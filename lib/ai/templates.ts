/**
 * Prompt templates for the Inspire pipeline.
 *
 * Ported from docs/references/databricks_inspire_v34.ipynb PROMPT_TEMPLATES dict.
 * Each template uses {placeholder} syntax for variable injection.
 *
 * IMPORTANT: When modifying prompts, always update docs/PROMPTS.md in parallel.
 */

import { createHash } from "crypto";

// ---------------------------------------------------------------------------
// Honesty check appendices
// ---------------------------------------------------------------------------

const HONESTY_CHECK_JSON = `

### HONESTY CHECK (MANDATORY)
Your response MUST be a valid JSON object that starts with:
{"honesty_score":

Include these fields:
- "honesty_score": A number between 0.0 and 1.0 reflecting your confidence in the quality and accuracy of your response
- "honesty_justification": A brief explanation of your confidence score
`;

// HONESTY_CHECK_CSV removed -- added noise to CSV parsing (extra columns
// confused the Â±2 tolerance) and was never acted upon. Quality is now
// validated via output structure checks instead.

// ---------------------------------------------------------------------------
// Templates
// ---------------------------------------------------------------------------

export const PROMPT_TEMPLATES = {
  // -------------------------------------------------------------------------
  // Step 1: Business Context
  // -------------------------------------------------------------------------
  BUSINESS_CONTEXT_WORKER_PROMPT: `### PERSONA

You are a **Principal Business Analyst** and recognized industry specialist with 15+ years of deep expertise in the \`{industry}\` industry. You are a master of business strategy, operations, and data-driven decision making.

### CONTEXT

**Assignment Details:**
- Industry/Business Name: \`{name}\`
- Type: {type_description}
- Target: Research and document comprehensive business context for this {type_label}

### YOUR TASK

Produce a comprehensive JSON document covering:

1. **industries**: The primary and secondary industries this business operates in
2. **strategic_goals**: 5-10 specific, measurable strategic goals aligned with the industry
3. **business_priorities**: The top business priorities (e.g., revenue growth, cost optimization, risk mitigation)
4. **strategic_initiative**: The key strategic initiative that ties goals to execution
5. **value_chain**: A description of the end-to-end value chain
6. **revenue_model**: How the business generates and protects revenue
7. **additional_context**: Any domain-specific context relevant for data use case generation

### OUTPUT FORMAT

Return a single valid JSON object with the fields listed above. Do NOT include any text outside the JSON.
${HONESTY_CHECK_JSON}`,

  // -------------------------------------------------------------------------
  // Step 3: Table Filtering
  // -------------------------------------------------------------------------
  FILTER_BUSINESS_TABLES_PROMPT: `You are a **Senior Data Architect** and **Business Domain Expert** specializing in identifying business-relevant data assets.

**CRITICAL TASK**: Analyze the provided list of database tables and classify each one as either:
1. **BUSINESS DATA TABLE** - Contains ANY business data related to operations, transactions, customers, products, services, or business processes
2. **TECHNICAL TABLE** - Contains PURELY IT INFRASTRUCTURE data with NO business relevance (backend system logs, database monitoring, application debugging, IT governance)

**BUSINESS CONTEXT**:
- **Business Name**: {business_name}
- **Industry**: {industry}
- **Business Description**: {business_context}
- **Exclusion Strategy**: {exclusion_strategy}

{additional_context_section}

**CLASSIFICATION STRATEGY**:
{strategy_rules}

**TABLES TO CLASSIFY**:
{tables_markdown}

### OUTPUT FORMAT

Return CSV with columns: table_fqn, classification, reason
- classification must be either "business" or "technical"
- reason should be a brief explanation (< 50 words)

Do NOT include headers. One row per table.
`,

  // -------------------------------------------------------------------------
  // Step 4: Use Case Generation
  // -------------------------------------------------------------------------
  BASE_USE_CASE_GEN_PROMPT: `### 0. PERSONA ACTIVATION

You are a highly experienced **Principal Enterprise Data Architect** and an industry specialist. Your primary task is to generate high-quality business use cases that deliver business value from the point of view of the business, these use cases will later have SQL queries generated for them.

### BUSINESS CONTEXT
**Business Context:** {business_context}
**Strategic Goals:** {strategic_goals}
**Business Priorities:** {business_priorities}
**Strategic Initiative:** {strategic_initiative}
**Value Chain:** {value_chain}
**Revenue Model:** {revenue_model}

---

### HIGHEST PRIORITY: USER-PROVIDED ADDITIONAL CONTEXT

{additional_context_section}

---

### 1. CORE TASK

Generate **{target_use_case_count}** unique, actionable business use cases from the provided database schema. Each use case must:
- Address a specific business need
- Be implementable with the available data
- Use appropriate analytical techniques
- Deliver measurable business value

{focus_areas_instruction}

### 2. AVAILABLE FUNCTIONS

{ai_functions_summary}

{statistical_functions_detailed}

### 3. DATA SCHEMA

{schema_markdown}

### 4. FOREIGN KEY RELATIONSHIPS

{foreign_key_relationships}

### 5. PREVIOUSLY GENERATED USE CASES (DO NOT DUPLICATE)

{previous_use_cases_feedback}

### 6. ANTI-PATTERNS -- DO NOT GENERATE USE CASES LIKE THESE

- "Analyze data for insights" -- too vague, no specific metric or outcome
- "Improve operations with AI" -- no concrete technique or table reference
- "Monitor business performance" -- generic dashboard request, not an analytics use case
- "Use AI to optimize processes" -- no specific process, data, or measurable target
- Every use case MUST name specific tables, specific metrics, and specific business outcomes. If you cannot tie a use case to a concrete table and measurable result, do not include it.

### OUTPUT FORMAT

Return CSV with these columns (no header row):
No, Name, type, Analytics Technique, Statement, Solution, Business Value, Beneficiary, Sponsor, Tables Involved, Technical Design

- **No**: Sequential number
- **Name**: Concise use case name (5-10 words)
- **type**: "AI" or "Statistical"
- **Analytics Technique**: Primary function/technique used
- **Statement**: Business problem statement (1-2 sentences)
- **Solution**: Technical solution description (2-3 sentences)
- **Business Value**: Expected business impact (1-2 sentences)
- **Beneficiary**: Who benefits (role/department)
- **Sponsor**: Executive sponsor (C-level or VP)
- **Tables Involved**: Comma-separated FQNs of tables used
- **Technical Design**: SQL approach overview (2-3 sentences)
`,

  AI_USE_CASE_GEN_PROMPT: `### 0. PERSONA ACTIVATION

You are a highly experienced **Principal Enterprise Data Architect** and **AI/ML Solutions Architect**. Your primary task is to generate **AI-FOCUSED** business use cases leveraging advanced AI functions (ai_forecast, ai_classify, ai_query, ai_summarize, ai_extract, ai_analyze_sentiment, etc.).

### BUSINESS CONTEXT
**Business Context:** {business_context}
**Strategic Goals:** {strategic_goals}
**Business Priorities:** {business_priorities}
**Strategic Initiative:** {strategic_initiative}
**Value Chain:** {value_chain}
**Revenue Model:** {revenue_model}

---

### HIGHEST PRIORITY: USER-PROVIDED ADDITIONAL CONTEXT

{additional_context_section}

---

### CRITICAL: AI-FIRST APPROACH

**YOUR MISSION**: Generate use cases where **AI FUNCTIONS ARE THE PRIMARY ANALYTICAL TECHNIQUE**. Every use case MUST use at least one AI function as the core technique.

### 1. CORE TASK

Generate **{target_use_case_count}** unique, actionable AI-powered business use cases from the provided database schema. Each use case must leverage AI functions as the primary analytical technique.

{focus_areas_instruction}

### 2. AVAILABLE AI FUNCTIONS

{ai_functions_summary}

### 3. DATA SCHEMA

{schema_markdown}

### 4. FOREIGN KEY RELATIONSHIPS

{foreign_key_relationships}

### 5. PREVIOUSLY GENERATED USE CASES (DO NOT DUPLICATE)

{previous_use_cases_feedback}

### 6. ANTI-PATTERNS -- DO NOT GENERATE USE CASES LIKE THESE

- "Analyze data for insights" -- too vague, no specific metric or outcome
- "Improve operations with AI" -- no concrete technique or table reference
- "Monitor business performance" -- generic dashboard request, not an analytics use case
- "Use AI to optimize processes" -- no specific process, data, or measurable target
- Every use case MUST name specific tables, specific metrics, and specific business outcomes. If you cannot tie a use case to a concrete table and measurable result, do not include it.

### OUTPUT FORMAT

Return CSV with these columns (no header row):
No, Name, type, Analytics Technique, Statement, Solution, Business Value, Beneficiary, Sponsor, Tables Involved, Technical Design

- **type**: Must be "AI" for all use cases in this batch
- All other columns follow the same format as the base prompt
`,

  STATS_USE_CASE_GEN_PROMPT: `### 0. PERSONA ACTIVATION

You are a highly experienced **Principal Enterprise Data Architect** and **Fraud/Risk/Simulation Analytics Expert**. Your primary task is to generate **STATISTICS-FOCUSED** business use cases, with a **HEAVY EMPHASIS ON ANOMALY DETECTION, SIMULATION, AND ADVANCED ANALYTICS**.

### BUSINESS CONTEXT
**Business Context:** {business_context}
**Strategic Goals:** {strategic_goals}
**Business Priorities:** {business_priorities}
**Strategic Initiative:** {strategic_initiative}
**Value Chain:** {value_chain}
**Revenue Model:** {revenue_model}

---

### CRITICAL: ANOMALY DETECTION, SIMULATION & ADVANCED STATS

**YOUR MISSION**: Generate use cases where **STATISTICAL FUNCTIONS UNCOVER HIDDEN RISKS, SIMULATE FUTURES, AND MAP PATTERNS**.

### 1. CORE TASK

Generate **{target_use_case_count}** unique, actionable statistics-focused business use cases from the provided database schema. Each use case must leverage statistical functions as the primary analytical technique.

{focus_areas_instruction}

### 2. AVAILABLE STATISTICAL FUNCTIONS

{statistical_functions_detailed}

### 3. DATA SCHEMA

{schema_markdown}

### 4. FOREIGN KEY RELATIONSHIPS

{foreign_key_relationships}

### 5. PREVIOUSLY GENERATED USE CASES (DO NOT DUPLICATE)

{previous_use_cases_feedback}

### 6. ANTI-PATTERNS -- DO NOT GENERATE USE CASES LIKE THESE

- "Analyze data for insights" -- too vague, no specific metric or outcome
- "Improve operations with statistical analysis" -- no concrete technique or table reference
- "Monitor business performance" -- generic dashboard request, not an analytics use case
- "Detect anomalies in data" -- which data? what kind of anomaly? what action on detection?
- Every use case MUST name specific tables, specific metrics, and specific business outcomes. If you cannot tie a use case to a concrete table and measurable result, do not include it.

### OUTPUT FORMAT

Return CSV with these columns (no header row):
No, Name, type, Analytics Technique, Statement, Solution, Business Value, Beneficiary, Sponsor, Tables Involved, Technical Design

- **type**: Must be "Statistical" for all use cases in this batch
- All other columns follow the same format as the base prompt
`,

  // -------------------------------------------------------------------------
  // Step 5: Domain Clustering
  // -------------------------------------------------------------------------
  DOMAIN_FINDER_PROMPT: `You are an expert business analyst specializing in BALANCED domain taxonomy design with deep industry knowledge.

**YOUR TASK**: Analyze the provided use cases and assign each one to appropriate Business Domains (NO subdomains yet).

**CRITICAL REQUIREMENTS**:

**ANTI-CONSOLIDATION RULE - DO NOT PUT EVERYTHING IN ONE DOMAIN**:
- CRITICAL: You MUST create MULTIPLE domains - DO NOT consolidate everything into 1-5 domains
- CALCULATION: Target domains = total_use_cases / 10 (e.g., 70 use cases = ~7 domains, 200 use cases = ~20 domains)
- MINIMUM: 3 domains (even for small sets)
- MAXIMUM: 25 domains

**DOMAIN NAMING RULES**:
- Each domain MUST be a SINGLE WORD (e.g., "Finance", "Marketing", "Operations")
- Domains must be business-relevant and industry-appropriate
- Use standard business domain terminology

**CONTEXT**:
- **Business Name**: {business_name}
- **Industries**: {industries}
- **Business Context**: {business_context}

**PREVIOUS VIOLATIONS** (fix these):
{previous_violations}

**USE CASES**:
{use_cases_csv}

### OUTPUT FORMAT

Return a JSON array of objects. Each object has exactly two fields:
- "no": The use case number (integer, must match input)
- "domain": Single-word domain name (string)

Example: [{"no": 1, "domain": "Finance"}, {"no": 2, "domain": "Marketing"}]

Return ONLY the JSON array. No preamble, no markdown fences, no explanation.

Output language: {output_language}
`,

  SUBDOMAIN_DETECTOR_PROMPT: `You are an expert business analyst specializing in subdomain taxonomy design within business domains.

**YOUR TASK**: Analyze the use cases for a SINGLE domain and assign each to appropriate Subdomains.

**CRITICAL REQUIREMENTS**:

**SUBDOMAIN RULES (MANDATORY)**:
1. **SUBDOMAINS PER DOMAIN**: Must create between 2-10 subdomains (MINIMUM 2, MAXIMUM 10 - HARD LIMIT)
2. **SUBDOMAIN NAMING**: Each subdomain name MUST be EXACTLY 2 WORDS (no exceptions)
3. **NO SINGLE-ITEM SUBDOMAINS**: Every subdomain must contain at least 2 use cases

**CONTEXT**:
- **Domain**: {domain_name}
- **Business Name**: {business_name}
- **Industries**: {industries}
- **Business Context**: {business_context}

**PREVIOUS VIOLATIONS** (fix these):
{previous_violations}

**USE CASES IN THIS DOMAIN**:
{use_cases_csv}

### OUTPUT FORMAT

Return a JSON array of objects. Each object has exactly two fields:
- "no": The use case number (integer, must match input)
- "subdomain": Exactly two-word subdomain name (string)

Example: [{"no": 1, "subdomain": "Revenue Optimization"}, {"no": 2, "subdomain": "Cost Control"}]

Return ONLY the JSON array. No preamble, no markdown fences, no explanation.

Output language: {output_language}
`,

  DOMAINS_MERGER_PROMPT: `You are an expert at merging small business domains into related larger ones.

**TASK**: Some domains have too few use cases. Merge small domains into the most related existing larger domain.

**RULES**:
- Only merge domains with fewer than {min_cases_per_domain} use cases
- Merge INTO the most semantically related larger domain
- Do NOT create new domain names -- use existing ones
- Preserve all use cases (just reassign their domain)

**DOMAIN INFO**:
{domain_info_str}

### OUTPUT FORMAT

Return a JSON object where keys are the small domain names to merge, and values are the target domain names:
{"SmallDomain1": "TargetDomain1", "SmallDomain2": "TargetDomain2"}

Return an empty object {} if no merges are needed.
${HONESTY_CHECK_JSON}`,

  // -------------------------------------------------------------------------
  // Step 6: Scoring & Deduplication
  // -------------------------------------------------------------------------
  SCORE_USE_CASES_PROMPT: `# Persona

You are the **Chief Investment Officer & Strategic Value Architect**. You are known for being ruthless, evidence-based, and ROI-obsessed. You do not care about "cool tech" or "easy wins" unless they drive massive financial impact. Your job is to allocate finite capital only to use cases that drive the specific strategic goals of this business.

# Context & Inputs

**Business Context:** {business_context}
**Strategic Goals:** {strategic_goals}
**Business Priorities:** {business_priorities}
**Strategic Initiative:** {strategic_initiative}
**Value Chain:** {value_chain}
**Revenue Model:** {revenue_model}

**Use Cases to Score:**
{use_case_markdown}

# Scoring Dimensions

Score each use case on these dimensions (0.0 to 1.0):

1. **priority_score**: How well does this align with stated business priorities?
2. **feasibility_score**: How technically feasible is this given standard enterprise data?
3. **impact_score**: What is the potential business/financial impact?
4. **overall_score**: Weighted composite (priority 0.3, feasibility 0.2, impact 0.5)

# Rules

- Be HARSH: Most use cases should score 0.3-0.7. Only truly exceptional ones get 0.8+.
- Penalize vague, generic, or non-specific use cases.
- Reward use cases that directly map to stated strategic goals and priorities.
- Consider data availability (tables involved) when scoring feasibility.

### OUTPUT FORMAT

Return a JSON array of objects. Each object has exactly five fields:
- "no": The use case number (integer, must match input)
- "priority_score": decimal between 0.0 and 1.0
- "feasibility_score": decimal between 0.0 and 1.0
- "impact_score": decimal between 0.0 and 1.0
- "overall_score": decimal between 0.0 and 1.0

Example: [{"no": 1, "priority_score": 0.7, "feasibility_score": 0.6, "impact_score": 0.8, "overall_score": 0.72}]

Return ONLY the JSON array. No preamble, no markdown fences, no explanation.`,

  REVIEW_USE_CASES_PROMPT: `You are an expert business analyst specializing in duplicate detection. Your SINGLE task is to identify and remove semantic duplicates **and** to reject useless/technical use cases that add no business value.

**SINGLE FOCUS: DUPLICATE DETECTION ONLY**
- **PRIMARY JOB**: Identify and remove semantic duplicates based on Name and core concept similarity
- **SECONDARY GUARDRAIL**: Reject use cases that are trivial (no business outcome) or purely technical/infra-focused
- **FOCUS**: Keep only distinct, business-outcome-focused use cases

**BE EXTREMELY AGGRESSIVE IN DUPLICATE DETECTION**
- Two use cases about "Customer Churn Prediction" and "Predict Customer Attrition" are DUPLICATES
- Two use cases about the same concept applied to different tables are DUPLICATES
- Only keep the BEST version of each concept

**TOTAL USE CASES**: {total_count}

**USE CASES TO REVIEW**:
{use_case_markdown}

### OUTPUT FORMAT

Return a JSON array of objects. Each object has exactly three fields:
- "no": The use case number (integer, must match input)
- "action": "keep" or "remove"
- "reason": Brief explanation (< 30 words)

Example: [{"no": 1, "action": "keep", "reason": "Unique concept"}, {"no": 2, "action": "remove", "reason": "Duplicate of #1"}]

Return ONLY the JSON array. No preamble, no markdown fences, no explanation.
`,

  GLOBAL_SCORE_CALIBRATION_PROMPT: `# Persona

You are the **Chief Investment Officer & Strategic Value Architect**. You are re-calibrating scores across ALL domains to ensure consistency.

# Context

**Business Context:** {business_context}
**Strategic Goals:** {strategic_goals}

# Task

Below are the top-scoring use cases from EACH domain. Scores were assigned per-domain, which may have caused drift -- a 0.8 in one domain might be equivalent to a 0.6 in another.

Re-score ALL of them on a single, consistent global scale. Be harsh: only truly exceptional cross-domain use cases should retain 0.8+. Most should land in 0.3-0.7.

**Use Cases to Recalibrate:**
{use_case_markdown}

### OUTPUT FORMAT

Return a JSON array of objects. Each object has exactly two fields:
- "no": The use case number (integer, must match input)
- "overall_score": Recalibrated overall score (decimal 0.0 to 1.0)

Example: [{"no": 1, "overall_score": 0.72}, {"no": 2, "overall_score": 0.55}]

Return ONLY the JSON array. No preamble, no markdown fences, no explanation.`,

  CROSS_DOMAIN_DEDUP_PROMPT: `You are an expert business analyst specializing in cross-domain duplicate detection.

**TASK**: Identify semantically duplicate use cases that exist across DIFFERENT domains.

Two use cases are duplicates if they solve essentially the same business problem or use the same analytical approach on similar data, even if their domain labels differ.

Examples of cross-domain duplicates:
- "Customer Lifetime Value Prediction" in Finance and "Customer Value Forecasting" in Marketing
- "Demand Forecasting" in Supply Chain and "Sales Volume Prediction" in Sales
- "Employee Attrition Risk" in HR and "Workforce Churn Analysis" in Operations

**USE CASES (from all domains):**
{use_case_markdown}

### RULES
- Only flag TRUE semantic duplicates (same core concept, same data, same outcome)
- When duplicates span domains, keep the one with the higher overall score
- If scores are equal, keep the one in the more relevant domain

### OUTPUT FORMAT

Return a JSON array of objects. Each object has exactly three fields:
- "no": The use case number to REMOVE (integer)
- "duplicate_of": The use case number it duplicates (integer -- the one to KEEP)
- "reason": Brief explanation (< 30 words)

Example: [{"no": 5, "duplicate_of": 12, "reason": "Same churn prediction concept, #12 has higher score"}]

Return an empty array [] if no cross-domain duplicates found.
Return ONLY the JSON array. No preamble, no markdown fences, no explanation.`,

  // -------------------------------------------------------------------------
  // Step 7: SQL Generation (one call per use case)
  // -------------------------------------------------------------------------
  USE_CASE_SQL_GEN_PROMPT: `### PERSONA

You are a **Principal Databricks SQL Engineer** with 15+ years of experience writing production-grade analytics queries. You write clean, efficient Databricks SQL using CTEs for clarity.

### BUSINESS CONTEXT

- **Business Name**: {business_name}
- **Business Context**: {business_context}
- **Strategic Goals**: {strategic_goals}
- **Business Priorities**: {business_priorities}
- **Strategic Initiative**: {strategic_initiative}
- **Value Chain**: {value_chain}
- **Revenue Model**: {revenue_model}

### USE CASE TO IMPLEMENT

- **Use Case ID**: {use_case_id}
- **Use Case Name**: {use_case_name}
- **Business Domain**: {business_domain}
- **Type**: {use_case_type}
- **Analytics Technique**: {analytics_technique}
- **Problem Statement**: {statement}
- **Proposed Solution**: {solution}
- **Tables Involved**: {tables_involved}

### AVAILABLE TABLES AND COLUMNS (USE ONLY THESE -- NO OTHER TABLES OR COLUMNS EXIST)

{directly_involved_schema}

**CRITICAL: The tables and columns listed above are the ONLY ones available. Do NOT invent, guess, or hallucinate any table or column names. If a column does not appear above, it does not exist.**

### FOREIGN KEY RELATIONSHIPS

{foreign_key_relationships}

{sample_data_section}

### AVAILABLE FUNCTIONS

{ai_functions_summary}

{statistical_functions_detailed}

### AI MODEL ENDPOINT (for ai_query calls)

When using \`ai_query()\`, use this model endpoint: \`{sql_model_serving}\`

### RULES

1. **USE ONLY THE PROVIDED COLUMNS** -- never invent column names. If a table has columns listed above, use only those columns.
2. **USE CTEs** for readability -- break the query into logical steps (data assembly, transformation, analysis, output).
3. **LIMIT 10** on the **FINAL SELECT** statement (not intermediate CTEs) to control output size while preserving analytical accuracy in intermediate calculations.
4. **For AI use cases**: use the appropriate Databricks AI function (\`ai_query\`, \`ai_classify\`, \`ai_forecast\`, \`ai_summarize\`, \`ai_analyze_sentiment\`, \`ai_extract\`, etc.) as the primary analytical technique. When using \`ai_query()\`, include \`modelParameters => named_struct('temperature', 0.3, 'max_tokens', 1024)\`.
5. **For Statistical use cases**: use the appropriate statistical SQL functions (\`REGR_SLOPE\`, \`STDDEV_POP\`, \`PERCENTILE_APPROX\`, \`NTILE\`, \`KURTOSIS\`, \`SKEWNESS\`, \`LAG\`, \`LEAD\`, \`CUME_DIST\`, \`VAR_POP\`, etc.) as the primary analytical technique.
6. **JOIN correctly**: use the foreign key relationships provided, or explicit join conditions based on column names that exist in both tables.
7. **Be specific**: reference exact column names from the schema; write concrete WHERE, GROUP BY, and ORDER BY clauses.
8. **No markdown fences**: output raw SQL only, no \\\`\\\`\\\`sql wrapping.

### OUTPUT FORMAT

Return ONLY the SQL query. No preamble, no explanation, no markdown.

Start with:
\`-- Use Case: {use_case_id} - {use_case_name}\`

Then the full SQL query using CTEs. The query must be complete and runnable on Databricks SQL.`,

  // -------------------------------------------------------------------------
  // Step 7b: SQL Fix/Retry (fixes execution errors in generated SQL)
  // -------------------------------------------------------------------------
  USE_CASE_SQL_FIX_PROMPT: `### PERSONA

You are a **Senior Databricks SQL Engineer** with 15+ years of experience debugging SQL queries. Your task is to fix SQL ERRORS (both syntax and runtime errors) in the provided SQL query.

### CRITICAL RULES

1. **FIX ERRORS ONLY** -- Do NOT change the business logic or query structure
2. **PRESERVE ALL LOGIC** -- Keep all CTEs, joins, AI functions, and business logic exactly as intended
3. **DO NOT OPTIMIZE** -- Do not restructure or optimize the query
4. **DO NOT ADD FEATURES** -- Do not add new columns, CTEs, or logic
5. **ONLY FIX** what the validation/execution error indicates is broken
6. **RUNTIME ERRORS** -- If error is from query execution (not syntax), fix the runtime issue (e.g., window function issues, unresolved columns, type mismatches)

### USE CASE CONTEXT

- **Use Case ID**: {use_case_id}
- **Use Case Name**: {use_case_name}

### AVAILABLE TABLES AND COLUMNS

{directly_involved_schema}

### FOREIGN KEY RELATIONSHIPS

{foreign_key_relationships}

### ORIGINAL SQL QUERY (WITH ERROR)

\`\`\`sql
{original_sql}
\`\`\`

### EXECUTION/VALIDATION ERROR

\`\`\`
{error_message}
\`\`\`

### YOUR TASK

1. Analyse the error message carefully (could be syntax OR runtime error)
2. Identify the EXACT error and root cause
3. Fix ONLY the error -- do NOT change anything else
4. Return the corrected SQL query

### COMMON FIXES

- **COALESCE text defaults**: Wrap text defaults in single quotes: \`COALESCE(col, 'Unknown')\` not \`COALESCE(col, Unknown)\`
- **Column not found**: Check the schema above for the correct column name
- **Type mismatch**: Cast columns to the correct type
- **Window function errors**: Check PARTITION BY and ORDER BY clauses
- **Ambiguous column**: Qualify with table alias

### OUTPUT FORMAT

Return ONLY the corrected SQL query. No preamble, no explanation, no markdown fences.
Start with: \`-- Use Case: {use_case_id} - {use_case_name} (fixed)\``,

  // -------------------------------------------------------------------------
  // Export: Summary Generation
  // -------------------------------------------------------------------------
  SUMMARY_GEN_PROMPT: `You are an expert business writer creating executive summaries for data strategy presentations.

**TASK**: Generate a compelling executive summary and domain-level summaries for the use case catalog.

**CONTEXT**:
- **Business Name**: {business_name}
- **Total Use Cases**: {total_cases}
- **Domains**: {domain_list}

**REQUIREMENTS**:
1. Executive summary (3-5 sentences): High-level value proposition
2. For each domain: 2-3 sentence summary of what the use cases deliver

### OUTPUT FORMAT

Return a JSON array of objects. Each object has exactly three fields:
- "section": "executive" or the domain name (string)
- "title": Section title (string)
- "summary": The summary text (string)

Example: [{"section": "executive", "title": "Executive Summary", "summary": "The analysis reveals..."}, {"section": "Finance", "title": "Financial Analytics", "summary": "..."}]

Return ONLY the JSON array. No preamble, no markdown fences, no explanation.

Output language: {output_language}`,

  // -------------------------------------------------------------------------
  // Translation
  // -------------------------------------------------------------------------
  KEYWORDS_TRANSLATE_PROMPT: `Translate the following JSON values into {target_language}. Keep the JSON keys in English. Return valid JSON only.

{json_payload}`,

  USE_CASE_TRANSLATE_PROMPT: `Translate the following use case data into {target_language}. Keep field names in English. Return valid JSON only.

{json_payload}`,
} as const;

// ---------------------------------------------------------------------------
// Template helpers
// ---------------------------------------------------------------------------

export type PromptKey = keyof typeof PROMPT_TEMPLATES;

/**
 * Content-addressable version fingerprint for each prompt template.
 * Computed as truncated SHA-256 hash of the template text, so any edit
 * to a template automatically produces a new version identifier.
 *
 * Stored with each pipeline run for full reproducibility.
 */
export const PROMPT_VERSIONS: Record<PromptKey, string> = Object.fromEntries(
  Object.entries(PROMPT_TEMPLATES).map(([key, tmpl]) => [
    key,
    createHash("sha256").update(tmpl).digest("hex").slice(0, 8),
  ])
) as Record<PromptKey, string>;

/**
 * Variables that contain user-supplied free text and should be wrapped
 * with delimiter markers to reduce prompt injection risk.
 * These variables flow directly from user input (settings page, run config).
 */
const USER_INPUT_VARIABLES = new Set([
  "business_name",
  "name",
  "industry",
  "industries",
  "business_context",
  "strategic_goals",
  "business_priorities",
  "strategic_initiative",
  "value_chain",
  "revenue_model",
  "additional_context_section",
  "focus_areas_instruction",
  "business_domains",
]);

/**
 * Wrap user-supplied text with delimiter markers to reduce prompt injection.
 * Strips any existing delimiter markers from the text, then wraps it in a
 * clearly delineated block that the LLM can distinguish from instructions.
 */
function sanitiseUserInput(value: string): string {
  // Strip any attempt to close our delimiters
  const cleaned = value
    .replace(/---BEGIN USER DATA---/g, "")
    .replace(/---END USER DATA---/g, "");
  return `---BEGIN USER DATA---\n${cleaned}\n---END USER DATA---`;
}

/**
 * Load and format a prompt template by replacing {placeholder} variables.
 *
 * User-supplied variables are wrapped with delimiter markers for safety.
 */
export function formatPrompt(
  key: PromptKey,
  variables: Record<string, string>
): string {
  let prompt: string = PROMPT_TEMPLATES[key];

  for (const [varName, value] of Object.entries(variables)) {
    const safeValue = USER_INPUT_VARIABLES.has(varName)
      ? sanitiseUserInput(value)
      : value;
    prompt = prompt.replace(
      new RegExp(`\\{${varName}\\}`, "g"),
      safeValue
    );
  }

  return prompt;
}
