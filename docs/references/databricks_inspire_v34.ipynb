{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eb385a7-505a-41f8-9166-563578f92e9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83D\uDE80 Databricks Inspire AI \uD83D\uDE80\n",
    "\n",
    "**Transform your metadata into actionable AI-generated use cases, documentation, and presentations.**\n",
    "\n",
    "---\n",
    "\n",
    "## Outputs\n",
    "- **Notebooks** – Ready-to-deploy SQL code\n",
    "- **PDF** – Professional documentation  \n",
    "- **PowerPoint** – Executive slides\n",
    "- **Excel** – Prioritized use cases catalog\n",
    "\n",
    "*Supports 20+ languages including English, Arabic, Chinese, French, Spanish, German, Japanese, and more.*\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Start\n",
    "1. **Configure** – Set **Business Name**, **UC Metadata** and **Operation**\n",
    "2. **Run** – Click **Run All**\n",
    "3. **Explore** – Find outputs in your **Generation Path**\n",
    "\n",
    "---\n",
    "\n",
    "## Configuration\n",
    "\n",
    "| # | Widget | Description | Default |\n",
    "|:--|:---|:---|:---|\n",
    "| 01 | **Business Name** | Organization/project name | *Required* |\n",
    "| 02 | **UC Metadata** | Catalogs, Schemas, or Tables (e.g., `main.finance`) or JSON path | *Required* |\n",
    "| 03 | **Operation** | `Discover Usecases`, `Re-generate SQL`, `Generate Sample Result` | `Discover Usecases` |\n",
    "| 04 | **Business Domains** | Focus domains (e.g., \"Risk, Finance\") | *Auto-detected* |\n",
    "| 05 | **Business Priorities** | `Increase Revenue`, `Reduce Cost`, `Optimize Operations`, `Mitigate Risk`, `Empower Talent`, `Enhance Experience`, `Drive Innovation`, `Achieve ESG`, `Protect Revenue`, `Execute Strategy` | `Increase Revenue` |\n",
    "| 06 | **Strategic Goals** | Custom goals for prioritization | *Auto-generated* |\n",
    "| 07 | **Generation Options** | `SQL Code`, `PDF Catalog`, `Presentation`, `dashboards`, `Unstructured Data Usecases` | `SQL Code` |\n",
    "| 08 | **Generation Path** | Output folder | `./inspire_gen/` |\n",
    "| 09 | **Documents Languages** | Target language(s) | `English` |\n",
    "| 10 | **AI Model** | Model endpoint for generated SQL ai_query | `databricks-gpt-oss-120b` |\n",
    "\n",
    "---\n",
    "\n",
    "## Privacy\n",
    "\n",
    "**Metadata Only**: Reads schemas, table & column names only. Does **NOT** access or sample your actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec3a16b2-4209-4a25-8c81-4d4209ec5456",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "InspireAI"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:19:04 - INFO - Creating widgets (retaining existing values)...\n01:19:04 - INFO - ✅ Widgets created successfully.\n01:19:04 - INFO - \n01:19:04 - INFO - >>> Fill in the widget values at the TOP of this notebook, then run main().\n01:19:06 - INFO - PROMPT_TEMPLATES dictionary defined successfully with all required prompts.\n"
     ]
    }
   ],
   "source": [
    "DATABRICKS_INSPIRE_BANNER = r\"\"\"\n",
    "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
    "┃    ____        _        _          _      _                             ┃\n",
    "┃   |  _ \\  __ _| |_ __ _| |__  _ __(_) ___| | _____                      ┃\n",
    "┃   | | | |/ _` | __/ _` | '_ \\| '__| |/ __| |/ / __|                     ┃\n",
    "┃   | |_| | (_| | || (_| | |_) | |  | | (__|   <\\__ \\                     ┃\n",
    "┃   |____/ \\__,_|\\__\\__,_|_.__/|_|  |_|\\___|_|\\_\\___/                     ┃\n",
    "┃       ___                      _                  _    ___              ┃\n",
    "┃      |_ _| _ __   ___  _ __   (_) _ __  ___      / \\  |_ _|             ┃\n",
    "┃       | | | '_ \\ / __|| '_ \\  | || '__|/ _ \\    / _ \\  | |              ┃\n",
    "┃       | | | | | |\\__ \\| |_) | | || |  |  __/   / ___ \\ | |              ┃\n",
    "┃      |___||_| |_||___/| .__/  |_||_|   \\___|  /_/   \\_\\___|             ┃\n",
    "┃                       |_|                                               ┃\n",
    "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
    "\"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# TECHNICAL CONTEXT - Prompt to Model Mapping Configuration\n",
    "# ==============================================================================\n",
    "# This structure maps all prompts to their assigned LLM models.\n",
    "# Modify the \"model\" field for each prompt to route it to a different model.\n",
    "# Available models are defined in the \"models\" section below.\n",
    "# ==============================================================================\n",
    "TECHNICAL_CONTEXT = {\n",
    "    \"prompts_models\": [\n",
    "        # === PHASE 1: INITIALIZATION & CONTEXT EXTRACTION ===\n",
    "        # Temperature: 0.3-0.4 for accurate extraction of business context\n",
    "        {\"prompt_name\": \"BUSINESS_CONTEXT_WORKER_PROMPT\", \"model\": \"claude-sonnet-4-5\", \"temperature\": 0.3},      # Step 1: Extract business context, goals, priorities\n",
    "        {\"prompt_name\": \"UNSTRUCTURED_DATA_DOCUMENTS_PROMPT\", \"model\": \"claude-sonnet-4-5\", \"temperature\": 0.4},  # Step 2: Generate unstructured doc list (if enabled)\n",
    "        {\"prompt_name\": \"FILTER_BUSINESS_TABLES_PROMPT\", \"model\": \"claude-sonnet-4-5\", \"temperature\": 0.2},       # Step 3: Filter business vs technical tables (precision needed)\n",
    "        \n",
    "        # === PHASE 2: USE CASE GENERATION (PARALLEL) ===\n",
    "        # Temperature: 0.7-0.8 for creative/innovative use case generation\n",
    "        {\"prompt_name\": \"BASE_USE_CASE_GEN_PROMPT\", \"model\": \"claude-sonnet-4-5\", \"temperature\": 0.7},            # Step 4: Main structured data use case generation (creative)\n",
    "        {\"prompt_name\": \"AI_USE_CASE_GEN_PROMPT\", \"model\": \"claude-sonnet-4-5\", \"temperature\": 0.8},              # Step 4: AI/ML focused use case generation (highly creative)\n",
    "        {\"prompt_name\": \"STATS_USE_CASE_GEN_PROMPT\", \"model\": \"claude-sonnet-4-5\", \"temperature\": 0.7},           # Step 4: Statistical use case generation (creative)\n",
    "        {\"prompt_name\": \"UNSTRUCTURED_DATA_USE_CASE_GEN_PROMPT\", \"model\": \"claude-sonnet-4-5\", \"temperature\": 0.7}, # Step 4: Unstructured data use cases (creative)\n",
    "        \n",
    "        # === PHASE 3: DOMAIN CLUSTERING ===\n",
    "        # Temperature: 0.4-0.5 for balanced clustering decisions\n",
    "        {\"prompt_name\": \"DOMAIN_FINDER_PROMPT\", \"model\": \"claude-sonnet-4-5\", \"temperature\": 0.5},                # Step 5: Cluster use cases into business domains\n",
    "        {\"prompt_name\": \"SUBDOMAIN_DETECTOR_PROMPT\", \"model\": \"claude-sonnet-4-5\", \"temperature\": 0.4},           # Step 6: Detect subdomains within each domain\n",
    "        {\"prompt_name\": \"DOMAINS_MERGER_PROMPT\", \"model\": \"claude-sonnet-4-5\", \"temperature\": 0.4},               # Step 7: Merge similar domains (optional)\n",
    "        \n",
    "        # === PHASE 4: SCORING & DEDUPLICATION ===\n",
    "        # Temperature: 0.2-0.3 for consistent, accurate scoring\n",
    "        {\"prompt_name\": \"SCORE_USE_CASES_PROMPT\", \"model\": \"claude-sonnet-4-5\", \"temperature\": 0.2},              # Step 8: Score use cases (ROI, Strategic Alignment) - precision\n",
    "        {\"prompt_name\": \"REVIEW_USE_CASES_PROMPT\", \"model\": \"claude-sonnet-4-5\", \"temperature\": 0.3},             # Step 9: Intelligent deduplication using scores\n",
    "        \n",
    "        # === PHASE 5: SQL GENERATION ===\n",
    "        # Temperature: 0.1-0.2 for accurate, syntactically correct SQL\n",
    "        {\"prompt_name\": \"USE_CASE_SQL_GEN_PROMPT\", \"model\": \"claude-sonnet-4-5\", \"temperature\": 0.1},             # Step 10: Generate SQL (accuracy critical)\n",
    "        {\"prompt_name\": \"USE_CASE_SQL_FIX_PROMPT\", \"model\": \"claude-sonnet-4-5\", \"temperature\": 0.1},             # Step 11: Fix SQL errors (precision critical)\n",
    "        {\"prompt_name\": \"INTERPRET_USER_SQL_REGENERATION_PROMPT\", \"model\": \"claude-sonnet-4-5\", \"temperature\": 0.2}, # SQL Regeneration mode (special)\n",
    "        \n",
    "        # === PHASE 6: SUMMARY & ARTIFACTS ===\n",
    "        # Temperature: 0.5-0.6 for engaging summaries and dashboards\n",
    "        {\"prompt_name\": \"SUMMARY_GEN_PROMPT\", \"model\": \"claude-sonnet-4-5\", \"temperature\": 0.5},                  # Step 12: Generate executive summary\n",
    "        {\"prompt_name\": \"DASHBOARDS_GEN_PROMPT\", \"model\": \"claude-sonnet-4-5\", \"temperature\": 0.6},               # Step 13: Generate dashboards (if enabled)\n",
    "        \n",
    "        # === PHASE 7: TRANSLATION (MULTI-LANGUAGE) ===\n",
    "        # Temperature: 0.2-0.3 for accurate translation\n",
    "        {\"prompt_name\": \"KEYWORDS_TRANSLATE_PROMPT\", \"model\": \"claude-sonnet-4-5\", \"temperature\": 0.2},           # Step 14: Translate keywords (accuracy)\n",
    "        {\"prompt_name\": \"USE_CASE_TRANSLATE_PROMPT\", \"model\": \"claude-sonnet-4-5\", \"temperature\": 0.3},           # Step 15: Translate use cases\n",
    "    ],\n",
    "    \"models\": [\n",
    "        {\n",
    "            \"name\": \"claude-sonnet-4-5\",\n",
    "            \"llm_endpoint_name\": \"databricks-claude-sonnet-4-5\",\n",
    "            \"llm_input_context_tokens_count\": 200000,\n",
    "            \"llm_output_context_tokens_count\": 128000\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"claude-opus-4-5\",\n",
    "            \"llm_endpoint_name\": \"databricks-claude-opus-4-5\",\n",
    "            \"llm_input_context_tokens_count\": 200000,\n",
    "            \"llm_output_context_tokens_count\": 64000\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"gpt-oss-120b\",\n",
    "            \"llm_endpoint_name\": \"databricks-gpt-oss-120b\",\n",
    "            \"llm_input_context_tokens_count\": 131000,\n",
    "            \"llm_output_context_tokens_count\": 131000\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"gpt-oss-20b\",\n",
    "            \"llm_endpoint_name\": \"databricks-gpt-oss-20b\",\n",
    "            \"llm_input_context_tokens_count\": 131000,\n",
    "            \"llm_output_context_tokens_count\": 32000\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "def get_model_endpoint_for_prompt(prompt_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the LLM endpoint name for a given prompt using TECHNICAL_CONTEXT.\n",
    "    \n",
    "    Args:\n",
    "        prompt_name: Name of the prompt (e.g., \"BUSINESS_CONTEXT_WORKER_PROMPT\")\n",
    "    \n",
    "    Returns:\n",
    "        The LLM endpoint name (e.g., \"databricks-gpt-oss-20b\")\n",
    "    \"\"\"\n",
    "    models_lookup = {m[\"name\"]: m[\"llm_endpoint_name\"] for m in TECHNICAL_CONTEXT[\"models\"]}\n",
    "    \n",
    "    for pm in TECHNICAL_CONTEXT[\"prompts_models\"]:\n",
    "        if pm[\"prompt_name\"] == prompt_name:\n",
    "            model_name = pm[\"model\"]\n",
    "            return models_lookup.get(model_name, \"databricks-gpt-oss-20b\")\n",
    "    \n",
    "    return \"databricks-gpt-oss-20b\"\n",
    "\n",
    "def get_model_config_for_prompt(prompt_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Get the full model configuration for a given prompt using TECHNICAL_CONTEXT.\n",
    "    \n",
    "    Args:\n",
    "        prompt_name: Name of the prompt (e.g., \"BUSINESS_CONTEXT_WORKER_PROMPT\")\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with model configuration including endpoint, input/output token limits\n",
    "    \"\"\"\n",
    "    models_lookup = {m[\"name\"]: m for m in TECHNICAL_CONTEXT[\"models\"]}\n",
    "    \n",
    "    for pm in TECHNICAL_CONTEXT[\"prompts_models\"]:\n",
    "        if pm[\"prompt_name\"] == prompt_name:\n",
    "            model_name = pm[\"model\"]\n",
    "            return models_lookup.get(model_name, models_lookup.get(\"claude-sonnet-4-5\", {}))\n",
    "    \n",
    "    return models_lookup.get(\"claude-sonnet-4-5\", {})\n",
    "\n",
    "def log_print(message: str, level: str = \"INFO\", flush: bool = True):\n",
    "    \"\"\"Print a message with timestamp in logger format for immediate console output.\n",
    "    For ERROR/CRITICAL levels, also writes to stderr for visibility.\n",
    "    \"\"\"\n",
    "    import time as _time\n",
    "    import sys as _sys\n",
    "    timestamp = _time.strftime('%H:%M:%S')\n",
    "    formatted_msg = f\"{timestamp} - {level} - {message}\"\n",
    "    print(formatted_msg, flush=flush)\n",
    "    if flush:\n",
    "        _sys.stdout.flush()\n",
    "    \n",
    "    level_upper = level.upper()\n",
    "    if level_upper in (\"ERROR\", \"CRITICAL\"):\n",
    "        print(formatted_msg, file=_sys.stderr, flush=True)\n",
    "\n",
    "def get_clean_error_message(exception: Exception, max_lines: int = 1) -> str:\n",
    "    \"\"\"\n",
    "    Extract clean error message from exception without ugly stack traces.\n",
    "    \n",
    "    Args:\n",
    "        exception: The exception object\n",
    "        max_lines: Maximum number of lines to include (default: 1 for first line only)\n",
    "    \n",
    "    Returns:\n",
    "        Clean error message string\n",
    "    \"\"\"\n",
    "    error_str = str(exception)\n",
    "    if '\\n' in error_str and 'JVM stacktrace' not in error_str:\n",
    "        # Multi-line error but no JVM trace - take first meaningful line\n",
    "        lines = [line.strip() for line in error_str.split('\\n') if line.strip()]\n",
    "        return ' '.join(lines[:max_lines])\n",
    "    elif 'JVM stacktrace' in error_str or len(error_str) > 500:\n",
    "        # Has JVM stack trace or very long - extract just first line\n",
    "        first_line = error_str.split('\\n')[0].strip()\n",
    "        # If first line mentions table/view not found, keep that\n",
    "        if 'TABLE_OR_VIEW_NOT_FOUND' in first_line or 'cannot be found' in first_line:\n",
    "            return first_line\n",
    "        # Otherwise extract the main error message\n",
    "        if ']:' in first_line:\n",
    "            return first_line.split(']:')[-1].strip()\n",
    "        return first_line[:500]  # Truncate to 500 chars\n",
    "    return error_str\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# TRULY ADAPTIVE PARALLELISM CALCULATOR\n",
    "# ==============================================================================\n",
    "# Parallelism is calculated dynamically based on ACTUAL DATA:\n",
    "# - Number of items to process (tables, use cases, domains)\n",
    "# - Estimated prompt/payload size\n",
    "# - LLM vs non-LLM operations\n",
    "# - Risk of rate limiting or timeouts\n",
    "#\n",
    "# BOUNDS: MIN=2, MAX=10 (enforced globally)\n",
    "# METADATA QUERIES: FIXED at 5 (no LLM, but too many connections can hang)\n",
    "# ==============================================================================\n",
    "\n",
    "# Fixed parallelism for metadata operations (no LLM, but DB connections can saturate)\n",
    "METADATA_PARALLELISM = 5\n",
    "\n",
    "def calculate_adaptive_parallelism(\n",
    "    step_name: str,\n",
    "    max_parallelism: int,\n",
    "    num_items: int = 0,\n",
    "    total_columns: int = 0,\n",
    "    avg_prompt_chars: int = 0,\n",
    "    num_domains: int = 0,\n",
    "    is_llm_operation: bool = True,\n",
    "    logger=None\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Calculate truly adaptive parallelism based on actual data characteristics.\n",
    "    \n",
    "    Args:\n",
    "        step_name: Name of the step (for logging)\n",
    "        max_parallelism: User-configured maximum parallelism\n",
    "        num_items: Number of items to process (tables, use cases, domains, etc.)\n",
    "        total_columns: Total columns involved (affects prompt size)\n",
    "        avg_prompt_chars: Average prompt size in characters\n",
    "        num_domains: Number of business domains\n",
    "        is_llm_operation: Whether this step makes LLM calls\n",
    "        logger: Optional logger\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (parallelism: int, reason: str)\n",
    "    \"\"\"\n",
    "    MIN_PARALLELISM = 4\n",
    "    MAX_PARALLELISM = 10\n",
    "    \n",
    "    # Start with user max, capped at global max\n",
    "    base = min(max_parallelism, MAX_PARALLELISM)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # METADATA OPERATIONS (Fixed at 5 - no LLM but DB connections can saturate)\n",
    "    # =========================================================================\n",
    "    if step_name in [\"schema_discovery\", \"table_discovery\", \"column_fetch\"]:\n",
    "        result = METADATA_PARALLELISM\n",
    "        reason = f\"FIXED={METADATA_PARALLELISM} for metadata queries (DB connection limit)\"\n",
    "        if logger:\n",
    "            logger.info(f\"\uD83D\uDD27 [{step_name.upper()}] Parallelism = {result} | {reason}\")\n",
    "        return (result, reason)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # FILE I/O OPERATIONS (Can use higher parallelism, but cap based on items)\n",
    "    # =========================================================================\n",
    "    if step_name in [\"notebook_generation\", \"artifact_writing\"]:\n",
    "        # Scale with number of items, but cap reasonably\n",
    "        if num_items <= 5:\n",
    "            result = min(base, num_items + 2)\n",
    "            reason = f\"few items ({num_items}), using {result} workers\"\n",
    "        elif num_items <= 15:\n",
    "            result = min(base, 6)\n",
    "            reason = f\"moderate items ({num_items}), capped at 6\"\n",
    "        else:\n",
    "            result = min(base, 8)\n",
    "            reason = f\"many items ({num_items}), capped at 8 for I/O stability\"\n",
    "        \n",
    "        result = max(MIN_PARALLELISM, min(MAX_PARALLELISM, result))\n",
    "        if logger:\n",
    "            logger.info(f\"\uD83D\uDD27 [{step_name.upper()}] Parallelism = {result} | {reason}\")\n",
    "        return (result, reason)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # LLM OPERATIONS - Truly adaptive based on workload\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Base calculation factors\n",
    "    factors = []\n",
    "    calculated = base\n",
    "    \n",
    "    # FACTOR 1: Number of items (more items = need more caution)\n",
    "    if num_items > 0:\n",
    "        if num_items <= 10:\n",
    "            item_factor = 0.8  # Small batch, can be aggressive\n",
    "            factors.append(f\"{num_items} items (small)\")\n",
    "        elif num_items <= 30:\n",
    "            item_factor = 0.6  # Medium batch\n",
    "            factors.append(f\"{num_items} items (medium)\")\n",
    "        elif num_items <= 100:\n",
    "            item_factor = 0.4  # Large batch, be conservative\n",
    "            factors.append(f\"{num_items} items (large)\")\n",
    "        else:\n",
    "            item_factor = 0.3  # Very large, very conservative\n",
    "            factors.append(f\"{num_items} items (very large)\")\n",
    "        calculated = int(calculated * item_factor)\n",
    "    \n",
    "    # FACTOR 2: Prompt size (larger prompts = more tokens = slower responses)\n",
    "    if avg_prompt_chars > 0:\n",
    "        if avg_prompt_chars > 100000:\n",
    "            prompt_factor = 0.4  # Very large prompts\n",
    "            factors.append(f\"~{avg_prompt_chars//1000}K chars/prompt (huge)\")\n",
    "        elif avg_prompt_chars > 50000:\n",
    "            prompt_factor = 0.5\n",
    "            factors.append(f\"~{avg_prompt_chars//1000}K chars/prompt (large)\")\n",
    "        elif avg_prompt_chars > 20000:\n",
    "            prompt_factor = 0.7\n",
    "            factors.append(f\"~{avg_prompt_chars//1000}K chars/prompt (medium)\")\n",
    "        else:\n",
    "            prompt_factor = 0.9\n",
    "            factors.append(f\"~{avg_prompt_chars//1000}K chars/prompt (small)\")\n",
    "        calculated = int(calculated * prompt_factor)\n",
    "    \n",
    "    # FACTOR 3: Number of domains (more domains = more parallel LLM calls)\n",
    "    if num_domains > 0:\n",
    "        if num_domains > 15:\n",
    "            domain_factor = 0.4  # Many domains, very conservative\n",
    "            factors.append(f\"{num_domains} domains (many)\")\n",
    "        elif num_domains > 8:\n",
    "            domain_factor = 0.5\n",
    "            factors.append(f\"{num_domains} domains (moderate)\")\n",
    "        else:\n",
    "            domain_factor = 0.7\n",
    "            factors.append(f\"{num_domains} domains (few)\")\n",
    "        calculated = int(calculated * domain_factor)\n",
    "    \n",
    "    # FACTOR 4: Total columns (more columns = bigger schema context)\n",
    "    if total_columns > 0:\n",
    "        if total_columns > 2000:\n",
    "            col_factor = 0.5\n",
    "            factors.append(f\"{total_columns} cols (massive schema)\")\n",
    "        elif total_columns > 1000:\n",
    "            col_factor = 0.6\n",
    "            factors.append(f\"{total_columns} cols (large schema)\")\n",
    "        elif total_columns > 500:\n",
    "            col_factor = 0.7\n",
    "            factors.append(f\"{total_columns} cols (medium schema)\")\n",
    "        else:\n",
    "            col_factor = 0.9\n",
    "            factors.append(f\"{total_columns} cols (small schema)\")\n",
    "        calculated = int(calculated * col_factor)\n",
    "    \n",
    "    # FACTOR 5: Step-specific adjustments\n",
    "    step_adjustments = {\n",
    "        \"scoring\": (0.6, \"scoring is rate-limit sensitive\"),\n",
    "        \"deduplication\": (0.7, \"dedup needs LLM per domain\"),\n",
    "        \"sql_generation\": (0.7, \"SQL gen is complex\"),\n",
    "        \"use_case_generation\": (0.6, \"LLM-intensive, 2-pass for transactional tables\"),\n",
    "        \"domain_clustering\": (0.6, \"domain detection is heavy\"),\n",
    "        \"subdomain_detection\": (0.7, \"subdomain per domain\"),\n",
    "        \"translation\": (0.7, \"translation LLM calls\"),\n",
    "        \"sql_validation\": (0.8, \"DB queries, not LLM\"),\n",
    "    }\n",
    "    \n",
    "    if step_name in step_adjustments:\n",
    "        adj_factor, adj_reason = step_adjustments[step_name]\n",
    "        calculated = int(calculated * adj_factor)\n",
    "        factors.append(adj_reason)\n",
    "    \n",
    "    # Apply bounds\n",
    "    result = max(MIN_PARALLELISM, min(MAX_PARALLELISM, calculated))\n",
    "    \n",
    "    # If no factors were applied and it's an LLM operation, use conservative default\n",
    "    if not factors and is_llm_operation:\n",
    "        result = max(MIN_PARALLELISM, min(4, base))\n",
    "        factors.append(\"LLM operation, conservative default\")\n",
    "    \n",
    "    # Build reason string\n",
    "    reason = \" + \".join(factors) if factors else \"default calculation\"\n",
    "    reason = f\"calculated={result} based on: {reason}\"\n",
    "    \n",
    "    if logger:\n",
    "        logger.info(f\"\uD83D\uDD27 [{step_name.upper()}] Parallelism = {result} (from max={max_parallelism}) | {reason}\")\n",
    "    \n",
    "    return (result, reason)\n",
    "\n",
    "\n",
    "def log_adaptive_parallelism_decision(step_name: str, parallelism: int, max_parallelism: int, reason: str):\n",
    "    \"\"\"\n",
    "    Log the adaptive parallelism decision with full context.\n",
    "    \"\"\"\n",
    "    log_print(f\"\uD83D\uDD27 [{step_name.upper()}] Workers: {parallelism} (max={max_parallelism})\")\n",
    "    log_print(f\"   └─ Reason: {reason}\")\n",
    "\n",
    "\n",
    "def create_widgets():\n",
    "    \"\"\"\n",
    "    Creates widgets if they don't exist. Retains existing widget values.\n",
    "    \n",
    "    Widget Order:\n",
    "    0- Business Name\n",
    "    1- UC Metadata\n",
    "    2- Operation\n",
    "    3- Business Domains\n",
    "    4- Business Priorities\n",
    "    5- Strategic Goals\n",
    "    6- Generation Options\n",
    "    7- Generation Path\n",
    "    8- Documents Languages\n",
    "    9- AI Model\n",
    "    \"\"\"\n",
    "    \n",
    "    log_print(\"Creating widgets (retaining existing values)...\")\n",
    "    \n",
    "    widget_errors = []\n",
    "    \n",
    "    # --- 0. Business Name (REQUIRED) ---\n",
    "    try:\n",
    "        dbutils.widgets.text(\"00_business_name\", \"\", \"01. Business Name\")\n",
    "    except Exception as e:\n",
    "        widget_errors.append(f\"Business Name: {e}\")\n",
    "    \n",
    "    # --- 1. UC Metadata (catalogs/schemas/tables OR JSON file path) ---\n",
    "    try:\n",
    "        dbutils.widgets.text(\"01_uc_metadata\", \"\", \"02. UC Metadata\")\n",
    "    except Exception as e:\n",
    "        widget_errors.append(f\"UC Metadata: {e}\")\n",
    "    \n",
    "    # --- 2. Operation (controls main operation mode) ---\n",
    "    try:\n",
    "        operation_options = [\n",
    "            \"Discover Usecases\",\n",
    "            \"Re-generate SQL\",\n",
    "            \"Generate Sample Result\"\n",
    "        ]\n",
    "        dbutils.widgets.dropdown(\"02_operation\", \"Discover Usecases\", operation_options, \"03. Operation\")\n",
    "    except Exception as e:\n",
    "        widget_errors.append(f\"Operation: {e}\")\n",
    "    \n",
    "    # --- 3. Business Domains (comma-separated list of domains) ---\n",
    "    try:\n",
    "        dbutils.widgets.text(\"03_business_domains\", \"\", \"04. Business Domains\")\n",
    "    except Exception as e:\n",
    "        widget_errors.append(f\"Business Domains: {e}\")\n",
    "    \n",
    "    # --- 4. Business Priorities (multi-select) ---\n",
    "    try:\n",
    "        business_priorities_options = [\n",
    "            \"Increase Revenue\",\n",
    "            \"Reduce Cost\",\n",
    "            \"Optimize Operations\",\n",
    "            \"Mitigate Risk\",\n",
    "            \"Empower Talent\",\n",
    "            \"Enhance Experience\",\n",
    "            \"Drive Innovation\",\n",
    "            \"Achieve ESG\",\n",
    "            \"Protect Revenue\",\n",
    "            \"Execute Strategy\"\n",
    "        ]\n",
    "        dbutils.widgets.multiselect(\"04_business_priorities\", \"Increase Revenue\", business_priorities_options, \"05. Business Priorities\")\n",
    "    except Exception as e:\n",
    "        widget_errors.append(f\"Business Priorities: {e}\")\n",
    "    \n",
    "    # --- 5. Strategic Goals ---\n",
    "    try:\n",
    "        dbutils.widgets.text(\"05_strategic_goals\", \"\", \"06. Strategic Goals\")\n",
    "    except Exception as e:\n",
    "        widget_errors.append(f\"Strategic Goals: {e}\")\n",
    "    \n",
    "    # --- 6. Generation Options (multiselect with generation choices) ---\n",
    "    try:\n",
    "        generation_options = [\n",
    "            \"SQL Code\",\n",
    "            \"PDF Catalog\",\n",
    "            \"Presentation\",\n",
    "            \"dashboards\",\n",
    "            \"Unstructured Data Usecases\"\n",
    "        ]\n",
    "        dbutils.widgets.multiselect(\n",
    "            \"06_generation_options\", \n",
    "            \"SQL Code\",\n",
    "            generation_options, \n",
    "            \"07. Generation Options\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        widget_errors.append(f\"Generation Options: {e}\")\n",
    "    \n",
    "    # --- 7. Generation Path ---\n",
    "    try:\n",
    "        dbutils.widgets.text(\"07_generation_path\", \"./inspire_gen/\", \"08. Generation Path\")\n",
    "    except Exception as e:\n",
    "        widget_errors.append(f\"Generation Path: {e}\")\n",
    "    \n",
    "    # --- 8. Documents Languages (multiselect) ---\n",
    "    try:\n",
    "        lang_choices = [\n",
    "            \"English\", \"French\", \"German\", \"Spanish\", \"Hindi\",\n",
    "            \"Chinese (Mandarin)\", \"Japanese\", \"Arabic\", \"Portuguese\", \"Russian\",\n",
    "            \"Swedish\", \"Danish\", \"Norwegian\", \"Finnish\",\n",
    "            \"Italian\", \"Polish\", \"Romanian\", \"Ukrainian\", \"Dutch\", \"Korean\",\n",
    "            \"Indonesian\", \"Malay\", \"Tamil\"\n",
    "        ]\n",
    "        dbutils.widgets.multiselect(\"08_documents_languages\", \"English\", lang_choices, \"09. Documents Languages\")\n",
    "    except Exception as e:\n",
    "        widget_errors.append(f\"Documents Languages: {e}\")\n",
    "    \n",
    "    # --- 9. AI Model (model endpoint for ai_query in generated SQL) ---\n",
    "    try:\n",
    "        dbutils.widgets.text(\"09_ai_model\", \"databricks-gpt-oss-120b\", \"10. AI Model\")\n",
    "    except Exception as e:\n",
    "        widget_errors.append(f\"AI Model: {e}\")\n",
    "    \n",
    "    if widget_errors:\n",
    "        log_print(f\"⚠️ Some widgets had errors during creation:\", level=\"WARNING\")\n",
    "        for err in widget_errors:\n",
    "            log_print(f\"   - {err}\", level=\"WARNING\")\n",
    "        log_print(\"   Try running: dbutils.widgets.removeAll() and then run this cell again\")\n",
    "    else:\n",
    "        log_print(\"✅ Widgets created successfully.\")\n",
    "    \n",
    "    log_print(\"\")\n",
    "    log_print(\">>> Fill in the widget values at the TOP of this notebook, then run main().\")\n",
    "\n",
    "# ---\n",
    "# Run this cell to create widgets.\n",
    "# Fill in the widget values at the TOP of the notebook.\n",
    "# Then, proceed to run the 'main()' cell below.\n",
    "# ---\n",
    "\n",
    "create_widgets()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Imports & Commons\n",
    "# ==============================================================================\n",
    "# 0. IMPORTS & CONFIGURATION\n",
    "# ==============================================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import json\n",
    "import csv\n",
    "import io\n",
    "import uuid\n",
    "import base64\n",
    "import random\n",
    "import tempfile\n",
    "import shutil\n",
    "import datetime\n",
    "import html\n",
    "import pkg_resources\n",
    "import warnings\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from collections import defaultdict\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import gc\n",
    "\n",
    "# --- Databricks SDK Imports for Notebook Creation ---\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service import workspace\n",
    "\n",
    "# --- Global Configuration ---\n",
    "AI_MODEL_NAME = \"databricks-gpt-oss-20b\"\n",
    "\n",
    "# Token-to-Character Ratios (for context limit calculations)\n",
    "# English: 1 token ≈ 4 characters\n",
    "# Non-English: 1 token ≈ 2 characters\n",
    "TOKEN_TO_CHAR_RATIO_ENGLISH = 4\n",
    "TOKEN_TO_CHAR_RATIO_NON_ENGLISH = 2\n",
    "\n",
    "def get_model_token_limit(prompt_name: str = None) -> int:\n",
    "    \"\"\"\n",
    "    Get the input token limit for a model based on TECHNICAL_CONTEXT.\n",
    "    \n",
    "    Args:\n",
    "        prompt_name: Name of the prompt to lookup model for. If None, returns default model limit.\n",
    "    \n",
    "    Returns:\n",
    "        Input token limit for the model assigned to this prompt\n",
    "    \"\"\"\n",
    "    if prompt_name:\n",
    "        model_config = get_model_config_for_prompt(prompt_name)\n",
    "        if model_config:\n",
    "            return model_config.get(\"llm_input_context_tokens_count\", 200000)\n",
    "    \n",
    "    default_model = next((m for m in TECHNICAL_CONTEXT[\"models\"] if m[\"name\"] == \"claude-sonnet-4-5\"), None)\n",
    "    return default_model.get(\"llm_input_context_tokens_count\", 200000) if default_model else 200000\n",
    "\n",
    "def get_model_output_token_limit(prompt_name: str = None) -> int:\n",
    "    \"\"\"\n",
    "    Get the OUTPUT token limit for a model based on TECHNICAL_CONTEXT.\n",
    "    This is critical to prevent LLM output truncation (Claude defaults to only 1000 tokens without max_tokens).\n",
    "    \n",
    "    Args:\n",
    "        prompt_name: Name of the prompt to lookup model for. If None, returns default model limit.\n",
    "    \n",
    "    Returns:\n",
    "        Output token limit for the model assigned to this prompt\n",
    "    \"\"\"\n",
    "    if prompt_name:\n",
    "        model_config = get_model_config_for_prompt(prompt_name)\n",
    "        if model_config:\n",
    "            return model_config.get(\"llm_output_context_tokens_count\", 32000)\n",
    "    \n",
    "    default_model = next((m for m in TECHNICAL_CONTEXT[\"models\"] if m[\"name\"] == \"claude-sonnet-4-5\"), None)\n",
    "    return default_model.get(\"llm_output_context_tokens_count\", 32000) if default_model else 32000\n",
    "\n",
    "def get_max_context_chars(language: str = \"English\", prompt_name: str = None) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the maximum character limit based on language and model's token limit.\n",
    "    Uses TECHNICAL_CONTEXT to determine model-specific token limits.\n",
    "    \n",
    "    Args:\n",
    "        language: Target language (default: \"English\")\n",
    "        prompt_name: Name of the prompt to lookup model for (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Maximum character limit for the given language and model\n",
    "    \"\"\"\n",
    "    max_tokens = get_model_token_limit(prompt_name)\n",
    "    \n",
    "    if language.lower() == \"english\":\n",
    "        return max_tokens * TOKEN_TO_CHAR_RATIO_ENGLISH\n",
    "    else:\n",
    "        return max_tokens * TOKEN_TO_CHAR_RATIO_NON_ENGLISH\n",
    "\n",
    "def get_safe_context_limit(language: str = \"English\", buffer_percent: float = 0.9, prompt_name: str = None) -> int:\n",
    "    \"\"\"\n",
    "    Calculate a safe context limit with buffer to proactively avoid LLM errors.\n",
    "    Uses TECHNICAL_CONTEXT to determine model-specific token limits.\n",
    "    \n",
    "    Formula: model_token_limit * char_ratio * buffer_percent\n",
    "    \n",
    "    Args:\n",
    "        language: Target language (default: \"English\")\n",
    "        buffer_percent: Safety buffer (default: 0.9 = 10% buffer)\n",
    "        prompt_name: Name of the prompt to lookup model for (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Safe character limit with buffer applied\n",
    "    \"\"\"\n",
    "    max_chars = get_max_context_chars(language, prompt_name)\n",
    "    safe_limit = int(max_chars * buffer_percent)\n",
    "    return safe_limit\n",
    "\n",
    "# Legacy constants for backward compatibility (uses default model's English ratio)\n",
    "MAX_CONTEXT_TOKENS = get_model_token_limit()\n",
    "MAX_CONTEXT_CHARS = MAX_CONTEXT_TOKENS * TOKEN_TO_CHAR_RATIO_ENGLISH\n",
    "\n",
    "# ==============================================================================\n",
    "# IDENTIFIER NORMALIZATION UTILITIES\n",
    "# ==============================================================================\n",
    "# These functions handle SQL identifier parsing and quoting consistently.\n",
    "# All user input should be normalized (backticks stripped) on input,\n",
    "# and backticks should be added when constructing SQL queries.\n",
    "# ==============================================================================\n",
    "\n",
    "def normalize_identifier(identifier: str) -> str:\n",
    "    \"\"\"\n",
    "    Strip backticks from an identifier.\n",
    "    \n",
    "    Args:\n",
    "        identifier: A SQL identifier that may or may not have backticks\n",
    "        \n",
    "    Returns:\n",
    "        The identifier without backticks\n",
    "        \n",
    "    Examples:\n",
    "        normalize_identifier(\"`my-schema`\") -> \"my-schema\"\n",
    "        normalize_identifier(\"my_table\") -> \"my_table\"\n",
    "    \"\"\"\n",
    "    if identifier is None:\n",
    "        return \"\"\n",
    "    return identifier.strip().strip('`')\n",
    "\n",
    "def quote_identifier(identifier: str) -> str:\n",
    "    \"\"\"\n",
    "    Add backticks to an identifier for safe SQL usage.\n",
    "    First normalizes (strips existing backticks) then adds fresh backticks.\n",
    "    \n",
    "    Args:\n",
    "        identifier: A SQL identifier (normalized or not)\n",
    "        \n",
    "    Returns:\n",
    "        The identifier wrapped in backticks\n",
    "        \n",
    "    Examples:\n",
    "        quote_identifier(\"my-schema\") -> \"`my-schema`\"\n",
    "        quote_identifier(\"`my-schema`\") -> \"`my-schema`\" (not double-quoted)\n",
    "    \"\"\"\n",
    "    if identifier is None:\n",
    "        return \"``\"\n",
    "    normalized = normalize_identifier(identifier)\n",
    "    return f\"`{normalized}`\"\n",
    "\n",
    "def parse_three_level_name(name: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Parse a three-level name (catalog.schema.table or catalog.schema.column) into parts.\n",
    "    Handles names with or without backticks at any level.\n",
    "    \n",
    "    Args:\n",
    "        name: A three-level name like \"catalog.schema.table\" or \"`cat`.`sch`.`tbl`\"\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (part1, part2, part3) with backticks stripped from each part,\n",
    "        or (None, None, None) if parsing fails\n",
    "        \n",
    "    Examples:\n",
    "        parse_three_level_name(\"cat.schema.table\") -> (\"cat\", \"schema\", \"table\")\n",
    "        parse_three_level_name(\"`cat`.`my-schema`.`table`\") -> (\"cat\", \"my-schema\", \"table\")\n",
    "        parse_three_level_name(\"invalid\") -> (None, None, None)\n",
    "    \"\"\"\n",
    "    if not name:\n",
    "        return (None, None, None)\n",
    "    \n",
    "    clean_name = name.replace('`', '')\n",
    "    parts = clean_name.split('.')\n",
    "    \n",
    "    if len(parts) == 3:\n",
    "        return (parts[0].strip(), parts[1].strip(), parts[2].strip())\n",
    "    return (None, None, None)\n",
    "\n",
    "def parse_two_level_name(name: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Parse a two-level name (catalog.schema) into parts.\n",
    "    Handles names with or without backticks.\n",
    "    \n",
    "    Args:\n",
    "        name: A two-level name like \"catalog.schema\" or \"`cat`.`my-schema`\"\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (part1, part2) with backticks stripped from each part,\n",
    "        or (None, None) if parsing fails\n",
    "        \n",
    "    Examples:\n",
    "        parse_two_level_name(\"cat.schema\") -> (\"cat\", \"schema\")\n",
    "        parse_two_level_name(\"`cat`.`my-schema`\") -> (\"cat\", \"my-schema\")\n",
    "    \"\"\"\n",
    "    if not name:\n",
    "        return (None, None)\n",
    "    \n",
    "    clean_name = name.replace('`', '')\n",
    "    parts = clean_name.split('.', 1)\n",
    "    \n",
    "    if len(parts) == 2:\n",
    "        return (parts[0].strip(), parts[1].strip())\n",
    "    return (None, None)\n",
    "\n",
    "def parse_four_level_name(name: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Parse a four-level name (catalog.schema.table.column) into parts.\n",
    "    Handles names with or without backticks at any level.\n",
    "    \n",
    "    Args:\n",
    "        name: A four-level name like \"catalog.schema.table.column\"\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (catalog, schema, table, column) with backticks stripped,\n",
    "        or (None, None, None, None) if parsing fails\n",
    "    \"\"\"\n",
    "    if not name:\n",
    "        return (None, None, None, None)\n",
    "    \n",
    "    clean_name = name.replace('`', '')\n",
    "    parts = clean_name.split('.')\n",
    "    \n",
    "    if len(parts) == 4:\n",
    "        return (parts[0].strip(), parts[1].strip(), parts[2].strip(), parts[3].strip())\n",
    "    return (None, None, None, None)\n",
    "\n",
    "def build_fqn(catalog: str, schema: str, table: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Build a fully qualified name with proper backtick quoting.\n",
    "    \n",
    "    Args:\n",
    "        catalog: Catalog name (will be normalized and quoted)\n",
    "        schema: Schema name (will be normalized and quoted)\n",
    "        table: Optional table name (will be normalized and quoted if provided)\n",
    "        \n",
    "    Returns:\n",
    "        Properly quoted FQN like `catalog`.`schema` or `catalog`.`schema`.`table`\n",
    "        \n",
    "    Examples:\n",
    "        build_fqn(\"cat\", \"my-schema\") -> \"`cat`.`my-schema`\"\n",
    "        build_fqn(\"cat\", \"schema\", \"table\") -> \"`cat`.`schema`.`table`\"\n",
    "    \"\"\"\n",
    "    cat_quoted = quote_identifier(catalog)\n",
    "    schema_quoted = quote_identifier(schema)\n",
    "    \n",
    "    if table:\n",
    "        table_quoted = quote_identifier(table)\n",
    "        return f\"{cat_quoted}.{schema_quoted}.{table_quoted}\"\n",
    "    return f\"{cat_quoted}.{schema_quoted}\"\n",
    "\n",
    "# --- LLM Model Configuration for Each Prompt (Derived from TECHNICAL_CONTEXT) ---\n",
    "# This dict is auto-generated from TECHNICAL_CONTEXT for backward compatibility.\n",
    "# To change model assignments, modify TECHNICAL_CONTEXT at the top of the file.\n",
    "LLM_MODEL_CONFIG = {\n",
    "    pm[\"prompt_name\"]: get_model_endpoint_for_prompt(pm[\"prompt_name\"])\n",
    "    for pm in TECHNICAL_CONTEXT[\"prompts_models\"]\n",
    "}\n",
    "\n",
    "# DBTITLE 1,Prompts\n",
    "# --- 1. Main Prompt Templates Dictionary ---\n",
    "PROMPT_TEMPLATES = {}\n",
    "\n",
    "HONESTY_CHECK_CSV = \"\"\"\n",
    "\n",
    "### \uD83C\uDFAF HONESTY SELF-ASSESSMENT (MANDATORY - INTEGRATED IN CSV) \uD83C\uDFAF\n",
    "\n",
    "You MUST include honesty self-assessment as TWO ADDITIONAL COLUMNS at the END of your CSV:\n",
    "- **\"honesty_score\"**: Your honest score 0-100 for the ENTIRE output quality\n",
    "- **\"honesty_justification\"**: Brief justification (max 250 chars)\n",
    "\n",
    "Another more powerful LLM will review your output and generate its own honesty score to compare against yours - BE EXTREMELY HONEST. Try EXTREMELY hard to achieve 100% honesty - do not inflate your score.\n",
    "\n",
    "**IMPORTANT**: Add these 2 columns to EVERY row. Use the SAME score and justification for all rows (it's for the entire output, not per-row).\n",
    "\"\"\"\n",
    "\n",
    "HONESTY_CHECK_JSON = \"\"\"\n",
    "\n",
    "### \uD83C\uDFAF HONESTY SELF-ASSESSMENT (MANDATORY - INTEGRATED IN JSON) \uD83C\uDFAF\n",
    "\n",
    "You MUST wrap your JSON output with honesty self-assessment fields. Your response must be a JSON object with this structure:\n",
    "```json\n",
    "{{\n",
    "  \"honesty_score\": <your score 0-100>,\n",
    "  \"honesty_justification\": \"<brief justification, max 250 chars>\",\n",
    "  \"data\": <your actual output here>\n",
    "}}\n",
    "```\n",
    "\n",
    "Another more powerful LLM will review your output and generate its own honesty score to compare against yours - BE EXTREMELY HONEST. Try EXTREMELY hard to achieve 100% honesty - do not inflate your score.\n",
    "\"\"\"\n",
    "\n",
    "HONESTY_CHECK_SQL = \"\"\"\n",
    "\n",
    "### \uD83C\uDFAF HONESTY SELF-ASSESSMENT (MANDATORY - AS SQL COMMENT) \uD83C\uDFAF\n",
    "\n",
    "You MUST include honesty self-assessment as the FIRST comment in your SQL output:\n",
    "```sql\n",
    "-- HONESTY_SCORE: <your score 0-100>\n",
    "-- HONESTY_JUSTIFICATION: <brief justification, max 250 chars>\n",
    "```\n",
    "\n",
    "Another more powerful LLM will review your output and generate its own honesty score to compare against yours - BE EXTREMELY HONEST. Try EXTREMELY hard to achieve 100% honesty - do not inflate your score.\n",
    "\"\"\"\n",
    "\n",
    "HONESTY_CHECK_TABLE = \"\"\"\n",
    "\n",
    "### \uD83C\uDFAF HONESTY SELF-ASSESSMENT (MANDATORY - AS TABLE FOOTER) \uD83C\uDFAF\n",
    "\n",
    "You MUST include honesty self-assessment as TWO ADDITIONAL COLUMNS at the END of your table:\n",
    "- **\"honesty_score\"**: Your honest score 0-100 for the ENTIRE output quality  \n",
    "- **\"honesty_justification\"**: Brief justification (max 250 chars)\n",
    "\n",
    "Another more powerful LLM will review your output and generate its own honesty score to compare against yours - BE EXTREMELY HONEST. Use the SAME score and justification for all rows.\n",
    "\"\"\"\n",
    "\n",
    "# --- 1. Business Context Worker Prompt ---\n",
    "PROMPT_TEMPLATES[\"BUSINESS_CONTEXT_WORKER_PROMPT\"] = \"\"\"\n",
    "### PERSONA\n",
    "\n",
    "You are a **Principal Business Analyst** and recognized industry specialist with 15+ years of deep expertise in the `{industry}` industry. You are a master of business strategy, operations, and data-driven decision making.\n",
    "\n",
    "### CONTEXT\n",
    "\n",
    "**Assignment Details:**\n",
    "- Industry/Business Name: `{name}`\n",
    "- Type: {type_description}\n",
    "- Target: Research and document comprehensive business context for this {type_label}\n",
    "\n",
    "### TASK DEFINITION\n",
    "\n",
    "Research and provide comprehensive business context information across 6 specific dimensions. Generate detailed, realistic, industry-specific information that will serve as the foundation for building a data model. Your output must be a single, well-structured JSON object.\n",
    "\n",
    "### WORKFLOW\n",
    "\n",
    "**Step 1: Research**\n",
    "Leverage your deep industry knowledge of `{industry}` to understand the {type_label} named `{name}`.\n",
    "\n",
    "**Step 2: Information Gathering**\n",
    "For each of the 6 required fields, identify comprehensive and specific details:\n",
    "1. **Business Context**: General overview of the business operations, market position, and key characteristics.\n",
    "2. **Strategic Goals**: The high-level long-term strategic objectives. You MUST select 3-7 goals from this standard list that are MOST relevant to this business:\n",
    "   - \"Reduce Cost\" (automation, efficiency, waste reduction)\n",
    "   - \"Boost Productivity\" (faster processes, better tools, streamlined workflows)\n",
    "   - \"Increase Revenue\" (new revenue streams, upselling, cross-selling, market expansion)\n",
    "   - \"Mitigate Risk\" (fraud detection, compliance, security, audit trails)\n",
    "   - \"Protect Revenue\" (churn prevention, retention, customer satisfaction)\n",
    "   - \"Align to Regulations\" (compliance automation, regulatory reporting, audit support)\n",
    "   - \"Improve Customer Experience\" (personalization, faster service, quality improvements)\n",
    "   - \"Enable Data-Driven Decisions\" (analytics, insights, forecasting, predictions)\n",
    "3. **Business Priorities**: Immediate and near-term focus areas for the organization.\n",
    "4. **Strategic Initiative**: Key initiatives currently underway to drive growth or transformation.\n",
    "5. **Value Chain**: The primary activities that create value for the customer.\n",
    "6. **Revenue Model**: How the business generates revenue (streams, pricing models, etc.).\n",
    "\n",
    "**Step 3: JSON Construction**\n",
    "Format all information as a single JSON object with 6 keys. Values should be descriptive strings (not lists).\n",
    "\n",
    "### RULES AND CONSTRAINTS\n",
    "\n",
    "1. **Descriptive Strings**: Provide clear, concise, but comprehensive descriptions for each field.\n",
    "2. **No Generic Placeholders**: Use specific, real-world terminology and examples.\n",
    "3. **Industry-Specific**: All information must be directly relevant to the `{industry}` industry.\n",
    "4. **Realistic and Plausible**: Information should reflect actual industry practices.\n",
    "5. **Strategic Goals Format**: The strategic_goals field MUST be a comma-separated list of 3-7 goals from the standard list above, with brief elaboration for each. Example: \"Reduce Cost (automate manual processes), Increase Revenue (expand digital channels), Mitigate Risk (enhance fraud detection)\"\n",
    "\n",
    "### OUTPUT FORMAT\n",
    "\n",
    "Your response must be a single valid JSON object with NO text before or after.\n",
    "\n",
    "**JSON Structure:**\n",
    "```json\n",
    "{{\n",
    "  \"business_context\": \"string description\",\n",
    "  \"strategic_goals\": \"Goal1 (elaboration), Goal2 (elaboration), Goal3 (elaboration)\",\n",
    "  \"business_priorities\": \"string description\",\n",
    "  \"strategic_initiative\": \"string description\",\n",
    "  \"value_chain\": \"string description\",\n",
    "  \"revenue_model\": \"string description\"\n",
    "}}\n",
    "```\n",
    "\n",
    "### EXECUTION INSTRUCTION\n",
    "\n",
    "Begin generation of the JSON output now. Ensure all 6 fields are present. Start with the opening brace.\n",
    "\n",
    "\uD83D\uDEA8 ABSOLUTE RULE - OUTPUT FORMAT \uD83D\uDEA8:\n",
    "❌ DO NOT INCLUDE:\n",
    "- Any conversational or explanatory text (\"Here is...\", \"I've generated...\", \"Based on...\")\n",
    "- Any thoughts or analysis descriptions\n",
    "\n",
    "✅ OUTPUT REQUIREMENTS:\n",
    "- Your response must be a valid JSON object with the honesty wrapper\n",
    "- Start with: {{\"honesty_score\":\n",
    "\"\"\" + HONESTY_CHECK_JSON\n",
    "\n",
    "# --- 1a. Use Case Generation Prompt (ENHANCED - REMOVED GUARDRAILS) ---\n",
    "# --- AI Functions Registry ---\n",
    "AI_FUNCTIONS = {\n",
    "    \"ai_analyze_sentiment\": {\n",
    "        \"function\": \"ai_analyze_sentiment(content)\",\n",
    "        \"business_value\": \"Analyzes sentiment (positive/negative/neutral) in text to understand customer emotion and prioritize responses. MUST be combined with ai_query for actionable recommendations.\",\n",
    "        \"example_use_cases\": \"Customer review analysis with emotion classification and response strategies • Social media sentiment monitoring with brand perception tracking • Support ticket triage with urgency classification • Employee feedback analysis with engagement insights • Product review sentiment with improvement recommendations. **Note: These are examples only - innovate with use cases specific to your business context and data.**\"\n",
    "    },\n",
    "    \"ai_classify\": {\n",
    "        \"function\": \"ai_classify(content, labels)\",\n",
    "        \"business_value\": \"Classifies text into predefined categories for automated routing, segmentation, and prioritization. MUST be combined with ai_query for actionable recommendations. Array MUST have max 20 items, each <50 characters.\",\n",
    "        \"example_use_cases\": \"Customer segmentation with retention strategies • Support ticket routing with resolution plans • Lead scoring with engagement tactics • Risk classification with mitigation strategies • Product categorization with marketing recommendations • Content tagging with engagement strategies. **Note: These are examples only - innovate with use cases specific to your business context and data.**\"\n",
    "    },\n",
    "    \"ai_extract\": {\n",
    "        \"function\": \"ai_extract(content, labels)\",\n",
    "        \"business_value\": \"Extracts specified entities from unstructured text to structure data for analysis and automation. Array MUST have max 20 items, each <50 characters.\",\n",
    "        \"example_use_cases\": \"Invoice detail extraction • Email parsing for CRM • Contract data extraction • Medical record entity extraction • Product specification parsing • Customer information extraction from notes. **Note: These are examples only - innovate with use cases specific to your business context and data.**\"\n",
    "    },\n",
    "    \"ai_fix_grammar\": {\n",
    "        \"function\": \"ai_fix_grammar(content)\",\n",
    "        \"business_value\": \"Corrects grammatical errors in text to improve communication quality and professionalism.\",\n",
    "        \"example_use_cases\": \"Customer feedback normalization • Report quality improvement • Email communication enhancement • Documentation cleanup • Survey response standardization. **Note: These are examples only - innovate with use cases specific to your business context and data.**\"\n",
    "    },\n",
    "    \"ai_mask\": {\n",
    "        \"function\": \"ai_mask(content, labels)\",\n",
    "        \"business_value\": \"Masks sensitive information (PII, PHI, financial data) for compliance and secure data sharing. MUST be combined with ai_query for compliance documentation and risk assessment.\",\n",
    "        \"example_use_cases\": \"PII data anonymization with compliance tracking • GDPR/CCPA compliance workflows • Secure data sharing with risk assessment • Medical record de-identification • Financial data protection with audit trails • Customer data masking for analytics. **Note: These are examples only - innovate with use cases specific to your business context and data.**\"\n",
    "    },\n",
    "    \"ai_parse_document\": {\n",
    "        \"function\": \"ai_parse_document(content, [options_map])\",\n",
    "        \"business_value\": \"Extracts structured text, layout, tables, and figures from unstructured document files (PDF, images, Word, PowerPoint). MUST ONLY be used with binary files from Unity Catalog volumes via READ_FILES().\",\n",
    "        \"example_use_cases\": \"Invoice processing from PDFs • Scanned contract digitization • Medical record extraction from images • Form data extraction • Receipt processing • Document archive digitization. **Note: These are examples only - innovate with use cases specific to your business context and data.**\"\n",
    "    },\n",
    "    \"ai_similarity\": {\n",
    "        \"function\": \"ai_similarity(string1, string2)\",\n",
    "        \"business_value\": \"Computes semantic similarity score (0-1) between two text strings for deduplication, matching, and record linkage. MUST be combined with ai_query for merge strategies and data quality recommendations.\",\n",
    "        \"example_use_cases\": \"Customer deduplication with merge strategies • Product matching across catalogs • Vendor record linkage • Duplicate detection with resolution plans • Entity resolution with data quality impact analysis. **Note: These are examples only - innovate with use cases specific to your business context and data.**\"\n",
    "    },\n",
    "    \"ai_summarize\": {\n",
    "        \"function\": \"ai_summarize(content[, max_words])\",\n",
    "        \"business_value\": \"Creates concise summaries of long text to improve information accessibility and decision-making speed.\",\n",
    "        \"example_use_cases\": \"Clinical notes summarization • Meeting transcript condensation • News article summarization • Research paper abstracts • Customer feedback summaries • Legal document summaries. **Note: These are examples only - innovate with use cases specific to your business context and data.**\"\n",
    "    },\n",
    "    \"ai_translate\": {\n",
    "        \"function\": \"ai_translate(content, to_lang)\",\n",
    "        \"business_value\": \"Translates text to specified target languages for global communication and localization.\",\n",
    "        \"example_use_cases\": \"Multi-lingual customer support • Product description localization • Document translation • Global marketing content • International compliance documentation. **Note: These are examples only - innovate with use cases specific to your business context and data.**\"\n",
    "    },\n",
    "    \"ai_query\": {\n",
    "        \"function\": \"ai_query(endpoint, request)\",\n",
    "        \"business_value\": \"Invokes custom model serving endpoints or LLMs for flexible AI-powered analysis, generation, and recommendations. Use the configured SQL model serving endpoint for generated SQL.\",\n",
    "        \"example_use_cases\": \"Custom business analysis with LLMs • Strategic recommendations generation • Complex reasoning tasks • Domain-specific model invocation • Multi-step AI workflows • Personalized content generation. **Note: These are examples only - innovate with use cases specific to your business context and data.**\"\n",
    "    },\n",
    "    \"ai_forecast\": {\n",
    "        \"function\": \"ai_forecast(...)\",\n",
    "        \"business_value\": \"Time series forecasting with prediction intervals for demand planning, capacity optimization, and trend prediction. MUST be combined with ai_query for strategic recommendations and action plans.\",\n",
    "        \"example_use_cases\": \"Revenue forecasting with investment strategies • Demand planning with inventory recommendations • Capacity planning with resource allocation • Churn prediction with retention strategies • Sales forecasting with tactical actions • Traffic prediction with infrastructure planning. **Note: These are examples only - innovate with use cases specific to your business context and data.**\"\n",
    "    },\n",
    "    \"vector_search\": {\n",
    "        \"function\": \"vector_search(...)\",\n",
    "        \"business_value\": \"Semantic search using vector embeddings for intelligent information retrieval and recommendation systems.\",\n",
    "        \"example_use_cases\": \"RAG (Retrieval Augmented Generation) applications • Semantic document search • Product recommendations • Similar content discovery • Knowledge base search. **Note: These are examples only - innovate with use cases specific to your business context and data.**\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# NOTE: Solution Accelerators (dbx-acc-*) have been removed.\n",
    "# The system now focuses 100% on AI Functions and Statistical Functions.\n",
    "\n",
    "# --- Statistical Functions Registry ---\n",
    "STATISTICAL_FUNCTIONS = {\n",
    "    # =========================================================\n",
    "    #  1. CENTRAL TENDENCY (Baselines & Norms)\n",
    "    # =========================================================\n",
    "    \"AVG(col)\": {\n",
    "        \"function\": \"AVG(col)\",\n",
    "        \"business_value\": \"Calculates the arithmetic mean to determine the central baseline of performance\",\n",
    "        \"use_cases\": \"Average Order Value (AOV): Revenue optimization • Session Duration: Baseline engagement • Capacity Planning: Average resource usage\",\n",
    "        \"category\": \"Central Tendency\"\n",
    "    },\n",
    "    \"MEDIAN(col)\": {\n",
    "        \"function\": \"MEDIAN(col)\",\n",
    "        \"business_value\": \"Finds the midpoint value, eliminating the impact of extreme outliers\",\n",
    "        \"use_cases\": \"Real Estate: Median Home Price avoiding mansion skew • Income Analysis: Typical Household Income • Performance Benchmarking: Middle-of-the-pack performance\",\n",
    "        \"category\": \"Central Tendency\"\n",
    "    },\n",
    "    \"MODE(col)\": {\n",
    "        \"function\": \"MODE(col)\",\n",
    "        \"business_value\": \"Returns the most frequent value (works on text and numbers)\",\n",
    "        \"use_cases\": \"Inventory: Identify most sold SKU • Error Logging: Find most frequent error code • UX: Most common 'First Action' by new users\",\n",
    "        \"category\": \"Central Tendency\"\n",
    "    },\n",
    "\n",
    "    # =========================================================\n",
    "    #  2. DISPERSION (Spread & Range)\n",
    "    # =========================================================\n",
    "    \"STDDEV_POP(col)\": {\n",
    "        \"function\": \"STDDEV_POP(col)\",\n",
    "        \"business_value\": \"Quantifies total deviation from the mean for entire populations\",\n",
    "        \"use_cases\": \"Manufacturing: 6-Sigma quality control • Service Levels: Consistency of delivery times • Risk: Portfolio volatility\",\n",
    "        \"category\": \"Dispersion\"\n",
    "    },\n",
    "    \"STDDEV_SAMP(col)\": {\n",
    "        \"function\": \"STDDEV_SAMP(col)\",\n",
    "        \"business_value\": \"Estimates volatility and risk from sample data\",\n",
    "        \"use_cases\": \"Survey Analysis: Response consensus • A/B Testing: Noise estimation in test groups\",\n",
    "        \"category\": \"Dispersion\"\n",
    "    },\n",
    "    \"VAR_POP(col)\": {\n",
    "        \"function\": \"VAR_POP(col)\",\n",
    "        \"business_value\": \"Quantifies total variance for stability assessment\",\n",
    "        \"use_cases\": \"Grid Load: Energy usage variance • Inventory: Stock level fluctuation across warehouses\",\n",
    "        \"category\": \"Dispersion\"\n",
    "    },\n",
    "    \"MIN(col)\": {\n",
    "        \"function\": \"MIN(col)\",\n",
    "        \"business_value\": \"Identifies the absolute floor of a dataset\",\n",
    "        \"use_cases\": \"System Checks: Minimum load during off-hours • Pricing: Lowest competitor price • SLA: Fastest response time recorded\",\n",
    "        \"category\": \"Dispersion\"\n",
    "    },\n",
    "    \"MAX(col)\": {\n",
    "        \"function\": \"MAX(col)\",\n",
    "        \"business_value\": \"Identifies the absolute ceiling of a dataset\",\n",
    "        \"use_cases\": \"Capacity Planning: Peak concurrent users • Sales: Record high revenue • Risk: Maximum historical drawdown\",\n",
    "        \"category\": \"Dispersion\"\n",
    "    },\n",
    "    \"RANGE\": {\n",
    "        \"function\": \"MAX(col) - MIN(col)\",\n",
    "        \"business_value\": \"Measures the full spread of data boundaries\",\n",
    "        \"use_cases\": \"Price Spread: High vs Low daily price • Jitter: Gap between best and worst latency • Salary Bands: Compensation width\",\n",
    "        \"category\": \"Dispersion\"\n",
    "    },\n",
    "\n",
    "    # =========================================================\n",
    "    #  3. DISTRIBUTION SHAPE (Asymmetry & Tails)\n",
    "    # =========================================================\n",
    "    \"SKEWNESS(col)\": {\n",
    "        \"function\": \"SKEWNESS(col)\",\n",
    "        \"business_value\": \"Detects asymmetry to flag fraud or operational bias\",\n",
    "        \"use_cases\": \"Fraud: Skewed claim amounts • Load Balancing: Uneven server requests • Pricing: Margin skew indicating systematic errors\",\n",
    "        \"category\": \"Distribution Shape\"\n",
    "    },\n",
    "    \"KURTOSIS(col)\": {\n",
    "        \"function\": \"KURTOSIS(col)\",\n",
    "        \"business_value\": \"Identifies probability of extreme 'Black Swan' events (fat tails)\",\n",
    "        \"use_cases\": \"Financial Risk: Crash probability detection • Manufacturing: Defect bursts • Security: DDoS attack traffic spikes\",\n",
    "        \"category\": \"Distribution Shape\"\n",
    "    },\n",
    "\n",
    "    # =========================================================\n",
    "    #  4. PERCENTILES (Thresholds & SLAs)\n",
    "    # =========================================================\n",
    "    \"PERCENTILE_APPROX(col, p)\": {\n",
    "        \"function\": \"PERCENTILE_APPROX(col, 0.95)\",\n",
    "        \"business_value\": \"Fast percentile calculation for SLAs (P95, P99)\",\n",
    "        \"use_cases\": \"SLA Monitoring: P99 Latency compliance • Pricing: 90th percentile competitor price • Wealth: Top 1% segmentation\",\n",
    "        \"category\": \"Percentiles\"\n",
    "    },\n",
    "    \"PERCENTILE(col, p)\": {\n",
    "        \"function\": \"PERCENTILE(col, 0.5)\",\n",
    "        \"business_value\": \"Exact percentile calculation (requires more compute)\",\n",
    "        \"use_cases\": \"Compliance: Regulatory capital requirements • Grading: Exact exam score boundaries\",\n",
    "        \"category\": \"Percentiles\"\n",
    "    },\n",
    "    \"APPROX_PERCENTILE(array)\": {\n",
    "        \"function\": \"APPROX_PERCENTILE(col, array(0.25, 0.5, 0.75))\",\n",
    "        \"business_value\": \"Calculates multiple percentiles in a single pass\",\n",
    "        \"use_cases\": \"Box Plots: Generate Q1, Median, Q3 simultaneously • Tiering: Define Bronze/Silver/Gold thresholds\",\n",
    "        \"category\": \"Percentiles\"\n",
    "    },\n",
    "\n",
    "    # =========================================================\n",
    "    #  5. TREND ANALYSIS (Regression)\n",
    "    #  *Requires Spark 3.3+ (Standard in Databricks)*\n",
    "    # =========================================================\n",
    "    \"REGR_SLOPE(y, x)\": {\n",
    "        \"function\": \"REGR_SLOPE(y, x)\",\n",
    "        \"business_value\": \"Calculates rate of change (trend direction)\",\n",
    "        \"use_cases\": \"Growth: Revenue per day slope • Elasticity: Demand change per dollar price change\",\n",
    "        \"category\": \"Trend Analysis\"\n",
    "    },\n",
    "    \"REGR_INTERCEPT(y, x)\": {\n",
    "        \"function\": \"REGR_INTERCEPT(y, x)\",\n",
    "        \"business_value\": \"Identifies baseline performance (y when x=0)\",\n",
    "        \"use_cases\": \"Baselines: Organic sales without marketing spend • Fixed Costs: Energy cost at zero production\",\n",
    "        \"category\": \"Trend Analysis\"\n",
    "    },\n",
    "    \"REGR_R2(y, x)\": {\n",
    "        \"function\": \"REGR_R2(y, x)\",\n",
    "        \"business_value\": \"Measures predictive power (0 to 1)\",\n",
    "        \"use_cases\": \"Driver Analysis: How well Price explains Churn • Forecast Validity: Reliability of trend projection\",\n",
    "        \"category\": \"Trend Analysis\"\n",
    "    },\n",
    "\n",
    "    # =========================================================\n",
    "    #  6. CORRELATION (Drivers)\n",
    "    # =========================================================\n",
    "    \"CORR(col1, col2)\": {\n",
    "        \"function\": \"CORR(col1, col2)\",\n",
    "        \"business_value\": \"Discovers relationships (-1 to 1) between metrics\",\n",
    "        \"use_cases\": \"Cannibalization: New product vs Old product sales • Marketing: Spend vs Acquisition correlation\",\n",
    "        \"category\": \"Correlation\"\n",
    "    },\n",
    "    \"COVAR_POP(col1, col2)\": {\n",
    "        \"function\": \"COVAR_POP(col1, col2)\",\n",
    "        \"business_value\": \"Measures joint variability across population\",\n",
    "        \"use_cases\": \"Systemic Risk: Sector A vs Sector B movement • Supply Chain: Fuel Price vs Shipping Cost linkage\",\n",
    "        \"category\": \"Correlation\"\n",
    "    },\n",
    "\n",
    "    # =========================================================\n",
    "    #  7. VOLATILITY & ANOMALY DETECTION\n",
    "    # =========================================================\n",
    "    \"COEFF_VAR\": {\n",
    "        \"function\": \"STDDEV(col) / AVG(col)\",\n",
    "        \"business_value\": \"Coefficient of Variation: Normalizes volatility for comparison\",\n",
    "        \"use_cases\": \"Risk Comparison: Compare volatility of High-Price vs Low-Price stock\",\n",
    "        \"category\": \"Volatility\"\n",
    "    },\n",
    "    \"Z_SCORE\": {\n",
    "        \"function\": \"(col - AVG(col) OVER ()) / STDDEV(col) OVER ()\",\n",
    "        \"business_value\": \"Calculates how many standard deviations a value is from mean (Window Func)\",\n",
    "        \"use_cases\": \"Universal Anomaly Detection: Flag > 3 Sigma • Normalization: Standardize different scales for scoring\",\n",
    "        \"category\": \"Outlier Detection\"\n",
    "    },\n",
    "    \"IQR_THRESHOLD\": {\n",
    "        \"function\": \"PERCENTILE(col, 0.75) - PERCENTILE(col, 0.25)\",\n",
    "        \"business_value\": \"Interquartile Range: Robust outlier detection ignoring extremes\",\n",
    "        \"use_cases\": \"Pricing: Middle 50% market range • Cleaning: Identify valid operating ranges excluding spikes\",\n",
    "        \"category\": \"Outlier Detection\"\n",
    "    },\n",
    "\n",
    "    # =========================================================\n",
    "    #  8. RANKING & SEGMENTATION\n",
    "    # =========================================================\n",
    "    \"CUME_DIST()\": {\n",
    "        \"function\": \"CUME_DIST() OVER (ORDER BY col)\",\n",
    "        \"business_value\": \"Cumulative distribution for relative standing\",\n",
    "        \"use_cases\": \"Loyalty: Top 10% customers by LTV • Inventory: Oldest 20% of stock\",\n",
    "        \"category\": \"Ranking\"\n",
    "    },\n",
    "    \"NTILE(n)\": {\n",
    "        \"function\": \"NTILE(n) OVER (ORDER BY col)\",\n",
    "        \"business_value\": \"Divides data into equal buckets (Quintiles, Deciles)\",\n",
    "        \"use_cases\": \"RFM Segmentation: 5 value groups • Risk Grading: Loan applicant quartiles\",\n",
    "        \"category\": \"Ranking\"\n",
    "    },\n",
    "    \"DENSE_RANK()\": {\n",
    "        \"function\": \"DENSE_RANK() OVER (ORDER BY col DESC)\",\n",
    "        \"business_value\": \"Ranks without gaps (ties get same rank)\",\n",
    "        \"use_cases\": \"Leaderboards: Sales rep rankings • Product Popularity: Top sellers handling ties\",\n",
    "        \"category\": \"Ranking\"\n",
    "    },\n",
    "    \"ROW_NUMBER()\": {\n",
    "        \"function\": \"ROW_NUMBER() OVER (PARTITION BY cat ORDER BY date DESC)\",\n",
    "        \"business_value\": \"Assigns unique ID to rows\",\n",
    "        \"use_cases\": \"Deduplication: Keep first record only • Latest Status: Get most recent change per user\",\n",
    "        \"category\": \"Ranking\"\n",
    "    },\n",
    "\n",
    "    # =========================================================\n",
    "    #  9. TIME SERIES (Window Functions)\n",
    "    # =========================================================\n",
    "    \"LAG(col, n)\": {\n",
    "        \"function\": \"LAG(col, 1) OVER (PARTITION BY entity ORDER BY time)\",\n",
    "        \"business_value\": \"Access previous row value\",\n",
    "        \"use_cases\": \"MoM Growth: Current vs Previous month • Churn: Days since last purchase\",\n",
    "        \"category\": \"Time Series\"\n",
    "    },\n",
    "    \"LEAD(col, n)\": {\n",
    "        \"function\": \"LEAD(col, 1) OVER (PARTITION BY entity ORDER BY time)\",\n",
    "        \"business_value\": \"Access next row value\",\n",
    "        \"use_cases\": \"Stockout Warning: Current inventory vs Next forecast • Sequencing: Next best action\",\n",
    "        \"category\": \"Time Series\"\n",
    "    },\n",
    "    \"RUNNING_SUM\": {\n",
    "        \"function\": \"SUM(col) OVER (PARTITION BY entity ORDER BY time)\",\n",
    "        \"business_value\": \"Cumulative total over time\",\n",
    "        \"use_cases\": \"Lifetime Value: Running spend total • Budget Burn: Cumulative spend vs Cap\",\n",
    "        \"category\": \"Time Series\"\n",
    "    },\n",
    "    \"MOVING_AVG\": {\n",
    "        \"function\": \"AVG(col) OVER (ORDER BY time ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)\",\n",
    "        \"business_value\": \"Smooths data to show underlying trends\",\n",
    "        \"use_cases\": \"7-Day Trend: Smoothing daily sales noise • Stock Analysis: Technical indicators\",\n",
    "        \"category\": \"Time Series\"\n",
    "    },\n",
    "\n",
    "    # =========================================================\n",
    "    #  10. OLAP & HIERARCHY\n",
    "    # =========================================================\n",
    "    \"ROLLUP(cols)\": {\n",
    "        \"function\": \"GROUP BY ROLLUP(region, country, city)\",\n",
    "        \"business_value\": \"Generates subtotals at every level\",\n",
    "        \"use_cases\": \"P&L Reporting: Global -> Region -> Country totals in one query\",\n",
    "        \"category\": \"OLAP\"\n",
    "    },\n",
    "    \"CUBE(cols)\": {\n",
    "        \"function\": \"GROUP BY CUBE(product, region)\",\n",
    "        \"business_value\": \"All possible combinations of dimensions\",\n",
    "        \"use_cases\": \"Cross-Analysis: Conversion rates for every Segment/Region/Device combo\",\n",
    "        \"category\": \"OLAP\"\n",
    "    },\n",
    "    \"PIVOT\": {\n",
    "        \"function\": \"PIVOT (SUM(sales) FOR month IN ('Jan', 'Feb'))\",\n",
    "        \"business_value\": \"Transposes rows to columns\",\n",
    "        \"use_cases\": \"Reporting: Monthly sales grid • Competitor Matrix: Features vs Competitors\",\n",
    "        \"category\": \"Reshaping\"\n",
    "    },\n",
    "\n",
    "    # =========================================================\n",
    "    #  11. ARRAY & COLLECTION ANALYTICS\n",
    "    # =========================================================\n",
    "    \"SIZE(col)\": {\n",
    "        \"function\": \"SIZE(array_col)\",\n",
    "        \"business_value\": \"Counts elements in an array\",\n",
    "        \"use_cases\": \"Basket Size: Items per order • Engagement: Features used per user\",\n",
    "        \"category\": \"Collections\"\n",
    "    },\n",
    "    \"EXPLODE(col)\": {\n",
    "        \"function\": \"EXPLODE(array_col)\",\n",
    "        \"business_value\": \"Unnests array into separate rows\",\n",
    "        \"use_cases\": \"Granularity: Analyze individual items inside a sales basket order\",\n",
    "        \"category\": \"Collections\"\n",
    "    },\n",
    "    \"COLLECT_SET(col)\": {\n",
    "        \"function\": \"COLLECT_SET(col)\",\n",
    "        \"business_value\": \"Aggregates unique values into a list\",\n",
    "        \"use_cases\": \"Journey Mapping: Unique pages visited • Cross-Sell: Categories purchased from\",\n",
    "        \"category\": \"Collections\"\n",
    "    },\n",
    "    \"ARRAYS_OVERLAP(a1, a2)\": {\n",
    "        \"function\": \"ARRAYS_OVERLAP(array1, array2)\",\n",
    "        \"business_value\": \"Checks if two arrays share elements\",\n",
    "        \"use_cases\": \"Targeting: Match User Interests with Product Tags • Fraud: Shared attributes\",\n",
    "        \"category\": \"Collections\"\n",
    "    },\n",
    "\n",
    "    # =========================================================\n",
    "    #  12. GEOSPATIAL (Databricks H3)\n",
    "    # =========================================================\n",
    "    \"H3_LONGLATASH3\": {\n",
    "        \"function\": \"h3_longlatash3(lon, lat, 10)\",\n",
    "        \"business_value\": \"Converts GPS to Hexagon Grid ID (Databricks Native)\",\n",
    "        \"use_cases\": \"Density Mapping: Delivery zones • Surge Pricing: Ride share grids\",\n",
    "        \"category\": \"Geospatial\"\n",
    "    },\n",
    "    \"H3_DISTANCE\": {\n",
    "        \"function\": \"h3_distance(cell1, cell2)\",\n",
    "        \"business_value\": \"Calculates grid steps between cells\",\n",
    "        \"use_cases\": \"Proximity: Store catchment analysis • Logistics: Delivery estimation\",\n",
    "        \"category\": \"Geospatial\"\n",
    "    }\n",
    "}\n",
    "\n",
    "PROMPT_TEMPLATES[\"BASE_USE_CASE_GEN_PROMPT\"] = \"\"\"### 0. PERSONA ACTIVATION\n",
    "\n",
    "You are a highly experienced **Principal Enterprise Data Architect** and an industry specialist. Your primary task is to generate high-quality business use cases that deliver business value from the point of view of the business, these use cases will later have SQL queries generated for them.\n",
    "\n",
    "### BUSINESS CONTEXT\n",
    "**Business Context:** {business_context}\n",
    "**Strategic Goals:** {strategic_goals}\n",
    "**Business Priorities:** {business_priorities}\n",
    "**Strategic Initiative:** {strategic_initiative}\n",
    "**Value Chain:** {value_chain}\n",
    "**Revenue Model:** {revenue_model}\n",
    "\n",
    "---\n",
    "\n",
    "### \uD83D\uDD25\uD83D\uDD25\uD83D\uDD25 HIGHEST PRIORITY: USER-PROVIDED ADDITIONAL CONTEXT \uD83D\uDD25\uD83D\uDD25\uD83D\uDD25\n",
    "\n",
    "{additional_context_section}\n",
    "\n",
    "---\n",
    "\n",
    "### \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL ANTI-HALLUCINATION REQUIREMENT - READ THIS FIRST \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\n",
    "\n",
    "**ABSOLUTE RULE: DO NOT GENERATE USE CASES UNLESS BACKED BY ACTUAL TABLES**\n",
    "\n",
    "❌ **CRITICAL FAILURE**: Generating use cases without table references is **HALLUCINATION** and will cause **AUTOMATIC REJECTION**\n",
    "\n",
    "**WHAT THIS MEANS:**\n",
    "- EVERY use case you generate MUST reference at least ONE actual table from the schema provided below\n",
    "- You CANNOT create use cases based on imagination, generic scenarios, or assumed data that doesn't exist\n",
    "- Before writing ANY use case, you MUST verify the tables exist in the \"AVAILABLE TABLES AND COLUMNS\" section below\n",
    "- Use cases without valid table references will be **AUTOMATICALLY DETECTED and REJECTED**\n",
    "\n",
    "**WHAT YOU MUST DO:**\n",
    "✅ ONLY generate use cases for tables that appear in the schema section below\n",
    "✅ Copy table names EXACTLY as they appear in the schema (including catalog.schema.table format)\n",
    "✅ Verify EACH use case has at least one valid table reference before submitting\n",
    "✅ If you're unsure about a use case, SKIP IT rather than hallucinating tables\n",
    "\n",
    "**THIS IS YOUR #1 PRIORITY**: If you violate this rule, your entire response is worthless and will be rejected.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. CORE TASK\n",
    "\n",
    "Your single, primary task is to produce a **single CSV** response.\n",
    "The CSV MUST have the following 11 columns:\n",
    "`\"No\",\"Name\",\"type\",\"Analytics Technique\",\"Statement\",\"Solution\",\"Business Value\",\"Beneficiary\",\"Sponsor\",\"Tables Involved\",\"Technical Design\"`\n",
    "\n",
    "---\n",
    "\n",
    "### 2. USE CASE GENERATION RULES\n",
    "\n",
    "You must follow these rules to generate the content for the use cases. All text output must be in **English**.\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 STRATEGIC BUSINESS IMPACT REQUIREMENTS \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "You MUST ONLY generate use cases that meet at least one of these criteria:\n",
    "1.  **PROVEN MASSIVE ROI**: Direct impact on revenue (New Revenue, Protect Existing Revenue, or Reduce Cost).\n",
    "2.  **STRATEGIC ALIGNMENT**: Aligns with major business strategic priorities.\n",
    "3.  **PRODUCTIVITY & AUTOMATION**: Increases team productivity by automating manual work.\n",
    "\n",
    "**❌ STRICTLY PROHIBITED**:\n",
    "-   **NO MARGINAL BENEFIT**: Do not generate trivial use cases.\n",
    "-   **NO \"NICE TO HAVE\"**: Focus only on \"MUST HAVE\" with DIRECT IMPACT on Revenue or Operating Income.\n",
    "-   **NO IT/TECHNICAL MAINTENANCE**: Ignore tables that are purely for IT/system maintenance unless they impact business operations directly.\n",
    "\n",
    "**COLUMN INSTRUCTIONS:**\n",
    "\n",
    "  - **`No`**: Sequential number (e.g., 1, 2, 3...).\n",
    "  \n",
    "  - **`Name`**: A short, clear name that emphasizes BUSINESS VALUE, not technical implementation.\n",
    "    *   **Use exciting business-oriented verbs**: Anticipate, Predict, Envision, Segment, Identify, Detect, Reveal.\n",
    "    *   **Example**: \"Anticipate Monthly Revenue Trends with Action Plans\" (NOT \"Forecast Revenue\").\n",
    "\n",
    "  - **`type`**: One of \"Problem\", \"Risk\", \"Opportunity\", \"Improvement\".\n",
    "\n",
    "  - **`Analytics Technique`**: The PRIMARY analytics technique used. MUST be ONE of these values:\n",
    "    * `Forecasting` - Time-series prediction using AI_FORECAST\n",
    "    * `Classification` - Categorizing data using ai_classify\n",
    "    * `Anomaly Detection` - Identifying outliers, deviations, unusual patterns\n",
    "    * `Cohort Analysis` - Grouping entities by shared characteristics over time\n",
    "    * `Segmentation` - Clustering customers/products into distinct groups\n",
    "    * `Sentiment Analysis` - Analyzing text for emotional tone\n",
    "    * `Trend Analysis` - Identifying patterns over time (growth, decline)\n",
    "    * `Correlation Analysis` - Finding relationships between variables\n",
    "    * `Pareto Analysis` - 80/20 rule, identifying top contributors\n",
    "    * `Funnel Analysis` - Conversion tracking through stages\n",
    "    * `Document Processing` - Extracting data from unstructured documents\n",
    "    * `Extraction` - Extracting specific entities from text\n",
    "    * `AI Analysis` - General AI-powered business analysis using ai_query\n",
    "\n",
    "  - **`Statement`**: 1-2 sentences on the business challenge/opportunity. Focus on IMPACT (Revenue, Cost, Risk).\n",
    "\n",
    "  - **`Solution`**: 1-2 sentences high-level business solution. **MUST explicitly highlight \"Databricks Agent Bricks\"**.\n",
    "\n",
    "  - **`Business Value`**: **CRITICAL**. Articulate the tangible business impact (Revenue, Cost, Efficiency).\n",
    "    *   **Focus on WHY this matters**.\n",
    "    *   **IMPORTANT CONSTRAINT**: Refrain from mentioning any specific values (e.g. \"10% more revenue\", \"reduce cost by 20%\"). Deliver the business value statement WITHOUT committing on any number.\n",
    "    *   **GOOD**: \"Reduces fuel costs and extends aircraft lifespan...\"\n",
    "    *   **BAD**: \"Optimizes performance...\" (Too generic).\n",
    "\n",
    "  - **`Beneficiary`**: The primary person/role (e.g., \"Loan Officer\").\n",
    "  - **`Sponsor`**: The main executive (e.g., \"CRO\").\n",
    "\n",
    "  - **`Tables Involved`**: Comma-separated **FULLY-QUALIFIED** table names (`catalog.schema.table`). MUST exist in schema.\n",
    "    *   **CRITICAL**: Use the EXACT THREE-LEVEL FORMAT shown in the schema.\n",
    "\n",
    "  - **`Technical Design`**: A high-level technical design guide (2-4 sentences) outlining the Logical Flow (CTEs).\n",
    "    *   **\uD83D\uDEA8 CTE1 MUST START WITH DISTINCT**: First CTE MUST use SELECT DISTINCT or GROUP BY to deduplicate source data.\n",
    "    *   **\uD83C\uDF10 OPTIONAL: external_api_for_* CTE (ONLY WHEN BUSINESS-RELEVANT)**: Include external data enrichment ONLY when there is a DIRECT, PROVABLE, INDUSTRY-RECOGNIZED cause-and-effect relationship between the external factor and the business metric. If you cannot explain WHY the external factor impacts the metric in one sentence, DO NOT include it.\n",
    "    *   Describe the approach as a sequence of logical steps.\n",
    "    *   Mention specific statistical/AI functions if relevant.\n",
    "    *   **\uD83E\uDDE0 ASK YOURSELF**: \"Is there a DIRECT cause-and-effect relationship?\" \"Would a domain expert agree this connection is logical?\" \"Would a CFO approve this without questioning the logic?\" Only include external_api CTE if ALL answers are YES.\n",
    "\n",
    "**FOCUS AREAS:**\n",
    "{focus_areas_instruction}\n",
    "\n",
    "**LOGICAL REQUIREMENTS:**\n",
    "  - **EXHAUSTIVE COVERAGE**: You MUST enumerate every distinct valid use case supported by the schema that meets the criteria above.\n",
    "  - **NO OMISSIONS**: Do NOT stop early or cap the number of use cases.\n",
    "  - **STRATEGIC GOALS**: If Strategic Goals are provided, include every use case that satisfies those goals.\n",
    "  - **JOIN OPPORTUNITIES**: Prioritize use cases that join multiple tables for cross-functional insights.\n",
    "  - **NO REDUNDANT EXTRACTION**: Do NOT use AI to extract/classify data that already exists in structured columns.\n",
    "  - **AGGRESSIVE ANALYSIS**: Squeeze every valuable use case from the tables.\n",
    "  - **BALANCE**: Ensure coverage of all tables if they offer business value.\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: MAXIMIZE BUSINESS VALUE - NOT QUANTITY \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**YOUR PRIMARY MISSION**: Extract EVERY use case that delivers **SIGNIFICANT BUSINESS VALUE** from the data.\n",
    "\n",
    "**VALUE-DRIVEN GENERATION RULES:**\n",
    "  - **REVENUE IMPACT FIRST**: Prioritize use cases that directly impact revenue (increase sales, reduce churn, optimize pricing, expand markets).\n",
    "  - **COST REDUCTION**: Include use cases that reduce operational costs, eliminate waste, or improve efficiency.\n",
    "  - **RISK MITIGATION**: Identify use cases that prevent losses (fraud detection, risk assessment, compliance).\n",
    "  - **STRATEGIC ALIGNMENT**: Every use case must align with business priorities and strategic goals.\n",
    "\n",
    "**EXHAUSTIVE EXPLORATION (NO ARTIFICIAL LIMITS):**\n",
    "  - **EXPLORE ALL TECHNIQUES**: Use ANY AI function, statistical method, or analytical approach that delivers business value. Do not limit yourself.\n",
    "  - **EXPLORE ALL ANGLES**: For each table, think about it from multiple business perspectives - operations, finance, sales, marketing, risk, strategy.\n",
    "  - **EXPLORE ALL RELATIONSHIPS**: Look for valuable insights from joining tables together - cross-functional analysis often yields the highest ROI.\n",
    "  - **EXPLORE ALL TIME HORIZONS**: Historical analysis, real-time monitoring, and future predictions all have value.\n",
    "\n",
    "**QUALITY OVER QUANTITY:**\n",
    "  - **NO FILLER**: Do NOT generate low-value use cases just to increase count. Every use case must deliver measurable business impact.\n",
    "  - **NO HALLUCINATION**: Only generate use cases that are ACTUALLY SUPPORTED by the tables and columns provided.\n",
    "  - **HIGH ROI FOCUS**: Ask yourself for each use case: \"Would a CFO approve budget for this? Does it move the revenue needle?\"\n",
    "  - **SKIP IF NO VALUE**: If a table has no high-value use cases, it's better to skip it than generate low-quality filler.\n",
    "\n",
    "**SELF-CHECK BEFORE FINALIZING:**\n",
    "  - ✅ Does each use case have CLEAR, MEASURABLE business value?\n",
    "  - ✅ Did I explore MULTIPLE valuable angles for tables with rich business data?\n",
    "  - ✅ Did I consider CROSS-TABLE opportunities that could unlock hidden value?\n",
    "  - ✅ Would a business executive actually want to implement these use cases?\n",
    "  - ❌ Did I generate any use cases just to fill space? (If yes, REMOVE them)\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL: FIRST CTE MUST RETURN UNIQUE/DISTINCT RECORDS \uD83D\uDEA8**\n",
    "  - **MANDATORY**: The FIRST CTE in every Technical Design MUST use `SELECT DISTINCT` or `GROUP BY` to ensure NO DUPLICATE RECORDS.\n",
    "  - **WHY**: Duplicates in source data will cascade errors through all downstream CTEs (forecasts, classifications, aggregations).\n",
    "  - **PATTERN**: `WITH base_data AS (SELECT DISTINCT col1, col2, ... FROM table WHERE ... LIMIT 10)` OR `GROUP BY` all non-aggregated columns.\n",
    "  - **VALIDATION**: Before any AI function or aggregation, the data MUST be deduplicated in the first CTE.\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL: LIMIT 10 SAMPLING RULES \uD83D\uDEA8**\n",
    "  - **FIRST CTE ONLY**: Use `LIMIT 10` at the END of the FIRST CTE that reads from tables\n",
    "  - **NO LIMIT IN OTHER CTEs**: DO NOT use `LIMIT 10` in any other CTE - only in the first CTE\n",
    "  - **LIMIT PLACEMENT**: LIMIT 10 MUST be the LAST clause in the SELECT statement (after WHERE, ORDER BY, etc.)\n",
    "  - **PATTERN**: `FROM catalog.schema.table AS t WHERE ... LIMIT 10`\n",
    "  - **EXAMPLE**:\n",
    "    ```sql\n",
    "    -- \uD83D\uDEA8 EVERY column must be COALESCE'd or have IS NOT NULL check!\n",
    "    WITH base_data AS (\n",
    "      SELECT DISTINCT \n",
    "        customer_id,                                            -- CRITICAL: filtered with IS NOT NULL\n",
    "        COALESCE(TRIM(customer_name), 'Unknown') AS customer_name  -- ✅ COALESCE'd\n",
    "        -- ... (all columns must be COALESCE'd or filtered) ...\n",
    "      FROM `catalog`.`schema`.`customers` AS c\n",
    "      WHERE customer_id IS NOT NULL\n",
    "      LIMIT 10  -- ✅ LIMIT 10 at the END of first CTE only\n",
    "    ),\n",
    "    enriched_data AS (\n",
    "      SELECT * FROM base_data  -- ✅ NO LIMIT here\n",
    "    ),\n",
    "    final_analysis AS (\n",
    "      SELECT * FROM enriched_data  -- ✅ NO LIMIT here\n",
    "    )\n",
    "    SELECT * FROM final_analysis;  -- ✅ NO LIMIT in final SELECT\n",
    "    ```\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: BUSINESS REALISM & RELEVANCY REQUIREMENT \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**EVERY USE CASE MUST PASS THE BUSINESS RELEVANCY TEST**\n",
    "\n",
    "Before generating ANY use case, you MUST verify it would be taken seriously by business stakeholders. Use cases with illogical correlations or far-fetched connections will be rejected by business teams and damage credibility.\n",
    "\n",
    "**MANDATORY REALISM TEST (APPLY TO EVERY USE CASE):**\n",
    "1. **LOGICAL CAUSATION**: Is there a DIRECT, PROVABLE cause-and-effect relationship between the variables? (Correlation is NOT causation)\n",
    "2. **INDUSTRY RECOGNITION**: Is this type of analysis recognized and practiced in the industry?\n",
    "3. **EXECUTIVE CREDIBILITY**: Would a senior executive approve budget for this analysis without questioning the logic?\n",
    "4. **DOMAIN EXPERT VALIDATION**: Would a 20-year industry veteran consider this analysis sensible and valuable?\n",
    "5. **BOARDROOM TEST**: Would you confidently present this use case in a boardroom without being challenged on its logic?\n",
    "\n",
    "**❌ STRICTLY PROHIBITED - IRRELEVANT OR NONSENSICAL USE CASES:**\n",
    "- Correlating variables that have NO logical business connection\n",
    "- Using external factors that do NOT directly impact the metric being analyzed\n",
    "- Inventing relationships just because two variables share temporal patterns\n",
    "- Adding external data enrichment when there is no clear cause-and-effect\n",
    "- Generating \"creative\" correlations that would be questioned by domain experts\n",
    "\n",
    "**\uD83E\uDDEA MANDATORY SELF-CHECK BEFORE EACH USE CASE:**\n",
    "Ask yourself these questions - if ANY answer is \"No\" or \"I'm not sure\", DO NOT generate the use case:\n",
    "1. \"Can I explain in ONE CLEAR SENTENCE why this factor directly impacts this metric?\"\n",
    "2. \"Would a domain expert in this industry agree this analysis makes business sense?\"\n",
    "3. \"Is this correlation industry-recognized, or am I inventing a relationship?\"\n",
    "4. \"Would a skeptical CFO approve budget for this without questioning the logic?\"\n",
    "5. \"Would I be embarrassed presenting this use case to a senior business leader?\"\n",
    "\n",
    "**✅ GOOD USE CASES HAVE:**\n",
    "- Clear, explainable cause-and-effect relationships\n",
    "- Industry-recognized analytical approaches\n",
    "- Direct relevance to the business metrics being analyzed\n",
    "- Logical connections that domain experts would validate\n",
    "\n",
    "---\n",
    "\n",
    "**\uD83C\uDF10\uD83C\uDF10\uD83C\uDF10 EXTERNAL PUBLIC DATA ENRICHMENT (ONLY WHEN BUSINESS-RELEVANT) \uD83C\uDF10\uD83C\uDF10\uD83C\uDF10**\n",
    "  - **ONLY include external data enrichment** when there is a CLEAR, LOGICAL, INDUSTRY-RECOGNIZED business connection\n",
    "  - External data is valuable ONLY when the external factor DIRECTLY and PROVABLY impacts the business metric\n",
    "  - **\uD83C\uDFAF YOUR MISSION: THINK LIKE A SKEPTICAL BUSINESS ANALYST!** If you cannot explain the connection in one sentence, DO NOT include it\n",
    "  \n",
    "  **\uD83E\uDDE0 BEFORE ADDING ANY EXTERNAL DATA, ASK:**\n",
    "  1. \"Is there a DIRECT cause-and-effect relationship that domain experts would recognize?\"\n",
    "  2. \"Can I explain WHY this external factor impacts this specific metric in one clear sentence?\"\n",
    "  3. \"Is this the type of enrichment that industry practitioners actually use?\"\n",
    "  4. \"Would a senior analyst in this industry include this external data?\"\n",
    "  5. \"If challenged by a business leader, can I defend this connection with logic?\"\n",
    "  \n",
    "  **IF YOU CANNOT ANSWER \"YES\" TO ALL QUESTIONS ABOVE, DO NOT INCLUDE THE EXTERNAL DATA.**\n",
    "  \n",
    "  **\uD83D\uDD25 RECOMMENDED TECHNICAL DESIGN PATTERN (When External Data is Business-Relevant) \uD83D\uDD25**\n",
    "  - When you identify missing information that would improve the analysis, include a dedicated CTE named `external_api_for_<scenario>`.\n",
    "  - Use a **PERSONA-BASED PROMPT** that establishes the AI as a domain expert with authority and credibility:\n",
    "    * Weather: \"You are a Principal Meteorologist at National Weather Service with 20 years expertise...\"\n",
    "    * Economic: \"You are a Chief Economist at World Bank with 18 years expertise...\"\n",
    "    * Competitive: \"You are a Senior Market Intelligence Analyst at McKinsey with 15 years expertise...\"\n",
    "    * Geographic: \"You are a Senior Demographer at UN Population Division with 20 years expertise...\"\n",
    "    * Events: \"You are a Risk Analyst at Lloyd's of London with 15 years expertise in disruption monitoring...\"\n",
    "  - **Include confidence scores** for each field: `<field>_confidence` (0.0-1.0), plus `as_of_date`, `source_note`, `is_estimate: true`, `requires_verification: true`\n",
    "  - Add a SQL comment: \"-- EXTERNAL DATA: For production, connect to a verified data source. LLM estimates are suitable for prototyping.\"\n",
    "  - **USE the external data** in downstream ai_query prompts - this is WHERE THE VALUE IS REALIZED!\n",
    "\n",
    "---\n",
    "\n",
    "### 3. AI FUNCTION DOCUMENTATION & SCHEMA\n",
    "\n",
    "#### CONTEXT: DATABRICKS AI FUNCTION DOCUMENTATION\n",
    "{ai_functions_summary}\n",
    "\n",
    "**AVAILABLE STATISTICAL FUNCTIONS:**\n",
    "{statistical_functions_detailed}\n",
    "\n",
    "#### INPUT DATA FORMAT\n",
    "##### 1. Structured Data Schema\n",
    "`| column | type | column_description |`\n",
    "{schema_markdown}\n",
    "\n",
    "##### 1b. Foreign Key Relationships\n",
    "{foreign_key_relationships}\n",
    "\n",
    "---\n",
    "\n",
    "### 4. CSV ROW EXAMPLES (PATTERN EXAMPLES - ADAPT TO YOUR INDUSTRY)\n",
    "\n",
    "**NOTE**: SQL will be generated separately.\n",
    "\n",
    "**\uD83D\uDEA8 IMPORTANT**: The examples below are GENERIC PATTERNS showing the CSV format and AI function usage. You MUST adapt them to:\n",
    "- The ACTUAL industry and business context provided above\n",
    "- The ACTUAL tables and columns available in the schema\n",
    "- The ACTUAL business terminology used by the organization\n",
    "\n",
    "  - **`ai_forecast` Pattern Example (WITH EXTERNAL DATA ENRICHMENT):**\n",
    "`\"1\",\"Forecast Monthly [METRIC] with Economic Context\",\"Risk\",\"The business risks inaccurate [METRIC] forecasts without understanding external factors.\",\"Implement predictive time-series forecasting enriched with economic indicators using Databricks Agent Bricks.\",\"Prevents costly errors by incorporating macro-economic factors that explain forecast variations.\",\"[Role]\",\"[Executive]\",\"[catalog.schema.table]\",\"CTE1: SELECT DISTINCT to deduplicate source data. CTE2: external_api_for_economic_context using ai_query with persona 'You are a Chief Economist at IMF...' to get GDP growth, inflation, exchange rates - ASK: What economic factors might explain the trends? CTE3: Parse economic JSON and join with base data. CTE4: Aggregate data by time period. CTE5: Apply ai_forecast. CTE6: Use ai_query for recommendations enriched with economic context.\"`\n",
    "\n",
    "  - **`ai_classify` Pattern Example (WITH EXTERNAL DATA ENRICHMENT):**\n",
    "`\"2\",\"Classify [ENTITY] by [CATEGORY] with Market Benchmarks\",\"Improvement\",\"Manual categorization lacks market context to compare against.\",\"Use Databricks Agent Bricks to classify [ENTITY] with industry benchmarks.\",\"Accelerates processing with context-aware classification and competitive positioning.\",\"[Role]\",\"[Executive]\",\"[catalog.schema.table]\",\"CTE1: SELECT DISTINCT to get unique entities. CTE2: external_api_for_market_benchmarks using ai_query with persona 'You are a Senior Market Analyst at McKinsey...' to get industry standards - ASK: What benchmarks would help understand if classification is good or bad? CTE3: Parse benchmark JSON. CTE4: Apply ai_classify with enriched benchmark context. CTE5: Use ai_query for actionable strategies informed by market position.\"`\n",
    "\n",
    "  - **`ai_query` Pattern Example (WITH EXTERNAL DATA ENRICHMENT):**\n",
    "`\"3\",\"Generate [RECOMMENDATIONS] for [ENTITY] with Competitive Intelligence\",\"Improvement\",\"Team lacks context to make informed [RECOMMENDATIONS].\",\"Use Databricks Agent Bricks to generate [RECOMMENDATIONS] enriched with competitive data.\",\"Improves outcomes with market-aware recommendations that consider competitive landscape.\",\"[Role]\",\"[Executive]\",\"[catalog.schema.table1], [catalog.schema.table2]\",\"CTE1: SELECT DISTINCT on joined data to eliminate duplicates. CTE2: external_api_for_competitor_intelligence using ai_query with persona 'You are a Competitive Intelligence Director at Gartner...' - ASK: What competitor info would make recommendations more actionable? CTE3: Parse competitive JSON. CTE4: Prepare enriched context combining internal data with competitive intelligence. CTE5: Use ai_query for analysis with benchmark comparisons and competitive positioning.\"`\n",
    "\n",
    "  - **`Statistical Analysis` Pattern Example (WITH EXTERNAL DATA ENRICHMENT):**\n",
    "`\"4\",\"Analyze [METRIC1]-[METRIC2] Correlation with [RELEVANT_EXTERNAL_FACTOR]\",\"Problem\",\"Business lacks understanding of external factors affecting [RELATIONSHIP].\",\"Use Databricks Agent Bricks to compute correlations enriched with relevant external data to explain patterns.\",\"Optimizes decision-making by identifying external drivers that explain unexpected variations.\",\"[Role]\",\"[Executive]\",\"[catalog.schema.table1], [catalog.schema.table2]\",\"CTE1: SELECT DISTINCT with GROUP BY to deduplicate joined data. CTE2: external_api_for_<relevant_context> using ai_query with appropriate domain expert persona - ONLY if external factor has DIRECT, PROVABLE impact on the metrics. CTE3: Parse external data JSON. CTE4: Calculate CORR/REGR_SLOPE between metrics and external variables. CTE5: Use ai_query for strategy recommendations.\"`\n",
    "\n",
    "  **⚠️ EXTERNAL DATA RELEVANCY REMINDER**: External data enrichment should ONLY be included when there is a CLEAR, LOGICAL, INDUSTRY-RECOGNIZED cause-and-effect relationship between the external factor and the business metric. If you cannot explain WHY the external factor impacts the metric in one sentence, DO NOT include it.\n",
    "\n",
    "**REMINDER**: Replace all [PLACEHOLDERS] with actual values from YOUR business context and schema.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. FINAL, CRITICAL FORMATTING RULES\n",
    "1.  **HEADER**: `\"No\",\"Name\",\"type\",\"Analytics Technique\",\"Statement\",\"Solution\",\"Business Value\",\"Beneficiary\",\"Sponsor\",\"Tables Involved\",\"Technical Design\"`\n",
    "2.  **QUOTING**: ALL values in EVERY field MUST be enclosed in **double quotes** (`\"`).\n",
    "3.  **LANGUAGE**: English.\n",
    "4.  **FORMAT**: ONLY CSV. No markdown, no text before/after.\n",
    "\n",
    "\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 **CRITICAL - DO NOT CHANGE COLUMN NAMES** \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\n",
    "- The column names MUST be EXACTLY as specified above\n",
    "- Do NOT rename \"Statement\" to \"Opportunity\" or any other name\n",
    "- Do NOT add extra columns like \"honesty_score\" or \"honesty_justification\" to the header\n",
    "- The header row must be EXACTLY: `\"No\",\"Name\",\"type\",\"Analytics Technique\",\"Statement\",\"Solution\",\"Business Value\",\"Beneficiary\",\"Sponsor\",\"Tables Involved\",\"Technical Design\"`\n",
    "\n",
    "### 6. PREVIOUS RUN FEEDBACK (ENSEMBLE PASS)\n",
    "\n",
    "{previous_use_cases_feedback}\n",
    "\n",
    "### 7. FINAL INSTRUCTION\n",
    "Begin generation now. Output ONLY the CSV with the additional honesty columns at the end.\n",
    "\"\"\" + HONESTY_CHECK_CSV\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATES[\"AI_USE_CASE_GEN_PROMPT\"] = \"\"\"### 0. PERSONA ACTIVATION\n",
    "\n",
    "You are a highly experienced **Principal Enterprise Data Architect** and an industry specialist. Your primary task is to generate high-quality **AI-FOCUSED** business use cases.\n",
    "\n",
    "### BUSINESS CONTEXT\n",
    "**Business Context:** {business_context}\n",
    "**Strategic Goals:** {strategic_goals}\n",
    "**Business Priorities:** {business_priorities}\n",
    "**Strategic Initiative:** {strategic_initiative}\n",
    "**Value Chain:** {value_chain}\n",
    "**Revenue Model:** {revenue_model}\n",
    "\n",
    "---\n",
    "\n",
    "### \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: AI FUNCTIONS AS PRIMARY FOCUS \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\n",
    "\n",
    "**YOUR MISSION**: Generate use cases where **AI FUNCTIONS ARE THE STAR**.\n",
    "- **PRIMARY**: ai_forecast, ai_classify, ai_extract, ai_query, ai_parse_document (if applicable), ai_similarity.\n",
    "- **SUPPORTING**: Stats functions for context.\n",
    "- **MANDATORY**: Pair ai_forecast/ai_classify with ai_query for recommendations.\n",
    "- **NEW CAPABILITIES**: Integrate **What-If Analysis**, **Simulation**, and **Geospatial Analysis** where AI can enhance them (e.g., \"Simulate AI-driven demand scenarios\", \"Geospatial sentiment mapping\").\n",
    "\n",
    "\"\"\" + PROMPT_TEMPLATES[\"BASE_USE_CASE_GEN_PROMPT\"].split(\"### 1. CORE TASK\")[1].replace(\n",
    "    \"### 2. USE CASE GENERATION RULES\",\n",
    "    \"\"\"### 2. AI-FOCUSED USE CASE GENERATION RULES\n",
    "\n",
    "**\uD83D\uDD25 AI FUNCTION PRIORITY \uD83D\uDD25**:\n",
    "1. **MANDATORY**: At least ONE AI function per use case.\n",
    "2. **DISTRIBUTION**:\n",
    "   - 40-50% Predictive (Forecast + Query)\n",
    "   - 20-30% Classification (Classify + Query)\n",
    "   - 15-20% Generative (Query)\n",
    "   - 5-10% Advanced (Similarity, Simulation, Geospatial)\n",
    "\n",
    "**\uD83D\uDD25 ADVANCED AI USE CASES (INTEGRATE THESE) \uD83D\uDD25**:\n",
    "- **AI-Driven What-If/Simulation**: Use AI to predict outcomes under different simulated scenarios.\n",
    "- **Geospatial AI**: Combine location data with AI (e.g., \"Predict demand by H3 hexagon\").\n",
    "- **Technical Design Hint**: In the \"Technical Design\" column, explicitly mention if a simulation or geospatial approach is used along with the AI function.\n",
    "\"\"\"\n",
    ") + HONESTY_CHECK_CSV\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATES[\"STATS_USE_CASE_GEN_PROMPT\"] = \"\"\"### 0. PERSONA ACTIVATION\n",
    "\n",
    "You are a highly experienced **Principal Enterprise Data Architect** and **Fraud/Risk/Simulation Analytics Expert**. Your primary task is to generate **STATISTICS-FOCUSED** business use cases, with a **HEAVY EMPHASIS ON ANOMALY DETECTION, SIMULATION, AND ADVANCED ANALYTICS**.\n",
    "\n",
    "### BUSINESS CONTEXT\n",
    "**Business Context:** {business_context}\n",
    "**Strategic Goals:** {strategic_goals}\n",
    "**Business Priorities:** {business_priorities}\n",
    "**Strategic Initiative:** {strategic_initiative}\n",
    "**Value Chain:** {value_chain}\n",
    "**Revenue Model:** {revenue_model}\n",
    "\n",
    "---\n",
    "\n",
    "### \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: ANOMALY DETECTION, SIMULATION & ADVANCED STATS \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\n",
    "\n",
    "**YOUR MISSION**: Generate use cases where **STATISTICAL FUNCTIONS UNCOVER HIDDEN RISKS, SIMULATE FUTURES, AND MAP PATTERNS**.\n",
    "- **PRIMARY FOCUS**: **ANOMALY DETECTION** (Transactional data), **SIMULATION** (What-If, Monte Carlo), **GEOSPATIAL**, and **MARKET BASKET**.\n",
    "- **AGGRESSIVENESS**: You must be **AGGRESSIVE** in finding anomalies and simulating risks.\n",
    "- **CORE FUNCTIONS**: Use ALL applicable functions from the STATISTICAL_FUNCTIONS registry (see Section 3 below).\n",
    "- **MANDATORY**: End with `ai_query` for detailed investigation (Root Cause, Action Plan).\n",
    "- **PROHIBITED**: Do NOT use ai_forecast here (use Regression/Trend stats instead).\n",
    "\n",
    "\"\"\" + PROMPT_TEMPLATES[\"BASE_USE_CASE_GEN_PROMPT\"].split(\"### 1. CORE TASK\")[1].replace(\n",
    "    \"### 2. USE CASE GENERATION RULES\",\n",
    "    \"\"\"### 2. STATISTICS, SIMULATION & ADVANCED ANALYTICS USE CASE RULES\n",
    "\n",
    "**\uD83D\uDD25 EXPANDED ANALYTICS SCOPE (MIX THESE APPROACHES) \uD83D\uDD25**:\n",
    "\n",
    "A. **SIMULATION & WHAT-IF ANALYSIS (HIGH PRIORITY)**:\n",
    "   - **What-If Analysis**: Test outcomes with simulated inputs (e.g., \"What if fuel cost rises 10%?\").\n",
    "   - **Scenario Modeling**: Compare multiple hypothetical situations simultaneously (Optimistic, Neutral, Pessimistic).\n",
    "   - **Monte Carlo Simulation**: Model risk by generating realistic synthetic data distributions using AI.\n",
    "   - **Sensitivity Analysis**: Measure how sensitive an output is to changes in specific inputs.\n",
    "\n",
    "B. **GEOSPATIAL & MARKET ANALYSIS**:\n",
    "   - **Geospatial Analysis**: Map data to physical locations using H3 or coordinates.\n",
    "   - **Market Basket Analysis**: Find products frequently bought together (Association Rules).\n",
    "\n",
    "C. **ANOMALY DETECTION (CORE)**:\n",
    "   - **TARGET**: Transactional tables (Orders, Logs, Clicks, Payments).\n",
    "   - **APPROACH**: Use functions from Distribution Shape, Outlier Detection, and Percentiles categories (see STATISTICAL_FUNCTIONS in Section 3).\n",
    "   - **OUTPUT**: Root Cause, Explanation, Recommendation via ai_query.\n",
    "\n",
    "D. **FREQUENCY & DISTRIBUTION**:\n",
    "   - Use Ranking functions (NTILE, DENSE_RANK) and Central Tendency functions (MODE) from STATISTICAL_FUNCTIONS.\n",
    "\n",
    "**\uD83D\uDD25 USE ALL APPLICABLE STATISTICAL FUNCTIONS \uD83D\uDD25**:\n",
    "Refer to the **AVAILABLE STATISTICAL FUNCTIONS** section below. You MUST use functions from ALL relevant categories:\n",
    "- Central Tendency, Dispersion, Distribution Shape, Percentiles\n",
    "- Trend Analysis, Correlation, Volatility, Outlier Detection\n",
    "- Ranking, Time Series\n",
    "\n",
    "**GUIDANCE**:\n",
    "- **Business Terms**: Make the use case Name reflect the approach (e.g., \"Simulate Impact of Pricing Changes\", \"Geospatial Hotspot Analysis\").\n",
    "- **Technical Design Column**: Reference specific statistical functions from the registry (e.g., \"Use SKEWNESS, KURTOSIS for anomaly detection, then ai_query for root cause analysis\").\n",
    "\"\"\"\n",
    ") + HONESTY_CHECK_CSV\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATES[\"UNSTRUCTURED_DATA_USE_CASE_GEN_PROMPT\"] = \"\"\"### 0. PERSONA ACTIVATION\n",
    "\n",
    "You are a highly experienced **Principal Enterprise Data Architect**. Your task is to generate business use cases for **UNSTRUCTURED DATA** (Documents).\n",
    "\n",
    "### BUSINESS CONTEXT\n",
    "**Business Context:** {business_context}\n",
    "**Strategic Goals:** {strategic_goals}\n",
    "**Business Priorities:** {business_priorities}\n",
    "**Strategic Initiative:** {strategic_initiative}\n",
    "**Value Chain:** {value_chain}\n",
    "**Revenue Model:** {revenue_model}\n",
    "\n",
    "---\n",
    "\n",
    "### \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: UNSTRUCTURED DATA FOCUS \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\n",
    "\n",
    "**YOUR MISSION**: Generate use cases that leverage `ai_parse_document` and `ai_extract` on the provided document files.\n",
    "\n",
    "**REQUIREMENTS**:\n",
    "1. **Source**: MUST use the Unity Catalog volume paths provided.\n",
    "2. **Function**: MUST use `ai_parse_document`.\n",
    "3. **Extraction**: Use `ai_extract` to get structured entities.\n",
    "4. **Action**: Use `ai_query` or `ai_classify` on the extracted data.\n",
    "\n",
    "\"\"\" + PROMPT_TEMPLATES[\"BASE_USE_CASE_GEN_PROMPT\"].split(\"### 1. CORE TASK\")[1].replace(\n",
    "    \"### 2. USE CASE GENERATION RULES\",\n",
    "    \"\"\"### 2. UNSTRUCTURED DATA USE CASE RULES\n",
    "\n",
    "**\uD83D\uDD25 DOCUMENT PROCESSING PRIORITY \uD83D\uDD25**:\n",
    "- **MANDATORY**: Use `ai_parse_document` for every use case here.\n",
    "- **Volume Paths**: Use strict Volume paths (not tables) for input.\n",
    "- **Entities**: Extract the specific entities listed in the document metadata.\n",
    "\"\"\"\n",
    ").replace(\n",
    "    \"##### 1. Structured Data Schema\",\n",
    "    \"\"\"##### 1. Unstructured Data Documents\n",
    "{unstructured_documents_markdown}\n",
    "\n",
    "##### 2. Structured Data Schema (Reference Only)\"\"\"\n",
    ") + HONESTY_CHECK_CSV\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATES[\"UNSTRUCTURED_DATA_DOCUMENTS_PROMPT\"] = \"\"\"You are a senior business analyst and data architect. Your task is to generate a comprehensive list of the TOP 20 most common unstructured documents that businesses in the specified industries would possess and store in a data lake or volume.\n",
    "\n",
    "**INDUSTRIES PROVIDED**:\n",
    "{industries_list}\n",
    "\n",
    "**CRITICAL REQUIREMENT**: You MUST generate EXACTLY 20 different document types across ALL industries provided. Ensure diversity across document formats.\n",
    "\n",
    "For each document, provide:\n",
    "1. **Document Name** - A clear, descriptive name for the document type\n",
    "2. **Description** - What the document contains and its business purpose\n",
    "3. **File Path** - A realistic Databricks Volumes path (format: `/Volumes/catalog/schema/volume_name/document_type/`)\n",
    "4. **Extracted Entities** - A comma-separated list of 4-8 key data fields that could be extracted from this document using ai_extract\n",
    "\n",
    "**Document Categories to Cover** (distribute the 20 documents across these categories):\n",
    "- **Financial Documents**: Invoices, receipts, statements, purchase orders (PDF, JPG)\n",
    "- **Customer Documents**: Feedback forms, surveys, support tickets, reviews (PDF, DOCX, JPG)\n",
    "- **Operational Documents**: Work orders, inspection reports, maintenance logs (PDF, PPTX, JPG)\n",
    "- **Legal/Compliance**: Regulatory filings, certificates, permits (PDF, DOCX)\n",
    "- **HR Documents**: Resumes, contracts, performance reviews (PDF, DOCX)\n",
    "- **Marketing/Media**: Campaign materials, testimonials, brochures (PPTX, PDF, JPG)\n",
    "- **Multimedia**: Training videos, customer calls, product demos (MP4 for videos, MP3/WAV for audio - represented as file paths)\n",
    "- **Spreadsheets**: Financial models, inventory sheets, reports (XLS, XLSX)\n",
    "\n",
    "**Important**:\n",
    "- Generate EXACTLY 20 documents total\n",
    "- Include diversity: ~8-10 PDFs, ~3-4 images (JPG/PNG), ~2-3 presentations (PPTX), ~2-3 documents (DOCX), ~1-2 spreadsheets (XLS/XLSX), ~1-2 videos, ~1-2 audio files\n",
    "- Video files should use format \"Video (MP4)\" and audio files \"Audio (MP3)\" or \"Audio (WAV)\"\n",
    "- Make File Paths realistic with proper catalog/schema/volume structure\n",
    "- Extracted Entities should be specific and useful for each document type\n",
    "- Cover ALL industries provided in the industries list\n",
    "\n",
    "**Output Format** (Markdown table):\n",
    "| Document Name | Description | Type | Extracted Entities | File Path |\n",
    "|---|---|---|---|---|\n",
    "| Vendor Invoices | PDF/image invoices from vendors containing itemized charges | PDF | vendor name, invoice number, date, total amount, items purchased, payment terms, tax amount, due date | /Volumes/finance/accounting/invoices/ |\n",
    "| Customer Training Videos | MP4 video recordings of customer onboarding and product training sessions | Video (MP4) | training_topic, duration, instructor, participant_count | /Volumes/training/videos/customer_onboarding/ |\n",
    "| Support Call Recordings | Audio recordings of customer support interactions for quality assurance | Audio (MP3) | call_id, customer_id, agent_id, call_duration, issue_category, resolution_status | /Volumes/support/audio/call_recordings/ |\n",
    "| Financial Planning Spreadsheets | Excel workbooks containing budget forecasts and financial models | XLS | budget_category, fiscal_year, projected_revenue, actual_spend, variance | /Volumes/finance/planning/budget_models/ |\n",
    "\n",
    "Your output **MUST** be a single markdown table with EXACTLY 20 document types.\n",
    "Ensure you include documents in these formats: PDF, JPG, PNG, DOCX, PPTX, XLS/XLSX, Video (MP4), Audio (MP3/WAV).\n",
    "Do not include *any* other text, preamble, or explanation.\n",
    "### Rules\n",
    "1.  Use the provided industries list to determine appropriate document types.\n",
    "2.  Generate EXACTLY 20 realistic unstructured documents, distributed across the industries.\n",
    "3.  For each document, provide a name, a **detailed description** of its content, a file type, a list of **key entities to extract**, and a directory path.\n",
    "4.  The file type **MUST** be one of: `PDF`, `JPG`, `PNG`, `DOCX`, `PPTX`, `XLS`, `XLSX`, `Video (MP4)`, `Audio (MP3)`, `Audio (WAV)`.\n",
    "5.  The `\"File Path\"` column **MUST** be a plausible Databricks Volume directory path where these files would be stored (e.g., `/Volumes/finance/invoices/unprocessed/`). It must end with a trailing slash.\n",
    "6. All table headers **MUST** be enclosed in double quotes.\n",
    "---\n",
    "### Example (for a \"sales\" schema)\n",
    "| \"Document Name\" | \"Description\" | \"Type\" | \"Extracted Entities\" | \"File Path\" |\n",
    "|---|---|---|---|---|\n",
    "| \"Customer Invoices\" | \"Scanned PDF copies of vendor invoices. Contains line items, PO number, vendor name, invoice date, and total amount due.\" | \"PDF\" | \"invoice_number, vendor_name, total_amount, due_date, line_items\" | \"/Volumes/finance/invoices/unprocessed/\" |\n",
    "| \"Product Spec Sheets\" | \"Multi-page technical datasheets for products, including specifications, performance metrics, and compliance information.\" | \"PDF\" | \"product_sku, technical_specs, compliance_standards\" | \"/Volumes/products/specifications/\" |\n",
    "| \"Marketing Brochures\" | \"Quarterly slide decks for product promotions, outlining key features, target audience, and pricing tiers.\" | \"PPTX\" | \"product_name, key_features, pricing\" | \"/Volumes/marketing/assets/brochures/\" |\n",
    "| \"Signed Contracts\" | \"Scanned copies of signed master service agreements (MSAs) with customers, detailing terms, conditions, and service levels.\" | \"PDF\" | \"customer_name, effective_date, contract_term, sla_details\" | \"/Volumes/legal/contracts/signed/\" |\n",
    "| \"Support Call Transcripts\" | \"Word documents containing full-text transcripts from customer support calls, auto-generated from an audio-to-text service.\" | \"DOCX\" | \"customer_id, support_agent, issue_type, resolution_steps, sentiment\" | \"/Volumes/support/transcripts/audio_to_text/\" |\n",
    "| \"Damaged Product Photos\" | \"Customer-submitted JPEG images showing defective or damaged products for warranty claims and RMA processing.\" | \"JPG\" | \"damage_type, product_area, serial_number (if visible)\" | \"/Volumes/claims/images/damaged_products/\" |\n",
    "---\n",
    "Begin generation now. Your response must start directly with the markdown table header.\n",
    "| \"Document Name\" | \"Description\" | \"Type\" | \"Extracted Entities\" | \"File Path\" |\n",
    "|---|---|---|---|---|\n",
    "\n",
    "\uD83D\uDEA8 ABSOLUTE RULE - OUTPUT FORMAT \uD83D\uDEA8:\n",
    "\n",
    "❌ DO NOT INCLUDE:\n",
    "- Any conversational or explanatory text (\"Here are...\", \"I've generated...\", \"Based on...\")\n",
    "- Any thoughts or analysis descriptions\n",
    "\n",
    "✅ OUTPUT REQUIREMENTS:\n",
    "- Your response must START with: | \"Document Name\" | ... | \"honesty_score\" | \"honesty_justification\" |\n",
    "- Include honesty_score and honesty_justification as the last 2 columns in header and all rows\n",
    "\"\"\" + HONESTY_CHECK_TABLE\n",
    "\n",
    "# --- 1c. PDF Summary Prompt (MODIFIED FOR REQUEST #1) ---\n",
    "PROMPT_TEMPLATES[\"SUMMARY_GEN_PROMPT\"] = \"\"\"\n",
    "You are a highly experienced Databricks Principal Strategist and business analyst.\n",
    "Your task is to generate a **CSV** response containing a strategic summary for a customer.\n",
    "The customer is: {business_name}\n",
    "We have identified {total_cases} use cases across these domains: {domain_list}\n",
    "Your output MUST be in **{output_language}**.\n",
    "\n",
    "### TASK\n",
    "Generate a **single CSV** with THREE columns: \"Type\", \"Summary\", \"TransliteratedBusinessName\"\n",
    "Your response MUST start with the header: `\"Type\",\"Summary\",\"TransliteratedBusinessName\"`\n",
    "The CSV must contain:\n",
    "1.s: One row for the \"Executive\" summary.\n",
    "2.s: One row for **EACH** business domain listed in {domain_list}.\n",
    "\n",
    "### COLUMN DEFINITIONS\n",
    "1.s: **\"Type\"**:\n",
    "    * Use the exact string \"Executive\" for the first row.\n",
    "    * For all other rows, use the exact Business Domain name from the list.\n",
    "2.s: **\"Summary\"**:\n",
    "    * **For the \"Executive\" row**: Write a 2-3 paragraph, professional executive summary. Start with a powerful opening. Emphasize the value of AI and Databricks Agent Bricks in unlocking the potential of their data.\n",
    "    * **For each \"Business Domain\" row**: Write a 2-3 paragraph professional and engaging summary in prose. This summary should narrate the domain's strategic importance, its core responsibilities, and the key opportunities AI (specifically Databricks Agent Bricks) can unlock. The tone should be identical to the Executive Summary.\n",
    "    * **FORMATTING**: ALL summaries (both Executive and Domain) **MUST** be enclosed in `<p>` HTML tags (e.g., `<p>Paragraph 1.</p><p>Paragraph 2.</p>`).\n",
    "3.s: **\"TransliteratedBusinessName\"**:\n",
    "    * **For the \"Executive\" row**: Provide the transliteration of {business_name} into {output_language}. If {output_language} is English, just repeat {business_name}.\n",
    "    * **For all \"Business Domain\" rows**: This field MUST be an empty string `\"\"`.\n",
    "\n",
    "### CRITICAL CSV FORMATTING RULES\n",
    "* **CSV Format**: The entire output MUST be a valid CSV. All values MUST be enclosed in **double quotes** (`\"`).\n",
    "* **NO CODE FENCES**: Do NOT include markdown code fences like ```csv or ``` at the beginning or end.\n",
    "* **ONLY CSV**: Your response must contain ONLY the CSV data, starting with the header line.\n",
    "* **Proper Escaping**: If a field value contains double quotes, escape them by doubling them (\"\").\n",
    "* **No Extra Text**: Do NOT include any explanatory text, comments, or anything other than the CSV data.\n",
    "\n",
    "\n",
    "### EXAMPLE CSV OUTPUT (for {output_language}=Arabic and {business_name}=Global Enterprises)\n",
    "\"Type\",\"Summary\",\"TransliteratedBusinessName\"\n",
    "\"Executive\",\"<p>تقف طيران الإمارات في طليعة التميز في مجال الطيران...</p><p>يحدد هذا الكتالوج مسارًا واضحًا للاستفادة من البيانات والذكاء الصناعي الخاص بداتا بريكس موزايك...</p>\",\"طيران الإمارات\"\n",
    "\"Customer Management\",\"<p>يعد مجال إدارة العملاء أمرًا محوريًا لنجاح طيران الإمارات...</p><p>من خلال الاستفادة من الذكاء الصناعي الخاص بداتا بريكس موزايك، يمكن لطيران الإمارات تحويل هذا المجال...</p>\",\"\"\n",
    "\"Finance & Billing\",\"<p>إدارة الصحة المالية للمؤسسة، يعد مجال المالية والفوترة أمرًا بالغ الأهمية...</p><p>يمثل الذكاء الصناعي الخاص بداتا بريكس موزايك فرصة كبيرة لأتمتة هذه العمليات...</p>\",\"\"\n",
    "\n",
    "### EXAMPLE CSV OUTPUT (for {output_language}=English and {business_name}=Global Enterprises)\n",
    "\"Type\",\"Summary\",\"TransliteratedBusinessName\"\n",
    "\"Executive\",\"<p>Global Enterprises stands at the forefront of its industry, and with {total_cases} identified use cases, the organization is poised to revolutionize its operations through the strategic implementation of Databricks Agent Bricks.</p><p>This catalog outlines a clear path to leveraging data and AI to drive innovation, enhance efficiency, and create significant business value across all key domains.</p>\",\"Global Enterprises\"\n",
    "\"Customer Management\",\"<p>The Customer Management domain is central to Global Enterprises's success, as it governs all direct interactions and the entire customer lifecycle. Its primary responsibility is to ensure high rates of acquisition, satisfaction, and retention.</p><p>By leveraging Databricks Agent Bricks, Global Enterprises can transform this domain, moving from reactive support to proactive engagement. Opportunities include developing sophisticated churn prediction models and deploying generative AI agents to provide instant, personalized customer service, dramatically improving loyalty and reducing operational costs.</p>\",\"\"\n",
    "\"Finance & Billing\",\"<p>Managing the financial health of the organization, the Finance & Billing domain is critical for ensuring revenue integrity, compliance, and accurate forecasting. This domain oversees everything from invoicing and payments to financial reporting and risk analysis.</p><p>Databricks Agent Bricks presents a significant opportunity to automate and intelligentize these processes. For instance, AI can be used to parse unstructured invoices, detect payment anomalies in real-time, and generate highly accurate revenue forecasts, thereby strengthening the organization's financial posture and decision-making capabilities.</p>\",\"\"\n",
    "\n",
    "### FINAL INSTRUCTION\n",
    "Begin generation now. Produce ONLY the CSV text, starting with the header `\"Type\",\"Summary\",\"TransliteratedBusinessName\"`.\n",
    "\n",
    "\uD83D\uDEA8 ABSOLUTE RULE - OUTPUT FORMAT \uD83D\uDEA8:\n",
    "\n",
    "❌ DO NOT INCLUDE:\n",
    "- Any conversational or explanatory text (\"Here is...\", \"I've generated...\", \"The...\")\n",
    "- Any thoughts or analysis descriptions\n",
    "\n",
    "✅ OUTPUT REQUIREMENTS:\n",
    "- Your response must START with: \"Type\",\"Summary\",\"TransliteratedBusinessName\",\"honesty_score\",\"honesty_justification\"\n",
    "- Include honesty columns in header and all rows\n",
    "\"\"\" + HONESTY_CHECK_CSV\n",
    "\n",
    "# --- 1d. Domain Consolidation Prompt (ENHANCED FOR 6-90 USE CASES REQUIREMENT) ---\n",
    "PROMPT_TEMPLATES[\"DOMAIN_FINDER_PROMPT\"] = \"\"\"\n",
    "You are an expert business analyst specializing in BALANCED domain taxonomy design with deep industry knowledge.\n",
    "\n",
    "**\uD83C\uDFAF YOUR TASK**: Analyze the provided use cases and assign each one to appropriate Business Domains (NO subdomains yet).\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL REQUIREMENTS - YOUR RESPONSE WILL BE REJECTED IF NOT FOLLOWED \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**:\n",
    "\n",
    "**\uD83D\uDEA8 ANTI-CONSOLIDATION RULE - DO NOT PUT EVERYTHING IN ONE DOMAIN \uD83D\uDEA8**:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "► **CRITICAL**: You MUST create MULTIPLE domains - DO NOT consolidate everything into 1-5 domains\n",
    "► **CALCULATION**: Target domains = total_use_cases ÷ 10 (e.g., 70 use cases = ~7 domains, 200 use cases = ~20 domains)\n",
    "► **\uD83D\uDEA8 ABSOLUTE HARD LIMIT \uD83D\uDEA8**: MAXIMUM 25 domains - NEVER EXCEED THIS LIMIT (will cause REJECTION)\n",
    "► **DOMAIN COUNT**: You MUST create between 3-25 domains (MINIMUM 3, MAXIMUM 25 - HARD LIMIT)\n",
    "► **GUIDELINE PER DOMAIN**: Aim for 6-80 use cases per domain (this is a guideline, not a strict requirement for small datasets)\n",
    "► **FLEXIBILITY**: If total use cases is small (e.g., 20-30), it's acceptable to have domains with fewer than 4 use cases\n",
    "► **REJECTION CRITERIA**: \n",
    "   - **HARD REJECTION**: If you create MORE than 25 domains → REJECTED (this is ABSOLUTE)\n",
    "   - **HARD REJECTION**: If you put all use cases into 1-2 domains → REJECTED\n",
    "   - **SOFT WARNING**: Domains with <6 use cases are acceptable if total use cases is low\n",
    "► **BALANCE REQUIREMENT**: Distribute use cases EVENLY across multiple domains\n",
    "► **DIVERSITY REQUIREMENT**: Create DIVERSE domain names that reflect DIFFERENT business areas\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "**REQUIREMENT #1 - EXACTLY ONE SIMPLE WORD FOR ALL DOMAIN NAMES (MANDATORY)**:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "► **ABSOLUTE RULE**: ALL domain names MUST be EXACTLY ONE SIMPLE WORD - NO EXCEPTIONS\n",
    "► **REJECTION CRITERIA**: If ANY domain name violates this rule, your ENTIRE response is REJECTED\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 ANTI-TRICK DETECTION - DO NOT TRY TO BYPASS THIS RULE \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**:\n",
    "► **NO CAMELCASE CONCATENATION**: Do NOT concatenate words using CamelCase to bypass the one-word rule\n",
    "   - ❌ WRONG: \"ProviderOperations\" (this is \"Provider\" + \"Operations\" concatenated - REJECTED)\n",
    "   - ❌ WRONG: \"CareTransitions\" (this is \"Care\" + \"Transitions\" concatenated - REJECTED)\n",
    "   - ❌ WRONG: \"NetworkManagement\" (this is \"Network\" + \"Management\" concatenated - REJECTED)\n",
    "   - ❌ WRONG: \"CustomerService\" (this is \"Customer\" + \"Service\" concatenated - REJECTED)\n",
    "   - ❌ WRONG: \"FlightOperations\" (this is \"Flight\" + \"Operations\" concatenated - REJECTED)\n",
    "   - ✅ CORRECT: Use the FIRST/PRIMARY word only: \"Provider\", \"Care\", \"Network\", \"Customer\", \"Flight\"\n",
    "\n",
    "► **DETECTION METHOD**: If a domain name contains CAPITAL LETTERS in the middle of the word, it is a CamelCase trick → REJECTED\n",
    "► **SIMPLE WORD DEFINITION**: A simple word has ONLY ONE capital letter (at the start) and NO capitals in the middle\n",
    "► **WORD COUNT METHOD**: Count spaces in domain names - ZERO spaces allowed\n",
    "► **ZERO CAPITAL LETTERS IN MIDDLE**: Domain names must have capital ONLY at the first character\n",
    "\n",
    "► ✅ CORRECT: \"Network\", \"Passengers\", \"Revenue\", \"Risk\", \"Maintenance\", \"Crew\", \"Finance\", \"Provider\", \"Care\"\n",
    "► ❌ WRONG (WILL CAUSE REJECTION): \n",
    "   - \"Network Management\" (has space - REJECTED)\n",
    "   - \"NetworkManagement\" (CamelCase concatenation - REJECTED)\n",
    "   - \"ProviderOperations\" (CamelCase concatenation - REJECTED)\n",
    "   - \"CareTransitions\" (CamelCase concatenation - REJECTED)\n",
    "   - \"CustomerService\" (CamelCase concatenation - REJECTED)\n",
    "\n",
    "► **RULE**: Remove all adjectives, verbs, descriptors, AND compound words - keep ONLY the core business noun\n",
    "► **SPLIT STRATEGY**: If you think of \"ProviderOperations\", split it and use ONLY \"Provider\"\n",
    "► **SPLIT STRATEGY**: If you think of \"CareTransitions\", split it and use ONLY \"Care\"\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "**OTHER DOMAIN REQUIREMENTS**:\n",
    "2. **\uD83D\uDEA8 DOMAIN COUNT (HARD LIMIT) \uD83D\uDEA8**: Domains MUST be between 3-25 (MINIMUM 3, **MAXIMUM 25 - ABSOLUTE HARD LIMIT**)\n",
    "3. **USE CASES PER DOMAIN (GUIDELINE)**: Aim for 6-80 use cases per domain. For small datasets (<50 total), domains with <6 use cases are acceptable.\n",
    "4. **TARGET DOMAINS**: Create MULTIPLE domains (calculate: total_use_cases ÷ 10 = target domains, but **NEVER EXCEED 25**)\n",
    "5. **ABSOLUTE RULE**: NO TWO DOMAINS CAN SHARE THE SAME CORE BUSINESS NAME\n",
    "6. **INDUSTRY ALIGNMENT REQUIRED**: Domain names MUST be aligned with the specific business and industry context provided below\n",
    "\n",
    "**BUSINESS CONTEXT**:\n",
    "Business Name: {business_name}\n",
    "Industries: {industries}\n",
    "Business Context: {business_context}\n",
    "\n",
    "**INPUT DATA**:\n",
    "You will receive a CSV of use cases with these columns:\n",
    "\"No\",\"Name\",\"type\",\"Analytics Technique\",\"Statement\",\"Solution\",\"Business Value\",\"Beneficiary\",\"Sponsor\",\"Tables Involved\"\n",
    "\n",
    "**YOUR DOMAIN DETECTION TASK**:\n",
    "1. Analyze the use case names, statements, solutions, analytics techniques, and tables involved\n",
    "2. Identify natural business groupings (domains) based on:\n",
    "   - Business functionality and purpose\n",
    "   - Tables and data domains involved\n",
    "   - Business beneficiaries and stakeholders\n",
    "   - Business value and outcomes\n",
    "3. Create appropriate domain names (EXACTLY 1 WORD, industry-specific)\n",
    "4. Ensure ALL validation rules are met (domain count, use cases per domain, word counts, etc.)\n",
    "\n",
    "**CLUSTERING STRATEGY**:\n",
    "- **GROUP BY BUSINESS FUNCTION**: Use cases that serve the same business function should be in the same domain\n",
    "- **USE TABLE ANALYSIS**: Use cases using similar tables should generally be in the same domain\n",
    "- **CONSIDER BENEFICIARIES**: Use cases serving the same business roles/departments often belong together\n",
    "- **BALANCE**: Aim for 6-20 use cases per domain (maximum 80 use cases per domain)\n",
    "- **DIVERSITY**: Create MULTIPLE distinct domains - DO NOT put everything into 1-5 mega-domains\n",
    "\n",
    "**DOMAIN RULES** (NOT AGGRESSIVE - MAINTAIN DIVERSITY):\n",
    "\n",
    "0. **\uD83D\uDEA8 BALANCE FIRST - DO NOT OVER-MERGE \uD83D\uDEA8**:\n",
    "   - **GOAL**: Create MULTIPLE DISTINCT domains, aim for 6-80 use cases per domain\n",
    "   - **\uD83D\uDEA8 ABSOLUTE HARD LIMIT \uD83D\uDEA8**: MAXIMUM 25 domains - NEVER EXCEED THIS (will cause REJECTION)\n",
    "   - **DOMAIN COUNT**: You MUST create between 3-25 domains (MINIMUM 3, **MAXIMUM 25 - HARD LIMIT**)\n",
    "   - **FLEXIBILITY**: For small datasets (<50 total use cases), domains with <6 use cases are acceptable\n",
    "   - **STRATEGY**: Only merge domains when they truly overlap - maintain business diversity\n",
    "   - **ANTI-PATTERN**: DO NOT merge everything into 1-5 mega-domains\n",
    "   - **CALCULATE**: Total use cases ÷ 10 = target number of domains (but **NEVER EXCEED 25**)\n",
    "   - **EXAMPLES**: \n",
    "     * 24 use cases ÷ 10 = ~2-3 domains, but MINIMUM is 3, so create 3-8 domains (small dataset, <6 per domain OK)\n",
    "     * 70 use cases ÷ 10 = ~7 domains (create 7-10 domains with 7-10 use cases each)\n",
    "     * 200 use cases ÷ 10 = ~20 domains (create 20-25 domains with 8-10 use cases each)\n",
    "     * 1700 use cases ÷ 10 = ~170 domains, but **MAX is 25 domains**, so create 25 domains with 68-80 use cases each\n",
    "   - **MAXIMUM DOMAINS**: Cap at 25 domains maximum. If calculation exceeds 25, distribute use cases evenly across 25 domains\n",
    "\n",
    "1. **\uD83D\uDEA8 CRITICAL: NO OVERLAPPING WORDS IN DOMAIN NAMES - MANDATORY MERGING \uD83D\uDEA8**:\n",
    "   - **ABSOLUTE RULE**: NO two domains can share the same word\n",
    "   - **LEAN TO SHORTEST NAME**: When merging domains with overlapping words, ALWAYS use the name with the LOWEST number of words\n",
    "   - **DETECTION**: Check EVERY domain name against all other domains. If ANY word appears in multiple domains, MERGE them immediately\n",
    "   - **EXAMPLES OF VIOLATIONS (MUST FIX)**:\n",
    "     * \"Network Management\" + \"Network Planning\" → Both share \"Network\" → MERGE into: \"Network\" (1 word - shortest)\n",
    "     * \"Customer Service\" + \"Customer Management\" + \"Customer Engagement\" → All share \"Customer\" → MERGE into: \"Customer\" or \"Passengers\" (industry-specific, 1 word)\n",
    "     * \"Sales Operations\" + \"Sales Analytics\" → Both share \"Sales\" → MERGE into: \"Sales\" (1 word - shortest)\n",
    "     * \"Risk Management\" + \"Risk Analysis\" → Both share \"Risk\" → MERGE into: \"Risk\" (1 word - shortest)\n",
    "   - **YOUR RESPONSE WILL BE REJECTED if any domains have overlapping words**\n",
    "\n",
    "2. **INDUSTRY-SPECIFIC NAMING (HIGHEST PRIORITY)**: Use domain names that reflect the ACTUAL business and industry from the data\n",
    "   - **INFER INDUSTRY FROM DATA**: Analyze the use case names, table names, and business context to determine the actual industry\n",
    "   - **USE DOMAIN-SPECIFIC TERMS**: Choose domain names that reflect the specific business operations in the data\n",
    "   - **EXAMPLES BY INDUSTRY TYPE** (use these as patterns, NOT defaults - always infer from actual data):\n",
    "     * Transportation: \"Fleet\", \"Routes\", \"Schedules\", \"Cargo\", \"Safety\", \"Maintenance\"\n",
    "     * Finance: \"Risk\", \"Credit\", \"Trading\", \"Compliance\", \"Wealth\", \"Fraud\"\n",
    "     * Healthcare: \"Patients\", \"Clinical\", \"Pharmacy\", \"Billing\", \"Records\"\n",
    "     * Retail: \"Inventory\", \"Sales\", \"Customers\", \"Pricing\", \"Suppliers\"\n",
    "     * Manufacturing: \"Production\", \"Quality\", \"Supply\", \"Maintenance\", \"Shipping\"\n",
    "     * Technology: \"Platform\", \"Users\", \"Security\", \"Performance\", \"Analytics\"\n",
    "     * Telecom: \"Network\", \"Subscribers\", \"Billing\", \"Coverage\", \"Support\"\n",
    "   - **CREATE DIVERSE DOMAINS** - use multiple business-specific terms to maintain variety\n",
    "   - **CRITICAL**: ALL domain names MUST be EXACTLY ONE WORD\n",
    "   - **CRITICAL**: Do NOT assume any industry - ALWAYS infer from the actual data provided\n",
    "\n",
    "3. **STRICTLY ONE-WORD NAMES MANDATORY**: ALL domain names MUST be EXACTLY ONE WORD\n",
    "   - \"Catering Operations\" + \"Flight Catering Operations\" + \"Catering Services\" → \"Catering\" (1 word)\n",
    "   - \"Maintenance Operations\" + \"Aircraft Maintenance\" → \"Maintenance\" (1 word)\n",
    "   - \"Network Planning\" + \"Network Operations\" + \"Network Optimization\" → \"Network\" (1 word)\n",
    "   - \"Customer Service\" + \"Customer Management\" → \"Passengers\" (1 word, industry-specific)\n",
    "   - **NO TWO-WORD DOMAIN NAMES ALLOWED** - response will be REJECTED\n",
    "\n",
    "4. **CORE NAME UNIQUENESS** (OVERLAPPING WORDS): Identify ANY shared words and merge ONLY those domains\n",
    "   - \"Customer Service\", \"Customer Support\", \"Customer Engagement\", \"Customer Management\" → ALL share \"Customer\" → MERGE into \"Passengers\" (industry-specific, 1 word)\n",
    "   - \"Risk Management\", \"Risk Analysis\", \"Risk Operations\" → ALL share \"Risk\" → MERGE into \"Risk\" (1 word)\n",
    "   - \"Sales Operations\", \"Sales Analytics\" → Both share \"Sales\" → MERGE into \"Sales\" (1 word)\n",
    "   - **BUT**: \"Risk\" and \"Compliance\" are DIFFERENT - do NOT merge them (maintain diversity)\n",
    "   - **BUT**: \"Revenue\" and \"Sales\" are DIFFERENT - do NOT merge them (maintain diversity)\n",
    "\n",
    "5. **SPLIT WHEN NEEDED**: Split domains that would have > 80 use cases to maintain balance\n",
    "   - **CRITICAL**: If a merged domain would exceed 80 use cases, you MUST split it into more specific domains\n",
    "   - **BALANCE**: Prefer creating MORE domains with 6-20 use cases rather than fewer domains with 80+ use cases\n",
    "   - Example: If \"Operations\" would have 180 use cases, split into \"Flight\" (80) and \"Ground\" (80) and \"Cargo\" (20)\n",
    "\n",
    "6. **BUSINESS-SPECIFIC NAMES**: Use names that reflect the actual business operations, NOT generic IT or data terms\n",
    "\n",
    "7. **DOMAIN COUNT RULE**: Target multiple domains (calculate: total÷10, cap at 25). Each domain needs 6-80 use cases\n",
    "\n",
    "**ANTI-PATTERNS TO AVOID**:\n",
    "❌ Putting all use cases into 1-3 domains (violates diversity requirement)\n",
    "❌ Creating domains with overlapping words (e.g., \"Customer\" and \"Customer Service\")\n",
    "❌ Using generic domain names like \"Operations\", \"Management\", \"Services\" when industry-specific terms exist\n",
    "\n",
    "**MERGE EXAMPLES WITH STRICTLY ONE-WORD DOMAIN NAMES** (adapt to YOUR actual industry from data):\n",
    "\n",
    "PATTERN: Merge domains that share a common concept into ONE WORD:\n",
    "- \"Customer Service\" + \"Customer Support\" + \"Customer Engagement\" → \"Customers\" (use industry-appropriate term) [ONE WORD - MANDATORY]\n",
    "- \"Operations Management\" + \"Operations Analytics\" + \"Operations Planning\" → \"Operations\" [ONE WORD - MANDATORY]\n",
    "- \"Risk Management\" + \"Risk Analysis\" + \"Risk Operations\" → \"Risk\" [ONE WORD - MANDATORY]\n",
    "- \"Sales Operations\" + \"Sales Analytics\" + \"Revenue Management\" → \"Sales\" or \"Revenue\" [ONE WORD - MANDATORY]\n",
    "- \"Maintenance Operations\" + \"Asset Maintenance\" + \"Preventive Maintenance\" → \"Maintenance\" [ONE WORD - MANDATORY]\n",
    "\n",
    "**CRITICAL: INFER FROM DATA** - The examples above are PATTERNS. You MUST:\n",
    "1. Analyze the actual use case names and business context provided\n",
    "2. Identify the REAL industry from the data (NOT assume any default)\n",
    "3. Use domain names that match the ACTUAL business terminology in the data\n",
    "\n",
    "**OUTPUT FORMAT: CSV (NOT JSON)**:\n",
    "Return a CSV with EXACTLY 2 columns (with header):\n",
    "  - Column 1: \"use_case_id\" - The \"No\" field from the input CSV\n",
    "  - Column 2: \"domain\" - The assigned domain name (EXACTLY 1 WORD)\n",
    "\n",
    "**\uD83D\uDEA8 VALIDATION CHECKLIST (AUTOMATED REJECTION IF FAILED) \uD83D\uDEA8**:\n",
    "☐ Domain count: 3-25 (MINIMUM 3, **MAXIMUM 25 - ABSOLUTE HARD LIMIT**)\n",
    "☐ Domain names: EXACTLY 1 word\n",
    "☐ Use cases per domain: Aim for 6-80 (flexible for small datasets <50 total use cases)\n",
    "☐ No overlapping words in domain names\n",
    "☐ All domain names are industry-specific (not generic)\n",
    "\n",
    "**CRITICAL**: The MAXIMUM 25 domains is an ABSOLUTE HARD LIMIT that will NEVER be waived.\n",
    "\n",
    "The output language for domain names should be {output_language}.\n",
    "\n",
    "Example Output (STRICTLY ONE-WORD DOMAINS - adapt to YOUR industry from data):\n",
    "use_case_id,domain\n",
    "N1-AI01,Revenue\n",
    "N1-AI02,Revenue\n",
    "N1-AI03,Revenue\n",
    "N1-AI04,Customers\n",
    "N1-AI05,Customers\n",
    "N1-AI06,Customers\n",
    "N1-AI07,Operations\n",
    "N1-AI08,Operations\n",
    "N1-AI09,Operations\n",
    "N1-AI10,Maintenance\n",
    "N1-AI11,Maintenance\n",
    "N1-AI12,Analytics\n",
    "N1-AI13,Analytics\n",
    "N1-AI14,Supply\n",
    "N1-AI15,Risk\n",
    "\n",
    "**NOTE**: The domain names above are EXAMPLES. Use domain names that match YOUR actual industry and data.\n",
    "\n",
    "**OUTPUT REQUIREMENTS**:\n",
    "\n",
    "**FORMAT**: Return ONLY the CSV (no explanations, no markdown fences, no additional text)\n",
    "\n",
    "**CONTENT**: \n",
    "- Use industry-specific names (see examples above)\n",
    "- Avoid generic terms (\"Management\", \"Operations\", \"Services\") \n",
    "- Use validation checklist above before submitting\n",
    "\n",
    "**INPUT USE CASES CSV**:\n",
    "{use_cases_csv}\n",
    "\n",
    "{previous_violations}\n",
    "\n",
    "\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL OUTPUT INSTRUCTION \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8:\n",
    "Your ENTIRE response must be ONLY the CSV in the format shown above.\n",
    "- START your response with the CSV header: use_case_id,domain\n",
    "- Follow with one row per use case\n",
    "- NO text before the CSV\n",
    "- NO text after the CSV\n",
    "- NO explanations or commentary\n",
    "- NO markdown code fences (```)\n",
    "- NO thoughts or reasoning\n",
    "- NO \"I need to analyze...\" or similar statements\n",
    "- NO \"Here is...\" or \"I have...\" statements\n",
    "- ONLY the pure CSV data\n",
    "\n",
    "\uD83D\uDEA8 ABSOLUTE RULE - OUTPUT FORMAT \uD83D\uDEA8:\n",
    "\n",
    "❌ DO NOT INCLUDE:\n",
    "- Any conversational or explanatory text (\"I need to...\", \"Let me...\", \"I'll...\", \"Here is...\", \"Based on...\")\n",
    "- Any thoughts or analysis descriptions\n",
    "\n",
    "✅ OUTPUT REQUIREMENTS:\n",
    "- Your response must START with: use_case_id,domain,honesty_score,honesty_justification\n",
    "- Include honesty columns in header and all rows\n",
    "\n",
    "Begin your CSV response now:\n",
    "\"\"\" + HONESTY_CHECK_CSV\n",
    "\n",
    "# --- 1d2. Subdomain Detector Prompt (NEW - PER-DOMAIN SUBDOMAIN ASSIGNMENT) ---\n",
    "PROMPT_TEMPLATES[\"SUBDOMAIN_DETECTOR_PROMPT\"] = \"\"\"\n",
    "You are an expert business analyst specializing in subdomain taxonomy design within business domains.\n",
    "\n",
    "**\uD83C\uDFAF YOUR TASK**: Analyze the use cases for a SINGLE domain and assign each to appropriate Subdomains.\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL REQUIREMENTS - YOUR RESPONSE WILL BE REJECTED IF NOT FOLLOWED \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**:\n",
    "\n",
    "**SUBDOMAIN RULES (MANDATORY - AUTOMATED VALIDATION WILL REJECT VIOLATIONS)**:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "1. **SUBDOMAINS PER DOMAIN**: Must create between 2-10 subdomains (MINIMUM 2, MAXIMUM 10 - HARD LIMIT)\n",
    "2. **SUBDOMAIN NAMING**: Each subdomain name MUST be EXACTLY 2 WORDS (no exceptions)\n",
    "3. **USE CASES PER SUBDOMAIN**: Each subdomain MUST have at least 2 use cases (MINIMUM 2)\n",
    "4. **CONSOLIDATE SINGLE-USE SUBDOMAINS**: If a subdomain has only 1 use case, you MUST merge it with another related subdomain. Do NOT create subdomains with only 1 use case.\n",
    "5. **NO OVERLAPPING WORDS**: Within this domain, NO two subdomains can share the same word\n",
    "6. **BUSINESS-FOCUSED**: Subdomains MUST be business-focused, NOT technical\n",
    "7. **BALANCED DISTRIBUTION**: Distribute use cases EVENLY across subdomains\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "**SUBDOMAIN NAMING - EXACTLY 2 WORDS (MANDATORY)**:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "► **ABSOLUTE RULE**: ALL subdomain names MUST be EXACTLY 2 WORDS\n",
    "► **REJECTION CRITERIA**: If ANY subdomain has 1 word OR 3+ words, your ENTIRE response is REJECTED\n",
    "► **WORD COUNT METHOD**: Count spaces - MUST have EXACTLY 1 space (= EXACTLY 2 words)\n",
    "► ✅ CORRECT (2 words): \"Crew Planning\", \"Special Assistance\", \"Menu Planning\", \"Route Optimization\", \"Quality Control\"\n",
    "► ❌ WRONG - 1 WORD (REJECTED): \"Scheduling\", \"Pricing\", \"Check-in\", \"Loyalty\", \"Baggage\"\n",
    "► ❌ WRONG - 3+ WORDS (REJECTED): \"Network Route Planning\" (3 words), \"Quality Control Management\" (3 words)\n",
    "► **RULE**: Use descriptive 2-word combinations like \"Crew Scheduling\" NOT \"Scheduling\"\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "**SUBDOMAIN NAMING EXAMPLES**:\n",
    "✅ CORRECT (EXACTLY 2 words, business-focused):\n",
    "- \"Crew Planning\", \"Special Assistance\", \"Menu Planning\", \"Route Optimization\", \"Quality Control\"\n",
    "- \"Revenue Pricing\", \"Fleet Management\", \"Customer Feedback\", \"Boarding Process\", \"Delay Recovery\"\n",
    "- \"Preventive Maintenance\", \"Safety Inspections\", \"Parts Management\", \"Work Orders\"\n",
    "- \"Pricing Strategy\", \"Yield Management\", \"Ancillary Revenue\", \"Revenue Forecasting\"\n",
    "- \"Schedule Optimization\", \"Capacity Management\", \"Network Planning\", \"Market Analysis\"\n",
    "\n",
    "❌ WRONG (1 word):\n",
    "- \"Scheduling\", \"Pricing\", \"Routes\", \"Maintenance\", \"Loyalty\"\n",
    "\n",
    "❌ WRONG (3+ words):\n",
    "- \"Quality Control Management\", \"Aircraft Assignment Management System\", \"Customer Service Operations\"\n",
    "\n",
    "**BUSINESS CONTEXT**:\n",
    "Domain Name: {domain_name}\n",
    "Business Name: {business_name}\n",
    "Industries: {industries}\n",
    "Business Context: {business_context}\n",
    "\n",
    "**INPUT DATA**:\n",
    "You will receive a CSV of use cases that ALL belong to the domain \"{domain_name}\":\n",
    "\"No\",\"Name\",\"type\",\"Analytics Technique\",\"Statement\",\"Solution\",\"Business Value\",\"Beneficiary\",\"Sponsor\",\"Tables Involved\"\n",
    "\n",
    "**YOUR SUBDOMAIN DETECTION TASK**:\n",
    "1. Analyze the use case names, statements, solutions, analytics techniques, and tables involved\n",
    "2. Identify natural BUSINESS groupings within the \"{domain_name}\" domain based on:\n",
    "   - Specific business functions or processes\n",
    "   - Related business activities\n",
    "   - Similar operational areas\n",
    "   - Common beneficiaries or stakeholders\n",
    "3. Create appropriate subdomain names (EXACTLY 2 WORDS, business-focused)\n",
    "4. Ensure ALL validation rules are met (2-10 subdomains, 2+ use cases per subdomain, 2-word names, etc.)\n",
    "\n",
    "**SUBDOMAIN STRATEGY**:\n",
    "- **BUSINESS PROCESSES**: Group use cases by specific business processes or workflows\n",
    "- **FUNCTIONAL AREAS**: Create subdomains for distinct functional areas within the domain\n",
    "- **RELATED ACTIVITIES**: Use cases with related business activities should be in the same subdomain\n",
    "- **CONSOLIDATION REQUIREMENT**: If any subdomain would have only 1 use case, you MUST merge it with the most related subdomain\n",
    "  * Example: \"Revenue Pricing\" (2 cases) is acceptable\n",
    "  * Example: \"Fleet Management\" (1 case) → MUST merge into \"Aircraft Maintenance\" (4 cases) → becomes (5 cases)\n",
    "  * Never leave a subdomain with only 1 use case\n",
    "- **MAXIMUM LIMIT**: Never exceed 10 subdomains per domain (this is a hard limit - consolidate if needed)\n",
    "- **BALANCE**: Aim for 2-10 use cases per subdomain (avoid very small or very large subdomains)\n",
    "- **CLARITY**: Subdomain names should clearly indicate what business function they represent\n",
    "\n",
    "**NO OVERLAPPING SUBDOMAIN WORDS**: \n",
    "- Within the \"{domain_name}\" domain, if subdomains share ANY word, merge them\n",
    "- Example: \"Network Planning\" + \"Network Optimization\" → Merge to \"Network Planning\" (2 words)\n",
    "- Always keep exactly 2 words when merging (never reduce to 1, never expand to 3+)\n",
    "\n",
    "**ANTI-PATTERNS TO AVOID**:\n",
    "❌ Creating single-word subdomains (e.g., \"Scheduling\", \"Pricing\")\n",
    "❌ Creating 3+ word subdomains (e.g., \"Quality Control Management\")\n",
    "❌ Creating subdomains with fewer than 2 use cases (MUST consolidate these)\n",
    "❌ Creating subdomains with only 1 use case (ABSOLUTELY FORBIDDEN - merge immediately)\n",
    "❌ Creating more than 10 subdomains (HARD LIMIT - consolidate if you exceed this)\n",
    "❌ Creating fewer than 2 subdomains\n",
    "❌ Using technical terms instead of business terms\n",
    "\n",
    "**OUTPUT FORMAT: CSV (NOT JSON)**:\n",
    "Return a CSV with EXACTLY 2 columns (with header):\n",
    "  - Column 1: \"use_case_id\" - The \"No\" field from the input CSV\n",
    "  - Column 2: \"subdomain\" - The assigned subdomain name (EXACTLY 2 WORDS)\n",
    "\n",
    "**\uD83D\uDEA8 VALIDATION CHECKLIST (BEFORE SUBMITTING) \uD83D\uDEA8**:\n",
    "☐ Subdomain count: 2-10 (MINIMUM 2, MAXIMUM 10 - HARD LIMIT)\n",
    "☐ Each subdomain name: EXACTLY 2 words\n",
    "☐ Use cases per subdomain: ≥2 (MINIMUM 2 - if any have only 1, must consolidate)\n",
    "☐ No subdomains with only 1 use case (MUST merge these)\n",
    "☐ Subdomains with 2 use cases are acceptable\n",
    "☐ Total subdomain count does not exceed 10 (if it does, consolidate)\n",
    "☐ No overlapping words in subdomain names\n",
    "☐ All subdomain names are business-focused (not technical)\n",
    "\n",
    "The output language for subdomain names should be {output_language}.\n",
    "\n",
    "Example Output for \"Revenue\" domain:\n",
    "use_case_id,subdomain\n",
    "N1-AI01,Pricing Strategy\n",
    "N1-AI02,Pricing Strategy\n",
    "N1-AI03,Pricing Strategy\n",
    "N1-AI04,Yield Management\n",
    "N1-AI05,Yield Management\n",
    "N1-AI06,Yield Management\n",
    "N1-AI07,Revenue Forecasting\n",
    "N1-AI08,Revenue Forecasting\n",
    "N1-AI09,Revenue Forecasting\n",
    "N1-AI10,Ancillary Revenue\n",
    "N1-AI11,Ancillary Revenue\n",
    "N1-AI12,Ancillary Revenue\n",
    "\n",
    "**OUTPUT REQUIREMENTS**:\n",
    "\n",
    "**FORMAT**: Return ONLY the CSV (no explanations, no markdown fences, no additional text)\n",
    "\n",
    "**CONTENT**: \n",
    "- Use business-focused 2-word subdomain names\n",
    "- Avoid technical terms\n",
    "- Use validation checklist above before submitting\n",
    "\n",
    "**INPUT USE CASES CSV FOR DOMAIN \"{domain_name}\"**:\n",
    "{use_cases_csv}\n",
    "\n",
    "{previous_violations}\n",
    "\n",
    "\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL OUTPUT INSTRUCTION \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8:\n",
    "Your ENTIRE response must be ONLY the CSV in the format shown above.\n",
    "- START your response with the CSV header: use_case_id,subdomain\n",
    "- Follow with one row per use case\n",
    "- NO text before the CSV\n",
    "- NO text after the CSV\n",
    "- NO explanations or commentary\n",
    "- NO markdown code fences (```)\n",
    "- NO thoughts or reasoning\n",
    "- NO \"I need to analyze...\" or similar statements\n",
    "- NO \"Here is...\" or \"I have...\" statements\n",
    "- NO \"It's mathematically impossible...\" or problem descriptions\n",
    "- ONLY the pure CSV data\n",
    "\n",
    "\uD83D\uDEA8 ABSOLUTE RULE - OUTPUT FORMAT \uD83D\uDEA8:\n",
    "\n",
    "❌ DO NOT INCLUDE:\n",
    "- Any conversational or explanatory text (\"I need to...\", \"It's mathematically...\", \"I only see...\", \"Let me...\", \"I'll...\", \"Here is...\", \"Based on...\")\n",
    "- Any thoughts or analysis descriptions\n",
    "- Any problem descriptions or concerns\n",
    "\n",
    "✅ OUTPUT REQUIREMENTS:\n",
    "- Your response must START with: use_case_id,subdomain,honesty_score,honesty_justification\n",
    "- Include honesty columns in header and all rows\n",
    "- If you cannot meet requirements, still output CSV (merge subdomains as needed)\n",
    "\n",
    "Begin your CSV response now:\n",
    "\"\"\" + HONESTY_CHECK_CSV\n",
    "# --- 1f. Translation Prompts (MODIFIED FOR REQUEST #1, #2) ---\n",
    "PROMPT_TEMPLATES[\"KEYWORDS_TRANSLATE_PROMPT\"] = \"\"\"You are an expert translator. Your task is to translate the *values* (and only the values) of the following JSON object into {target_language}.\n",
    "Do NOT translate the keys.\n",
    "\n",
    "**CRITICAL JSON SYNTAX VALIDATION**:\n",
    "- Every key-value pair MUST have a colon (:) separator\n",
    "- Format: \"key\": \"translated value\"\n",
    "- **MOST COMMON ERROR**: Missing colon - `\"key\" \"value\"` is WRONG, must be `\"key\": \"value\"`\n",
    "- **FOR ARABIC/CHINESE/JAPANESE/HINDI**: Verify the colon `:` exists between key and value\n",
    "- Count your colons - you need ONE colon per key-value pair\n",
    "- Validate your JSON structure before responding\n",
    "\n",
    "SPECIAL INSTRUCTIONS:\n",
    "\uD83D\uDEA8 CRITICAL: You MUST translate ALL text values in the JSON object. Do NOT leave ANY value in English (except for the specific exceptions listed below).\n",
    "\n",
    "**MANDATORY TRANSLATION REQUIREMENT:**\n",
    "- EVERY single value in the JSON MUST be translated to {target_language}\n",
    "- This includes short words like \"Type\", \"Priority\", \"Subdomain\", \"Statement\", \"Solution\", \"Beneficiary\", \"Sponsor\", etc.\n",
    "- DO NOT assume any value should stay in English just because it's short or seems technical\n",
    "- If a value is in English in the input, it MUST be in {target_language} in the output (except for the specific exceptions below)\n",
    "\n",
    "**ONLY THESE SPECIFIC EXCEPTIONS - DO NOT TRANSLATE:**\n",
    "1. \"Databricks Agent Bricks Strategic AI Use Cases\" - Keep this EXACT text in English\n",
    "2. \"N/A\" - Keep as \"N/A\" in all languages\n",
    "3. If you see \"pdf_title\" or \"pptx_main_title\" keys with value \"Databricks Agent Bricks Strategic AI Use Cases\", keep the value EXACTLY as is\n",
    "\n",
    "**EVERYTHING ELSE MUST BE TRANSLATED WITHOUT EXCEPTION**\n",
    "\n",
    "**FOR ALL LANGUAGES - UNIVERSAL REQUIREMENTS:**\n",
    "- Provide EXACT, DIRECT translations of the English text\n",
    "- Do NOT use special phrasings or adaptations\n",
    "- Translate literally and accurately\n",
    "- Use standard terminology for the target language\n",
    "- Do NOT leave any English words in the translation (except the 3 exceptions above)\n",
    "- Ensure complete and proper character encoding for all scripts (Arabic, Chinese, Japanese, Hindi, Cyrillic, etc.)\n",
    "\n",
    "**MANDATORY TRANSLATIONS FOR ALL LANGUAGES (these MUST be translated, NOT left in English):**\n",
    "\n",
    "**Common UI Terms (translate to your target language):**\n",
    "  * \"Type\" → MUST translate\n",
    "  * \"Subdomain\" → MUST translate\n",
    "  * \"Analytics Technique\" → MUST translate\n",
    "  * \"Primary Table\" → MUST translate\n",
    "  * \"Priority\" → MUST translate\n",
    "  * \"Statement\" → MUST translate\n",
    "  * \"Solution\" → MUST translate\n",
    "  * \"Business Value\" → MUST translate\n",
    "  * \"Beneficiary\" → MUST translate\n",
    "  * \"Sponsor\" → MUST translate\n",
    "  * \"Business Domain\" → MUST translate\n",
    "  * \"Tables Involved\" → MUST translate\n",
    "\n",
    "**Value Terms (translate to your target language):**\n",
    "  * \"Risk\" → MUST translate\n",
    "  * \"Problem\" → MUST translate\n",
    "  * \"Opportunity\" → MUST translate\n",
    "  * \"Improvement\" → MUST translate\n",
    "  * \"Very High\" → MUST translate\n",
    "  * \"High\" → MUST translate\n",
    "  * \"Medium\" → MUST translate\n",
    "  * \"Low\" → MUST translate\n",
    "  * \"Very Low\" → MUST translate\n",
    "\n",
    "**REFERENCE TRANSLATIONS BY LANGUAGE:**\n",
    "\n",
    "**Arabic:**\n",
    "  * \"Type\" = \"النوع\", \"Subdomain\" = \"المجال الفرعي\", \"Analytics Technique\" = \"تقنية التحليل\", \"Primary Table\" = \"الجدول الرئيسي\", \"Priority\" = \"الأولوية\"\n",
    "  * \"Statement\" = \"البيان\", \"Solution\" = \"الحل\", \"Business Value\" = \"القيمة التجارية\"\n",
    "  * \"Beneficiary\" = \"المستفيد\", \"Sponsor\" = \"الراعي\", \"Business Domain\" = \"مجال الأعمال\"\n",
    "  * \"Risk\" = \"مخاطرة\", \"Problem\" = \"مشكلة\", \"Opportunity\" = \"فرصة\", \"Improvement\" = \"تحسين\"\n",
    "  * \"High\" = \"عالية\", \"Very High\" = \"عالية جداً\", \"Medium\" = \"متوسطة\", \"Low\" = \"منخفضة\", \"Very Low\" = \"منخفضة جداً\"\n",
    "\n",
    "**Spanish:**\n",
    "  * \"Type\" = \"Tipo\", \"Subdomain\" = \"Subdominio\", \"Analytics Technique\" = \"Técnica de Análisis\", \"Primary Table\" = \"Tabla Principal\", \"Priority\" = \"Prioridad\"\n",
    "  * \"Statement\" = \"Declaración\", \"Solution\" = \"Solución\", \"Business Value\" = \"Valor Comercial\"\n",
    "  * \"Beneficiary\" = \"Beneficiario\", \"Sponsor\" = \"Patrocinador\", \"Business Domain\" = \"Dominio Empresarial\"\n",
    "  * \"Risk\" = \"Riesgo\", \"Problem\" = \"Problema\", \"Opportunity\" = \"Oportunidad\", \"Improvement\" = \"Mejora\"\n",
    "  * \"High\" = \"Alto\", \"Very High\" = \"Muy Alto\", \"Medium\" = \"Medio\", \"Low\" = \"Bajo\", \"Very Low\" = \"Muy Bajo\"\n",
    "\n",
    "**French:**\n",
    "  * \"Type\" = \"Type\", \"Subdomain\" = \"Sous-domaine\", \"Analytics Technique\" = \"Technique d'Analyse\", \"Primary Table\" = \"Table Principale\", \"Priority\" = \"Priorité\"\n",
    "  * \"Statement\" = \"Déclaration\", \"Solution\" = \"Solution\", \"Business Value\" = \"Valeur Commerciale\"\n",
    "  * \"Beneficiary\" = \"Bénéficiaire\", \"Sponsor\" = \"Sponsor\", \"Business Domain\" = \"Domaine d'Activité\"\n",
    "  * \"Risk\" = \"Risque\", \"Problem\" = \"Problème\", \"Opportunity\" = \"Opportunité\", \"Improvement\" = \"Amélioration\"\n",
    "  * \"High\" = \"Élevé\", \"Very High\" = \"Très Élevé\", \"Medium\" = \"Moyen\", \"Low\" = \"Faible\", \"Very Low\" = \"Très Faible\"\n",
    "\n",
    "**German:**\n",
    "  * \"Type\" = \"Typ\", \"Subdomain\" = \"Unterbereich\", \"Analytics Technique\" = \"Analysetechnik\", \"Primary Table\" = \"Haupttabelle\", \"Priority\" = \"Priorität\"\n",
    "  * \"Statement\" = \"Aussage\", \"Solution\" = \"Lösung\", \"Business Value\" = \"Geschäftswert\"\n",
    "  * \"Beneficiary\" = \"Begünstigter\", \"Sponsor\" = \"Sponsor\", \"Business Domain\" = \"Geschäftsbereich\"\n",
    "  * \"Risk\" = \"Risiko\", \"Problem\" = \"Problem\", \"Opportunity\" = \"Chance\", \"Improvement\" = \"Verbesserung\"\n",
    "  * \"High\" = \"Hoch\", \"Very High\" = \"Sehr Hoch\", \"Medium\" = \"Mittel\", \"Low\" = \"Niedrig\", \"Very Low\" = \"Sehr Niedrig\"\n",
    "\n",
    "**Chinese (Simplified):**\n",
    "  * \"Type\" = \"类型\", \"Subdomain\" = \"子域\", \"Analytics Technique\" = \"分析技术\", \"Primary Table\" = \"主表\", \"Priority\" = \"优先级\"\n",
    "  * \"Statement\" = \"声明\", \"Solution\" = \"解决方案\", \"Business Value\" = \"业务价值\"\n",
    "  * \"Beneficiary\" = \"受益人\", \"Sponsor\" = \"赞助者\", \"Business Domain\" = \"业务领域\"\n",
    "  * \"Risk\" = \"风险\", \"Problem\" = \"问题\", \"Opportunity\" = \"机会\", \"Improvement\" = \"改进\"\n",
    "  * \"High\" = \"高\", \"Very High\" = \"非常高\", \"Medium\" = \"中等\", \"Low\" = \"低\", \"Very Low\" = \"非常低\"\n",
    "\n",
    "**Japanese:**\n",
    "  * \"Type\" = \"タイプ\", \"Subdomain\" = \"サブドメイン\", \"Analytics Technique\" = \"分析技術\", \"Primary Table\" = \"主要テーブル\", \"Priority\" = \"優先度\"\n",
    "  * \"Statement\" = \"ステートメント\", \"Solution\" = \"ソリューション\", \"Business Value\" = \"ビジネス価値\"\n",
    "  * \"Beneficiary\" = \"受益者\", \"Sponsor\" = \"スポンサー\", \"Business Domain\" = \"ビジネスドメイン\"\n",
    "  * \"Risk\" = \"リスク\", \"Problem\" = \"問題\", \"Opportunity\" = \"機会\", \"Improvement\" = \"改善\"\n",
    "  * \"High\" = \"高\", \"Very High\" = \"非常に高い\", \"Medium\" = \"中程度\", \"Low\" = \"低\", \"Very Low\" = \"非常に低い\"\n",
    "\n",
    "**Portuguese:**\n",
    "  * \"Type\" = \"Tipo\", \"Subdomain\" = \"Subdomínio\", \"Analytics Technique\" = \"Técnica de Análise\", \"Primary Table\" = \"Tabela Principal\", \"Priority\" = \"Prioridade\"\n",
    "  * \"Statement\" = \"Declaração\", \"Solution\" = \"Solução\", \"Business Value\" = \"Valor Comercial\"\n",
    "  * \"Beneficiary\" = \"Beneficiário\", \"Sponsor\" = \"Patrocinador\", \"Business Domain\" = \"Domínio de Negócio\"\n",
    "  * \"Risk\" = \"Risco\", \"Problem\" = \"Problema\", \"Opportunity\" = \"Oportunidade\", \"Improvement\" = \"Melhoria\"\n",
    "  * \"High\" = \"Alto\", \"Very High\" = \"Muito Alto\", \"Medium\" = \"Médio\", \"Low\" = \"Baixo\", \"Very Low\" = \"Muito Baixo\"\n",
    "\n",
    "**Russian:**\n",
    "  * \"Type\" = \"Тип\", \"Subdomain\" = \"Поддомен\", \"Analytics Technique\" = \"Аналитическая техника\", \"Primary Table\" = \"Основная таблица\", \"Priority\" = \"Приоритет\"\n",
    "  * \"Statement\" = \"Заявление\", \"Solution\" = \"Решение\", \"Business Value\" = \"Бизнес-ценность\"\n",
    "  * \"Beneficiary\" = \"Бенефициар\", \"Sponsor\" = \"Спонсор\", \"Business Domain\" = \"Бизнес-домен\"\n",
    "  * \"Risk\" = \"Риск\", \"Problem\" = \"Проблема\", \"Opportunity\" = \"Возможность\", \"Improvement\" = \"Улучшение\"\n",
    "  * \"High\" = \"Высокий\", \"Very High\" = \"Очень Высокий\", \"Medium\" = \"Средний\", \"Low\" = \"Низкий\", \"Very Low\" = \"Очень Низкий\"\n",
    "\n",
    "**Hindi:**\n",
    "  * \"Type\" = \"प्रकार\", \"Subdomain\" = \"उपडोमेन\", \"Analytics Technique\" = \"विश्लेषण तकनीक\", \"Primary Table\" = \"प्राथमिक तालिका\", \"Priority\" = \"प्राथमिकता\"\n",
    "  * \"Statement\" = \"कथन\", \"Solution\" = \"समाधान\", \"Business Value\" = \"व्यावसायिक मूल्य\"\n",
    "  * \"Beneficiary\" = \"लाभार्थी\", \"Sponsor\" = \"प्रायोजक\", \"Business Domain\" = \"व्यावसायिक डोमेन\"\n",
    "  * \"Risk\" = \"जोखिम\", \"Problem\" = \"समस्या\", \"Opportunity\" = \"अवसर\", \"Improvement\" = \"सुधार\"\n",
    "  * \"High\" = \"उच्च\", \"Very High\" = \"बहुत उच्च\", \"Medium\" = \"मध्यम\", \"Low\" = \"निम्न\", \"Very Low\" = \"बहुत निम्न\"\n",
    "\n",
    "**VALIDATION CHECKLIST - Before submitting your translation:**\n",
    "✓ Every value that was in English is now in {target_language} (except the 3 exceptions)\n",
    "✓ \"Type\" is translated (NOT left as \"Type\")\n",
    "✓ \"Subdomain\" is translated (NOT left as \"Subdomain\")\n",
    "✓ \"Analytics Technique\" is translated (NOT left as \"Analytics Technique\")\n",
    "✓ \"Primary Table\" is translated (NOT left as \"Primary Table\")\n",
    "✓ \"Priority\" is translated (NOT left as \"Priority\")\n",
    "✓ \"Statement\" is translated (NOT left as \"Statement\")\n",
    "✓ \"Solution\" is translated (NOT left as \"Solution\")\n",
    "✓ \"Beneficiary\" is translated (NOT left as \"Beneficiary\")\n",
    "✓ \"Sponsor\" is translated (NOT left as \"Sponsor\")\n",
    "✓ \"Business Value\" is translated (NOT left as \"Business Value\")\n",
    "✓ \"Business Domain\" is translated (NOT left as \"Business Domain\")\n",
    "✓ \"Tables Involved\" is translated (NOT left as \"Tables Involved\")\n",
    "✓ ALL keys with \"Type\", \"Priority\" values are translated (including \"type\", \"priority\", \"aspect_priority\")\n",
    "✓ All other English words are translated\n",
    "✓ ONLY \"Databricks Agent Bricks Strategic AI Use Cases\" and \"N/A\" remain in English\n",
    "\n",
    "**CRITICAL - COUNT YOUR TRANSLATIONS:**\n",
    "The input JSON has approximately 70+ key-value pairs. Your output JSON must have THE SAME number of key-value pairs with 99% of values in {target_language} (only \"Databricks Agent Bricks Strategic AI Use Cases\" and \"N/A\" stay in English).\n",
    "\n",
    "If you see ANY English words in your output other than the 3 exceptions, you FAILED. Go back and translate them.\n",
    "\n",
    "Return ONLY a single, valid JSON object with the exact same structure, but with the values translated.\n",
    "\n",
    "\uD83D\uDEA8 ABSOLUTE RULE - OUTPUT FORMAT \uD83D\uDEA8:\n",
    "\n",
    "❌ DO NOT INCLUDE:\n",
    "- Any conversational or explanatory text (\"Here is...\", \"I've...\", \"The...\")\n",
    "- Any thoughts or analysis descriptions\n",
    "\n",
    "✅ OUTPUT REQUIREMENTS:\n",
    "- Your response must be wrapped in honesty JSON: {{\"honesty_score\": XX, \"honesty_justification\": \"...\", \"data\": <your_translated_json>}}\n",
    "- Start with: {{\"honesty_score\":\n",
    "\n",
    "Input JSON:\n",
    "{json_payload}\n",
    "\n",
    "Respond with the honesty-wrapped JSON object.\n",
    "\"\"\" + HONESTY_CHECK_JSON\n",
    "\n",
    "PROMPT_TEMPLATES[\"USE_CASE_TRANSLATE_PROMPT\"] = \"\"\"You are an expert translator. Your task is to translate a list of use cases into {target_language}.\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL OUTPUT REQUIREMENT - READ THIS FIRST \uD83D\uDEA8**\n",
    "Your response MUST be PURE CSV DATA ONLY. \n",
    "- NO explanations before or after the CSV\n",
    "- NO conversational text\n",
    "- NO SQL code fragments\n",
    "- NO commentary\n",
    "- NO markdown fences (```)\n",
    "- ONLY the CSV header and data rows\n",
    "- Start immediately with the header row: \"No\",\"Name\",...\n",
    "\n",
    "**FIELDS TO TRANSLATE** (translate these 10 fields ONLY):\n",
    "\"Name\", \"Business Domain\", \"Subdomain\", \"type\", \"Statement\", \"Solution\", \"Business Value\", \"Beneficiary\", \"Sponsor\", \"Business Priority Alignment\"\n",
    "\n",
    "Do NOT translate: \"No\", \"Tables Involved\", \"Analytics Technique\", \"Priority\"\n",
    "\n",
    "**SPECIAL RULES FOR STRATEGIC GOAL ALIGNMENT FIELD**:\n",
    "- This field contains strategic goal values like: \"Reduce Cost\", \"Increase Revenue\", \"Boost Productivity\", \"Mitigate Risk\", \"Protect Revenue\", \"Align to Regulations\", \"Improve Customer Experience\", \"Enable Data-Driven Decisions\", \"General Improvement\"\n",
    "- Translate these goal values to {target_language} using proper business terminology\n",
    "- If multiple goals are comma-separated (e.g., \"Reduce Cost, Increase Revenue\"), translate each goal separately and keep them comma-separated\n",
    "- Examples for MULTIPLE languages:\n",
    "  * Arabic: \"Reduce Cost\" → \"تقليل التكلفة\", \"Increase Revenue\" → \"زيادة الإيرادات\"\n",
    "  * French: \"Reduce Cost\" → \"Réduire les coûts\", \"Boost Productivity\" → \"Améliorer la productivité\"\n",
    "  * Spanish: \"Reduce Cost\" → \"Reducir costos\", \"Mitigate Risk\" → \"Mitigar riesgos\"\n",
    "  * Chinese: \"Reduce Cost\" → \"降低成本\", \"Increase Revenue\" → \"增加收入\"\n",
    "  * German: \"Reduce Cost\" → \"Kosten senken\", \"Protect Revenue\" → \"Umsatz schützen\"\n",
    "\n",
    "**NOTE**: The SQL field is NOT included in the input to reduce payload size. You only need to translate the fields listed above.\n",
    "\n",
    "**SPECIAL RULES FOR SPONSOR FIELD**:\n",
    "- If the Sponsor field contains a name in the format \"Name (Title)\" (e.g., \"John Smith (Chief Technology Officer)\"), you MUST transliterate/translate BOTH the name AND the title for ALL languages\n",
    "- Person names should be transliterated phonetically into the target language writing system\n",
    "- **CRITICAL**: This applies to ALL languages, not just non-Latin scripts. Transliterate names appropriately for each target language.\n",
    "- Examples for MULTIPLE languages:\n",
    "  * French: \"John Smith (Chief Technology Officer)\" → \"Jean Smith (Directeur de la Technologie)\"\n",
    "  * Spanish: \"John Smith (Chief Technology Officer)\" → \"Juan Smith (Director de Tecnología)\"\n",
    "  * Arabic: \"John Smith (Chief Technology Officer)\" → \"جون سميث (المدير التنفيذي للتكنولوجيا)\"\n",
    "  * Chinese: \"John Smith (Chief Technology Officer)\" → \"约翰·史密斯 (首席技术官)\"\n",
    "  * Japanese: \"John Smith (Chief Technology Officer)\" → \"ジョン・スミス (最高技術責任者)\"\n",
    "  * German: \"John Smith (Chief Technology Officer)\" → \"Johann Schmidt (Technischer Leiter)\"\n",
    "  * Russian: \"John Smith (Chief Technology Officer)\" → \"Джон Смит (Технический директор)\"\n",
    "  * Hindi: \"John Smith (Chief Technology Officer)\" → \"जॉन स्मिथ (मुख्य प्रौद्योगिकी अधिकारी)\"\n",
    "- If the Sponsor field contains ONLY a title (no name), translate it normally\n",
    "- Always adapt the name to sound natural in the target language and culture\n",
    "\n",
    "**OUTPUT FORMAT: CSV (NOT JSON)**\n",
    "Your response MUST be in CSV format with the following structure:\n",
    "- First line: Header row with 14 column names (SQL field is not included)\n",
    "- Subsequent lines: One row per use case with 14 fields\n",
    "\n",
    "**CSV HEADER (MUST BE EXACTLY THIS)**:\n",
    "\"No\",\"Name\",\"Business Domain\",\"Subdomain\",\"type\",\"Analytics Technique\",\"Statement\",\"Solution\",\"Business Value\",\"Beneficiary\",\"Sponsor\",\"Business Priority Alignment\",\"Tables Involved\",\"Priority\"\n",
    "\n",
    "**CRITICAL CSV FORMATTING RULES**:\n",
    "1. ALL fields MUST be enclosed in double quotes (\")\n",
    "2. Use comma (,) as the field separator\n",
    "3. For fields containing commas or quotes, keep them inside the double quotes\n",
    "4. Each row must have exactly 14 fields (SQL is NOT included)\n",
    "5. Keep untranslated fields (No, Tables Involved, Analytics Technique, Priority) EXACTLY as they appear in the input\n",
    "6. The Analytics Technique field should be copied EXACTLY as-is in English (do not translate)\n",
    "7. The Priority field should be copied EXACTLY as-is in English (do not translate \"Very High\", \"High\", \"Medium\", \"Low\", \"Very Low\")\n",
    "\n",
    "**IMPORTANT**: Analytics Technique and Priority values remain in English in the CSV. The UI will handle translation separately.\n",
    "\n",
    "**TRANSLATION REQUIREMENTS**:\n",
    "1. Translate ALL 10 designated fields for EVERY use case\n",
    "2. \"Business Domain\" - MUST be translated to {target_language}\n",
    "3. \"Subdomain\" - MUST be translated to {target_language}\n",
    "4. \"Name\" - MUST be translated to {target_language}\n",
    "5. \"Business Priority Alignment\" - MUST be translated to {target_language} (translate the priority values like \"Reduce Cost\" → appropriate translation)\n",
    "\n",
    "**SPECIAL INSTRUCTIONS**:\n",
    "- \"Databricks Agent Bricks\" → Keep as-is for all languages EXCEPT Arabic: \"الذكاء الصناعي الخاص بداتا بريكس\"\n",
    "- Preserve all technical terms in SQL exactly as they appear\n",
    "- Keep numeric IDs (like \"AI-001\") unchanged\n",
    "- Keep Analytics Technique and Priority values in English (not translated in CSV)\n",
    "- For Arabic: Ensure complete and proper Arabic text encoding. Double-check all Arabic characters are properly formed.\n",
    "\n",
    "**\uD83D\uDED1 ABSOLUTELY FORBIDDEN - DO NOT INCLUDE \uD83D\uDED1**:\n",
    "- ❌ NO text before the CSV header\n",
    "- ❌ NO explanatory text like \"Here is the translation...\" or \"I've translated...\"\n",
    "- ❌ NO SQL code fragments or queries outside the CSV structure\n",
    "- ❌ NO examples or demonstrations\n",
    "- ❌ NO line breaks or empty lines before the header\n",
    "- ❌ NO commentary about the translation process\n",
    "- ❌ NO additional rows beyond the exact number provided in the input\n",
    "- ✅ ONLY: Pure CSV starting with the header, nothing else\n",
    "\n",
    "**VALIDATION CHECKLIST**:\n",
    "✓ Response starts with the exact 14-column header (no text before it)\n",
    "✓ All fields are enclosed in double quotes\n",
    "✓ Each row has exactly 14 comma-separated fields (SQL is NOT included)\n",
    "✓ No markdown fences (```) around the CSV\n",
    "✓ Analytics Technique field is copied exactly as received (do not translate it)\n",
    "✓ Priority field is copied exactly as received (do not translate it)\n",
    "✓ Business Priority Alignment field is translated to target language\n",
    "✓ All 10 translatable fields are translated for every row\n",
    "\n",
    "**CRITICAL ROW COUNT REQUIREMENT**:\n",
    "- You will receive EXACTLY a specific number of use cases in the input\n",
    "- Your CSV output MUST contain EXACTLY the same number of data rows (plus the header row)\n",
    "- DO NOT add any extra rows beyond what was provided in the input\n",
    "- DO NOT omit any rows from the input\n",
    "- Each row in your output MUST correspond to exactly one row from the input, matched by the \"No\" field\n",
    "\n",
    "**INPUT USE CASES** (as JSON for readability):\n",
    "{json_payload}\n",
    "\n",
    "\uD83D\uDEA8 FINAL INSTRUCTION \uD83D\uDEA8\n",
    "Your ENTIRE response must be ONLY the CSV data.\n",
    "- Start your response with: \"No\",\"Name\",\"Business Domain\",\"Subdomain\",\"type\",\"Statement\",...\n",
    "- Do NOT write anything before this header\n",
    "- Do NOT write anything after the last data row\n",
    "- Do NOT include markdown code fences (```)\n",
    "- Do NOT include any explanatory text\n",
    "- NO thoughts or reasoning\n",
    "- NO \"Here is the translation...\" or similar statements\n",
    "- The number of data rows MUST exactly match the number of use cases above\n",
    "\n",
    "\uD83D\uDEA8 ABSOLUTE RULE - OUTPUT FORMAT \uD83D\uDEA8:\n",
    "\n",
    "❌ DO NOT INCLUDE:\n",
    "- Any conversational or explanatory text (\"Here is...\", \"I've...\", \"The...\")\n",
    "- Any thoughts or analysis descriptions\n",
    "\n",
    "✅ OUTPUT REQUIREMENTS:\n",
    "- Your response must START with: \"No\",\"Name\",\"Business Domain\",...,\"honesty_score\",\"honesty_justification\"\n",
    "- Include honesty columns in header and all rows\n",
    "\n",
    "Begin your CSV response now (header row first, include honesty columns):\n",
    "\"\"\" + HONESTY_CHECK_CSV\n",
    "\n",
    "# --- 1g. SQL Syntax Reviewer Prompt (NEW) ---\n",
    "PROMPT_TEMPLATES[\"USE_CASE_SQL_FIX_PROMPT\"] = \"\"\"You are a **Senior Databricks SQL Engineer** with 15+ years of experience debugging SQL queries. Your task is to fix SQL ERRORS (both syntax and runtime errors) in the provided SQL query.\n",
    "\n",
    "**CRITICAL RULES:**\n",
    "1. **FIX ERRORS ONLY** - Do NOT change the business logic or query structure\n",
    "2. **PRESERVE ALL LOGIC** - Keep all CTEs, joins, AI functions, and business logic exactly as intended\n",
    "3. **DO NOT OPTIMIZE** - Do not restructure or optimize the query\n",
    "4. **DO NOT ADD FEATURES** - Do not add new columns, CTEs, or logic\n",
    "5. **ONLY FIX** what the validation/execution error indicates is broken\n",
    "6. **RUNTIME ERRORS** - If error is from query execution (not syntax), fix the runtime issue (e.g., window function issues, unresolved columns, type mismatches)\n",
    "\n",
    "**USE CASE CONTEXT:**\n",
    "- **Use Case ID**: {use_case_id}\n",
    "- **Use Case Name**: {use_case_name}\n",
    "- **Business Domain**: {business_domain}\n",
    "- **Statement**: {statement}\n",
    "- **Tables Involved**: {tables_involved}\n",
    "\n",
    "**AVAILABLE SCHEMA:**\n",
    "{directly_involved_schema}\n",
    "\n",
    "**COLUMNS FROM USE CASE (allowed set):**\n",
    "{use_case_columns}\n",
    "- Use only these columns plus the ones in the schema above. Do not add new columns.\n",
    "\n",
    "**ORIGINAL SQL QUERY (WITH ERROR):**\n",
    "```sql\n",
    "{original_sql}\n",
    "```\n",
    "\n",
    "**EXECUTION/VALIDATION ERROR:**\n",
    "```\n",
    "{explain_error}\n",
    "```\n",
    "\n",
    "**YOUR TASK:**\n",
    "1. Analyze the error message carefully (could be syntax OR runtime error)\n",
    "2. Identify the EXACT error and root cause\n",
    "3. Fix ONLY the error - do NOT change anything else\n",
    "4. Return the corrected SQL query\n",
    "\n",
    "**COMMON RUNTIME ERRORS TO FIX:**\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 #1 MOST COMMON ERROR: COALESCE STRING DEFAULTS WITHOUT QUOTES \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**IF YOU SEE THIS ERROR:** `[PARSE_SYNTAX_ERROR] Syntax error at or near 'Material'/'Unknown'/'Description'/'Supplier'/etc.`\n",
    "**THE CAUSE IS**: You forgot 'single quotes' around text defaults in COALESCE!\n",
    "\n",
    "**LOOK FOR THIS WRONG PATTERN AND FIX IT:**\n",
    "```sql\n",
    "-- ❌ WRONG - Text defaults missing quotes (THIS IS THE ERROR!)\n",
    "COALESCE(TRIM(material_type), Unknown Material) AS material_type     -- ❌ FAILS!\n",
    "COALESCE(TRIM(supplier_name), Unknown Supplier) AS supplier_name     -- ❌ FAILS!\n",
    "COALESCE(TRIM(status), Pending Review) AS status                     -- ❌ FAILS!\n",
    "\n",
    "-- ✅ CORRECT - Text defaults MUST have 'single quotes'\n",
    "COALESCE(TRIM(material_type), 'Unknown Material') AS material_type   -- ✅ WORKS!\n",
    "COALESCE(TRIM(supplier_name), 'Unknown Supplier') AS supplier_name   -- ✅ WORKS!\n",
    "COALESCE(TRIM(status), 'Pending Review') AS status                   -- ✅ WORKS!\n",
    "```\n",
    "\n",
    "**1. STRING LITERAL QUOTING ERRORS (MOST COMMON - CHECK THIS FIRST):**\n",
    "- `[PARSE_SYNTAX_ERROR] Syntax error at or near 'Type'/'Cert'/'PN'/'Status'/'Owner'/'Policy'/'Config'/'Material'/'Unknown'/'Description'` → String literal missing quotes\n",
    "- **ROOT CAUSE**: String values in SQL are not quoted with single quotes\n",
    "- **\uD83D\uDEA8 MANDATORY CORRECT PATTERNS (COPY THESE EXACTLY) \uD83D\uDEA8**:\n",
    "  * `CASE WHEN type = 'Policy' THEN 'Covered'` -- ✅ String values in single quotes\n",
    "  * `CASE WHEN status = 'Active' THEN 'Yes'` -- ✅ String values in single quotes  \n",
    "  * `COALESCE(category, 'Premium')` -- ✅ Default value in single quotes\n",
    "  * `ARRAY('Type', 'Status')` -- ✅ Array elements in single quotes\n",
    "  * `CASE WHEN level = 'High' THEN 'Urgent'` -- ✅ All strings quoted\n",
    "- **\uD83D\uDEA8 MANDATORY COALESCE PATTERNS (COPY THESE EXACTLY) \uD83D\uDEA8**:\n",
    "  * `COALESCE(TRIM(name), 'Unknown')` -- ✅ Default in single quotes\n",
    "  * `COALESCE(TRIM(category), 'Not Specified')` -- ✅ Multi-word default in single quotes\n",
    "  * `COALESCE(TRIM(status), 'Pending Review')` -- ✅ Multi-word default in single quotes\n",
    "  * `COALESCE(TRIM(region), 'Unassigned Region')` -- ✅ Multi-word default in single quotes\n",
    "  * `COALESCE(CAST(date AS STRING), 'No Date Available')` -- ✅ Text default in single quotes\n",
    "  * `COALESCE(TRIM(type), 'UNKNOWN')` -- ✅ Uppercase text in single quotes\n",
    "  * `COALESCE(TRIM(supplier_name), 'Unknown Supplier')` -- ✅ Multi-word default in single quotes\n",
    "  * `COALESCE(TRIM(material_type), 'Unknown Material')` -- ✅ Multi-word default in single quotes\n",
    "- **CRITICAL**: Scan the ENTIRE query for any string values without quotes and add single quotes around them\n",
    "- **VALIDATION**: Every string value must have single quotes: `'value'`\n",
    "- **REMEMBER**: ALL COALESCE default values that are TEXT are STRING literals and MUST have single quotes around them\n",
    "- **EXCEPTION**: Numbers (0.0, 0, 123) and booleans (TRUE, FALSE) do NOT need quotes\n",
    "\n",
    "**2. AI_FORECAST SYNTAX ERRORS:**\n",
    "- `[UNRESOLVED_COLUMN] A column with name 'ds'/'column_name' cannot be resolved` → **MOST COMMON ERROR**: Column names in AI_FORECAST parameters are NOT quoted as STRING LITERALS\n",
    "- **ROOT CAUSE**: `time_col => ds` treats `ds` as a column reference instead of a string literal\n",
    "- **\uD83D\uDEA8 MANDATORY FIX**: ALL column names in time_col, value_col, group_col MUST be STRING LITERALS (in single quotes):\n",
    "  * ❌ WRONG: `time_col => ds` → ✅ CORRECT: `time_col => 'ds'`\n",
    "  * ❌ WRONG: `value_col => revenue` → ✅ CORRECT: `value_col => 'revenue'`\n",
    "  * ❌ WRONG: `value_col => ARRAY(col1, col2)` → ✅ CORRECT: `value_col => ARRAY('col1', 'col2')`\n",
    "  * ❌ WRONG: `group_col => ARRAY(id, type)` → ✅ CORRECT: `group_col => ARRAY('id', 'type')`\n",
    "- `[PARSE_SYNTAX_ERROR] Syntax error at or near '=>'` → Wrong parameter syntax in ai_forecast or wrong date_add syntax\n",
    "- **FIX**: Ensure date_add units are NOT quoted: `date_add(MONTH, 3, MAX(ds))` not `date_add('MONTH', 3, MAX(ds))`\n",
    "- **FIX**: Ensure named parameters use `=>` correctly: `time_col => 'ds'` not `time_col='ds'`\n",
    "- `[INVALID_PARAMETER_VALUE.DATETIME_UNIT]` → date_add unit is quoted when it must be unquoted\n",
    "- **FIX**: Remove quotes: `date_add(QUARTER, 4, MAX(ds))` not `date_add('QUARTER', 4, MAX(ds))`\n",
    "- `[PYTHON_TVF_ARGUMENT_MUST_BE_CONSTANT_FOLDABLE]` → ai_forecast parameters must be constant literals\n",
    "- **FIX**: Use literal strings: `value_col => 'revenue'` not `value_col => (SELECT 'revenue')`\n",
    "- `[PYTHON_TVF_COLUMN_VALUES_MUST_BE_UNIQUE_WITHIN_PARTITION]` → Duplicate time values in AI_FORECAST input\n",
    "- **FIX**: You MUST use `GROUP BY time_col` in the input CTE to deduplicate rows for the same timestamp.\n",
    "- `[PYTHON_TVF_INCOMPATIBLE_COLUMN_TYPE]` → value_col is not DOUBLE\n",
    "- **FIX**: You MUST cast the value column to DOUBLE: `CAST(col AS DOUBLE)` before passing to AI_FORECAST.\n",
    "\n",
    "**DATE/TIME INTERVAL ERRORS:**\n",
    "- `[INVALID_PARAMETER_VALUE.DATETIME_UNIT]` → date_add or other interval functions used with quoted units\n",
    "- **FIX**: Never quote units. Use `date_add(DAY, 7, some_date)` or `add_months(some_date, -3)`. Do NOT call `date_add('MONTH', ...)`.\n",
    "- `DATEDIFF` only takes two arguments (`DATEDIFF(end_date, start_date)`). Do NOT pass a unit. Use `months_between` for month differences.\n",
    "\n",
    "**3. WINDOW FUNCTION ERRORS:**\n",
    "- `[INTERNAL_ERROR] Cannot evaluate expression: corr(...)` → Aggregate window functions cannot use ROWS BETWEEN frames\n",
    "- **FIX**: Remove `ROWS BETWEEN` clause: `CORR(col1, col2) OVER (PARTITION BY group)` not `CORR(col1, col2) OVER (PARTITION BY group ROWS BETWEEN...)`\n",
    "- **FIX**: Apply to ALL aggregate window functions: AVG, CORR, COVAR_POP, COVAR_SAMP, PERCENTILE_APPROX, STDDEV, VARIANCE, etc.\n",
    "- **CRITICAL**: NEVER use ROWS BETWEEN or RANGE BETWEEN with these functions in window specifications\n",
    "- **DECIMAL WINDOW AGGREGATES**: For AVG/STDDEV/CORR/COVAR on DECIMAL columns in windows, CAST inputs to DOUBLE to avoid internal evaluation errors.\n",
    "- `[DISTINCT_WINDOW_FUNCTION_UNSUPPORTED]` → `COUNT(DISTINCT col) OVER (...)` is NOT supported\n",
    "- **FIX**: Use a subquery/CTE to pre-aggregate using GROUP BY, then window over the result.\n",
    "- **FIX (Alternative)**: `size(collect_set(col) OVER (...))` (only for small data volumes).\n",
    "\n",
    "**4. GROUP BY ERRORS:**\n",
    "- `[MISSING_AGGREGATION]` → Non-aggregated columns in SELECT must be in GROUP BY\n",
    "- **FIX**: Add missing column OR expression to GROUP BY: `GROUP BY col1, col2, complex_expression`\n",
    "- **FIX**: Wrap non-aggregated column in `ANY_VALUE()` if it's constant per group.\n",
    "- Example: `SELECT customer_id, region, SUM(sales) ... GROUP BY customer_id, region` (not just customer_id)\n",
    "\n",
    "**5. TYPE MISMATCH ERRORS:**\n",
    "- `[DATATYPE_MISMATCH.DATA_DIFF_TYPES] Cannot resolve coalesce(bool_col, 'N')` → Mixing BOOLEAN and STRING types\n",
    "- **FIX**: Cast to common type: `COALESCE(CAST(bool_col AS STRING), 'N')` or `COALESCE(bool_col, FALSE)`\n",
    "\n",
    "**6. COLUMN RESOLUTION ERRORS:**\n",
    "- `[UNRESOLVED_COLUMN.WITH_SUGGESTION]` → Column doesn't exist in the table/CTE, use suggested column or join to get it\n",
    "- `[TABLE_OR_VIEW_NOT_FOUND]` → Table reference is wrong, check catalog.schema.table format\n",
    "\n",
    "**OUTPUT FORMAT:**\n",
    "Return ONLY the corrected SQL query with NO explanations, NO markdown fences around the SQL, NO commentary.\n",
    "\n",
    "**EXAMPLE 1 - UNRESOLVED COLUMN:**\n",
    "\n",
    "Original SQL with error:\n",
    "```sql\n",
    "SELECT service_type FROM forecast_results\n",
    "```\n",
    "\n",
    "Execution Error:\n",
    "```\n",
    "[UNRESOLVED_COLUMN] service_type cannot be resolved\n",
    "```\n",
    "\n",
    "Corrected SQL:\n",
    "```sql\n",
    "-- Error fixed: Column service_type not available in forecast results\n",
    "-- Fix: Join back to original table to get service_type\n",
    "SELECT \n",
    "  f.*,\n",
    "  t.service_type\n",
    "FROM `catalog`.`schema`.`forecast_results` AS f\n",
    "LEFT JOIN `catalog`.`schema`.`original_table` AS t\n",
    "  ON f.airport_code = t.airport_code\n",
    "```\n",
    "\n",
    "**EXAMPLE 2 - WINDOW FUNCTION ERROR:**\n",
    "\n",
    "Original SQL with error:\n",
    "```sql\n",
    "SELECT \n",
    "  aircraft_id,\n",
    "  CORR(flight_hours, maintenance_cost) OVER (PARTITION BY aircraft_id ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS correlation\n",
    "FROM aircraft_data\n",
    "```\n",
    "\n",
    "Execution Error:\n",
    "```\n",
    "[INTERNAL_ERROR] Cannot evaluate expression: corr(input[22, double, true], input[23, double, true]) windowspecdefinition(input[5, string, false], specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) SQLSTATE: XX000\n",
    "```\n",
    "\n",
    "Corrected SQL:\n",
    "```sql\n",
    "-- Error fixed: CORR function cannot use ROWS BETWEEN frame with window\n",
    "-- Fix: Remove ROWS BETWEEN clause for aggregate window functions\n",
    "SELECT \n",
    "  a.aircraft_id,\n",
    "  CORR(a.flight_hours, a.maintenance_cost) OVER (PARTITION BY a.aircraft_id) AS correlation\n",
    "FROM `catalog`.`schema`.`aircraft_data` AS a\n",
    "```\n",
    "\n",
    "\uD83D\uDEA8 ABSOLUTE RULE - OUTPUT FORMAT \uD83D\uDEA8:\n",
    "\n",
    "❌ DO NOT INCLUDE:\n",
    "- Any conversational or explanatory text before or after the SQL\n",
    "- Markdown code fences (```)\n",
    "- Any thoughts or reasoning\n",
    "\n",
    "✅ OUTPUT REQUIREMENTS:\n",
    "- Return ONLY the corrected SQL query\n",
    "- Start with honesty comments: -- HONESTY_SCORE: XX and -- HONESTY_JUSTIFICATION: ...\n",
    "- Then: -- Error fixed: [brief description of what was wrong]\n",
    "- Keep ALL original business logic intact\n",
    "- Only fix the specific error mentioned in the validation/execution error\n",
    "\n",
    "Begin your corrected SQL now:\n",
    "\"\"\" + HONESTY_CHECK_SQL\n",
    "\n",
    "# --- 1g-2. Interpret User SQL Regeneration Instructions Prompt (NEW) ---\n",
    "PROMPT_TEMPLATES[\"INTERPRET_USER_SQL_REGENERATION_PROMPT\"] = \"\"\"You are a **Senior Data Engineer and SQL Architect** with deep expertise in interpreting business requirements and translating them into technical specifications for SQL generation.\n",
    "\n",
    "**YOUR TASK**: Interpret the user's SQL regeneration instructions and produce a structured output that will guide the SQL generation process.\n",
    "\n",
    "**USE CASE INFORMATION:**\n",
    "- **Use Case ID**: {use_case_id}\n",
    "- **Use Case Name**: {use_case_name}\n",
    "- **Business Domain**: {business_domain}\n",
    "- **Statement**: {statement}\n",
    "- **Solution**: {solution}\n",
    "- **Original Tables Involved**: {original_tables_involved}\n",
    "\n",
    "**PREVIOUS SQL QUERY (if exists):**\n",
    "```sql\n",
    "{previous_sql}\n",
    "```\n",
    "\n",
    "**AVAILABLE TABLES IN THE REGISTRY (you can request to include any of these):**\n",
    "{available_tables_registry}\n",
    "\n",
    "**USER'S REGENERATION INSTRUCTIONS:**\n",
    "{user_regeneration_instructions}\n",
    "\n",
    "---\n",
    "\n",
    "**YOUR ANALYSIS TASK:**\n",
    "\n",
    "1. **UNDERSTAND THE USER'S INTENT**: What is the user trying to achieve? What changes do they want?\n",
    "\n",
    "2. **IDENTIFY TABLE CHANGES**:\n",
    "   - **New Tables to Add**: If the user mentions tables that are not in \"Original Tables Involved\" but ARE in the \"Available Tables Registry\", list them\n",
    "   - **Tables to Remove**: If the user explicitly asks to exclude certain tables\n",
    "   - **Tables to Keep**: Which original tables should remain\n",
    "\n",
    "3. **EXTRACT TECHNICAL REQUIREMENTS**:\n",
    "   - What specific columns, joins, or aggregations does the user want?\n",
    "   - What filtering conditions are implied?\n",
    "   - What statistical or AI functions should be used?\n",
    "\n",
    "4. **FORMULATE GENERATION INSTRUCTIONS**:\n",
    "   - Create a clear, technical specification that the SQL generator can follow\n",
    "\n",
    "---\n",
    "\n",
    "**OUTPUT FORMAT (JSON):**\n",
    "\n",
    "You MUST return a valid JSON object with the following structure:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"interpretation_summary\": \"Brief summary of what the user wants\",\n",
    "  \"tables_to_add\": [\"catalog.schema.table1\", \"catalog.schema.table2\"],\n",
    "  \"tables_to_remove\": [\"catalog.schema.table3\"],\n",
    "  \"final_tables_involved\": [\"catalog.schema.table1\", \"catalog.schema.table2\", \"catalog.schema.table4\"],\n",
    "  \"new_tables_need_loading\": true,\n",
    "  \"technical_design_instructions\": \"Detailed technical instructions for SQL generation including: specific joins needed, aggregations, filters, AI functions to use, statistical calculations, etc.\",\n",
    "  \"column_focus\": [\"specific_column1\", \"specific_column2\"],\n",
    "  \"special_requirements\": \"Any special requirements like specific date ranges, business rules, etc.\"\n",
    "}}\n",
    "```\n",
    "\n",
    "**RULES:**\n",
    "1. **ONLY** include tables in \"tables_to_add\" if they are present in the \"Available Tables Registry\"\n",
    "2. If the user asks for a table that is NOT in the registry, set `\"new_tables_need_loading\": true` and include a note in \"special_requirements\"\n",
    "3. \"final_tables_involved\" should be the complete list of tables for the regenerated query (original + added - removed)\n",
    "4. Be precise in \"technical_design_instructions\" - the SQL generator will follow these exactly\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL: Return ONLY the JSON object, no markdown fences, no explanation before or after.**\n",
    "\n",
    "Begin your JSON response now:\n",
    "\"\"\"\n",
    "\n",
    "# --- 1f. Use Case Review Prompt (RENAMED, ENHANCED) ---\n",
    "PROMPT_TEMPLATES[\"REVIEW_USE_CASES_PROMPT\"] = \"\"\"You are an expert business analyst specializing in duplicate detection. Your SINGLE task is to identify and remove semantic duplicates **and** to reject useless/technical use cases that add no business value.\n",
    "\n",
    "**\uD83D\uDEA8 SINGLE FOCUS: DUPLICATE DETECTION ONLY \uD83D\uDEA8**\n",
    "- **PRIMARY JOB**: Identify and remove semantic duplicates based on Name and core concept similarity\n",
    "- **SECONDARY GUARDRAIL**: Reject use cases that are trivial (no business outcome) or purely technical/infra-focused\n",
    "- **FOCUS**: Keep only distinct, business-outcome-focused use cases\n",
    "\n",
    "**\uD83D\uDD25 BE EXTREMELY AGGRESSIVE IN DUPLICATE DETECTION \uD83D\uDD25**\n",
    "\n",
    "**\uD83D\uDEAB ADDITIONAL FILTERS (MANDATORY) \uD83D\uDEAB**\n",
    "- Remove **TRIVIAL** use cases that have no business value (e.g., \"count rows\", \"list tables\", \"display schema\", \"dump data\") unless clearly tied to a business decision\n",
    "- Remove **TECHNICAL/INFRA** use cases that only deliver platform/IT value (monitoring pipelines, cluster/job status, DevOps telemetry) with no clear business beneficiary\n",
    "- Keep only use cases that articulate a business outcome or decision; everything else is rejected\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: BUSINESS RELEVANCY & REALISM CHECK \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "- Remove **IRRELEVANT CORRELATIONS**: Use cases that correlate variables with NO logical, provable cause-and-effect relationship\n",
    "- Remove **NONSENSICAL EXTERNAL DATA**: Use cases that add external data enrichment without a clear, industry-recognized business connection to the metric being analyzed\n",
    "- Remove **FAR-FETCHED CONNECTIONS**: Use cases where the relationship between factors would be questioned by domain experts or laughed out of a boardroom\n",
    "- **ASK FOR EACH USE CASE**: \"Can I explain in ONE sentence why these variables/factors are logically connected?\" If NO, REMOVE IT.\n",
    "- **BOARDROOM TEST**: Would a senior executive approve budget for this analysis without questioning the logic? If the correlation seems invented or far-fetched, REMOVE IT.\n",
    "\n",
    "**DUPLICATE DETECTION RULES** (Apply ALL of these):\n",
    "1. **Semantic Duplicates**: Names that mean the same thing with different wording\n",
    "   - Pattern: \"Forecast [X]\" = \"Predict [X]\" = \"[X] Forecasting\" = \"[X] Prediction\" → ALL DUPLICATES\n",
    "2. **Synonym Duplicates**: Names using synonyms for the same concept\n",
    "   - Pattern: \"Classify [Entity] [Attribute]\" = \"Categorize [Entity] [Attribute]\" = \"[Entity] [Attribute] Classification\" → ALL DUPLICATES\n",
    "3. **Action-Object Duplicates**: Same action on same object, different phrasing\n",
    "   - Pattern: \"Analyze [Entity] [Metric]\" = \"[Metric] Analysis for [Entity]\" = \"[Entity] [Metric] Analysis\" → ALL DUPLICATES\n",
    "4. **Abbreviation Duplicates**: Full form and abbreviated versions\n",
    "   - Pattern: \"AI-Powered [Feature]\" = \"[Feature]\" → DUPLICATES\n",
    "5. **Similar Core Concepts**: Names that address the same core business problem even with slightly different wording\n",
    "   - Pattern: \"Match Similar [Entity] Records\" = \"Find Similar [Entity] Profiles\" = \"Identify Similar [Entity]\" → ALL DUPLICATES\n",
    "   - Pattern: \"Detect Fraudulent [X]\" = \"Identify Fraud in [X]\" = \"Find Fraudulent [X]\" → ALL DUPLICATES\n",
    "\n",
    "**EXAMPLES OF DUPLICATES TO REMOVE** (keep only ONE from each group):\n",
    "- \"Forecast Revenue\", \"Predict Revenue\", \"Revenue Forecasting\", \"Revenue Prediction\" → Keep FIRST occurrence ONLY\n",
    "- \"Classify Support Tickets\", \"Categorize Support Tickets\", \"Support Ticket Classification\", \"Ticket Categorization\" → Keep FIRST occurrence ONLY\n",
    "- \"Detect Fraud\", \"Fraud Detection\", \"Identify Fraudulent Transactions\", \"Find Fraud Cases\" → Keep FIRST occurrence ONLY\n",
    "- \"Optimize Inventory\", \"Inventory Optimization\", \"Improve Inventory Management\", \"Enhance Inventory Control\" → Keep FIRST occurrence ONLY\n",
    "- \"Match Similar Passenger Records\", \"Find Similar Passenger Profiles\", \"Identify Similar Passengers\" → Keep FIRST occurrence ONLY\n",
    "\n",
    "**EXAMPLES OF NON-DUPLICATES TO KEEP** (these are DIFFERENT):\n",
    "- \"Forecast Sales Revenue\" vs \"Forecast Customer Demand\" → Different objects (revenue vs demand), KEEP BOTH\n",
    "- \"Classify Customer Feedback\" vs \"Classify Product Reviews\" → Different data sources (customer feedback vs product reviews), KEEP BOTH\n",
    "- \"Predict Customer Churn\" vs \"Predict Sales Trends\" → Different predictions (churn vs sales), KEEP BOTH\n",
    "- \"Count Total Orders\" vs \"Forecast Order Volume\" → Different actions (counting vs forecasting), KEEP BOTH\n",
    "- \"Extract Customer Name\" vs \"Classify Customer Segment\" → Different operations (extraction vs classification), KEEP BOTH\n",
    "\n",
    "**ONLY REMOVE IF TRULY DUPLICATE OR VALUELESS:**\n",
    "- ❌ REMOVE: \"Forecast Revenue\" AND \"Predict Revenue\" → DUPLICATES (same action, same object)\n",
    "- ❌ REMOVE: \"Classify Feedback\" AND \"Categorize Feedback\" → DUPLICATES (synonyms)\n",
    "- ❌ REMOVE: \"Analyze Customer Sentiment\" AND \"Customer Sentiment Analysis\" → DUPLICATES (same concept)\n",
    "- ❌ REMOVE: \"Monitor ETL Pipeline\" / \"Check Job Status\" / \"List Tables\" → TECHNICAL/NO BUSINESS VALUE\n",
    "- ✅ KEEP: \"Count Orders\" AND \"Forecast Orders\" → NOT DUPLICATES (different actions and business value)\n",
    "- ✅ KEEP: \"Extract Names\" AND \"Classify Segments\" → NOT DUPLICATES (different operations)\n",
    "\n",
    "**YOUR MANDATE:**\n",
    "- Remove semantic duplicates\n",
    "- Remove trivial/no-business-value use cases\n",
    "- Remove technical/platform/infra-only use cases that do not deliver business outcomes\n",
    "- Keep the remaining distinct, business-oriented use cases\n",
    "\n",
    "**IMPORTANT RULES**:\n",
    "- When duplicates are found, keep the FIRST occurrence (earliest ID)\n",
    "- Be EXTREMELY AGGRESSIVE in detecting semantic duplicates - err on the side of removing duplicates\n",
    "- Remove trivial/technical items even if they are unique\n",
    "- You are reviewing ALL {total_count} use cases in one pass\n",
    "- **TARGET: Remove 20-30% as duplicates PLUS any trivial/technical items**\n",
    "\n",
    "**OUTPUT FORMAT: CSV (NOT JSON)**:\n",
    "Your output **MUST** be a simple CSV with ONE column (with header) containing the 'ID' of every use case to KEEP.\n",
    "Do NOT include any text, explanation, or markdown - ONLY the CSV.\n",
    "\n",
    "Example Input:\n",
    "| ID | Name | Business Value | Tables |\n",
    "|---|---|---|---|\n",
    "| AI-001 | Forecast Sales Revenue | Enables data-driven financial planning | sales.orders, sales.products |\n",
    "| AI-002 | Classify Support Tickets | Automates ticket routing and prioritization | support.tickets |\n",
    "| AI-003 | Predict Sales | Improves forecasting | sales.orders |\n",
    "| AI-004 | Sales Forecasting | Better predictions | sales.orders |\n",
    "| AI-005 | Categorize Support Tickets | Routes tickets faster | support.tickets |\n",
    "| AI-006 | Count Database Records | View data | system.metadata |\n",
    "| AI-007 | Extract Refund Type | Gets refund category | refunds.transactions |\n",
    "\n",
    "(Assuming refunds.transactions has columns: refund_id, refund_reason_text, refund_type)\n",
    "\n",
    "Example Output (keeping high-value, non-duplicates):\n",
    "use_case_id\n",
    "AI-001\n",
    "AI-002\n",
    "\n",
    "**Removal Rationale**:\n",
    "- AI-003, AI-004: Duplicates of AI-001 (same concept: sales forecasting)\n",
    "- AI-005: Duplicate of AI-002 (same concept: ticket classification)\n",
    "- AI-006: KEPT (not a duplicate, even if trivial - scoring will handle value assessment)\n",
    "- AI-007: KEPT (not a duplicate, even if useless - scoring will handle value assessment)\n",
    "\n",
    "Here is the markdown table of ALL use cases to analyze:\n",
    "{use_case_markdown}\n",
    "\n",
    "Produce ONLY the valid CSV with one column of IDs to keep. Be EXTREMELY AGGRESSIVE in removing:\n",
    "1. Semantic duplicates (keep first occurrence only)\n",
    "\n",
    "Target: Remove 20-30% as duplicates, keep 70-80% for scoring.\n",
    "\n",
    "\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 OUTPUT INSTRUCTION \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8:\n",
    "- START with CSV header: use_case_id\n",
    "- Follow with one ID per line\n",
    "- NO text before or after the CSV\n",
    "- NO explanations, NO markdown fences\n",
    "- NO thoughts or reasoning\n",
    "- NO \"I have reviewed...\" or similar statements\n",
    "- ONLY the pure CSV data\n",
    "\n",
    "\uD83D\uDEA8 ABSOLUTE RULE - OUTPUT FORMAT \uD83D\uDEA8:\n",
    "\n",
    "❌ DO NOT INCLUDE:\n",
    "- Any conversational or explanatory text (\"I have...\", \"After...\", \"Based on...\", \"Here are...\")\n",
    "- Any thoughts or analysis descriptions\n",
    "\n",
    "✅ OUTPUT REQUIREMENTS:\n",
    "- Your response must START with: use_case_id,honesty_score,honesty_justification\n",
    "- Include honesty columns in header and all rows\n",
    "\"\"\" + HONESTY_CHECK_CSV\n",
    "\n",
    "# --- 1g. Use Case Scoring Prompt (COMPREHENSIVE WITH BUSINESS CONTEXT) ---\n",
    "PROMPT_TEMPLATES[\"SCORE_USE_CASES_PROMPT\"] = \"\"\"# Persona\n",
    "\n",
    "You are the **Chief Investment Officer & Strategic Value Architect**. You are known for being ruthless, evidence-based, and ROI-obsessed. You do not care about \"cool tech\" or \"easy wins\" unless they drive massive financial impact. Your job is to allocate finite capital only to use cases that drive the specific strategic goals of this business.\n",
    "\n",
    "# Context & Inputs\n",
    "\n",
    "**Business Context:** {business_context}\n",
    "**Strategic Goals:** {strategic_goals}\n",
    "**Business Priorities:** {business_priorities}\n",
    "**Strategic Initiative:** {strategic_initiative}\n",
    "**Value Chain:** {value_chain}\n",
    "**Revenue Model:** {revenue_model}\n",
    "\n",
    "**Use Cases to Score:**\n",
    "{use_case_markdown}\n",
    "\n",
    "# Instructions\n",
    "\n",
    "### Task Overview\n",
    "\n",
    "Score the provided use cases based strictly on their potential to drive the specific **Business Priorities**, **Strategic Goals**, and **Revenue Model** provided in the **Context & Inputs** section above.\n",
    "\n",
    "\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 **CRITICAL: BUSINESS PRIORITIES DRIVE RANKING** \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\n",
    "\n",
    "Use cases that directly achieve any of the Business Priorities MUST receive significantly higher scores. Strategic Goals define the intended outcomes and must be used for alignment.\n",
    "\n",
    "**Common Strategic Goals to Consider:**\n",
    "- **Reduce Cost**: Automation, efficiency improvements, waste reduction\n",
    "- **Boost Productivity**: Faster processes, better tools, streamlined workflows  \n",
    "- **Increase Revenue**: New revenue streams, upselling, cross-selling, market expansion\n",
    "- **Mitigate Risk**: Fraud detection, compliance, security, audit trails\n",
    "- **Protect Revenue**: Churn prevention, retention, customer satisfaction\n",
    "- **Align to Regulations**: Compliance automation, regulatory reporting, audit support\n",
    "- **Improve Customer Experience**: Personalization, faster service, quality improvements\n",
    "- **Enable Data-Driven Decisions**: Analytics, insights, forecasting, predictions\n",
    "\n",
    "**For EVERY use case, you MUST:**\n",
    "1. Identify which Strategic Goal(s) it aligns to (if any)\n",
    "2. Score higher if it DIRECTLY achieves a stated Business Priority\n",
    "3. In the justification, EXPLICITLY mention which Business Priority and Strategic Goal(s) the use case supports\n",
    "\n",
    "**\uD83D\uDEA8 SCORING RULES: AGGRESSIVE BUSINESS VALUE \uD83D\uDEA8**\n",
    "\n",
    "1.  **NO CURVE / NO DISTRIBUTION:** Do not force a normal distribution. If all use cases are weak, score them all low. If all are critical, score them all high. Score based on **Absolute Merit**.\n",
    "2.  **ZERO-BASED SCORING:** Start every score at 1.0. The use case must *earn* points by showing explicit alignment to the data provided in the Context. Do not assume value exists unless clearly demonstrated.\n",
    "3.  **IGNORE \"NICE TO HAVES\":** If a use case improves a process that does not directly impact revenue, margin, or strategic competitive advantage, it is a **Low Value** case, regardless of how easy it is to implement.\n",
    "4.  **STRATEGIC GOAL BONUS:** Use cases that DIRECTLY achieve a stated Strategic Goal should receive a +0.5 to +1.0 bonus to their Strategic Alignment score.\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: BUSINESS RELEVANCY & REALISM PENALTY \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "5.  **IRRELEVANT CORRELATIONS = LOW SCORE:** Use cases that correlate variables with NO logical, provable cause-and-effect relationship MUST receive low scores (ROI ≤ 2.0, Alignment ≤ 2.0).\n",
    "6.  **NONSENSICAL EXTERNAL DATA = LOW SCORE:** Use cases that include external data enrichment without a clear, industry-recognized business connection MUST be penalized heavily.\n",
    "7.  **RELEVANCY TEST:** For EVERY use case, ask: \"Can I explain in ONE sentence why these variables/factors are logically connected?\" If NO, score LOW.\n",
    "8.  **BOARDROOM TEST:** Would a senior executive approve budget for this analysis without questioning the logic? If the correlation seems invented or far-fetched, score LOW and note in the justification: \"Irrelevant correlation - no logical business connection.\"\n",
    "\n",
    "### The 75/25 Priority Formula (Critical)\n",
    "\n",
    "You must use a weighted formula for the final Priority Score to heavily favor Business Value over Feasibility.\n",
    "\n",
    "1.  **Calculate Value Score (1.0 - 5.0):** Weighted average of ROI (60%), Alignment (25%), TTV (7.5%), Reusability (7.5%).\n",
    "2.  **Calculate Feasibility Score (1.0 - 5.0):** Simple average of the 8 feasibility factors.\n",
    "3.  **CALCULATE PRIORITY SCORE (2.0 - 10.0):**\n",
    "    \n",
    "    $$ Priority = (Value * 1.5) + (Feasibility * 0.5) $$\n",
    "\n",
    "*Note: This formula ensures that Business Value accounts for 7.5 points of the total score, while Feasibility only accounts for 2.5 points.*\n",
    "\n",
    "---\n",
    "\n",
    "### Scoring Factors (Detailed Assessment Criteria)\n",
    "\n",
    "**I. VALUE FACTORS (The \"Why\")**\n",
    "\n",
    "**1. Return on Investment (ROI)** \uD83D\uDEA8 **WEIGHT: 60% of Value Score** \uD83D\uDEA8\n",
    "    * **Contextual ROI Check:** Compare the use case against the **Revenue Model** listed in the Context. Does this use case directly impact the way this specific company makes money?\n",
    "    * **4.8 - 5.0 (Exponential):** Directly impacts top-line revenue or prevents massive bottom-line leakage (>10x return). *Examples: Dynamic Pricing, Demand Forecasting, Churn Prevention for high-value customers.*\n",
    "    * **4.0 - 4.7 (High):** Significant measurable impact on P&L (5-10x return). *Examples: Supply Chain Optimization, Fraud Detection, Predictive Maintenance.*\n",
    "    * **3.0 - 3.9 (Moderate):** Incremental efficiency gains (2-5x return). *Examples: Automated Invoice Processing, Intelligent Document Classification.*\n",
    "    * **1.0 - 2.9 (Low/Soft):** \"Soft\" benefits (efficiency, happiness) that do not clearly translate to dollars in the **Revenue Model**. *Examples: Internal Wiki Search, Employee Sentiment Dashboard.*\n",
    "    * **CRITICAL**: Evaluate ROI based on the ACTUAL industry and business model from the provided context - do NOT assume any specific industry.\n",
    "\n",
    "**2. Strategic Alignment** \uD83D\uDEA8 **WEIGHT: 25% of Value Score** \uD83D\uDEA8\n",
    "    * **Strict Alignment Check:** Look at the **Business Priorities** and **Strategic Goals** listed in the Context. Is this use case mentioned?\n",
    "    * **4.8 - 5.0 (Direct Hit):** The use case is EXPLICITLY named in or required by the **Business Priorities** or **Strategic Goals**. *Pattern: If priority mentions \"[X]\", use case directly addresses \"[X]\".*\n",
    "    * **3.5 - 4.7 (Strong Link):** Supports a stated **Business Priority** directly. *Pattern: Priority is about retention/growth/efficiency, use case directly enables that outcome.*\n",
    "    * **1.0 - 3.4 (Weak/None):** Generic improvement (e.g., \"Better Reporting\") that does not touch the specific **Business Priorities** listed in the Context.\n",
    "    * **CRITICAL**: Evaluate alignment based on the ACTUAL Business Priorities and Strategic Goals provided in the context - do NOT assume any default goals.\n",
    "\n",
    "**3. Time to Value (TTV)** (Weight: 7.5%)\n",
    "    * **Definition:** How fast until the business *sees* the money?\n",
    "    * **4.8 - 5.0 (Immediate):** < 4 weeks. Quick wins, dashboarding existing data.\n",
    "    * **3.0 - 4.7 (Quarterly):** 1-3 months. Standard agile cycle.\n",
    "    * **1.0 - 2.9 (Long Term):** > 6 months. Long infrastructure build-outs before any value is realized.\n",
    "\n",
    "**4. Reusability** (Weight: 7.5%)\n",
    "    * **Definition:** Does this create a permanent asset (Feature Store, Data Product)?\n",
    "    * **4.8 - 5.0 (Platform Asset):** Creates a \"Customer 360\" or \"Product Master\" table that 10+ other use cases *will* leverage.\n",
    "    * **3.0 - 4.7 (Modular):** Code is clean and reusable, but data is specific to this use case.\n",
    "    * **1.0 - 2.9 (One-Off):** Ad-hoc analysis or script solving exactly one isolated problem.\n",
    "\n",
    "**II. FEASIBILITY FACTORS (The \"How\" - Average of all 8)**\n",
    "\n",
    "**5. Data Availability**\n",
    "    * **Check:** Does the specific data required exist in this industry/business context?\n",
    "    * **4.8 - 5.0 (Perfect):** Data is standard, transactional, and historically logged (e.g., Sales Records, ERP logs).\n",
    "    * **3.0 - 4.7 (Likely):** Data likely exists but might be scattered or require some engineering to consolidate.\n",
    "    * **1.0 - 2.9 (Missing):** Requires new sensors, external purchases, or starting logs from scratch.\n",
    "\n",
    "**6. Data Accessibility**\n",
    "    * **Check:** Are there Legal, Privacy, or Tech barriers?\n",
    "    * **4.8 - 5.0 (Open):** Internal, non-PII, open access data.\n",
    "    * **3.0 - 4.7 (Restricted):** PII present but manageable via standard RBAC/Masking.\n",
    "    * **1.0 - 2.9 (Blocked):** Highly sensitive (Medical/Banking) or owned by a 3rd party refusing to share.\n",
    "\n",
    "**7. Architecture Fitness**\n",
    "    * **Check:** Does it fit the standard Lakehouse/Spark stack?\n",
    "    * **4.8 - 5.0 (Native):** Solvable using standard SQL/Python. Fits Medallion Architecture perfectly.\n",
    "    * **3.0 - 4.7 (Adaptable):** Requires specific library installs or external API calls.\n",
    "    * **1.0 - 2.9 (Incompatible):** Requires mainframe, on-prem appliances, or non-cloud tech.\n",
    "\n",
    "**8. Team Skills**\n",
    "    * **Check:** Does a typical team in this industry have these skills?\n",
    "    * **4.8 - 5.0 (Standard):** SQL, Python, Basic Regression/Classification.\n",
    "    * **3.0 - 4.7 (Specialized):** NLP, Computer Vision, GenAI prompting.\n",
    "    * **1.0 - 2.9 (Niche):** Requires PhD-level Research Math or archaic languages (COBOL).\n",
    "\n",
    "**9. Domain Knowledge**\n",
    "    * **Check:** Is the business logic clear?\n",
    "    * **4.8 - 5.0 (Documented):** Logic is clear, rules are written, SMEs are available.\n",
    "    * **3.0 - 4.7 (Tribal):** \"Head knowledge\" exists but isn't written down.\n",
    "    * **1.0 - 2.9 (Unknown):** Logic is a \"Black Box\" or lost.\n",
    "\n",
    "**10. People Allocation**\n",
    "    * **Check:** Staffing difficulty.\n",
    "    * **4.8 - 5.0 (Minimal):** 1-2 Engineers.\n",
    "    * **3.0 - 4.7 (Squad):** Full agile squad (4-6 people).\n",
    "    * **1.0 - 2.9 (Heavy):** Requires massive cross-functional teams or external consultants.\n",
    "\n",
    "**11. Budget Allocation**\n",
    "    * **Check:** Likelihood of funding.\n",
    "    * **4.8 - 5.0 (Secured):** Critical path for the **Strategic Initiative** listed in the Context.\n",
    "    * **3.0 - 4.7 (Discretionary):** Funded via normal OPEX.\n",
    "    * **1.0 - 2.9 (CapEx Required):** Needs board approval for new money.\n",
    "\n",
    "**12. Time to Production**\n",
    "    * **Check:** Engineering effort magnitude.\n",
    "    * **4.8 - 5.0 (Sprint):** < 2 weeks dev time.\n",
    "    * **3.0 - 4.7 (Quarterly):** 1-3 months dev time.\n",
    "    * **1.0 - 2.9 (Major Project):** > 6 months dev time.\n",
    "\n",
    "-----\n",
    "\n",
    "### Scoring Methodology - Execution Steps\n",
    "\n",
    "**STEP 1: CALCULATE RAW VALUE (High Precision)**\n",
    "* Score ROI (0-5) based on the **Revenue Model** in the Context.\n",
    "* Score Alignment (0-5) based on the **Strategic Goals** in the Context.\n",
    "* Score TTV and Reusability.\n",
    "* Calculate:\n",
    "  $$ Value = (ROI * 0.60) + (Alignment * 0.25) + (TTV * 0.075) + (Reusability * 0.075) $$\n",
    "\n",
    "**STEP 2: CALCULATE RAW FEASIBILITY**\n",
    "* Average the 8 feasibility factors (Factors 5 through 12).\n",
    "* Calculate:\n",
    "  $$ Feasibility = (Sum of 8 Factors) / 8 $$\n",
    "\n",
    "**STEP 3: APPLY THE \"VALUE-FIRST\" PRIORITY FORMULA**\n",
    "* Calculate:\n",
    "  $$ Priority Score = (Value * 1.5) + (Feasibility * 0.5) $$\n",
    "* *Validation Logic:*\n",
    "    * If Value is 5.0 and Feasibility is 1.0 -> Priority = 8.0 (High Priority)\n",
    "    * If Value is 1.0 and Feasibility is 5.0 -> Priority = 4.0 (Low Priority)\n",
    "    * **This mathematically forces High Value cases to always outrank High Feasibility cases.**\n",
    "\n",
    "**STEP 4: GENERATE JUSTIFICATION**\n",
    "* Write a sharp, executive summary (max 200 chars).\n",
    "* **Must** reference specific **Strategic Goals** or **Revenue Model** elements found in the Context.\n",
    "* **Must** justify the score based on BUSINESS IMPACT, not technical ease.\n",
    "\n",
    "\uD83D\uDEA8 **JUSTIFICATION QUALITY RULES - CRITICAL** \uD83D\uDEA8:\n",
    "1. **USE CASE SPECIFIC**: The justification MUST be specific to THIS use case. It should mention the core capability or outcome (e.g., \"network congestion prediction\", \"churn prevention\", \"demand forecasting\").\n",
    "2. **NO GENERIC BUZZWORDS**: Do NOT use generic phrases that could apply to any use case. The following are PROHIBITED unless directly relevant to the use case domain:\n",
    "   - \"digital transformation\" (too vague)\n",
    "   - \"workflow automation\" (unless the use case IS about workflow automation)\n",
    "   - \"revenue recognition\" (unless the use case IS about revenue recognition)\n",
    "   - \"operational efficiency\" (too generic)\n",
    "   - \"data-driven insights\" (too vague)\n",
    "3. **CONNECT TO USE CASE DOMAIN**: If the use case is about \"Network Congestion Prediction\", the justification MUST mention network, capacity, congestion, or infrastructure concepts - NOT unrelated benefits like \"CSAT\" or \"invoice processing\".\n",
    "4. **EXAMPLES OF BAD vs GOOD JUSTIFICATIONS**:\n",
    "   - ❌ BAD for \"Predict Network Congestion\": \"Accelerates revenue recognition and supports digital transformation through workflow automation.\"\n",
    "   - ✅ GOOD for \"Predict Network Congestion\": \"Proactively prevents service degradation by predicting network hotspots, directly reducing churn and protecting recurring revenue from enterprise clients.\"\n",
    "   - ❌ BAD for \"Churn Prediction\": \"Improves operational efficiency and enables data-driven decision making.\"\n",
    "   - ✅ GOOD for \"Churn Prediction\": \"Identifies at-risk customers 30 days before cancellation, enabling targeted retention campaigns that protect $2M annual recurring revenue.\"\n",
    "\n",
    "-----\n",
    "\n",
    "### Output Format\n",
    "\n",
    "Return **ONLY** a valid CSV.\n",
    "\n",
    "**Columns:**\n",
    "\"No\",\"Strategic Alignment\",\"Return on Investment\",\"Reusability\",\"Time to Value\",\"Data Availability\",\"Data Accessibility\",\"Architecture Fitness\",\"Team Skills\",\"Domain Knowledge\",\"People Allocation\",\"Budget Allocation\",\"Time to Production\",\"Value\",\"Feasibility\",\"Priority Score\",\"Business Priority Alignment\",\"Strategic Goals Alignment\",\"Justification\",\"AI_Confidence\",\"AI_Feedback\"\n",
    "\n",
    "**CRITICAL - Business Priority Alignment Column:**\n",
    "For each use case, identify which business priority(ies) it aligns to. Use the following format:\n",
    "- If aligned to ONE priority: \"Reduce Cost\" or \"Increase Revenue\" etc.\n",
    "- If aligned to MULTIPLE priorities: \"Reduce Cost, Mitigate Risk\" (comma-separated)\n",
    "- If NO clear alignment: \"General Improvement\"\n",
    "\n",
    "Standard business priorities: Increase Revenue | Reduce Cost | Optimize Operations | Mitigate Risk | Empower Talent | Enhance Experience | Drive Innovation | Achieve ESG | Protect Revenue | Execute Strategy\n",
    "\n",
    "**CRITICAL - Strategic Goals Alignment Column:**\n",
    "For each use case, identify which strategic goal(s) it aligns to.\n",
    "- Look at the **Strategic Goals** provided in the Context section above (these are either user-provided OR generated from business context)\n",
    "- Match each use case to one or more strategic goals from the context\n",
    "- If aligned to ONE goal: output that goal exactly as written\n",
    "- If aligned to MULTIPLE goals: list them comma-separated\n",
    "- If the use case does NOT align to ANY of the strategic goals in context: output \"General Improvement\"\n",
    "- Be specific: use the EXACT wording of the strategic goals from the context\n",
    "\n",
    "**CRITICAL - AI_Confidence and AI_Feedback Columns (LAST 2 COLUMNS - MANDATORY):**\n",
    "For EACH use case, you MUST provide:\n",
    "- **AI_Confidence**: A decimal score from 0.0 to 1.0 representing your honesty score - how truthfully and completely you achieved this scoring task. Consider: data quality, domain expertise applied, clarity of the use case statement.\n",
    "- **AI_Feedback**: A comprehensive explanation that MUST include: 1) All reasons justifying your AI_Confidence score, 2) If score < 1.0, what specific improvements are needed to reach 1.0, 3) A MANDATORY \"MISSING DATA\" section listing all data/context that if provided would have improved your scoring accuracy. Be 100% honest - your output will be reviewed by another more powerful AI to judge your score.\n",
    "\n",
    "**Example Output:**\n",
    "```csv\n",
    "\"No\",\"Strategic Alignment\",\"Return on Investment\",\"Reusability\",\"Time to Value\",\"Data Availability\",\"Data Accessibility\",\"Architecture Fitness\",\"Team Skills\",\"Domain Knowledge\",\"People Allocation\",\"Budget Allocation\",\"Time to Production\",\"Value\",\"Feasibility\",\"Priority Score\",\"Business Priority Alignment\",\"Strategic Goals Alignment\",\"Justification\",\"AI_Confidence\",\"AI_Feedback\"\n",
    "\"AI-U001\",4.9,4.8,4.5,4.2,4.0,4.0,4.0,4.0,4.0,4.0,4.0,4.0,4.76,4.00,9.14,\"Increase Revenue, Reduce Cost\",\"Improve carbon footprint tracking, Optimize workforce efficiency\",\"Directly drives Revenue Growth by optimizing pricing engine. Achieves Increase Revenue priority and aligns to strategic goals.\",0.85,\"Score Justification: High confidence due to clear business value proposition and well-defined table relationships. Improvements Needed: Historical implementation success rates would raise score to 0.95. MISSING DATA: Industry benchmark ROI metrics, competitor pricing data, historical pricing model performance statistics.\"\n",
    "\"AI-U002\",1.2,1.5,2.0,3.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,5.0,1.58,5.00,4.87,\"General Improvement\",\"General Improvement\",\"Does not align with any stated Business Priorities or Strategic Goals. Purely administrative 'nice-to-have'.\",0.72,\"Moderate confidence. Use case statement is vague - would benefit from specific metrics and expected outcomes.\"\n",
    "\n",
    "```\n",
    "\n",
    "**OUTPUT RULES:**\n",
    "\n",
    "* Return ONLY the CSV data (with header row).\n",
    "* The `Priority Score` in the CSV MUST reflect the `(Value * 1.5) + (Feasibility * 0.5)` formula.\n",
    "* Do not normalize. Real scores only.\n",
    "* The `Business Priority Alignment` column MUST specify which business priority(ies) the use case achieves.\n",
    "* The `Strategic Goals Alignment` column MUST specify which strategic goal(s) from the context the use case achieves, or \"General Improvement\" if none match.\n",
    "* The `AI_Confidence` column MUST be a decimal between 0.0 and 1.0 (your honesty score).\n",
    "* The `AI_Feedback` column MUST include: reasons for score, improvements needed if < 1.0, and MISSING DATA section.\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: SCORE EVERY SINGLE USE CASE \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "* You MUST output a CSV row for EVERY use case in the input table\n",
    "* Count the use cases: If there are N use cases in the input, you MUST output EXACTLY N data rows (plus header)\n",
    "* DO NOT skip any use case. DO NOT truncate the output.\n",
    "* If you're running low on output space, use shorter justifications but NEVER omit rows\n",
    "* Missing use case scores = CRITICAL FAILURE\n",
    "\n",
    "Begin your CSV response now:\n",
    "\"\"\"\n",
    "# --- 1h. SQL Generation Prompt (ENHANCED - DATABRICKS SQL EXPERT) ---\n",
    "PROMPT_TEMPLATES[\"USE_CASE_SQL_GEN_PROMPT\"] = \"\"\"You are a **Principal Databricks SQL Engineer** and **AI/ML Solutions Architect** with 15+ years of experience. You are an absolute EXPERT in Databricks SQL dialect and AI functions. Your task is to generate SOPHISTICATED, PRODUCTION-READY, syntactically PERFECT SQL queries.\n",
    "\n",
    "**\uD83D\uDD25\uD83D\uDD25\uD83D\uDD25 CRITICAL: GENERATE COMPREHENSIVE SQL - NO ARTIFICIAL LIMITS \uD83D\uDD25\uD83D\uDD25\uD83D\uDD25**\n",
    "- There is **NO LINE LIMIT** for the SQL query - generate as many lines as needed\n",
    "- Generate as **MANY CTEs** as required to fully implement the use case (3-10 CTEs is normal)\n",
    "- A typical sophisticated query should be **200 to 600 lines** of code\n",
    "- Include **ALL** statistical functions, **ALL** AI functions, **ALL** transformations\n",
    "- Do **NOT** artificially shorten or simplify the SQL\n",
    "- Do **NOT** skip steps to reduce code length\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: FIRST CTE MUST USE SELECT DISTINCT \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "- The **FIRST CTE** MUST ALWAYS use `SELECT DISTINCT` to ensure NO DUPLICATE RECORDS\n",
    "- **WHY**: Duplicates in source data will cascade errors through all downstream CTEs\n",
    "- **PATTERN**: `WITH base_data AS (SELECT DISTINCT col1, col2, ... FROM table WHERE ... LIMIT 10)`\n",
    "- **ALTERNATIVE**: If aggregating, use `GROUP BY` on all non-aggregated columns\n",
    "- **VALIDATION**: Before any AI function or analysis, data MUST be deduplicated in the first CTE\n",
    "- **LIMIT PLACEMENT**: LIMIT 10 MUST be the LAST clause in the SELECT (after WHERE, ORDER BY, etc.)\n",
    "- **COMPLETENESS over brevity** - comprehensive analysis is the goal\n",
    "\n",
    "**\uD83C\uDFE2 BUSINESS CONTEXT (CRITICAL - READ THIS FIRST!):**\n",
    "- **Company/Customer Name**: {business_name}\n",
    "- **Business Context**: {enriched_business_context}\n",
    "- **Strategic Goals**: {enriched_strategic_goals}\n",
    "- **Business Priorities**: {enriched_business_priorities}\n",
    "- **Strategic Initiative**: {enriched_strategic_initiative}\n",
    "- **Value Chain**: {enriched_value_chain}\n",
    "- **Revenue Model**: {enriched_revenue_model}\n",
    "- This analysis is being generated FOR {business_name}\n",
    "- When generating external_api CTEs, get information ABOUT the entities in your data (customers, suppliers, locations), NOT about {business_name} itself\n",
    "- Example: If {business_name} is \"Databricks\" and you're analyzing Databricks' customers, get competitor info for THOSE CUSTOMERS, not for Databricks\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: ENRICH ALL PERSONAS WITH BUSINESS CONTEXT \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "Every persona in ai_query prompts MUST be enriched with the business context above. Do NOT use generic personas like \"You are a Chief Revenue Officer with 20 years of experience\". \n",
    "Instead, ALWAYS create business-specific personas like: \"You are a Chief Revenue Officer for {business_name} which is aiming to [use strategic goals and business context above] with 20 years of experience in [relevant domain].\"\n",
    "\n",
    "**PERSONA ENRICHMENT PATTERN (MANDATORY):**\n",
    "```sql\n",
    "-- ❌ WRONG - Generic persona without business context:\n",
    "ai_query('{sql_model_serving}',\n",
    "  CONCAT('You are a Chief Revenue Officer with 20 years of experience in enterprise software sales strategy. ',\n",
    "         'Analyze...'))\n",
    "\n",
    "-- ❌ WRONG - Empty/malformed business context placeholders:\n",
    "ai_query('{sql_model_serving}',\n",
    "  CONCAT('You are a Chief Revenue Officer for Acme Corp which is focused on General business operations. ',\n",
    "         'The organization''s strategic goals include: . ',  -- ❌ EMPTY! Must have actual goals\n",
    "         'Business priorities are: Digital transformation. ',\n",
    "         'Analyze...'))\n",
    "\n",
    "-- ✅ CORRECT - Persona enriched with ALL business context (NONE can be empty):\n",
    "ai_query('{sql_model_serving}',\n",
    "  CONCAT('You are a Chief Revenue Officer for {business_name} which is focused on {enriched_business_context}. ',\n",
    "         'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "         'Business priorities are: {enriched_business_priorities}. ',\n",
    "         'You have 20 years of experience in enterprise software sales strategy, revenue forecasting, and go-to-market planning. ',\n",
    "         'Your expertise in [relevant expertise] aligns with the strategic initiative: [relevant initiative from goals]. ',\n",
    "         'Analyze...'))\n",
    "```\n",
    "\n",
    "**\uD83D\uDEA8 PERSONA CONTEXT VALIDATION (ALL fields MUST have non-empty values):**\n",
    "- `{business_name}` - MUST be the actual company name, NEVER empty or \"Unknown\"\n",
    "- `{enriched_business_context}` - MUST describe what the business does, NEVER just \"General business operations\"\n",
    "- `{enriched_strategic_goals}` - MUST list actual strategic goals, NEVER empty (e.g., \"include: .\" is WRONG)\n",
    "- `{enriched_business_priorities}` - MUST list actual priorities, NEVER generic placeholders\n",
    "\n",
    "**ALL ai_query personas MUST include (in this order):**\n",
    "1. The business name: {business_name}\n",
    "2. Relevant business context: {enriched_business_context}\n",
    "3. Strategic goals alignment: {enriched_strategic_goals}\n",
    "4. Business priorities: {enriched_business_priorities}\n",
    "5. The professional role and years of experience\n",
    "6. Expertise alignment with a specific strategic initiative\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 MANDATORY: ai_sys_prompt COLUMN - CAPTURE THE EXACT PROMPT \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**EVERY SQL that uses ai_query MUST include `ai_sys_prompt` as the LAST column in the final output.**\n",
    "This column captures the exact prompt sent to the AI for auditability, debugging, and reproducibility.\n",
    "\n",
    "**PATTERN FOR ai_sys_prompt (MANDATORY):**\n",
    "```sql\n",
    "-- Step N: Build the AI prompt in a CTE (generate prompt as a column FIRST)\n",
    "prompt_generation AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    CONCAT(\n",
    "      'You are a [ROLE] for {business_name} which is focused on {enriched_business_context}. ',\n",
    "      'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "      'Business priorities are: {enriched_business_priorities}. ',\n",
    "      'You have [X] years of experience in [domain]. ',\n",
    "      'Your expertise in [specific expertise] aligns with the strategic initiative: [initiative]. ',\n",
    "      '[Analysis instructions...]',\n",
    "      'Output ONLY JSON...'\n",
    "    ) AS ai_sys_prompt  -- MUST be named ai_sys_prompt\n",
    "  FROM previous_cte\n",
    "),\n",
    "-- Step N+1: Call ai_query using the prompt column\n",
    "ai_analysis AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS insights_json\n",
    "  FROM prompt_generation\n",
    "),\n",
    "-- Final output: ai_sys_prompt MUST be the LAST column\n",
    "final_output AS (\n",
    "  SELECT \n",
    "    -- Data columns...\n",
    "    -- AI extracted columns (ai_cat_, ai_txt_)...\n",
    "    -- Mandatory system columns (ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data)...\n",
    "    ai_sys_prompt  -- MUST BE LAST COLUMN\n",
    "  FROM ai_analysis\n",
    ")\n",
    "```\n",
    "\n",
    "**WHY ai_sys_prompt IS MANDATORY:**\n",
    "1. **Auditability** - Know exactly what prompt generated each AI response\n",
    "2. **Debugging** - Identify prompt issues when AI output is unexpected\n",
    "3. **Reproducibility** - Recreate the exact analysis conditions\n",
    "4. **Compliance** - Document AI decision-making inputs for regulatory requirements\n",
    "5. **Optimization** - Analyze prompts to improve AI response quality\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: VALIDATE PERSONA PLACEHOLDERS ARE NOT EMPTY \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**Before generating SQL, VERIFY that these placeholders have actual values:**\n",
    "- `{business_name}` - MUST NOT be empty or \"Unknown\"\n",
    "- `{enriched_business_context}` - MUST NOT be empty or just \"General business operations\"\n",
    "- `{enriched_strategic_goals}` - MUST NOT be empty (e.g., \"include: .\" is INVALID!)\n",
    "- `{enriched_business_priorities}` - MUST NOT be empty or generic\n",
    "\n",
    "**❌ MALFORMED PERSONA EXAMPLE (THIS IS WRONG - EMPTY STRATEGIC GOALS!):**\n",
    "```sql\n",
    "CONCAT('You are a Director for Acme Corp which is focused on General business operations. ',\n",
    "       'The organization''s strategic goals include: . ',  -- ❌ EMPTY! This is malformed!\n",
    "       'Business priorities are: Digital transformation. ',\n",
    "       '...')\n",
    "```\n",
    "\n",
    "**✅ CORRECT PERSONA EXAMPLE (ALL FIELDS HAVE VALUES):**\n",
    "```sql\n",
    "CONCAT('You are a Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "       'The organization''s strategic goals include: {enriched_strategic_goals}. ',  -- ✅ Must have actual goals!\n",
    "       'Business priorities are: {enriched_business_priorities}. ',\n",
    "       '...')\n",
    "```\n",
    "\n",
    "**IF ANY PLACEHOLDER IS EMPTY:**\n",
    "1. The resulting prompt will be malformed and produce poor AI responses\n",
    "2. Check the upstream data source providing these values\n",
    "3. Add fallback defaults that are meaningful (NOT just \"Unknown\" or empty)\n",
    "\n",
    "**PLACEHOLDER VALIDATION CHECKLIST:**\n",
    "☐ `{business_name}` resolves to an actual company name\n",
    "☐ `{enriched_business_context}` describes what the business does (NOT \"General business operations\")\n",
    "☐ `{enriched_strategic_goals}` contains actual strategic goals (NOT empty, NOT just \". \")\n",
    "☐ `{enriched_business_priorities}` lists actual priorities\n",
    "☐ All placeholders produce meaningful text when substituted\n",
    "\n",
    "**USE CASE INFORMATION:**\n",
    "- **ID**: {use_case_id}\n",
    "- **Name**: {use_case_name}\n",
    "- **Business Domain**: {business_domain}\n",
    "- **Statement**: {statement}\n",
    "- **Solution**: {solution}\n",
    "- **Tables Involved**: {tables_involved}\n",
    "\n",
    "**Columns From Use Case (use exactly these, no additions):**\n",
    "{use_case_columns}\n",
    "- If blank, derive columns only from the provided schema context.\n",
    "- Every column you use must appear here and must belong to the tables above.\n",
    "- Exception: columns derived in `external_api_for_<scenario>` via ai_query are allowed and must be explicitly passed forward.\n",
    "\n",
    "**YOUR TASK**: Analyze the use case information above and identify the OPTIMAL combination of:\n",
    "1. **AI Functions** - Choose the best Databricks AI functions for the task\n",
    "2. **Statistical, Simulation & Advanced Analytics** - Use Monte Carlo, What-If, Geospatial, or Market Basket analysis to uncover hidden patterns (MUST be well-documented)\n",
    "\n",
    "You have full autonomy to innovate and mix these capabilities to deliver maximum business value.\n",
    "\n",
    "**\uD83D\uDD25\uD83D\uDD25\uD83D\uDD25 CRITICAL: COMPREHENSIVE STATISTICS - NO LAZINESS ALLOWED \uD83D\uDD25\uD83D\uDD25\uD83D\uDD25**\n",
    "\n",
    "**YOU MUST USE EVERY STATISTICAL FUNCTION FROM THE AVAILABLE STATISTICAL FUNCTIONS SECTION THAT APPLIES TO THE DATA.**\n",
    "\n",
    "Refer to the **AVAILABLE STATISTICAL FUNCTIONS** section below for the complete registry. You MUST use functions from ALL applicable categories:\n",
    "- Central Tendency, Dispersion, Distribution Shape, Percentiles\n",
    "- Trend Analysis, Correlation, Volatility, Outlier Detection\n",
    "- Ranking, Time Series\n",
    "\n",
    "**\uD83D\uDEAB ZERO TOLERANCE FOR LAZINESS \uD83D\uDEAB**:\n",
    "- \"I could add more statistics\" → UNACCEPTABLE! You MUST add them!\n",
    "- \"Additional metrics could help\" → UNACCEPTABLE! Include them NOW!\n",
    "- \"Basic analysis is sufficient\" → UNACCEPTABLE! We need COMPREHENSIVE!\n",
    "\n",
    "**✅ MANDATORY BEHAVIOR**:\n",
    "- Use ALL applicable functions from EVERY category in the statistical functions registry\n",
    "- Generate 15-25+ statistical metrics per analysis CTE\n",
    "- Feed statistical results into ai_query prompts for AI-enhanced insights\n",
    "- NEVER leave out a statistic that could reveal business value\n",
    "\n",
    "**\uD83D\uDCDD DOCUMENTATION REQUIREMENTS:**\n",
    "1.  **First CTE Filtering Guidance**: In the first CTE's WHERE clause, you MUST add a commented-out TODO line suggesting how to filter the data slice (e.g., `-- AND status = 'Active'`).\n",
    "2.  **Statistical CTE Documentation**: If you use complex statistical functions (REGR_SLOPE, CORR, etc.) or AI functions, you MUST add a comment block before the CTE explaining what the statistics represent and how they are calculated.\n",
    "\n",
    "**\uD83C\uDF10\uD83C\uDF10\uD83C\uDF10 REQUIRED FOR ACCURACY: EXTERNAL PUBLIC DATA ENRICHMENT CTE \uD83C\uDF10\uD83C\uDF10\uD83C\uDF10**\n",
    "\n",
    "**The `external_api_for_<scenario>` CTE is REQUIRED for generating accurate ai_txt_business_outcome calculations. This CTE provides market rates, benchmarks, and external factors that transform internal data into actionable business intelligence with measurable ROI.**\n",
    "\n",
    "**⚠️ WITHOUT external_api: Your analysis will lack market context, making ai_txt_business_outcome calculations less accurate and less credible.**\n",
    "**✅ WITH external_api: Your analysis includes verified market rates (fuel prices, labor costs, industry benchmarks) enabling precise ROI calculations.**\n",
    "\n",
    "**\uD83E\uDDE0 BEFORE GENERATING SQL, ASK YOURSELF THESE CRITICAL QUESTIONS:**\n",
    "\n",
    "1. **\"WHAT INFORMATION IS MISSING?\"** - What external context would explain WHY the patterns exist in the data? What would a human analyst naturally look up?\n",
    "\n",
    "2. **\"WHAT INFORMATION, IF ADDED, WOULD REVEAL NEW ANALYSIS AND PROVIDE VALUE?\"** - What public data would transform basic numbers into actionable intelligence?\n",
    "\n",
    "3. **\"WHAT WOULD MAKE THE LLM'S RECOMMENDATIONS MORE ACCURATE?\"** - What context would help the AI make better, more informed business recommendations?\n",
    "\n",
    "4. **\"WHAT WOULD A DOMAIN EXPERT BRING TO THIS ANALYSIS?\"** - What external knowledge would a 20-year industry veteran consider essential?\n",
    "\n",
    "5. **\"WHAT EXTERNAL CONTEXT WOULD ADD BUSINESS VALUE?\"** - Only include external data when there is a DIRECT, PROVABLE, INDUSTRY-RECOGNIZED cause-and-effect relationship with the metric. External data enrichment is valuable for ANY use case where the connection is RELEVANT and REALISTIC.\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: BUSINESS RELEVANCY REQUIREMENT FOR EXTERNAL DATA \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**BEFORE adding ANY external data enrichment, you MUST pass ALL of these tests:**\n",
    "1. Is there a DIRECT, PROVABLE cause-and-effect relationship between the external factor and the metric?\n",
    "2. Would a domain expert in this industry agree this connection is logical and valuable?\n",
    "3. Can you explain WHY the external factor impacts the metric in ONE clear sentence?\n",
    "4. Is this type of enrichment recognized and practiced in the industry?\n",
    "5. Would a senior executive approve this analysis without questioning the logic?\n",
    "\n",
    "**IF ANY ANSWER IS \"NO\" OR \"UNCERTAIN\", DO NOT INCLUDE THE EXTERNAL DATA.**\n",
    "\n",
    "**❌ STRICTLY PROHIBITED:**\n",
    "- Correlating variables that have NO logical business connection\n",
    "- Adding external factors that do NOT directly impact the metric being analyzed\n",
    "- Inventing relationships just because two variables share temporal patterns\n",
    "- Using external data enrichment when you cannot explain the cause-and-effect in one sentence\n",
    "- Generating \"creative\" correlations that would be questioned by domain experts\n",
    "\n",
    "**\uD83C\uDFAF THE BUSINESS VALUE OF EXTERNAL DATA (ONLY WHEN RELEVANT):**\n",
    "- Internal data tells you WHAT happened; RELEVANT external data helps explain WHY\n",
    "- External benchmarks add value ONLY when they have a direct relationship to the metric\n",
    "- The connection must be INDUSTRY-RECOGNIZED, not invented\n",
    "\n",
    "**\uD83D\uDCCB EXTERNAL DATA - RELEVANCY PRINCIPLE:**\n",
    "\n",
    "**GOLDEN RULE: \"Can I explain in ONE SENTENCE why this external factor DIRECTLY impacts this specific metric?\"**\n",
    "\n",
    "Before including ANY external data, ask:\n",
    "- Does this external factor have a PROVEN, DIRECT impact on this business metric?\n",
    "- Would a 20-year industry veteran include this enrichment in their analysis?\n",
    "- Is this correlation INDUSTRY-RECOGNIZED or am I inventing a relationship?\n",
    "- Would I be confident defending this connection to a skeptical business leader?\n",
    "\n",
    "**IF YOU CANNOT ANSWER \"YES\" TO ALL OF THESE, DO NOT INCLUDE THE EXTERNAL DATA.**\n",
    "\n",
    "**\uD83D\uDD25 MANDATORY PERSONA-BASED PROMPT PATTERN \uD83D\uDD25**\n",
    "\n",
    "You MUST use a PERSONA-BASED prompt that establishes the AI as a specialist with domain expertise. This ensures accurate, authoritative external data.\n",
    "\n",
    "**\uD83D\uDD25 AI_QUERY MODEL SELECTION AND TEMPERATURE GUIDE \uD83D\uDD25**\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL: USE THE CONFIGURED MODEL FOR ALL ai_query CALLS \uD83D\uDEA8**\n",
    "**User-configured SQL Model Serving endpoint: `{sql_model_serving}`**\n",
    "\n",
    "You MUST use `{sql_model_serving}` for ALL ai_query calls in the generated SQL. This is the model endpoint configured by the user.\n",
    "\n",
    "**TEMPERATURE GUIDE FOR GENERATED SQL:**\n",
    "- **0.1-0.2**: Factual extraction, precise classifications, data parsing (no creativity needed)\n",
    "- **0.3-0.4**: Structured analysis, JSON output, business intelligence (balanced)\n",
    "- **0.5-0.6**: Recommendations, insights, strategic advice (some creativity)\n",
    "- **0.7-0.8**: Creative content, innovative suggestions, brainstorming (high creativity)\n",
    "\n",
    "**ai_query SYNTAX WITH modelParameters:**\n",
    "```sql\n",
    "ai_query(\n",
    "  '{sql_model_serving}',\n",
    "  prompt_text,\n",
    "  modelParameters => named_struct('temperature', 0.4)\n",
    ") AS result\n",
    "```\n",
    "\n",
    "**PERSONA TEMPLATE:**\n",
    "```sql\n",
    "-- Step X: Fetch external public data for contextual enrichment\n",
    "-- NOTE: For production, connect to verified data sources (weather APIs, market data feeds, etc.). \n",
    "-- LLM estimates are suitable for prototyping but require verification before business decisions.\n",
    "external_api_for_<scenario> AS (\n",
    "  SELECT \n",
    "    *,  -- Keep all columns from previous CTE\n",
    "    ai_query(\n",
    "      '{sql_model_serving}',  -- User-configured model endpoint\n",
    "      CONCAT(\n",
    "        'You are a [SPECIALIST_ROLE] at [AUTHORITATIVE_ORGANIZATION] with [X] years of expertise in [DOMAIN]. ',\n",
    "        'Your task is to provide accurate, factual public information for business analysis. ',\n",
    "        'Context: [SPECIFIC_CONTEXT_FROM_DATA]. ',\n",
    "        'Required information: [LIST_OF_REQUIRED_FIELDS]. ',\n",
    "        'Return ONLY a single-line JSON object, no extra text. NO HALLUCINATION. ',\n",
    "        'Use public information only. Always return a value for each field (use \"Unknown\" or \"Data Not Available\" if evidence is insufficient). ',\n",
    "        'Include confidence scores for each field. ',\n",
    "        'Required JSON format: {{\"field1\": \"value\", \"field1_confidence\": 0.0-1.0, ..., \"as_of_date\": \"YYYY-MM-DD\", \"source_note\": \"text\", \"is_estimate\": true, \"requires_verification\": true}}. ',\n",
    "        'Output ONLY the JSON object, nothing else.'\n",
    "      ),\n",
    "      modelParameters => named_struct('temperature', 0.3)  -- Low temperature for factual data\n",
    "    ) AS external_data_json\n",
    "  FROM base_data  -- ✅ MANDATORY: Must reference previous CTE!\n",
    ")\n",
    "```\n",
    "\n",
    "**\uD83C\uDF1F PERSONA EXAMPLES BY DOMAIN (USE ONLY WHEN BUSINESS-RELEVANT) \uD83C\uDF1F**\n",
    "\n",
    "**⚠️ REMINDER: Before using ANY of these patterns, you MUST verify there is a DIRECT, PROVABLE, INDUSTRY-RECOGNIZED cause-and-effect relationship between the external factor and your business metric. If you cannot explain the connection in one sentence, DO NOT use external data enrichment.**\n",
    "\n",
    "**1. EXTERNAL DATA PATTERN (GENERIC TEMPLATE):**\n",
    "\n",
    "```sql\n",
    "-- ONLY use external data when there is a DIRECT, PROVABLE business connection\n",
    "-- ASK: \"Can I explain in ONE sentence why this external factor impacts this metric?\"\n",
    "-- Assumes previous CTE 'base_data' exists with relevant context columns\n",
    "external_api_for_<relevant_context> AS (\n",
    "  SELECT \n",
    "    *,  -- Keep all columns from previous CTE\n",
    "    ai_query(\n",
    "      '{sql_model_serving}',  -- User-configured model endpoint\n",
    "      CONCAT(\n",
    "        'You are a [DOMAIN_EXPERT_ROLE] at [AUTHORITATIVE_ORGANIZATION] with [X] years of expertise in [RELEVANT_DOMAIN]. ',\n",
    "        'Provide [RELEVANT_EXTERNAL_DATA] for: [CONTEXT_FROM_DATA]. ',\n",
    "        'Return ONLY a single-line JSON object. NO HALLUCINATION. Use public records only. ',\n",
    "        'Required JSON format: {{\"field1\": value, \"field1_confidence\": 0.0-1.0, ..., \"as_of_date\": \"YYYY-MM-DD\", \"source_note\": \"[DATA_SOURCE]\", \"is_estimate\": true, \"requires_verification\": true}}. ',\n",
    "        'Output ONLY the JSON object, nothing else.'\n",
    "      ),\n",
    "      modelParameters => named_struct('temperature', 0.2)  -- Low temp for factual data\n",
    "    ) AS external_data_json\n",
    "  FROM base_data  -- ✅ MANDATORY: Must reference previous CTE!\n",
    ")\n",
    "```\n",
    "\n",
    "**2. COMPETITOR/MARKET DATA (ABOUT THE ENTITY IN YOUR DATA, NOT YOUR COMPANY!):**\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL: When analyzing customers/entities, get info about THOSE entities, not about the company you're working for!**\n",
    "\n",
    "```sql\n",
    "-- CORRECT: Get info about the CUSTOMER being analyzed, using data from the query\n",
    "external_api_for_customer_market_intelligence AS (\n",
    "  SELECT \n",
    "    c.customer_id,\n",
    "    c.customer_name,\n",
    "    c.industry,\n",
    "    ai_query(\n",
    "      '{sql_model_serving}',  -- User-configured model endpoint\n",
    "      CONCAT(\n",
    "        'You are a Senior Market Intelligence Analyst at Gartner with 15 years of expertise in competitive analysis. ',\n",
    "        'Provide business intelligence for company: ', c.customer_name, ' in industry: ', c.industry, '. ',\n",
    "        'Required: This company''s market cap, their main competitors (companies competing WITH them), ',\n",
    "        'estimated annual revenue, number of employees, strategic priorities, key business risks, market position. ',\n",
    "        'Return ONLY a single-line JSON object. NO HALLUCINATION. Use public information only. ',\n",
    "        'Required JSON format: {{\"company_market_cap_usd\": value, \"market_cap_confidence\": 0.0-1.0, ',\n",
    "        '\"company_competitors\": \"Competitor1, Competitor2, Competitor3\", \"competitors_confidence\": 0.0-1.0, ',\n",
    "        '\"estimated_revenue_usd\": value, \"revenue_confidence\": 0.0-1.0, ',\n",
    "        '\"employee_count\": value, \"employee_confidence\": 0.0-1.0, ',\n",
    "        '\"strategic_priorities\": \"text\", \"priorities_confidence\": 0.0-1.0, ',\n",
    "        '\"key_business_risks\": \"text\", \"risks_confidence\": 0.0-1.0, ',\n",
    "        '\"market_position\": \"Leader/Challenger/Follower/Niche\", \"position_confidence\": 0.0-1.0, ',\n",
    "        '\"as_of_date\": \"YYYY-MM-DD\", \"source_note\": \"Public filings/News/Industry reports\", \"is_estimate\": true, \"requires_verification\": true}}. ',\n",
    "        'Output ONLY the JSON object, nothing else.'\n",
    "      ),\n",
    "      modelParameters => named_struct('temperature', 0.3)  -- Balanced for market analysis\n",
    "    ) AS customer_intel_json\n",
    "  FROM customer_base AS c  -- Use data from your query!\n",
    ")\n",
    "```\n",
    "\n",
    "**3. ECONOMIC DATA:**\n",
    "```sql\n",
    "-- Assumes previous CTE 'base_data' exists with country_name, analysis_period columns\n",
    "external_api_for_economic_context AS (\n",
    "  SELECT \n",
    "    *,  -- Keep all columns from previous CTE\n",
    "    ai_query(\n",
    "      '{sql_model_serving}',  -- User-configured model endpoint\n",
    "      CONCAT(\n",
    "        'You are a Chief Economist at the World Bank with 18 years of expertise in macroeconomic analysis, currency markets, and regional economic forecasting. ',\n",
    "        'Provide economic context for: Country ', country_name, ', Date/Period ', COALESCE(CAST(analysis_period AS STRING), 'Unknown Period'), '. ',\n",
    "        'Return ONLY a single-line JSON object. NO HALLUCINATION. Use public economic data only. ',\n",
    "        'Required JSON format: {{\"gdp_growth_rate_pct\": value, \"gdp_confidence\": 0.0-1.0, \"inflation_rate_pct\": value, \"inflation_confidence\": 0.0-1.0, \"unemployment_rate_pct\": value, \"unemployment_confidence\": 0.0-1.0, \"interest_rate_pct\": value, \"interest_confidence\": 0.0-1.0, \"currency_code\": \"XXX\", \"exchange_rate_to_usd\": value, \"exchange_confidence\": 0.0-1.0, \"economic_outlook\": \"Strong/Moderate/Weak/Recession\", \"outlook_confidence\": 0.0-1.0, \"key_economic_factors\": \"text\", \"factors_confidence\": 0.0-1.0, \"as_of_date\": \"YYYY-MM-DD\", \"source_note\": \"World Bank/IMF/Central Bank data\", \"is_estimate\": true, \"requires_verification\": true}}. ',\n",
    "        'Output ONLY the JSON object, nothing else.'\n",
    "      ),\n",
    "      modelParameters => named_struct('temperature', 0.2)  -- Low temp for economic facts\n",
    "    ) AS economic_json\n",
    "  FROM base_data  -- ✅ MANDATORY: Must reference previous CTE!\n",
    ")\n",
    "```\n",
    "\n",
    "**4. EVENTS/DISRUPTIONS DATA:**\n",
    "```sql\n",
    "-- Assumes previous CTE 'base_data' exists with location_name, event_date columns\n",
    "external_api_for_events_disruptions AS (\n",
    "  SELECT \n",
    "    *,  -- Keep all columns from previous CTE\n",
    "    ai_query(\n",
    "      '{sql_model_serving}',  -- User-configured model endpoint\n",
    "      CONCAT(\n",
    "        'You are a Senior Risk and Disruption Analyst at Lloyd''s of London with 15 years of expertise in global event monitoring, operational risk assessment, and business continuity analysis. ',\n",
    "        'Provide event/disruption context for: Location ', location_name, ', Date ', COALESCE(CAST(event_date AS STRING), 'Unknown Date'), '. ',\n",
    "        'Return ONLY a single-line JSON object. NO HALLUCINATION. Use public information only. ',\n",
    "        'Required JSON format: {{\"major_events\": \"Event1, Event2 or None\", \"events_confidence\": 0.0-1.0, \"event_type\": \"Holiday/Strike/Sports/Conference/Weather/Political/None\", \"type_confidence\": 0.0-1.0, \"expected_impact\": \"High/Medium/Low/None\", \"impact_confidence\": 0.0-1.0, \"affected_sectors\": \"Transport/Retail/Hospitality/All/None\", \"sectors_confidence\": 0.0-1.0, \"disruption_narrative\": \"text explaining any disruptions\", \"narrative_confidence\": 0.0-1.0, \"as_of_date\": \"YYYY-MM-DD\", \"source_note\": \"News/Event calendars/Public records\", \"is_estimate\": true, \"requires_verification\": true}}. ',\n",
    "        'Output ONLY the JSON object, nothing else.'\n",
    "      ),\n",
    "      modelParameters => named_struct('temperature', 0.3)  -- Balanced for event analysis\n",
    "    ) AS events_json\n",
    "  FROM base_data  -- ✅ MANDATORY: Must reference previous CTE!\n",
    ")\n",
    "```\n",
    "\n",
    "**5. GEOGRAPHIC/DEMOGRAPHIC DATA:**\n",
    "```sql\n",
    "-- Assumes previous CTE 'base_data' exists with city_name, country_name columns\n",
    "external_api_for_geographic_context AS (\n",
    "  SELECT \n",
    "    *,  -- Keep all columns from previous CTE\n",
    "    ai_query(\n",
    "      '{sql_model_serving}',  -- User-configured model endpoint\n",
    "      CONCAT(\n",
    "        'You are a Senior Demographer and Urban Planning Analyst at the United Nations Population Division with 20 years of expertise in population statistics, urban development, and regional demographics. ',\n",
    "        'Provide geographic and demographic context for: Location ', city_name, ', Country ', country_name, '. ',\n",
    "        'Return ONLY a single-line JSON object. NO HALLUCINATION. Use public census and geographic data only. ',\n",
    "        'Required JSON format: {{\"population\": value, \"population_confidence\": 0.0-1.0, \"population_density_per_sqkm\": value, \"density_confidence\": 0.0-1.0, \"median_household_income_usd\": value, \"income_confidence\": 0.0-1.0, \"urban_classification\": \"Metro/Urban/Suburban/Rural\", \"classification_confidence\": 0.0-1.0, \"timezone\": \"UTC+X\", \"timezone_confidence\": 0.0-1.0, \"latitude\": value, \"longitude\": value, \"coordinates_confidence\": 0.0-1.0, \"climate_zone\": \"Tropical/Temperate/Arid/Continental/Polar\", \"climate_confidence\": 0.0-1.0, \"key_industries\": \"text\", \"industries_confidence\": 0.0-1.0, \"as_of_date\": \"YYYY-MM-DD\", \"source_note\": \"UN/Census/Geographic databases\", \"is_estimate\": true, \"requires_verification\": true}}. ',\n",
    "        'Output ONLY the JSON object, nothing else.'\n",
    "      ),\n",
    "      modelParameters => named_struct('temperature', 0.2)  -- Low temp for factual geographic data\n",
    "    ) AS geo_json\n",
    "  FROM base_data  -- ✅ MANDATORY: Must reference previous CTE!\n",
    ")\n",
    "```\n",
    "\n",
    "**\uD83C\uDF1F BEST PRACTICES FOR EXTERNAL DATA ENRICHMENT \uD83C\uDF1F**\n",
    "\n",
    "When including an `external_api_for_<scenario>` CTE, follow these practices:\n",
    "\n",
    "1. **Use PERSONA-BASED prompts** with specific role, organization, and expertise (e.g., \"You are a Principal Meteorologist at NOAA...\")\n",
    "2. **Include CONFIDENCE SCORES** for every field: `<field>_confidence` (0.0-1.0) - this helps users understand data reliability\n",
    "3. **Always include metadata**: `as_of_date`, `source_note`, `is_estimate: true`, `requires_verification: true`\n",
    "4. **Add SQL comment** explaining the external data source and verification needs for production use\n",
    "5. **Parse JSON** using `get_json_object()` and pass fields to downstream CTEs\n",
    "6. **USE external data in ai_query prompts** for the final analysis - this is WHERE THE VALUE IS REALIZED!\n",
    "7. **HIGHLIGHT the source** in the prompt so the LLM knows the authority behind the data\n",
    "8. **USE CONFIGURED MODEL**: Always use `{sql_model_serving}` for all ai_query calls in generated SQL\n",
    "9. **SET TEMPERATURE**: Use `modelParameters => named_struct('temperature', X)` - low (0.1-0.3) for facts, higher (0.4-0.6) for insights\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: USE ACTUAL DATA CONTEXT IN EXTERNAL_API CTEs \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**The external_api CTE MUST use actual data values from your query (customer_name, company_name, location, dates, etc.) to fetch RELEVANT external information!**\n",
    "\n",
    "**❌ WRONG - Generic context without using actual data:**\n",
    "```sql\n",
    "-- ❌ WRONG: No entity-specific context passed to the LLM prompt\n",
    "external_api_for_competitor_intelligence AS (\n",
    "  SELECT \n",
    "    customer_id,  -- At least pass through the entity ID\n",
    "    ai_query('{sql_model_serving}',\n",
    "      'Provide competitive intelligence for the data platform market...'  -- ❌ NO ACTUAL DATA CONTEXT! Should reference customer_name, industry, etc.\n",
    "    ) AS json\n",
    "  FROM customer_base  -- ✅ EVERY CTE MUST have a FROM clause!\n",
    ")\n",
    "```\n",
    "\n",
    "**✅ CORRECT - Use actual entity data from your query:**\n",
    "```sql\n",
    "-- First, get base data with the entity you're analyzing (ALWAYS use DISTINCT)\n",
    "WITH customer_base AS (\n",
    "  SELECT DISTINCT \n",
    "    customer_id,  -- CRITICAL: filtered with IS NOT NULL\n",
    "    COALESCE(TRIM(customer_name), 'Unknown') AS customer_name,  -- ✅ COALESCE'd\n",
    "    COALESCE(TRIM(industry), 'Unknown') AS industry,            -- ✅ COALESCE'd\n",
    "    COALESCE(TRIM(region), 'Unknown') AS region                 -- ✅ COALESCE'd\n",
    "    -- ... (all columns must be COALESCE'd or have IS NOT NULL) ...\n",
    "  FROM table AS t\n",
    "  WHERE customer_id IS NOT NULL  -- ✅ Filter critical columns\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at the END of first CTE only\n",
    "),\n",
    "-- Then, use that entity's data to fetch RELEVANT external info\n",
    "external_api_for_customer_intelligence AS (\n",
    "  SELECT \n",
    "    b.customer_id,\n",
    "    b.customer_name,\n",
    "    ai_query('{sql_model_serving}',\n",
    "      CONCAT(\n",
    "        'You are a Senior Market Intelligence Analyst at Gartner... ',\n",
    "        'Provide business intelligence for company: ', b.customer_name, ' in industry: ', b.industry, '. ',\n",
    "        'Required information: company market cap, main competitors, revenue estimate, strategic priorities, market risks. ',\n",
    "        'Return JSON: {{\"market_cap_usd\": value, \"main_competitors\": \"Comp1, Comp2, Comp3\", \"estimated_revenue_usd\": value, ',\n",
    "        '\"strategic_priorities\": \"text\", \"market_risks\": \"text\", \"industry_position\": \"Leader/Challenger/Follower/Niche\", ...}}. '\n",
    "      )\n",
    "    ) AS company_intel_json\n",
    "  FROM customer_base AS b\n",
    ")\n",
    "```\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL RULE: CONTEXT AWARENESS \uD83D\uDEA8**\n",
    "- **DO NOT** generate insights about the company you are working for (e.g., if analyzing Databricks customers, don't generate Databricks competitor insights)\n",
    "- **DO** generate insights about the ENTITIES IN YOUR DATA (customers, suppliers, partners, locations)\n",
    "- **ALWAYS** pass entity identifiers (customer_name, company_name, location, etc.) from your base CTE into the external_api prompt\n",
    "- **ASK**: \"What would I want to know about THIS SPECIFIC customer/entity to make better recommendations?\"\n",
    "\n",
    "**ENTITY-AWARE EXTERNAL DATA EXAMPLES:**\n",
    "\n",
    "**For Customer Analysis - Get info about THE CUSTOMER:**\n",
    "```sql\n",
    "external_api_for_customer_profile AS (\n",
    "  SELECT \n",
    "    c.customer_id,\n",
    "    c.customer_name,\n",
    "    ai_query('{sql_model_serving}',\n",
    "      CONCAT(\n",
    "        'You are a Business Intelligence Analyst at D&B with 15 years expertise in company research. ',\n",
    "        'Provide business profile for company: ', c.customer_name, '. ',\n",
    "        'Required: market_cap_usd, employee_count, founded_year, headquarters_location, main_business_segments, ',\n",
    "        'top_3_competitors (companies competing WITH this customer, NOT your company), annual_revenue_estimate_usd, ',\n",
    "        'growth_trajectory (Growing/Stable/Declining), strategic_priorities, technology_stack, market_position. ',\n",
    "        'Return JSON with confidence scores. NO HALLUCINATION. Use public information only.'\n",
    "      )\n",
    "    ) AS customer_profile_json\n",
    "  FROM customer_base AS c\n",
    ")\n",
    "```\n",
    "\n",
    "**For Location Analysis - Get info about THE LOCATION:**\n",
    "```sql\n",
    "external_api_for_location_context AS (\n",
    "  SELECT \n",
    "    l.location_id,\n",
    "    l.city_name,\n",
    "    l.country,\n",
    "    ai_query('{sql_model_serving}',\n",
    "      CONCAT(\n",
    "        'You are a Geographic Analyst at UN Population Division. ',\n",
    "        'Provide context for: City ', l.city_name, ', Country ', l.country, '. ',\n",
    "        'Required: population, gdp_per_capita, major_industries, business_climate, infrastructure_rating, ',\n",
    "        'timezone, climate, key_economic_indicators. Return JSON with confidence scores.'\n",
    "      )\n",
    "    ) AS location_context_json\n",
    "  FROM location_base AS l\n",
    ")\n",
    "```\n",
    "\n",
    "**\uD83E\uDDE0 REMEMBER: The goal is to answer \"WHAT INFORMATION IS MISSING?\" and \"WHAT WOULD MAKE THIS ANALYSIS MORE VALUABLE?\"**\n",
    "**Use the ACTUAL ENTITIES in your data to fetch RELEVANT external context!**\n",
    "\n",
    "**EXAMPLE: INTEGRATING EXTERNAL DATA INTO ANALYSIS (CONTEXT-AWARE):**\n",
    "```sql\n",
    "-- Step 1: Get base data with LIMIT 10 (only at END of first CTE!)\n",
    "-- \uD83D\uDEA8 EVERY column must be COALESCE'd or have IS NOT NULL check!\n",
    "WITH customer_metrics AS (\n",
    "  SELECT DISTINCT\n",
    "    customer_id,                                           -- CRITICAL: filtered with IS NOT NULL\n",
    "    COALESCE(TRIM(customer_name), 'Unknown Customer') AS customer_name,  -- ✅ COALESCE'd\n",
    "    COALESCE(TRIM(industry), 'Unknown Industry') AS industry,            -- ✅ COALESCE'd\n",
    "    COALESCE(TRIM(region), 'Unknown Region') AS region,                  -- ✅ COALESCE'd\n",
    "    COALESCE(total_revenue, 0.0) AS total_revenue                        -- ✅ COALESCE'd\n",
    "  FROM `catalog`.`schema`.`customers` AS c\n",
    "  WHERE customer_id IS NOT NULL\n",
    "    AND customer_name IS NOT NULL  -- ✅ Critical identifier also filtered\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at the END of first CTE only\n",
    "),\n",
    "\n",
    "-- Step 2: Fetch external context FOR EACH CUSTOMER (using their data!)\n",
    "-- NOTE: For production, connect to verified data sources. LLM estimates are for prototyping.\n",
    "external_api_for_customer_intelligence AS (\n",
    "  SELECT \n",
    "    cm.customer_id,\n",
    "    cm.customer_name,\n",
    "    ai_query('{sql_model_serving}',\n",
    "      CONCAT(\n",
    "        'You are a Business Intelligence Analyst at D&B with 15 years expertise. ',\n",
    "        'Provide business profile for company: ', cm.customer_name, ' in industry: ', cm.industry, '. ',\n",
    "        'Required: market_cap_usd, competitors (companies competing WITH this customer), ',\n",
    "        'estimated_revenue, strategic_priorities, key_risks, market_position. ',\n",
    "        'Return JSON with confidence scores. NO HALLUCINATION.'\n",
    "      )\n",
    "    ) AS customer_intel_json\n",
    "  FROM customer_metrics AS cm  -- ✅ Uses actual customer data from the query!\n",
    "),\n",
    "\n",
    "-- Step 3: Parse external data\n",
    "customer_intel_parsed AS (\n",
    "  SELECT \n",
    "    customer_id,\n",
    "    customer_name,\n",
    "    get_json_object(customer_intel_json, '$.market_cap_usd') AS customer_market_cap,\n",
    "    COALESCE(get_json_object(customer_intel_json, '$.competitors'), 'Unknown') AS customer_competitors,\n",
    "    COALESCE(get_json_object(customer_intel_json, '$.strategic_priorities'), 'Unknown') AS customer_priorities,\n",
    "    COALESCE(TRY_CAST(get_json_object(customer_intel_json, '$.market_cap_confidence') AS DECIMAL(3,2)), 0.0) AS intel_confidence  -- ✅ TRY_CAST for safety\n",
    "  FROM external_api_for_customer_intelligence\n",
    "),\n",
    "\n",
    "-- Step 4: Combine internal data with external context and generate ai_sys_prompt\n",
    "analysis_prompts AS (\n",
    "  SELECT \n",
    "    c.*,\n",
    "    p.customer_market_cap,\n",
    "    p.customer_competitors,\n",
    "    p.customer_priorities,\n",
    "    CONCAT(\n",
    "      'You are a Strategic Account Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "      'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "      'Business priorities are: {enriched_business_priorities}. ',\n",
    "      'With 18 years of experience in enterprise account strategy and customer success, ',\n",
    "      'your expertise in strategic account planning and competitive positioning aligns with the strategic initiative: Customer growth. ',\n",
    "      'Analyze customer ', c.customer_name, ' (Market Cap: $', p.customer_market_cap, '). ',\n",
    "      'Their main competitors are: ', p.customer_competitors, '. ',\n",
    "      'Their strategic priorities: ', p.customer_priorities, '. ',\n",
    "      'Internal metrics: Revenue $', c.total_revenue, ', Region: ', c.region, '. ',\n",
    "      'Use both internal AND external context to provide actionable recommendations. ',\n",
    "      'Output ONLY JSON with NO markdown. ',\n",
    "      'Format: {{\"ai_cat_account_priority\": \"value\", \"ai_txt_growth_strategy\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", ',\n",
    "      '\"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "      'Output ONLY the JSON object, nothing else.'\n",
    "    ) AS ai_sys_prompt  -- ✅ Named ai_sys_prompt for auditability\n",
    "  FROM customer_metrics AS c\n",
    "  LEFT JOIN customer_intel_parsed AS p ON c.customer_id = p.customer_id\n",
    "),\n",
    "-- Step 5: Call ai_query with the prompt\n",
    "analysis_with_insights AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS insights_json\n",
    "  FROM analysis_prompts\n",
    ")\n",
    "-- Final analysis uses BOTH internal data AND external customer intelligence\n",
    "-- ai_sys_prompt MUST be the LAST column\n",
    "SELECT \n",
    "  customer_id,\n",
    "  customer_name,\n",
    "  region,\n",
    "  total_revenue,\n",
    "  customer_market_cap,\n",
    "  customer_competitors,\n",
    "  get_json_object(insights_json, '$.ai_cat_account_priority') AS ai_cat_account_priority,\n",
    "  get_json_object(insights_json, '$.ai_txt_growth_strategy') AS ai_txt_growth_strategy,\n",
    "  -- MANDATORY LAST 7 COLUMNS (including ai_txt_business_outcome):\n",
    "  COALESCE(get_json_object(insights_json, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "  COALESCE(get_json_object(insights_json, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "  COALESCE(get_json_object(insights_json, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "  COALESCE(get_json_object(insights_json, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "  COALESCE(TRY_CAST(get_json_object(insights_json, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "  COALESCE(get_json_object(insights_json, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "  COALESCE(get_json_object(insights_json, '$.ai_sys_missing_data'), 'No missing data identified') AS ai_sys_missing_data,\n",
    "  ai_sys_prompt  -- ✅ LAST COLUMN: The exact prompt used for auditability\n",
    "FROM analysis_with_insights;  -- ✅ NO LIMIT - data already sampled\n",
    "```\n",
    "\n",
    "**\uD83D\uDD25 THE VALUE: External data transforms basic analytics into CONTEXTUAL INTELLIGENCE that drives better business decisions! \uD83D\uDD25**\n",
    "\n",
    "**\uD83E\uDDE0 FINAL CHECK BEFORE GENERATING SQL:**\n",
    "- \"What information is MISSING from this analysis?\"\n",
    "- \"What external context would help EXPLAIN the patterns I'm seeing?\"\n",
    "- \"What would make the LLM's recommendations MORE ACCURATE and ACTIONABLE?\"\n",
    "- \"What would a 20-year industry veteran want to know before making a decision?\"\n",
    "\n",
    "**If the answer to any of these questions points to external data, YOU MUST include an `external_api_for_<scenario>` CTE to ensure accurate ai_txt_business_outcome calculations!**\n",
    "\n",
    "**\uD83C\uDFE2\uD83C\uDFE2\uD83C\uDFE2 REQUIRED FOR COMPREHENSIVE ANALYSIS: INTERNAL DATA ENRICHMENT CTE \uD83C\uDFE2\uD83C\uDFE2\uD83C\uDFE2**\n",
    "\n",
    "**The `internal_data_for_<scenario>` CTE is REQUIRED for comprehensive business analysis. Use this CTE to fetch internal information that provides organizational context for accurate ai_txt_business_outcome calculations.**\n",
    "\n",
    "**⚠️ WITHOUT internal_data: Analysis misses organizational policies, historical patterns, and internal benchmarks needed for accurate business impact calculations.**\n",
    "**✅ WITH internal_data: Analysis incorporates company-specific rates, SLAs, policies, and historical performance enabling precise ai_txt_business_outcome projections.**\n",
    "\n",
    "**\uD83E\uDDE0 BEFORE GENERATING SQL, ASK YOURSELF THESE INTERNAL DATA QUESTIONS:**\n",
    "\n",
    "1. **\"WHAT INTERNAL POLICIES OR GUIDELINES ARE NEEDED?\"** - Sales playbooks, pricing guidelines, approval thresholds, SLA definitions\n",
    "2. **\"WHAT HISTORICAL INTERACTIONS WOULD HELP?\"** - Support tickets, email communications, meeting notes, call logs\n",
    "3. **\"WHAT INSTITUTIONAL KNOWLEDGE IS MISSING?\"** - Best practices, lessons learned, tribal knowledge from experienced staff\n",
    "4. **\"WHAT OPERATIONAL CONTEXT WOULD IMPROVE RECOMMENDATIONS?\"** - Current inventory levels, team capacity, budget constraints, ongoing initiatives\n",
    "\n",
    "**INTERNAL DATA CTE PATTERN:**\n",
    "```sql\n",
    "-- Step X: Anticipate and fetch internal data that would improve analysis\n",
    "-- NOTE: This CTE fetches internal organizational knowledge, policies, and context\n",
    "-- that would typically be reported as missing in ai_sys_missing_data\n",
    "internal_data_for_<scenario> AS (\n",
    "  SELECT \n",
    "    *,  -- Keep all columns from previous CTE\n",
    "    ai_query(\n",
    "      '{sql_model_serving}',  -- User-configured model endpoint\n",
    "      CONCAT(\n",
    "        'You are an Internal Knowledge Manager for {business_name} with deep understanding of ',\n",
    "        '{enriched_business_context}. ',\n",
    "        'The organization is focused on strategic goals: {enriched_strategic_goals}. ',\n",
    "        'Provide internal organizational context for: [ENTITY_CONTEXT]. ',\n",
    "        'Required internal information: [SALES_GUIDELINES/SUPPORT_HISTORY/POLICY_INFO/BEST_PRACTICES]. ',\n",
    "        'Return ONLY a single-line JSON object. ',\n",
    "        'Required JSON format: {{\"internal_guidelines\": \"text\", \"historical_context\": \"text\", ',\n",
    "        '\"recommended_approach\": \"text\", \"risk_factors\": \"text\", \"success_criteria\": \"text\", ',\n",
    "        '\"stakeholders_to_consult\": \"text\", \"precedents\": \"text\", \"confidence\": 0.0-1.0}}. ',\n",
    "        'Output ONLY the JSON object, nothing else.'\n",
    "      ),\n",
    "      modelParameters => named_struct('temperature', 0.3)\n",
    "    ) AS internal_context_json\n",
    "  FROM base_data  -- ✅ MANDATORY: Must reference previous CTE!\n",
    ")\n",
    "```\n",
    "\n",
    "**\uD83C\uDF1F INTERNAL DATA CTE EXAMPLES BY SCENARIO \uD83C\uDF1F**\n",
    "\n",
    "**1. SALES PLAY RECOMMENDATIONS:**\n",
    "```sql\n",
    "-- Anticipate internal sales guidelines and best practices\n",
    "internal_data_for_sales_playbook AS (\n",
    "  SELECT \n",
    "    c.customer_id,\n",
    "    c.customer_name,\n",
    "    c.deal_stage,\n",
    "    ai_query('{sql_model_serving}',\n",
    "      CONCAT(\n",
    "        'You are a Sales Enablement Director for {business_name} with expertise in {enriched_business_context}. ',\n",
    "        'Strategic goals include: {enriched_strategic_goals}. ',\n",
    "        'Provide recommended sales approach for customer: ', c.customer_name, ' at deal stage: ', c.deal_stage, '. ',\n",
    "        'Include: sales playbook recommendations, objection handling strategies, competitive positioning, ',\n",
    "        'discount approval guidelines, key stakeholders to engage, success stories to reference, ',\n",
    "        'technical resources needed, timeline expectations. ',\n",
    "        'Return JSON: {{\"recommended_play\": \"text\", \"objection_handling\": \"text\", \"competitive_positioning\": \"text\", ',\n",
    "        '\"discount_guidelines\": \"text\", \"key_stakeholders\": \"text\", \"reference_stories\": \"text\", ',\n",
    "        '\"technical_resources\": \"text\", \"expected_timeline\": \"text\", \"confidence\": 0.0-1.0}}.'\n",
    "      )\n",
    "    ) AS sales_context_json\n",
    "  FROM customer_base AS c\n",
    ")\n",
    "```\n",
    "\n",
    "**2. CUSTOMER SUPPORT HISTORY:**\n",
    "```sql\n",
    "-- Anticipate internal support history and customer health context\n",
    "internal_data_for_customer_health AS (\n",
    "  SELECT \n",
    "    c.customer_id,\n",
    "    c.customer_name,\n",
    "    ai_query('{sql_model_serving}',\n",
    "      CONCAT(\n",
    "        'You are a Customer Success Manager for {business_name} focused on {enriched_business_context}. ',\n",
    "        'Business priorities include: {enriched_business_priorities}. ',\n",
    "        'Provide internal context for customer: ', c.customer_name, '. ',\n",
    "        'Include: typical support patterns, known pain points, escalation history, ',\n",
    "        'relationship health indicators, renewal risk factors, expansion opportunities, ',\n",
    "        'key contacts and their preferences, communication history highlights. ',\n",
    "        'Return JSON: {{\"support_patterns\": \"text\", \"known_pain_points\": \"text\", ',\n",
    "        '\"escalation_history\": \"text\", \"health_indicators\": \"text\", \"renewal_risks\": \"text\", ',\n",
    "        '\"expansion_opportunities\": \"text\", \"key_contacts\": \"text\", \"communication_notes\": \"text\", \"confidence\": 0.0-1.0}}.'\n",
    "      )\n",
    "    ) AS customer_health_json\n",
    "  FROM customer_base AS c\n",
    ")\n",
    "```\n",
    "\n",
    "**3. OPERATIONAL GUIDELINES:**\n",
    "```sql\n",
    "-- Anticipate internal operational policies and thresholds\n",
    "internal_data_for_operational_context AS (\n",
    "  SELECT \n",
    "    o.operation_id,\n",
    "    o.operation_type,\n",
    "    ai_query('{sql_model_serving}',\n",
    "      CONCAT(\n",
    "        'You are an Operations Manager for {business_name} with expertise in {enriched_value_chain}. ',\n",
    "        'Strategic initiative: {enriched_strategic_initiative}. ',\n",
    "        'Provide operational guidelines for: ', o.operation_type, '. ',\n",
    "        'Include: approval thresholds, escalation procedures, SLA requirements, ',\n",
    "        'resource allocation guidelines, quality checkpoints, compliance requirements, ',\n",
    "        'documentation standards, reporting requirements. ',\n",
    "        'Return JSON: {{\"approval_thresholds\": \"text\", \"escalation_procedures\": \"text\", ',\n",
    "        '\"sla_requirements\": \"text\", \"resource_guidelines\": \"text\", \"quality_checkpoints\": \"text\", ',\n",
    "        '\"compliance_requirements\": \"text\", \"documentation_standards\": \"text\", \"confidence\": 0.0-1.0}}.'\n",
    "      )\n",
    "    ) AS operational_context_json\n",
    "  FROM operation_base AS o\n",
    ")\n",
    "```\n",
    "\n",
    "**\uD83D\uDD25 BEST PRACTICES FOR INTERNAL DATA CTEs \uD83D\uDD25**\n",
    "\n",
    "1. **Anticipate ai_sys_missing_data**: Think about what data you would report as missing in the final analysis, and proactively fetch it\n",
    "2. **Use business context**: Always include {business_name}, {enriched_business_context}, and {enriched_strategic_goals} in internal data prompts\n",
    "3. **Be specific to the use case**: Tailor internal data requests to the specific analysis being performed\n",
    "4. **Combine with external_api**: Use BOTH internal_data and external_api CTEs for comprehensive enrichment\n",
    "5. **Parse and use the data**: Extract JSON fields and include them in downstream ai_query prompts\n",
    "\n",
    "**\uD83E\uDDE0 INTERNAL vs EXTERNAL DATA DECISION:**\n",
    "- **external_api_for_<scenario>**: Use for PUBLIC information about entities (market data, competitor info, economic indicators)\n",
    "- **internal_data_for_<scenario>**: Use for ORGANIZATIONAL knowledge (policies, guidelines, historical context, best practices)\n",
    "\n",
    "**\uD83C\uDFAF AVAILABLE TABLES AND COLUMNS (USE ONLY THESE - NO OTHER TABLES OR COLUMNS EXIST):**\n",
    "{directly_involved_schema}\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL: The tables and columns listed above are the ONLY ones available. Do NOT use any other table or column names. If you need a column that is not listed above, you CANNOT generate the query - add a comment explaining what is missing (exception: columns generated inside `external_api_for_<scenario>` via ai_query).**\n",
    "\n",
    "**\uD83D\uDCCB TABLES INCLUDED IN THIS CONTEXT:**\n",
    "The tables listed above include:\n",
    "1. **Tables directly specified in \"Tables Involved\"** field for this use case\n",
    "2. **Tables with foreign key relationships** to the directly involved tables (if they exist)\n",
    "\n",
    "**FOREIGN KEY RELATIONSHIPS (auto-included, never drop):**\n",
    "{foreign_key_relationships}\n",
    "- If a relationship is listed, you MUST include the referenced table(s) in FROM/JOIN and join using the provided key pairs.\n",
    "- Do NOT omit a referenced table when it appears here, even if not explicitly listed in \"Tables Involved\".\n",
    "- Automatically pull every referenced table above into your join plan and leverage the relationships to avoid missing required columns.\n",
    "\n",
    "**IMPORTANT**: If a table or column you think you need is not listed above, it means:\n",
    "- Either it doesn't exist in the database\n",
    "- Or it has no foreign key relationship to the involved tables\n",
    "- DO NOT hallucinate or invent table/column names\n",
    "- Use ONLY what is explicitly provided above\n",
    "\n",
    "**UNSTRUCTURED DOCUMENTS** (if applicable):\n",
    "{unstructured_docs}\n",
    "\n",
    "{previous_feedback}\n",
    "\n",
    "{interpreted_regeneration_context}\n",
    "\n",
    "---\n",
    "\n",
    "### \uD83C\uDFAF ABSOLUTE PRIORITY RULES (FAILURE = CRITICAL ERROR)\n",
    "\n",
    "#### 0. **SCHEMA ADHERENCE - ZERO HALLUCINATION TOLERANCE** (MOST CRITICAL):\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: YOU MUST USE ONLY THE EXACT TABLES, COLUMNS, CATALOGS, AND SCHEMAS PROVIDED IN THE \"AVAILABLE TABLES AND COLUMNS\" SECTION ABOVE. ABSOLUTELY NO HALLUCINATION ALLOWED. NO OTHER TABLES OR COLUMNS EXIST. THIS IS THE #1 FAILURE POINT. \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**\uD83D\uDEA8 MANDATORY: ONLY USE PROVIDED SCHEMA \uD83D\uDEA8**\n",
    "- The **AVAILABLE TABLES AND COLUMNS** section contains the ONLY tables and columns you can use\n",
    "- These are the ONLY tables that exist in the database for this query\n",
    "\n",
    "#### 0.1. **STRING LITERAL QUOTING - CRITICAL SYNTAX RULE**:\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: ALL STRING LITERALS MUST BE QUOTED WITH SINGLE QUOTES \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "This is the **#1 MOST COMMON ERROR** - forgetting to quote string literals in SQL.\n",
    "\n",
    "**MANDATORY RULES:**\n",
    "- **ANY text value** used in comparisons, CASE statements, or expressions MUST be quoted with single quotes\n",
    "- **Column names** are NOT quoted (unless they have spaces/keywords, then use backticks)\n",
    "- **String values** MUST ALWAYS be quoted with single quotes\n",
    "\n",
    "**✅ MANDATORY CORRECT PATTERNS (COPY THESE EXACTLY):**\n",
    "```sql\n",
    "-- CORRECT: String literals with single quotes\n",
    "WHERE certificate_type = 'Policy'         -- ✅ 'Policy' is quoted\n",
    "WHERE status = 'Active'                   -- ✅ 'Active' is quoted  \n",
    "WHEN category = 'Premium' THEN ...        -- ✅ 'Premium' is quoted\n",
    "COALESCE(risk_level, 'High')             -- ✅ 'High' is quoted\n",
    "COALESCE(TRIM(name), 'Unknown')          -- ✅ 'Unknown' is quoted\n",
    "COALESCE(TRIM(category), 'Not Specified') -- ✅ 'Not Specified' is quoted\n",
    "```\n",
    "\n",
    "**✅ CORRECT CASE STATEMENT PATTERN (COPY THIS EXACTLY):**\n",
    "```sql\n",
    "-- ✅ CORRECT: CASE statement with quoted strings\n",
    "CASE \n",
    "  WHEN status = 'Pending' THEN 'Low'       -- ✅ All quoted\n",
    "  WHEN status = 'Approved' THEN 'Medium'   -- ✅ All quoted\n",
    "  ELSE 'High'                              -- ✅ Quoted\n",
    "END\n",
    "\n",
    "-- ✅ CORRECT: Array with quoted strings (COPY THIS EXACTLY)\n",
    "ARRAY('Type', 'Status', 'Category')       -- ✅ All quoted\n",
    "\n",
    "-- ✅ CORRECT: COALESCE with quoted default (COPY THESE PATTERNS EXACTLY)\n",
    "COALESCE(customer_name, 'Unknown')        -- ✅ 'Unknown' is quoted\n",
    "\n",
    "-- ✅ CORRECT: COALESCE with properly quoted STRING defaults (COPY THESE PATTERNS EXACTLY)\n",
    "COALESCE(TRIM(c.charge_code), 'UNKNOWN') AS charge_code              -- ✅ 'UNKNOWN' is quoted\n",
    "COALESCE(TRIM(c.category), 'Not Specified') AS category              -- ✅ 'Not Specified' is quoted\n",
    "COALESCE(TRIM(c.status), 'Pending Review') AS status                 -- ✅ 'Pending Review' is quoted\n",
    "COALESCE(TRIM(c.region), 'Unassigned Region') AS region              -- ✅ 'Unassigned Region' is quoted\n",
    "COALESCE(CAST(date_col AS STRING), 'No Date Available') AS date_str  -- ✅ 'No Date Available' is quoted\n",
    "```\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 STOP! READ THIS BEFORE WRITING ANY COALESCE! THE #1 ERROR IS MISSING QUOTES! \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**⛔⛔⛔ IF YOU FORGET SINGLE QUOTES AROUND STRING DEFAULTS, YOUR SQL WILL FAIL! ⛔⛔⛔**\n",
    "\n",
    "**LOOK AT THIS REAL EXAMPLE OF THE ERROR YOU MUST NOT MAKE:**\n",
    "```sql\n",
    "-- ❌❌❌ CATASTROPHICALLY WRONG - THIS EXACT PATTERN WILL FAIL - DO NOT COPY ❌❌❌\n",
    "WITH base_data AS (\n",
    "  SELECT DISTINCT\n",
    "    material_usage_id,\n",
    "    COALESCE(TRIM(material_type), Unknown Material) AS material_type,          -- ❌ SYNTAX ERROR! Unknown Material needs quotes!\n",
    "    COALESCE(TRIM(material_description), No Description) AS material_description,  -- ❌ SYNTAX ERROR! No Description needs quotes!\n",
    "    COALESCE(quantity_used, 0.0) AS quantity_used,\n",
    "    COALESCE(TRIM(unit_of_measure), Unknown) AS unit_of_measure,               -- ❌ SYNTAX ERROR! Unknown needs quotes!\n",
    "    COALESCE(waste_factor_percent, 0.0) AS waste_factor_percent,\n",
    "    COALESCE(TRIM(waste_reason), No Reason) AS waste_reason,                   -- ❌ SYNTAX ERROR! No Reason needs quotes!\n",
    "    COALESCE(CAST(delivery_date AS STRING), Unknown Date) AS delivery_date_str,  -- ❌ SYNTAX ERROR! Unknown Date needs quotes!\n",
    "    COALESCE(TRIM(storage_location), Unknown Location) AS storage_location,    -- ❌ SYNTAX ERROR! Unknown Location needs quotes!\n",
    "    COALESCE(TRIM(supplier_name), Unknown Supplier) AS supplier_name,          -- ❌ SYNTAX ERROR! Unknown Supplier needs quotes!\n",
    "    COALESCE(TRIM(inspection_status), Unknown) AS inspection_status            -- ❌ SYNTAX ERROR! Unknown needs quotes!\n",
    "  FROM `catalog`.`schema`.`table` AS t\n",
    ")\n",
    "\n",
    "-- ✅✅✅ CORRECT - EVERY STRING DEFAULT HAS SINGLE QUOTES - COPY THIS PATTERN EXACTLY ✅✅✅\n",
    "WITH base_data AS (\n",
    "  SELECT DISTINCT\n",
    "    material_usage_id,\n",
    "    COALESCE(TRIM(material_type), 'Unknown Material') AS material_type,          -- ✅ 'Unknown Material' has quotes!\n",
    "    COALESCE(TRIM(material_description), 'No Description') AS material_description,  -- ✅ 'No Description' has quotes!\n",
    "    COALESCE(quantity_used, 0.0) AS quantity_used,                                -- ✅ Numbers don't need quotes\n",
    "    COALESCE(TRIM(unit_of_measure), 'Unknown') AS unit_of_measure,               -- ✅ 'Unknown' has quotes!\n",
    "    COALESCE(waste_factor_percent, 0.0) AS waste_factor_percent,                 -- ✅ Numbers don't need quotes\n",
    "    COALESCE(TRIM(waste_reason), 'No Reason') AS waste_reason,                   -- ✅ 'No Reason' has quotes!\n",
    "    COALESCE(CAST(delivery_date AS STRING), 'Unknown Date') AS delivery_date_str,  -- ✅ 'Unknown Date' has quotes!\n",
    "    COALESCE(TRIM(storage_location), 'Unknown Location') AS storage_location,    -- ✅ 'Unknown Location' has quotes!\n",
    "    COALESCE(TRIM(supplier_name), 'Unknown Supplier') AS supplier_name,          -- ✅ 'Unknown Supplier' has quotes!\n",
    "    COALESCE(TRIM(inspection_status), 'Unknown') AS inspection_status            -- ✅ 'Unknown' has quotes!\n",
    "  FROM `catalog`.`schema`.`table` AS t\n",
    ")\n",
    "```\n",
    "\n",
    "**\uD83D\uDD34\uD83D\uDD34\uD83D\uDD34 RULE: ANY TEXT after the comma in COALESCE MUST have 'single quotes' around it! \uD83D\uDD34\uD83D\uDD34\uD83D\uDD34**\n",
    "\n",
    "**MORE EXAMPLES OF WRONG vs CORRECT:**\n",
    "```sql\n",
    "-- ❌ WRONG (will cause PARSE_SYNTAX_ERROR)     |  ✅ CORRECT (will execute successfully)\n",
    "COALESCE(TRIM(name), Unknown)                   |  COALESCE(TRIM(name), 'Unknown')\n",
    "COALESCE(TRIM(status), Active)                  |  COALESCE(TRIM(status), 'Active')\n",
    "COALESCE(TRIM(region), North America)           |  COALESCE(TRIM(region), 'North America')\n",
    "COALESCE(TRIM(type), Type A)                    |  COALESCE(TRIM(type), 'Type A')\n",
    "COALESCE(CAST(date AS STRING), No Date)         |  COALESCE(CAST(date AS STRING), 'No Date')\n",
    "COALESCE(TRIM(category), Uncategorized)         |  COALESCE(TRIM(category), 'Uncategorized')\n",
    "COALESCE(TRIM(owner), Unassigned)               |  COALESCE(TRIM(owner), 'Unassigned')\n",
    "COALESCE(TRIM(priority), Low)                   |  COALESCE(TRIM(priority), 'Low')\n",
    "COALESCE(TRIM(description), N/A)                |  COALESCE(TRIM(description), 'N/A')\n",
    "COALESCE(TRIM(po_number), Unknown PO)           |  COALESCE(TRIM(po_number), 'Unknown PO')\n",
    "```\n",
    "\n",
    "**THE SIMPLE RULE:**\n",
    "- `COALESCE(..., 0.0)` - Numbers: NO quotes needed\n",
    "- `COALESCE(..., 0)` - Numbers: NO quotes needed  \n",
    "- `COALESCE(..., FALSE)` - Booleans: NO quotes needed\n",
    "- `COALESCE(..., 'Any Text')` - **TEXT: ALWAYS needs 'single quotes'!**\n",
    "- `COALESCE(TRIM(...), 'Any Text')` - **TEXT: ALWAYS needs 'single quotes'!**\n",
    "- `COALESCE(CAST(... AS STRING), 'Any Text')` - **TEXT: ALWAYS needs 'single quotes'!**\n",
    "\n",
    "**\uD83D\uDD25\uD83D\uDD25\uD83D\uDD25 VALIDATION STEP - DO THIS BEFORE SUBMITTING \uD83D\uDD25\uD83D\uDD25\uD83D\uDD25**\n",
    "1. Search your SQL for the pattern `COALESCE(`\n",
    "2. For EACH COALESCE found, check: Is the second argument text?\n",
    "3. If YES → Does it have 'single quotes' around it?\n",
    "4. If NO quotes → ADD THEM NOW!\n",
    "\n",
    "**VALIDATION CHECKLIST - BEFORE SUBMITTING SQL:**\n",
    "☐ Every string value in WHERE clause has single quotes: `WHERE col = 'value'`\n",
    "☐ Every string in CASE/WHEN/THEN/ELSE has single quotes: `WHEN col = 'value' THEN 'result'`\n",
    "☐ **\uD83D\uDEA8 CRITICAL \uD83D\uDEA8**: Every COALESCE string default has single quotes: `COALESCE(col, 'Unknown')`, `COALESCE(TRIM(name), 'Not Specified')`\n",
    "☐ Every string in ARRAY has single quotes: `ARRAY('val1', 'val2')`\n",
    "☐ Every string literal anywhere in the query has single quotes\n",
    "☐ **\uD83D\uDEA8 CRITICAL \uD83D\uDEA8**: Multi-word defaults need quotes: `'No Data Available'`, `'Unknown Customer'`, `'Pending Review'`\n",
    "\n",
    "**REMEMBER**: \n",
    "- Column names = NO quotes (or backticks if spaces/keywords)\n",
    "- String values = **ALWAYS** single quotes `'...'`\n",
    "- Numbers/booleans = NO quotes (TRUE, FALSE, 123, 45.67)\n",
    "- You MUST NOT use any table or column name that is not explicitly listed above\n",
    "- If you think you need a table or column that is not listed, it DOES NOT EXIST - add a comment explaining what is missing\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "#### 0.2. **CRITICAL DATABRICKS SQL SYNTAX RULES** (ZERO FAILURES ALLOWED):\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 AI_FORECAST FUNCTION SYNTAX - CRITICAL QUOTING RULES \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**\uD83D\uDD25 #1 MOST COMMON ERROR: Column names in AI_FORECAST parameters MUST be STRING LITERALS (quoted) \uD83D\uDD25**\n",
    "\n",
    "The `time_col`, `value_col`, and `group_col` parameters expect **STRING LITERALS** containing the column name, NOT column references!\n",
    "\n",
    "**✅ CORRECT - Column names are STRING LITERALS (with single quotes):**\n",
    "```sql\n",
    "AI_FORECAST(\n",
    "  TABLE(past),\n",
    "  time_col => 'ds',                                    -- ✅ 'ds' is a STRING LITERAL\n",
    "  value_col => 'revenue',                             -- ✅ 'revenue' is a STRING LITERAL\n",
    "  group_col => 'customer_id',                         -- ✅ 'customer_id' is a STRING LITERAL\n",
    "  horizon => (SELECT date_add(MONTH, 3, MAX(ds)) FROM past)\n",
    ")\n",
    "```\n",
    "\n",
    "**✅ CORRECT - ARRAY parameters also use STRING LITERALS:**\n",
    "```sql\n",
    "AI_FORECAST(\n",
    "  TABLE(past),\n",
    "  time_col => 'ds',\n",
    "  value_col => ARRAY('total_dbus', 'total_net_dbus'),           -- ✅ Array of STRING LITERALS\n",
    "  group_col => ARRAY('workspaceId', 'cloudType', 'workloadType'), -- ✅ Array of STRING LITERALS\n",
    "  horizon => (SELECT add_months(MAX(ds), 3) FROM past)\n",
    ")\n",
    "```\n",
    "\n",
    "**❌❌❌ WRONG - Column names WITHOUT quotes (CAUSES [UNRESOLVED_COLUMN] ERROR!) ❌❌❌:**\n",
    "```sql\n",
    "-- THIS IS WRONG AND WILL FAIL!\n",
    "AI_FORECAST(\n",
    "  TABLE(past),\n",
    "  time_col => ds,                                    -- ❌ WRONG! ds without quotes is a COLUMN REFERENCE\n",
    "  value_col => revenue,                              -- ❌ WRONG! revenue without quotes is a COLUMN REFERENCE\n",
    "  group_col => customer_id,                          -- ❌ WRONG! customer_id without quotes is a COLUMN REFERENCE\n",
    "  horizon => (SELECT date_add(MONTH, 3, MAX(ds)) FROM past)\n",
    ")\n",
    "-- ERROR: [UNRESOLVED_COLUMN] A column with name 'ds' cannot be resolved\n",
    "\n",
    "-- THIS IS ALSO WRONG!\n",
    "AI_FORECAST(\n",
    "  TABLE(past),\n",
    "  time_col => 'ds',\n",
    "  value_col => ARRAY(total_dbus, total_net_dbus),              -- ❌ WRONG! Unquoted inside ARRAY!\n",
    "  group_col => ARRAY(workspaceId, cloudType, workloadType),    -- ❌ WRONG! Unquoted inside ARRAY!\n",
    "  horizon => (SELECT add_months(MAX(ds), 3) FROM past)\n",
    ")\n",
    "-- ERROR: [UNRESOLVED_COLUMN] A column with name 'total_dbus' cannot be resolved\n",
    "```\n",
    "\n",
    "**WHY THIS HAPPENS:**\n",
    "- `time_col => ds` → SQL tries to find a column named `ds` in the CURRENT SCOPE (not inside the TABLE)\n",
    "- `time_col => 'ds'` → Passes the STRING 'ds' which AI_FORECAST uses to find the column INSIDE the TABLE\n",
    "- This is the #1 most common AI_FORECAST error - ALWAYS USE SINGLE QUOTES around column names!\n",
    "\n",
    "**VALIDATION CHECKLIST FOR AI_FORECAST:**\n",
    "☐ `time_col => 'column_name'` - column name in SINGLE QUOTES\n",
    "☐ `value_col => 'column_name'` OR `value_col => ARRAY('col1', 'col2')` - ALL in SINGLE QUOTES\n",
    "☐ `group_col => 'column_name'` OR `group_col => ARRAY('col1', 'col2')` - ALL in SINGLE QUOTES\n",
    "☐ `horizon` parameter is present (REQUIRED - no default)\n",
    "☐ `parameters` uses single quotes outside: `parameters => '{{\"key\": value}}'`\n",
    "\n",
    "- **DATE_ADD SYNTAX**: Do NOT quote the unit parameter:\n",
    "  * ✅ CORRECT: `date_add(MONTH, 3, MAX(ds))`\n",
    "  * ✅ CORRECT: `date_add(DAY, 7, MAX(ds))`\n",
    "  * ✅ CORRECT: `date_add(QUARTER, 4, MAX(ds))`\n",
    "  * ❌ WRONG: `date_add('MONTH', 3, MAX(ds))` - unit must be unquoted\n",
    "  * ❌ WRONG: `date_add('QUARTER', 4, MAX(ds))` - unit must be unquoted\n",
    "- **CONSTANT VALUES IN ai_forecast**: Parameters like `value_col` and `group_col` MUST be constant literal strings, NOT subqueries:\n",
    "  * ✅ CORRECT: `value_col => 'revenue'`\n",
    "  * ❌ WRONG: `value_col => (SELECT 'revenue')` - must be literal constant!\n",
    "  \n",
    "**\uD83D\uDEA8 DATE/TIME INTERVAL RULES (STRICT) \uD83D\uDEA8**\n",
    "- Do NOT quote date/time units anywhere. Use `date_add(DAY, 7, some_date)` or `date_add(MONTH, 3, some_date)` with unquoted units.\n",
    "- For month arithmetic use `add_months(date_expr, n)`; never use `date_add` with a quoted 'MONTH' literal.\n",
    "- `DATEDIFF` in Databricks SQL only takes two arguments: `DATEDIFF(end_date, start_date)`. Do NOT pass a unit parameter. For month differences use `months_between(end_date, start_date)` instead of `DATEDIFF('month', ...)`.\n",
    "- If you need weeks or quarters, compute with `date_add` using unquoted units or use `months_between`/`datediff` plus division, never a three-argument `DATEDIFF`.\n",
    "\n",
    "**\uD83D\uDEA8 WINDOW FUNCTION SYNTAX \uD83D\uDEA8**\n",
    "- **AGGREGATE WINDOW FUNCTIONS**: Functions like CORR(), COVAR_POP(), COVAR_SAMP(), AVG(), STDDEV(), VAR(), PERCENTILE_APPROX(), MEDIAN() CANNOT use ROWS BETWEEN or RANGE BETWEEN frames:\n",
    "  * ✅ CORRECT: `CORR(col1, col2) OVER ()`\n",
    "  * ✅ CORRECT: `AVG(col1) OVER ()`\n",
    "  * ✅ CORRECT: `STDDEV(col1) OVER (PARTITION BY group_col)`\n",
    "  * ✅ CORRECT: `PERCENTILE_APPROX(col1, 0.5) OVER ()`\n",
    "  * ❌ WRONG: `CORR(col1, col2) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)`\n",
    "  * ❌ WRONG: `AVG(col1) OVER (PARTITION BY group_col ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)`\n",
    "  * ❌ WRONG: `MEDIAN(col1) OVER (RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)`\n",
    "  * **FIX**: Remove the `ROWS BETWEEN` and `RANGE BETWEEN` clauses entirely for aggregate window functions - use simple `OVER ()` or `OVER (PARTITION BY col)`\n",
    "- **DECIMAL WINDOW AGGREGATES**: When using AVG/STDDEV/CORR/COVAR on DECIMAL columns in a window, CAST the inputs to DOUBLE (e.g., `AVG(CAST(dec_col AS DOUBLE)) OVER (...)`) to avoid internal decimal evaluation errors.\n",
    "- **NO DISTINCT IN WINDOW FUNCTIONS**: `COUNT(DISTINCT col) OVER (...)` is NOT supported.\n",
    "  * ❌ WRONG: `COUNT(DISTINCT user_id) OVER (PARTITION BY region)`\n",
    "  * ✅ CORRECT: Aggregation in a CTE/Subquery first, then join back or window over the aggregated results.\n",
    "  * ✅ CORRECT: `size(collect_set(user_id) OVER (...))` (ONLY IF strict requirement and set is small).\n",
    "  * **PREFERRED**: Pre-aggregate in a CTE using `GROUP BY`, then window over that CTE.\n",
    "\n",
    "**\uD83D\uDEA8 GROUP BY CLAUSE RULES \uD83D\uDEA8**\n",
    "- **ALL NON-AGGREGATED COLUMNS MUST BE IN GROUP BY**:\n",
    "  * If a column OR EXPRESSION appears in SELECT but is NOT inside an aggregate function (SUM, COUNT, AVG, etc.), it MUST be in GROUP BY\n",
    "  * ✅ CORRECT: `SELECT customer_id, region, SUM(revenue) ... GROUP BY customer_id, region`\n",
    "  * ❌ WRONG: `SELECT customer_id, region, SUM(revenue) ... GROUP BY customer_id` - Missing region!\n",
    "- **MISSING_AGGREGATION ERROR**: If you see this error (non-aggregating expression based on columns not in GROUP BY), you MUST:\n",
    "  * ✅ FIX OPTION 1: Add the missing column/expression to GROUP BY: `GROUP BY customer_id, region, country`\n",
    "  * ✅ FIX OPTION 2: Use ANY_VALUE() if the value is constant per group: `SELECT customer_id, ANY_VALUE(region), SUM(revenue) ... GROUP BY customer_id`\n",
    "  * ✅ FIX OPTION 3: Aggregate the expression using MIN/MAX: `SELECT customer_id, MAX(region), SUM(revenue) ... GROUP BY customer_id`\n",
    "\n",
    "**\uD83D\uDEA8 TYPE MATCHING IN COALESCE \uD83D\uDEA8**\n",
    "- **ALL ARGUMENTS MUST BE SAME TYPE**:\n",
    "  * COALESCE requires all arguments to be the same type\n",
    "  * ✅ CORRECT: `COALESCE(bool_column, FALSE)` - both BOOLEAN\n",
    "  * ✅ CORRECT: `COALESCE(string_column, 'N')` - both STRING\n",
    "  * ✅ CORRECT: `COALESCE(CAST(bool_column AS STRING), 'N')` - both STRING after CAST\n",
    "  * ❌ WRONG: `COALESCE(bool_column, 'N')` - mixing BOOLEAN and STRING!\n",
    "  * ❌ WRONG: `COALESCE(int_column, '0')` - mixing INTEGER and STRING!\n",
    "- **FIX**: Cast to common type before COALESCE:\n",
    "  * `COALESCE(CAST(bool_column AS STRING), 'N')`\n",
    "  * `COALESCE(int_column, 0)` - both INT\n",
    "  * `COALESCE(CAST(int_column AS STRING), '0')` - both STRING\n",
    "\n",
    "**\uD83D\uDEA8 SCHEMA VALIDATION PROCESS (ZERO HALLUCINATION TOLERANCE) \uD83D\uDEA8**\n",
    "\n",
    "**⚠️ IMPORTANT: This validation is INTERNAL ONLY. DO NOT output any validation messages, status checks, or confirmations in your response.**\n",
    "\n",
    "**BEFORE WRITING ANY SQL - MANDATORY 3-STEP PROCESS (DO THIS INTERNALLY - DO NOT OUTPUT):**\n",
    "\n",
    "**STEP 1: INVENTORY** - Internally check all available tables and columns from \"AVAILABLE TABLES AND COLUMNS\" section above\n",
    "\n",
    "**STEP 2: VERIFY** - For EVERY table/column you want to use, internally confirm it exists in Step 1's list\n",
    "- ✅ Column exists in schema → Use exact name (case-sensitive)\n",
    "- ❌ Column doesn't exist → DO NOT invent it. Add comment: `-- MISSING: column_name`\n",
    "- \uD83D\uDEA8 CRITICAL: UNRESOLVED_COLUMN errors are the #1 cause of SQL failures - VERIFY EVERY COLUMN EXISTS before using it\n",
    "\n",
    "**STEP 3: VALIDATE** - After writing SQL, check each clause:\n",
    "- SELECT: All columns exist in schema\n",
    "- FROM/JOIN: All tables exist, fully qualified (catalog.schema.table), have aliases\n",
    "- WHERE: Only IS NULL / IS NOT NULL (no value comparisons)\n",
    "- JOIN ON: Both join columns exist in their respective tables\n",
    "- GROUP BY / ORDER BY: All columns exist in schema\n",
    "- AI functions: Arrays ≤20 items, each <50 chars; all referenced columns exist\n",
    "\n",
    "**COMMON HALLUCINATION ERRORS TO AVOID:**\n",
    "❌ Assuming `id` exists → Check if it's `customer_id`, `order_id`, etc.\n",
    "❌ Assuming `name` exists → Check if it's `first_name`, `product_name`, etc.\n",
    "❌ Assuming `date` exists → Check if it's `order_date`, `created_at`, etc.\n",
    "❌ Assuming `status` exists → Check if it's `order_status`, `payment_status`, etc.\n",
    "❌ Using \"typical\" column names → Use ONLY exact names from schema\n",
    "\n",
    "**VALIDATION CHECKLIST (MANDATORY BEFORE SUBMITTING SQL):**\n",
    "☐ \uD83D\uDEA8 Every table name exists in \"AVAILABLE TABLES AND COLUMNS\" section above (VERIFY FIRST)\n",
    "☐ \uD83D\uDEA8 Every column name exists in its table in \"AVAILABLE TABLES AND COLUMNS\" section above (VERIFY FIRST - #1 FAILURE CAUSE)\n",
    "☐ Catalog.schema.table names match EXACTLY (case-sensitive)\n",
    "☐ No invented/hallucinated names - ZERO TOLERANCE\n",
    "☐ JOIN keys exist in BOTH tables being joined (VERIFY BOTH SIDES)\n",
    "☐ AI function parameters reference actual columns (not assumed ones)\n",
    "☐ Columns used in WHERE, GROUP BY, ORDER BY all exist in schema (VERIFY EACH ONE)\n",
    "☐ WHERE clauses only use IS NULL / IS NOT NULL (no value comparisons)\n",
    "☐ All CONCAT parameters have proper quotes (single quotes for literals, no quotes for columns)\n",
    "☐ Array parameters have ≤20 items, each <50 characters\n",
    "☐ Step documentation included for all CTEs\n",
    "☐ All columns in final SELECT exist in the last CTE (use SELECT * in intermediate CTEs to preserve columns)\n",
    "\n",
    "**IF SCHEMA IS MISSING REQUIRED DATA:**\n",
    "- Add comment: `-- MISSING: column_name - cannot generate without it`\n",
    "- If substitute exists: `-- NOTE: Using substitute_col for missing original_col`\n",
    "\n",
    "---\n",
    "\n",
    "#### 0.3. **ADVANCED ANALYTICS & SIMULATION RULES** (IF APPLICABLE):\n",
    "\n",
    "**A. MONTE CARLO SIMULATION (AI-DRIVEN - RICH OUTPUT)**:\n",
    "- **Implementation**: Do NOT use raw `RAND()`. Use `ai_query` to generate realistic simulation scenarios with NARRATIVES.\n",
    "- **Steps**:\n",
    "  1. Calculate historical stats (Min, Max, Avg, StdDev) in a CTE.\n",
    "  2. Use `ai_query` to generate a JSON array of 20-50 rich simulation objects:\n",
    "     - Prompt: \"Generate 50 realistic simulation scenarios for [Metric] based on Mean=[X], StdDev=[Y]. Return JSON array of objects: {{ \"simulation_id\": 1, \"simulated_value\": 123.45, \"scenario_narrative\": \"Market rally driven by...\", \"explanation\": \"Value is +1.5 sigma due to...\" }}.\"\n",
    "  3. `EXPLODE()` the JSON array.\n",
    "  4. Extract columns: `simulated_value`, `scenario_narrative`, `explanation`.\n",
    "  5. Show detailed simulation rows (Long Format) as output columns.\n",
    "\n",
    "**B. WHAT-IF / SCENARIO ANALYSIS (AI-DEFINED - RICH OUTPUT)**:\n",
    "- **Implementation**: Use `ai_query` to define 5-10 distinct business scenarios.\n",
    "- **Steps**:\n",
    "  1. Use `ai_query` to generate scenarios:\n",
    "     - Prompt: \"Generate 5 distinct business scenarios (e.g. Supply Chain Disruption, Competitor Entry). Return JSON array: {{ \"scenario_name\": \"...\", \"impact_factor\": 0.85, \"narrative\": \"...\", \"explanation\": \"...\" }}.\"\n",
    "  2. Parse into `scenarios` CTE using `from_json`.\n",
    "  3. `CROSS JOIN` main data with `scenarios`.\n",
    "  4. Calculate: `projected_metric = actual_metric * impact_factor`.\n",
    "  5. **MANDATORY OUTPUT COLUMNS**: `scenario_name`, `projected_metric`, `scenario_narrative`, `explanation`.\n",
    "\n",
    "**C. GEOSPATIAL ANALYSIS**:\n",
    "- **Implementation**: Use H3 functions for efficient spatial indexing.\n",
    "- **Functions**: `h3_longlatash3(lon, lat, resolution)`, `h3_centeraswkt(h3_cell)`.\n",
    "- **Logic**: Group data by H3 cells (`GROUP BY h3_cell`) to find regional hotspots.\n",
    "\n",
    "**D. MARKET BASKET ANALYSIS**:\n",
    "- **Implementation**: Self-Join or Array Intersection.\n",
    "- **Logic**:\n",
    "  1. `COLLECT_SET(product_id)` per transaction.\n",
    "  2. Join transactions to find co-occurrences.\n",
    "  3. Calculate Support, Confidence, Lift.\n",
    "\n",
    "---\n",
    "\n",
    "#### 0a. **CRITICAL vs OPTIONAL COLUMNS - DATA QUALITY FILTERING** (CRITICAL):\n",
    "\n",
    "**\uD83D\uDEA8 MANDATORY: Distinguish between CRITICAL and OPTIONAL columns \uD83D\uDEA8**\n",
    "\n",
    "**RULE**: Before applying COALESCE for NULL handling, you MUST first filter out rows with NULL/empty values in CRITICAL columns.\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 MANDATORY RULE: EVERY COLUMN IN FIRST CTE MUST HAVE NULL PROTECTION \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**⛔ ZERO EXCEPTIONS - THIS RULE MUST BE ENFORCED FOR EVERY SINGLE COLUMN ⛔**\n",
    "\n",
    "**Every single column selected in the FIRST CTE must be EITHER:**\n",
    "1. ✅ Filtered with `IS NOT NULL` in the WHERE clause (for CRITICAL columns), OR\n",
    "2. ✅ Wrapped with `COALESCE(column, 'default_value')` (for OPTIONAL columns)\n",
    "\n",
    "**⚠️ NO COLUMN CAN BE SELECTED WITHOUT NULL PROTECTION - NOT EVEN ONE! ⚠️**\n",
    "\n",
    "**COMMON MISTAKE - FORGETTING TO PROTECT A COLUMN:**\n",
    "```sql\n",
    "-- ❌❌❌ WRONG - workspaceName has NO NULL protection! ❌❌❌\n",
    "SELECT DISTINCT\n",
    "    workspaceId,           -- ✅ Filtered with IS NOT NULL\n",
    "    workspaceName,         -- ❌❌❌ MISSING COALESCE OR IS NOT NULL! ❌❌❌\n",
    "    COALESCE(TRIM(cloudType), 'Unknown Cloud') AS cloudType  -- ✅ COALESCE'd\n",
    "FROM table\n",
    "WHERE workspaceId IS NOT NULL  -- workspaceName NOT checked!\n",
    "\n",
    "-- ✅✅✅ CORRECT - EVERY column has NULL protection ✅✅✅\n",
    "SELECT DISTINCT\n",
    "    workspaceId,           -- ✅ Filtered with IS NOT NULL\n",
    "    COALESCE(TRIM(workspaceName), 'Unknown Workspace') AS workspaceName,  -- ✅ COALESCE'd!\n",
    "    COALESCE(TRIM(cloudType), 'Unknown Cloud') AS cloudType  -- ✅ COALESCE'd\n",
    "FROM table\n",
    "WHERE workspaceId IS NOT NULL\n",
    "```\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: COALESCE DEFAULT VALUES MUST BE QUOTED STRINGS \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**ALL COALESCE default values MUST have SINGLE QUOTES around them!**\n",
    "\n",
    "```sql\n",
    "-- ❌❌❌ WRONG - Default values are NOT quoted (SYNTAX ERROR!) ❌❌❌\n",
    "COALESCE(TRIM(cloudType), Unknown Cloud) AS cloudType        -- ❌ FAILS! Missing quotes!\n",
    "COALESCE(TRIM(workloadType), Unknown Workload) AS workloadType  -- ❌ FAILS! Missing quotes!\n",
    "COALESCE(TRIM(status), Unknown) AS status                    -- ❌ FAILS! Missing quotes!\n",
    "\n",
    "-- ✅✅✅ CORRECT - Default values have SINGLE QUOTES ✅✅✅\n",
    "COALESCE(TRIM(cloudType), 'Unknown Cloud') AS cloudType           -- ✅ Quoted!\n",
    "COALESCE(TRIM(workloadType), 'Unknown Workload') AS workloadType  -- ✅ Quoted!\n",
    "COALESCE(TRIM(status), 'Unknown') AS status                       -- ✅ Quoted!\n",
    "COALESCE(numeric_col, 0.0) AS numeric_col                         -- ✅ Numbers don't need quotes\n",
    "COALESCE(CAST(bool_col AS STRING), 'false') AS bool_col           -- ✅ String default quoted\n",
    "```\n",
    "\n",
    "**WHY THIS MATTERS:**\n",
    "- ONE NULL column = ENTIRE CONCAT prompt becomes NULL = QUERY FAILS OR RETURNS GARBAGE\n",
    "- NULL values propagate downstream through all CTEs\n",
    "- LEFT JOINs can introduce NULLs even for previously non-NULL columns\n",
    "- AI functions receiving NULL input produce unpredictable results\n",
    "- Unquoted default values cause SYNTAX ERRORS\n",
    "\n",
    "**VALIDATION CHECKLIST FOR FIRST CTE (CHECK EVERY COLUMN!):**\n",
    "☐ **EVERY** column has NULL protection - scan each column one by one!\n",
    "☐ Every ID/key column: Has `IS NOT NULL` in WHERE clause\n",
    "☐ Every string column: Has `COALESCE(TRIM(col), 'Default')` OR `IS NOT NULL` in WHERE\n",
    "☐ Every numeric column: Has `COALESCE(col, 0.0)` OR `IS NOT NULL` in WHERE  \n",
    "☐ Every date column: Has `IS NOT NULL` in WHERE (dates rarely have safe defaults)\n",
    "☐ Every column used in AI_FORECAST group_col: Has `IS NOT NULL` in WHERE (CRITICAL!)\n",
    "☐ **ALL COALESCE default STRING values have SINGLE QUOTES**\n",
    "\n",
    "**CRITICAL COLUMNS** - Use IS NOT NULL and TRIM() checks:\n",
    "- Primary keys and foreign keys (customer_id, product_id, route_id, etc.)\n",
    "- Required business identifiers (order_number, transaction_id, flight_number, etc.)\n",
    "- Essential dimensions for grouping (category, region, store_id, etc.)\n",
    "- Date/time columns used for time_col in AI_FORECAST\n",
    "- Columns used in group_col for AI_FORECAST\n",
    "- Core business metrics that define the record's validity\n",
    "- **\uD83D\uDEA8 ALL date-related columns** including:\n",
    "  - `date`, `timestamp`, `created_at`, `updated_at`\n",
    "  - **Year-month columns like `yyyymm`, `year_month`, `fiscal_period`** - these MUST have `IS NOT NULL` in WHERE\n",
    "  - Date components like `year`, `month`, `quarter`, `week`\n",
    "\n",
    "**DATE/TIME COLUMN NULL HANDLING:**\n",
    "```sql\n",
    "-- For date columns that are CRITICAL for time-series or grouping:\n",
    "WHERE date IS NOT NULL\n",
    "  AND yyyymm IS NOT NULL          -- ✅ CRITICAL: Don't forget period columns!\n",
    "  AND fiscal_period IS NOT NULL   -- ✅ If used for grouping/analysis\n",
    "\n",
    "-- For date columns that are OPTIONAL (can have defaults):\n",
    "COALESCE(CAST(created_date AS STRING), 'Unknown') AS created_date_str  -- ✅ For display\n",
    "```\n",
    "\n",
    "**OPTIONAL COLUMNS** - Use COALESCE with appropriate defaults:\n",
    "- Descriptive text fields (descriptions, comments, notes)\n",
    "- Status fields that have reasonable defaults\n",
    "- Supplementary metrics\n",
    "- Attributes that enhance context but aren't essential\n",
    "\n",
    "**CORRECT PATTERN ✅:**\n",
    "```sql\n",
    "WITH clean_data AS (\n",
    "  SELECT \n",
    "    route_id,                    -- CRITICAL: primary key\n",
    "    flight_number,               -- CRITICAL: required identifier\n",
    "    departure_airport_iata,      -- CRITICAL: required for business logic\n",
    "    arrival_airport_iata,        -- CRITICAL: required for business logic\n",
    "    flight_date,                 -- CRITICAL: time dimension\n",
    "    -- CRITICAL columns checked with IS NOT NULL\n",
    "    COALESCE(aircraft_type, 'Unknown Aircraft') AS aircraft_type,     -- OPTIONAL: can default\n",
    "    COALESCE(passenger_count, 0) AS passenger_count,                   -- OPTIONAL: keep as INT\n",
    "    COALESCE(delay_minutes, 0) AS delay_minutes,                       -- OPTIONAL: keep as INT\n",
    "    COALESCE(on_time_indicator, 'Unknown') AS on_time_indicator,       -- OPTIONAL: can default\n",
    "    COALESCE(weather_condition, 'CLEAR') AS weather_condition          -- OPTIONAL: can default\n",
    "  FROM `catalog`.`schema`.`flights` AS f\n",
    "  WHERE route_id IS NOT NULL                          -- CRITICAL\n",
    "    AND flight_number IS NOT NULL                      -- CRITICAL\n",
    "    AND TRIM(flight_number) <> ''                      -- CRITICAL: not empty string\n",
    "    AND departure_airport_iata IS NOT NULL             -- CRITICAL\n",
    "    AND TRIM(departure_airport_iata) <> ''             -- CRITICAL\n",
    "    AND arrival_airport_iata IS NOT NULL               -- CRITICAL\n",
    "    AND TRIM(arrival_airport_iata) <> ''               -- CRITICAL\n",
    "    AND flight_date IS NOT NULL                        -- CRITICAL\n",
    "    -- TODO: Add suitable filtering to load data that matches the intended slice for this use case (keep commented until confirmed)\n",
    "    -- AND lower(trim(route_status)) = 'running'  -- Example placeholder; adjust column/value\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at the END of first CTE only\n",
    ")\n",
    "SELECT * FROM clean_data;  -- ✅ NO LIMIT in final SELECT\n",
    "```\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL: LIMIT 10 SAMPLING RULES \uD83D\uDEA8**\n",
    "\n",
    "**MANDATORY RULES FOR DATA SAMPLING:**\n",
    "1. **FIRST CTE ONLY**: Use `LIMIT 10` at the END of the FIRST CTE that reads from tables\n",
    "2. **NO LIMIT IN OTHER CTEs**: DO NOT use `LIMIT 10` in any other CTE - only in the first CTE\n",
    "3. **LIMIT PLACEMENT**: LIMIT 10 MUST be the LAST clause in the SELECT (after WHERE, ORDER BY, GROUP BY, etc.)\n",
    "4. **SYNTAX**: `FROM catalog.schema.table AS t WHERE ... LIMIT 10`\n",
    "\n",
    "**✅ CORRECT PATTERN:**\n",
    "```sql\n",
    "-- \uD83D\uDEA8 EVERY column must be COALESCE'd or have IS NOT NULL check!\n",
    "WITH base_data AS (\n",
    "  SELECT DISTINCT \n",
    "    customer_id,                                            -- CRITICAL: filtered with IS NOT NULL\n",
    "    COALESCE(TRIM(customer_name), 'Unknown') AS customer_name  -- ✅ COALESCE'd\n",
    "    -- ... (all columns must be COALESCE'd or have IS NOT NULL) ...\n",
    "  FROM `catalog`.`schema`.`customers` AS c\n",
    "  WHERE customer_id IS NOT NULL\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at the END of first CTE only\n",
    "),\n",
    "enriched_data AS (\n",
    "  SELECT * FROM base_data  -- ✅ NO LIMIT\n",
    "),\n",
    "final_analysis AS (\n",
    "  SELECT * FROM enriched_data  -- ✅ NO LIMIT\n",
    ")\n",
    "SELECT * FROM final_analysis;  -- ✅ NO LIMIT in final SELECT\n",
    "```\n",
    "\n",
    "**❌ WRONG PATTERNS (DO NOT USE LIMIT ANYWHERE!):**\n",
    "```sql\n",
    "-- ❌ WRONG: LIMIT not at the END of the statement\n",
    "WITH base_data AS (SELECT * FROM table LIMIT 10 WHERE x = 1)  -- LIMIT must be LAST\n",
    "\n",
    "-- ❌ WRONG: LIMIT in intermediate CTE\n",
    "enriched_data AS (SELECT * FROM base_data LIMIT 10)\n",
    "\n",
    "-- ❌ WRONG: LIMIT in final SELECT\n",
    "SELECT * FROM final_analysis LIMIT 10;\n",
    "\n",
    "-- ❌ WRONG: LIMIT in intermediate CTE\n",
    "enriched AS (SELECT * FROM base_data LIMIT 10)  -- NO LIMIT in intermediate CTEs\n",
    "```\n",
    "\n",
    "**WRONG PATTERN ❌:**\n",
    "```sql\n",
    "-- BAD: COALESCing critical columns without filtering\n",
    "SELECT \n",
    "  COALESCE(route_id, 'UNKNOWN') AS route_id,           -- ❌ Primary keys should never be NULL!\n",
    "  COALESCE(flight_date, CURRENT_DATE()) AS flight_date -- ❌ Time columns should never be defaulted!\n",
    "FROM flights\n",
    "-- No WHERE clause to filter out bad data\n",
    "```\n",
    "\n",
    "**VALIDATION CHECKLIST:**\n",
    "☐ Primary keys/foreign keys: IS NOT NULL\n",
    "☐ Required identifiers: IS NOT NULL AND TRIM() <> ''\n",
    "☐ time_col for AI_FORECAST: IS NOT NULL\n",
    "☐ group_col columns for AI_FORECAST: IS NOT NULL\n",
    "☐ Essential business dimensions: IS NOT NULL\n",
    "☐ Optional attributes: COALESCE with appropriate defaults\n",
    "☐ **Add commented-out filtering suggestion (TODO) in the first CTE's WHERE clause** to guide user customization.\n",
    "\n",
    "**WHY THIS MATTERS:**\n",
    "- NULL critical columns indicate data quality issues - don't hide them with COALESCE\n",
    "- Filtering ensures AI_FORECAST gets high-quality training data\n",
    "- Prevents garbage-in-garbage-out scenarios\n",
    "- Makes data quality issues visible and actionable\n",
    "- Commented filters help users quickly adapt the query to their specific slice of data (e.g. status='active', region='NA')\n",
    "\n",
    "#### 0b. **NULL HANDLING - MANDATORY FOR ALL AI FUNCTION PROMPTS** (CRITICAL):\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL NULL BEHAVIOR: `CONCAT(2, 3, 1.0, 'hello', NULL)` → NULL (the entire result is NULL!) \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "When using CONCAT to build prompts for ai_query, ai_gen, or any AI function, NULL values in ANY column will nullify the ENTIRE concatenated string. You MUST handle NULL values with COALESCE for EVERY SINGLE VALUE used in the prompt.\n",
    "\n",
    "**\uD83D\uDD25 ZERO TOLERANCE POLICY: EVERY VALUE IN CONCAT MUST BE NULL-SAFE \uD83D\uDD25**\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 MANDATORY: NO COALESCE INSIDE CONCAT - NULL HANDLING MUST BE DONE BEFORE \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**RULE: NEVER use COALESCE(), CAST(), ROUND(), or TRIM() inside CONCAT() calls!**\n",
    "All NULL handling, type conversions, and formatting MUST be done in PREVIOUS CTEs or in the SELECT clause BEFORE the CONCAT.\n",
    "\n",
    "```sql\n",
    "-- ❌❌❌ WRONG: COALESCE inside CONCAT ❌❌❌\n",
    "SELECT ai_query('model', \n",
    "  CONCAT('Customer: ', COALESCE(customer_name, 'Unknown'),  -- ❌ COALESCE in CONCAT!\n",
    "         ', Amount: $', COALESCE(ROUND(amount, 2), 0.0)))   -- ❌ COALESCE+ROUND in CONCAT!\n",
    "FROM raw_data;\n",
    "\n",
    "-- ✅✅✅ CORRECT: COALESCE in previous CTE, then use clean columns in CONCAT ✅✅✅\n",
    "WITH null_safe_data AS (\n",
    "  SELECT \n",
    "    COALESCE(TRIM(customer_name), 'Unknown') AS customer_name,  -- ✅ COALESCE'd HERE\n",
    "    COALESCE(ROUND(amount, 2), 0.0) AS amount                   -- ✅ COALESCE'd + ROUND'd HERE\n",
    "  FROM raw_data AS r\n",
    "  WHERE customer_id IS NOT NULL\n",
    "  LIMIT 10\n",
    ")\n",
    "SELECT ai_query('model',\n",
    "  CONCAT('Customer: ', customer_name,  -- ✅ Already NULL-safe from previous CTE\n",
    "         ', Amount: $', amount))        -- ✅ Already NULL-safe from previous CTE\n",
    "FROM null_safe_data;\n",
    "```\n",
    "\n",
    "**WHY THIS MATTERS:**\n",
    "- Cleaner, more readable SQL\n",
    "- NULL handling is explicit and visible\n",
    "- Easier to debug and maintain\n",
    "- Follows separation of concerns (data prep vs. prompt building)\n",
    "- Prevents duplicate COALESCE operations\n",
    "\n",
    "**KEY INSIGHT: CONCAT SUPPORTS MIXED TYPES - NO CASTING TO STRING NEEDED:**\n",
    "```sql\n",
    "SELECT CONCAT(2, 3, 1.0, 'hello') → '231.0hello'  -- ✅ Works! No CAST needed\n",
    "SELECT CONCAT(2, 3, 1.0, 'hello', NULL) → NULL    -- ❌ ONE NULL ruins everything!\n",
    "```\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL ANTI-PATTERNS TO AVOID - READ CAREFULLY! \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**⛔⛔⛔ ANTI-PATTERN #0: UNQUOTED COALESCE DEFAULT VALUES (THE #1 MOST COMMON ERROR!) ⛔⛔⛔**\n",
    "\n",
    "**THIS IS THE EXACT ERROR PATTERN THAT KEEPS HAPPENING - DO NOT MAKE THIS MISTAKE:**\n",
    "```sql\n",
    "-- ❌❌❌ CATASTROPHICALLY WRONG - EVERY STRING DEFAULT IS MISSING QUOTES ❌❌❌\n",
    "WITH base_material_data AS (\n",
    "  SELECT DISTINCT\n",
    "    material_usage_id,\n",
    "    COALESCE(TRIM(material_type), Unknown Material) AS material_type,     -- ❌ SYNTAX ERROR!\n",
    "    COALESCE(TRIM(material_description), No Description) AS material_description,  -- ❌ SYNTAX ERROR!\n",
    "    COALESCE(quantity_used, 0.0) AS quantity_used,\n",
    "    COALESCE(TRIM(unit_of_measure), Unknown) AS unit_of_measure,          -- ❌ SYNTAX ERROR!\n",
    "    COALESCE(waste_factor_percent, 0.0) AS waste_factor_percent,\n",
    "    COALESCE(TRIM(waste_reason), No Reason) AS waste_reason,              -- ❌ SYNTAX ERROR!\n",
    "    COALESCE(CAST(delivery_date AS STRING), Unknown Date) AS delivery_date_str,  -- ❌ SYNTAX ERROR!\n",
    "    COALESCE(TRIM(storage_location), Unknown Location) AS storage_location,  -- ❌ SYNTAX ERROR!\n",
    "    COALESCE(TRIM(supplier_name), Unknown Supplier) AS supplier_name,     -- ❌ SYNTAX ERROR!\n",
    "    COALESCE(TRIM(inspection_status), Unknown) AS inspection_status       -- ❌ SYNTAX ERROR!\n",
    "  FROM `catalog`.`schema`.`table` AS m\n",
    ")\n",
    "-- ⛔ ALL OF THE ABOVE WILL FAIL WITH: [PARSE_SYNTAX_ERROR] Syntax error at or near 'Material'/'Description'/etc.\n",
    "\n",
    "-- ✅✅✅ CORRECT - EVERY STRING DEFAULT HAS 'SINGLE QUOTES' ✅✅✅\n",
    "WITH base_material_data AS (\n",
    "  SELECT DISTINCT\n",
    "    material_usage_id,\n",
    "    COALESCE(TRIM(material_type), 'Unknown Material') AS material_type,     -- ✅ QUOTED!\n",
    "    COALESCE(TRIM(material_description), 'No Description') AS material_description,  -- ✅ QUOTED!\n",
    "    COALESCE(quantity_used, 0.0) AS quantity_used,                          -- Numbers: no quotes\n",
    "    COALESCE(TRIM(unit_of_measure), 'Unknown') AS unit_of_measure,          -- ✅ QUOTED!\n",
    "    COALESCE(waste_factor_percent, 0.0) AS waste_factor_percent,            -- Numbers: no quotes\n",
    "    COALESCE(TRIM(waste_reason), 'No Reason') AS waste_reason,              -- ✅ QUOTED!\n",
    "    COALESCE(CAST(delivery_date AS STRING), 'Unknown Date') AS delivery_date_str,  -- ✅ QUOTED!\n",
    "    COALESCE(TRIM(storage_location), 'Unknown Location') AS storage_location,  -- ✅ QUOTED!\n",
    "    COALESCE(TRIM(supplier_name), 'Unknown Supplier') AS supplier_name,     -- ✅ QUOTED!\n",
    "    COALESCE(TRIM(inspection_status), 'Unknown') AS inspection_status       -- ✅ QUOTED!\n",
    "  FROM `catalog`.`schema`.`table` AS m\n",
    ")\n",
    "```\n",
    "\n",
    "**\uD83D\uDD34 THE RULE IS SIMPLE: Text after comma in COALESCE = MUST have 'single quotes' \uD83D\uDD34**\n",
    "\n",
    "**More examples of WRONG vs CORRECT:**\n",
    "```sql\n",
    "-- ❌ WRONG (SYNTAX ERROR)                      |  ✅ CORRECT (will work)\n",
    "COALESCE(TRIM(name), Unknown)                   |  COALESCE(TRIM(name), 'Unknown')\n",
    "COALESCE(TRIM(type), Type A)                    |  COALESCE(TRIM(type), 'Type A')\n",
    "COALESCE(TRIM(status), Pending Review)          |  COALESCE(TRIM(status), 'Pending Review')\n",
    "COALESCE(TRIM(owner), Not Assigned)             |  COALESCE(TRIM(owner), 'Not Assigned')\n",
    "COALESCE(TRIM(po), Unknown PO)                  |  COALESCE(TRIM(po), 'Unknown PO')\n",
    "COALESCE(CAST(date AS STRING), No Date)         |  COALESCE(CAST(date AS STRING), 'No Date')\n",
    "```\n",
    "\n",
    "**ANTI-PATTERN #0b: FORGETTING TO PROTECT A COLUMN (NULL PROPAGATES!):**\n",
    "```sql\n",
    "-- ❌❌❌ WRONG - workspaceName has NO NULL protection! ❌❌❌\n",
    "SELECT DISTINCT\n",
    "  workspaceId,                                           -- ✅ Filtered with IS NOT NULL\n",
    "  workspaceName,                                         -- ❌❌❌ NO PROTECTION! NULL will propagate!\n",
    "  COALESCE(TRIM(cloudType), 'Unknown Cloud') AS cloudType  -- ✅ COALESCE'd\n",
    "FROM table\n",
    "WHERE workspaceId IS NOT NULL   -- workspaceName is NOT checked!\n",
    "\n",
    "-- ✅✅✅ CORRECT - EVERY column has NULL protection ✅✅✅\n",
    "SELECT DISTINCT\n",
    "  workspaceId,                                           -- ✅ Filtered with IS NOT NULL\n",
    "  COALESCE(TRIM(workspaceName), 'Unknown Workspace') AS workspaceName,  -- ✅ COALESCE'd!\n",
    "  COALESCE(TRIM(cloudType), 'Unknown Cloud') AS cloudType  -- ✅ COALESCE'd\n",
    "FROM table\n",
    "WHERE workspaceId IS NOT NULL\n",
    "```\n",
    "\n",
    "**ANTI-PATTERN #1: SELECTING BOTH ORIGINAL AND COALESCED VALUE (NEVER DO THIS!):**\n",
    "```sql\n",
    "-- ❌ WRONG: Selecting the same column twice (original + coalesced version)\n",
    "SELECT\n",
    "  a.account_name,  -- ❌ Original value\n",
    "  COALESCE(TRIM(a.account_name), 'Unknown') AS account_name_str,  -- ❌ Duplicate!\n",
    "  a.arr,  -- ❌ Original value\n",
    "  COALESCE(CAST(a.arr AS STRING), '0.00') AS arr_str  -- ❌ Duplicate!\n",
    "FROM accounts AS a\n",
    "\n",
    "-- ✅ CORRECT: COALESCE once, use that version everywhere\n",
    "SELECT\n",
    "  COALESCE(TRIM(a.account_name), 'Unknown') AS account_name,  -- ✅ One version\n",
    "  COALESCE(a.arr, 0.0) AS arr  -- ✅ One version, correct type (DOUBLE)\n",
    "FROM accounts AS a\n",
    "```\n",
    "\n",
    "**ANTI-PATTERN #2: COALESCE TO STRING THEN CAST BACK TO DOUBLE (WASTEFUL!):**\n",
    "```sql\n",
    "-- ❌ WRONG: Converting to STRING then back to DOUBLE for calculations\n",
    "WITH base AS (\n",
    "  SELECT COALESCE(CAST(arr AS STRING), '0.00') AS arr_str  -- ❌ Why STRING?\n",
    "  FROM accounts\n",
    "),\n",
    "stats AS (\n",
    "  SELECT AVG(CAST(arr_str AS DOUBLE)) OVER () AS avg_arr  -- ❌ Casting back to DOUBLE!\n",
    "  FROM base\n",
    ")\n",
    "\n",
    "-- ✅ CORRECT: Keep numeric values as DOUBLE, only use STRING for text-only columns\n",
    "WITH base AS (\n",
    "  SELECT \n",
    "    COALESCE(arr, 0.0) AS arr,  -- ✅ Keep as DOUBLE for calculations\n",
    "    COALESCE(TRIM(account_name), 'Unknown') AS account_name  -- ✅ STRING for text\n",
    "  FROM accounts\n",
    "),\n",
    "stats AS (\n",
    "  SELECT \n",
    "    arr,\n",
    "    account_name,\n",
    "    COALESCE(ROUND(AVG(arr) OVER (), 2), 0.0) AS avg_arr  -- ✅ Direct DOUBLE calculation\n",
    "  FROM base\n",
    ")\n",
    "-- In CONCAT, DOUBLE values auto-convert: CONCAT('ARR: $', arr, ' vs avg $', avg_arr)\n",
    "```\n",
    "\n",
    "**ANTI-PATTERN #3: SEPARATE CTE JUST FOR COALESCE (NO BUSINESS VALUE!):**\n",
    "```sql\n",
    "-- ❌ WRONG: First CTE plain select, second CTE only for COALESCE\n",
    "WITH base_data AS (\n",
    "  SELECT account_id, account_name, arr, vertical  -- ❌ Plain select\n",
    "  FROM accounts\n",
    "  LIMIT 10\n",
    "),\n",
    "null_safe_data AS (  -- ❌ CTE with NO business value - only COALESCE\n",
    "  SELECT\n",
    "    account_id,\n",
    "    COALESCE(TRIM(account_name), 'Unknown') AS account_name,\n",
    "    COALESCE(arr, 0.0) AS arr,\n",
    "    COALESCE(TRIM(vertical), 'Unknown') AS vertical\n",
    "  FROM base_data\n",
    ")\n",
    "\n",
    "-- ✅ CORRECT: Apply COALESCE in the SAME CTE that retrieves data\n",
    "WITH account_data AS (\n",
    "  SELECT\n",
    "    account_id,  -- CRITICAL: filtered with WHERE, not COALESCE'd\n",
    "    COALESCE(TRIM(account_name), 'Unknown') AS account_name,\n",
    "    COALESCE(arr, 0.0) AS arr,\n",
    "    COALESCE(TRIM(vertical), 'Unknown') AS vertical\n",
    "  FROM accounts AS a\n",
    "  WHERE account_id IS NOT NULL AND account_name IS NOT NULL\n",
    "  LIMIT 10\n",
    ")\n",
    "-- One CTE does both: filtering + NULL handling\n",
    "```\n",
    "\n",
    "**ANTI-PATTERN #4: NOT COALESCING VALUES FROM LEFT JOIN (CAUSES NULL PROMPT!):**\n",
    "```sql\n",
    "-- ❌ WRONG: LEFT JOIN columns can be NULL and are used in CONCAT without COALESCE\n",
    "WITH accounts AS (...),\n",
    "benchmarks AS (...),\n",
    "combined AS (\n",
    "  SELECT a.*, b.industry_avg, b.segment_median  -- ❌ b.* columns can be NULL from LEFT JOIN!\n",
    "  FROM accounts AS a\n",
    "  LEFT JOIN benchmarks AS b ON a.industry = b.industry\n",
    "),\n",
    "prompt_cte AS (\n",
    "  SELECT CONCAT('Industry avg: ', industry_avg, ', Median: ', segment_median)  -- ❌ NULL if no match!\n",
    "  FROM combined\n",
    ")\n",
    "\n",
    "-- ✅ CORRECT: COALESCE all LEFT JOIN columns when used in CONCAT\n",
    "WITH accounts AS (...),\n",
    "benchmarks AS (...),\n",
    "combined AS (\n",
    "  SELECT \n",
    "    a.*,\n",
    "    COALESCE(b.industry_avg, 0.0) AS industry_avg,  -- ✅ COALESCE joined columns\n",
    "    COALESCE(b.segment_median, 0.0) AS segment_median  -- ✅ COALESCE joined columns\n",
    "  FROM accounts AS a\n",
    "  LEFT JOIN benchmarks AS b ON a.industry = b.industry\n",
    "),\n",
    "prompt_cte AS (\n",
    "  SELECT CONCAT('Industry avg: ', industry_avg, ', Median: ', segment_median)  -- ✅ NULL-safe\n",
    "  FROM combined\n",
    ")\n",
    "```\n",
    "\n",
    "**\uD83D\uDD25 CORRECT PATTERN - COMPREHENSIVE EXAMPLE \uD83D\uDD25:**\n",
    "\n",
    "```sql\n",
    "-- Step 1: Retrieve data with COALESCE applied ONCE, keeping correct types\n",
    "WITH account_data AS (\n",
    "  SELECT \n",
    "    -- CRITICAL columns: filter with WHERE, don't COALESCE\n",
    "    account_id,\n",
    "    \n",
    "    -- String columns: COALESCE to STRING defaults\n",
    "    COALESCE(TRIM(account_name), 'Unknown Account') AS account_name,\n",
    "    COALESCE(TRIM(vertical), 'Unknown Vertical') AS vertical,\n",
    "    COALESCE(TRIM(account_tier), 'Not Classified') AS account_tier,\n",
    "    \n",
    "    -- Numeric columns: COALESCE to DOUBLE/INT defaults (NOT STRING!)\n",
    "    COALESCE(arr, 0.0) AS arr,  -- ✅ Keep as DOUBLE\n",
    "    COALESCE(t3m_annualized, 0.0) AS t3m_annualized,  -- ✅ Keep as DOUBLE\n",
    "    COALESCE(customer_age_years, 0.0) AS customer_age_years,  -- ✅ Keep as DOUBLE\n",
    "    \n",
    "    -- Boolean columns: COALESCE to BOOLEAN default\n",
    "    COALESCE(strategic_account, FALSE) AS strategic_account,\n",
    "    COALESCE(fortune_500, FALSE) AS fortune_500,\n",
    "    \n",
    "    -- Date columns: COALESCE to STRING for display\n",
    "    COALESCE(CAST(next_renewal_date AS STRING), 'No Renewal Date') AS next_renewal_date\n",
    "    \n",
    "  FROM `catalog`.`schema`.`accounts` AS a\n",
    "  WHERE account_id IS NOT NULL  -- CRITICAL: Filter NULL primary keys\n",
    "    AND account_name IS NOT NULL  -- CRITICAL: Filter NULL required fields\n",
    "  LIMIT 10\n",
    "),\n",
    "\n",
    "-- Step 2: Calculate statistics (business value CTE) - keep numeric types\n",
    "account_statistics AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    -- Statistical metrics - keep as DOUBLE, COALESCE the result\n",
    "    COALESCE(ROUND(AVG(arr) OVER (), 2), 0.0) AS avg_arr,\n",
    "    COALESCE(ROUND(MEDIAN(arr) OVER (), 2), 0.0) AS median_arr,\n",
    "    COALESCE(ROUND(STDDEV_POP(arr) OVER (), 2), 0.0) AS stddev_arr,\n",
    "    COALESCE(ROUND(PERCENTILE_APPROX(arr, 0.75) OVER (), 2), 0.0) AS p75_arr,\n",
    "    COALESCE(ROUND(PERCENT_RANK() OVER (ORDER BY arr), 3), 0.0) AS arr_percentile_rank,\n",
    "    COALESCE(NTILE(10) OVER (ORDER BY arr), 5) AS arr_decile\n",
    "  FROM account_data\n",
    "),\n",
    "\n",
    "-- Step 3: Build AI prompt - Generate ai_sys_prompt column FIRST\n",
    "-- CONCAT handles mixed types automatically\n",
    "account_prompt_generation AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    CONCAT(\n",
    "      'You are an Account Strategy Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "      'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "      'Business priorities are: {enriched_business_priorities}. ',\n",
    "      'With 18 years of experience in enterprise account management, revenue optimization, and strategic planning, ',\n",
    "      'your expertise in account health analysis and growth strategy aligns with the strategic initiative: Customer success and retention. ',\n",
    "      'Analyze account: ', account_name, ' (ID: ', account_id, '). ',\n",
    "      'Vertical: ', vertical, ', Tier: ', account_tier, '. ',\n",
    "      'ARR: $', ROUND(arr, 2), ' (Percentile: ', ROUND(arr_percentile_rank * 100, 1), '%, Decile: ', arr_decile, '). ',\n",
    "      'T3M: $', ROUND(t3m_annualized, 2), ', Age: ', ROUND(customer_age_years, 1), ' years. ',\n",
    "      'Strategic: ', strategic_account, ', Fortune 500: ', fortune_500, '. ',\n",
    "      'Benchmarks - Avg ARR: $', avg_arr, ', Median: $', median_arr, ', P75: $', p75_arr, '. ',\n",
    "      'Next Renewal: ', next_renewal_date, '. ',\n",
    "      'Output ONLY JSON: {{\"ai_cat_priority\": \"value\", \"ai_txt_recommendation\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"I assessed my confidence at 85% because...\", \"ai_sys_missing_data\": \"I can get higher confidence than 85% if I can get access to [narrative]. {{\\\"missing_data\\\": [\\\"dataset1\\\", \\\"dataset2\\\"]}}\"}}. ',\n",
    "      'MANDATORY LAST 7 FIELDS: ai_txt_business_outcome (CALCULATED MEASURABLE IMPACT with Daily/Weekly/Monthly/Yearly breakdown and DISCLAIMER), ai_txt_executive_summary (references business outcome numbers), ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data.'\n",
    "    ) AS ai_sys_prompt  -- ✅ Named ai_sys_prompt for auditability\n",
    "  FROM account_statistics\n",
    "),\n",
    "\n",
    "-- Step 4: AI analysis - pass ai_sys_prompt to ai_query\n",
    "account_analysis AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS insights_json\n",
    "  FROM account_prompt_generation\n",
    "),\n",
    "\n",
    "-- Step 5: Final output with extracted fields and ai_sys_prompt as LAST column\n",
    "final_output AS (\n",
    "  SELECT \n",
    "    account_id,\n",
    "    account_name,\n",
    "    vertical,\n",
    "    account_tier,\n",
    "    arr,\n",
    "    t3m_annualized,\n",
    "    customer_age_years,\n",
    "    strategic_account,\n",
    "    fortune_500,\n",
    "    next_renewal_date,\n",
    "    avg_arr,\n",
    "    median_arr,\n",
    "    p75_arr,\n",
    "    arr_percentile_rank,\n",
    "    arr_decile,\n",
    "    get_json_object(insights_json, '$.ai_cat_priority') AS ai_cat_priority,\n",
    "    get_json_object(insights_json, '$.ai_txt_recommendation') AS ai_txt_recommendation,\n",
    "    -- MANDATORY LAST 7 COLUMNS (including ai_txt_business_outcome + ai_sys_prompt):\n",
    "    COALESCE(get_json_object(insights_json, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "    COALESCE(get_json_object(insights_json, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "    COALESCE(get_json_object(insights_json, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "    COALESCE(get_json_object(insights_json, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "    COALESCE(TRY_CAST(get_json_object(insights_json, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "    COALESCE(get_json_object(insights_json, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "    COALESCE(get_json_object(insights_json, '$.ai_sys_missing_data'), 'No missing data identified') AS ai_sys_missing_data,\n",
    "    ai_sys_prompt  -- ✅ LAST COLUMN: The exact prompt used for auditability\n",
    "  FROM account_analysis\n",
    ")\n",
    "SELECT * FROM final_output\n",
    "-- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "-- WHERE ai_cat_priority IN ('Critical', 'High', 'Medium', 'Low')\n",
    "-- AND ai_sys_importance IN ('Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical')\n",
    "-- AND ai_sys_urgency IN ('Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical')\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "\n",
    "**\uD83D\uDD25 MANDATORY RULES FOR NULL HANDLING \uD83D\uDD25:**\n",
    "\n",
    "1. **COALESCE ONCE at first retrieval** - don't select both original and coalesced versions\n",
    "2. **Keep numeric types as DOUBLE** - COALESCE(arr, 0.0), NOT COALESCE(CAST(arr AS STRING), '0.00')\n",
    "3. **CONCAT auto-converts types** - no need to CAST to STRING before CONCAT\n",
    "4. **COALESCE statistical results** - window functions can return NULL, always COALESCE\n",
    "5. **COALESCE LEFT JOIN columns** - joined columns can be NULL when no match\n",
    "6. **Filter critical columns with WHERE** - use WHERE IS NOT NULL, not COALESCE for IDs/keys\n",
    "7. **Every CTE must have business value** - no CTE just for COALESCE transformations\n",
    "\n",
    "**TYPE-APPROPRIATE COALESCE DEFAULTS:**\n",
    "- **DOUBLE columns**: `COALESCE(numeric_col, 0.0)` - keep as DOUBLE for calculations\n",
    "- **INT columns**: `COALESCE(int_col, 0)` - keep as INT\n",
    "- **BOOLEAN columns**: `COALESCE(bool_col, FALSE)` - keep as BOOLEAN\n",
    "- **STRING columns**: `COALESCE(TRIM(string_col), 'Unknown')` - STRING with business-friendly default\n",
    "- **DATE columns**: `COALESCE(CAST(date_col AS STRING), 'No Date')` - convert to STRING for display\n",
    "- **Statistical results**: `COALESCE(ROUND(AVG(x) OVER (), 2), 0.0)` - always COALESCE window functions\n",
    "\n",
    "**VALIDATION CHECKLIST BEFORE SUBMITTING SQL:**\n",
    "☐ **\uD83D\uDEA8 NO COALESCE/ROUND/CAST/TRIM inside CONCAT** - all must be done in previous CTE\n",
    "☐ Every column in CONCAT has been COALESCEd in a PREVIOUS CTE (not inside CONCAT)\n",
    "☐ No column appears twice (once original, once coalesced)\n",
    "☐ Numeric values remain as DOUBLE/INT (not converted to STRING for calculations)\n",
    "☐ LEFT JOIN columns are COALESCEd in the CTE that performs the join, not in CONCAT\n",
    "☐ Statistical window function results are COALESCEd in the CTE that calculates them\n",
    "☐ Every CTE has business value (no CTE just for COALESCE)\n",
    "☐ CRITICAL columns (IDs, keys) are filtered with WHERE IS NOT NULL, not COALESCEd\n",
    "\n",
    "**Example where separate NULL handling is needed (after JOIN):**\n",
    "```sql\n",
    "-- Step 1: Join multiple tables first\n",
    "WITH joined_data AS (\n",
    "  SELECT \n",
    "    c.customer_id,\n",
    "    c.customer_name,\n",
    "    o.order_count,\n",
    "    r.avg_rating\n",
    "  FROM `catalog`.`schema`.`customers` AS c\n",
    "  LEFT JOIN `catalog`.`schema`.`order_summary` AS o ON c.customer_id = o.customer_id\n",
    "  LEFT JOIN `catalog`.`schema`.`ratings` AS r ON c.customer_id = r.customer_id\n",
    "  WHERE c.customer_id IS NOT NULL\n",
    "  LIMIT 10\n",
    "),\n",
    "-- Step 2: Apply NULL handling to joined result (business value: data enrichment + null safety)\n",
    "customer_enriched AS (\n",
    "  SELECT \n",
    "    customer_id,\n",
    "    COALESCE(TRIM(customer_name), 'Unknown') AS customer_name,\n",
    "    COALESCE(order_count, 0) AS order_count,  -- ✅ NULL from LEFT JOIN\n",
    "    COALESCE(avg_rating, 0.0) AS avg_rating   -- ✅ NULL from LEFT JOIN\n",
    "  FROM joined_data\n",
    "),\n",
    "-- Step 3: Build prompts - CONCAT auto-converts numeric types\n",
    "prompt_cte AS (\n",
    "  SELECT *,\n",
    "    CONCAT('Customer: ', customer_name, ', Orders: ', order_count, ', Rating: ', avg_rating) AS prompt\n",
    "  FROM customer_enriched\n",
    ")\n",
    "```\n",
    "\n",
    "**\uD83D\uDD25 REMEMBER: MISS ONE COALESCE = ENTIRE PROMPT IS NULL = QUERY FAILS \uD83D\uDD25**\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: CTE BUSINESS VALUE RULE - NO TECHNICAL-ONLY CTEs \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**ABSOLUTE RULE: Every CTE in your SQL MUST have BUSINESS VALUE, not just technical transformation value.**\n",
    "\n",
    "**DO NOT create separate CTEs ONLY for:**\n",
    "- COALESCE operations (NULL handling)\n",
    "- CAST operations (type conversions)\n",
    "- TRIM operations (whitespace cleaning)\n",
    "- Renaming columns\n",
    "- Concatenating strings\n",
    "- Any purely technical transformation\n",
    "\n",
    "**WHY THIS MATTERS:**\n",
    "- Separate COALESCE CTEs increase LOC without adding business value\n",
    "- They make SQL harder to read and maintain\n",
    "- They can introduce naming inconsistencies (column_name vs column_name_str)\n",
    "- They add unnecessary query complexity and potential performance overhead\n",
    "\n",
    "**✅ CORRECT: Keep numeric types as DOUBLE for calculations, CONCAT handles mixed types:**\n",
    "```sql\n",
    "-- Step 1: Retrieve data with COALESCE applied, keeping correct types\n",
    "WITH account_metrics AS (\n",
    "  SELECT \n",
    "    account_id,\n",
    "    COALESCE(TRIM(account_name), 'Unknown Account') AS account_name,\n",
    "    COALESCE(ROUND(arr, 2), 0.0) AS arr,  -- ✅ COALESCE + ROUND done HERE\n",
    "    COALESCE(TRIM(brag_status), 'Not Classified') AS brag_status,\n",
    "    COALESCE(CAST(next_renewal_date AS STRING), 'No Renewal Date') AS next_renewal_date\n",
    "  FROM `catalog`.`schema`.`accounts` AS a\n",
    "  WHERE account_id IS NOT NULL AND account_name IS NOT NULL\n",
    "  LIMIT 10\n",
    "),\n",
    "-- Step 2: BUSINESS VALUE CTE - Statistical analysis (calculations on DOUBLE values)\n",
    "account_statistics AS (\n",
    "  SELECT *,\n",
    "    COALESCE(ROUND(AVG(arr) OVER (), 2), 0.0) AS avg_arr_portfolio,  -- ✅ Direct DOUBLE calc\n",
    "    COALESCE(ROUND(PERCENTILE_APPROX(arr, 0.75) OVER (), 2), 0.0) AS p75_arr  -- ✅ Direct DOUBLE calc\n",
    "  FROM account_metrics\n",
    "),\n",
    "-- Step 3: Build prompt - CONCAT uses columns already NULL-safe from previous CTEs\n",
    "-- \uD83D\uDEA8 NO COALESCE, ROUND, or CAST inside CONCAT!\n",
    "prompt_cte AS (\n",
    "  SELECT *,\n",
    "    CONCAT('Account: ', account_name, ', ARR: $', arr,   -- ✅ Already rounded in CTE\n",
    "           ', Avg ARR: $', avg_arr_portfolio, ', P75: $', p75_arr) AS prompt  -- ✅ All NULL-safe\n",
    "  FROM account_statistics\n",
    ")\n",
    "-- No CAST to STRING needed! CONCAT auto-converts: CONCAT('Value: ', 123.45) → 'Value: 123.45'\n",
    "```\n",
    "\n",
    "**❌ WRONG: COALESCE to STRING then CAST back to DOUBLE (ANTI-PATTERN!):**\n",
    "```sql\n",
    "-- ❌ Step 1: Converting numeric to STRING unnecessarily\n",
    "WITH account_metrics AS (\n",
    "  SELECT \n",
    "    account_id,\n",
    "    COALESCE(CAST(ROUND(arr, 2) AS STRING), '0.00') AS arr_str  -- ❌ Why STRING?\n",
    "  FROM accounts\n",
    "),\n",
    "-- ❌ Step 2: Casting STRING back to DOUBLE for calculations - WASTEFUL!\n",
    "account_statistics AS (\n",
    "  SELECT *,\n",
    "    AVG(CAST(arr_str AS DOUBLE)) OVER () AS avg_arr  -- ❌ Unnecessary round-trip!\n",
    "  FROM account_metrics\n",
    ")\n",
    "-- This pattern is WASTEFUL and ERROR-PRONE!\n",
    "```\n",
    "\n",
    "**❌ WRONG: Separate CTE ONLY for NULL handling (NO BUSINESS VALUE):**\n",
    "```sql\n",
    "-- Step 1: Raw data retrieval\n",
    "WITH base_accounts_data AS (\n",
    "  SELECT account_id, account_name, arr, brag_status\n",
    "  FROM `catalog`.`schema`.`accounts` AS a\n",
    "  WHERE account_id IS NOT NULL\n",
    "  LIMIT 10\n",
    "),\n",
    "-- ❌ Step 2: WRONG - This CTE has NO BUSINESS VALUE, only COALESCE\n",
    "accounts_null_safe AS (\n",
    "  SELECT\n",
    "    account_id,\n",
    "    COALESCE(TRIM(account_name), 'Unknown Account') AS account_name,\n",
    "    COALESCE(arr, 0.0) AS arr,\n",
    "    COALESCE(TRIM(brag_status), 'Not Classified') AS brag_status\n",
    "  FROM base_accounts_data\n",
    ")\n",
    "-- WRONG! The COALESCE should be in base_accounts_data, not a separate CTE!\n",
    "```\n",
    "\n",
    "**❌ WRONG: Selecting BOTH original AND coalesced value:**\n",
    "```sql\n",
    "-- ❌ WRONG: Duplicate columns (original + coalesced)\n",
    "SELECT \n",
    "  account_name,  -- ❌ Original\n",
    "  COALESCE(TRIM(account_name), 'Unknown') AS account_name_str,  -- ❌ Duplicate!\n",
    "  arr,  -- ❌ Original\n",
    "  COALESCE(arr, 0.0) AS arr_safe  -- ❌ Duplicate!\n",
    "FROM accounts\n",
    "-- Pick ONE version and use it everywhere!\n",
    "```\n",
    "\n",
    "**VALID REASONS TO CREATE A SEPARATE CTE:**\n",
    "1. **JOIN operations** - Combining data from multiple tables (then COALESCE joined columns)\n",
    "2. **Statistical calculations** - Computing metrics, percentiles, correlations (business value!)\n",
    "3. **AI function calls** - Calling ai_query, ai_forecast, etc. (business value!)\n",
    "4. **JSON extraction** - Parsing JSON results from AI functions (business value!)\n",
    "5. **Business logic** - Applying business rules, classifications, calculations\n",
    "6. **Aggregations** - GROUP BY operations for summarization\n",
    "7. **Window functions** - Computing rankings, running totals, etc.\n",
    "\n",
    "**INVALID REASONS TO CREATE A SEPARATE CTE (DO NOT DO THIS):**\n",
    "1. ❌ Only applying COALESCE to columns\n",
    "2. ❌ Only casting data types\n",
    "3. ❌ Only trimming strings\n",
    "4. ❌ Only renaming columns with suffixes\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: COLUMN NAMING CONSISTENCY - ZERO TOLERANCE FOR HALLUCINATED NAMES \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**ABSOLUTE RULE: Once a column is named (with or without suffix), use EXACTLY that name throughout ALL subsequent CTEs and the final SELECT.**\n",
    "\n",
    "**PROBLEM PATTERN (CAUSES ERRORS):**\n",
    "```sql\n",
    "-- CTE 1: Column named with _str suffix\n",
    "SELECT COALESCE(CAST(next_renewal_quarter AS STRING), 'Unknown') AS next_renewal_quarter_str\n",
    "-- CTE 2: ❌ WRONG - Referencing original name without suffix\n",
    "SELECT next_renewal_quarter  -- ❌ ERROR! This column doesn't exist, only next_renewal_quarter_str exists\n",
    "```\n",
    "\n",
    "**MANDATORY NAMING RULES:**\n",
    "\n",
    "1. **DECIDE ONCE, USE EVERYWHERE**: When you name a column in a CTE, use that EXACT name in ALL subsequent CTEs and the final SELECT.\n",
    "\n",
    "2. **PREFER ORIGINAL NAMES WHERE POSSIBLE**: If a string column remains a string, keep its original name:\n",
    "   - ✅ `COALESCE(TRIM(account_name), 'Unknown') AS account_name`  -- Same name, no confusion\n",
    "   - ❌ `COALESCE(TRIM(account_name), 'Unknown') AS account_name_str`  -- Unnecessary suffix\n",
    "\n",
    "3. **KEEP NUMERIC TYPES AS DOUBLE/INT - NO _str SUFFIX NEEDED**: CONCAT auto-converts numeric types:\n",
    "   - ✅ `COALESCE(arr, 0.0) AS arr` -- Keep as DOUBLE, CONCAT auto-converts\n",
    "   - ✅ `COALESCE(CAST(date_col AS STRING), 'No Date') AS date_display` -- DATE to STRING for display only\n",
    "\n",
    "4. **PASS ALL COLUMNS THROUGH CTEs**: When building multi-CTE queries, use `SELECT *` plus new columns to preserve all column names:\n",
    "   ```sql\n",
    "   SELECT *, new_calculated_column FROM previous_cte  -- ✅ Preserves all column names\n",
    "   ```\n",
    "\n",
    "5. **\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: EVERY SELECT MUST HAVE A FROM CLAUSE \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**:\n",
    "   - **ABSOLUTE RULE**: EVERY SELECT statement inside a CTE MUST have a `FROM` clause referencing the previous CTE or table.\n",
    "   - **NO EXCEPTIONS**: Even when using `SELECT *, ...new_columns...`, you MUST include `FROM previous_cte_name`.\n",
    "   - **SYNTAX ERROR WITHOUT FROM**: SQL will fail with syntax error if FROM is missing!\n",
    "   \n",
    "   ```sql\n",
    "   -- ✅ CORRECT: FROM clause present\n",
    "   cte_name AS (\n",
    "     SELECT *, \n",
    "       COALESCE(ROUND(AVG(arr) OVER (), 2), 0.0) AS avg_arr\n",
    "     FROM previous_cte  -- MANDATORY!\n",
    "   )\n",
    "   \n",
    "   -- ❌ WRONG: Missing FROM clause (SYNTAX ERROR!)\n",
    "   cte_name AS (\n",
    "     SELECT *, \n",
    "       COALESCE(ROUND(AVG(arr) OVER (), 2), 0.0) AS avg_arr\n",
    "     -- ERROR! No FROM clause - this is INVALID SQL!\n",
    "   )\n",
    "   ```\n",
    "\n",
    "6. **VALIDATE BEFORE SUBMITTING**: Search your SQL for every column referenced in the final SELECT or ai_query prompts, and verify it exists with EXACTLY that name in the source CTE.\n",
    "\n",
    "**ANTI-PATTERN DETECTION CHECKLIST:**\n",
    "☐ **EVERY CTE SELECT has a FROM clause** - No SELECT without FROM!\n",
    "☐ No column is named `foo_str` in one CTE and referenced as `foo` in another\n",
    "☐ No column is named `foo` in one CTE and referenced as `foo_str` in another\n",
    "☐ All columns in ai_query CONCAT exist in the immediate source CTE\n",
    "☐ All columns in final SELECT exist in the last CTE\n",
    "☐ Column names are consistent from definition to usage\n",
    "\n",
    "**\uD83D\uDD25 CLEAN CTE STRUCTURE - SEPARATE CONCERNS \uD83D\uDD25:**\n",
    "\n",
    "**OPTIMIZED PATTERN: Merge NULL handling into first CTE when possible:**\n",
    "\n",
    "1. **Data Retrieval with NULL Handling**: Apply COALESCE at time of first read from table\n",
    "2. **Statistical Analysis CTE** (if needed): ONLY compute statistical metrics - no prompts\n",
    "3. **Prompt Building CTE**: ONLY build prompts using CONCAT - keep this CTE clean and readable\n",
    "4. **AI Function CTE**: ONLY call ai_query with the prepared prompts\n",
    "5. **JSON Extraction CTE**: ONLY extract JSON fields using get_json_object\n",
    "\n",
    "**EXAMPLE - Optimized with Merged NULL Handling:**\n",
    "\n",
    "```sql\n",
    "-- Step 1: Retrieve data with NULL handling applied immediately - keep correct types\n",
    "WITH order_data_with_defaults AS (\n",
    "  SELECT \n",
    "    order_id,\n",
    "    COALESCE(customer_name, 'Unknown Customer') AS customer_name,\n",
    "    COALESCE(order_amount, 0.0) AS order_amount,  -- ✅ Keep as DOUBLE\n",
    "    COALESCE(CAST(order_date AS STRING), 'No Date') AS order_date,  -- DATE to STRING for display\n",
    "    COALESCE(product_category, 'Uncategorized') AS product_category\n",
    "  FROM `catalog`.`schema`.`orders` AS o\n",
    "  WHERE order_id IS NOT NULL\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at END of first CTE only\n",
    "    -- TODO: Add suitable filtering to load data that matches the business slice for this use case (keep commented until confirmed)\n",
    "    -- AND lower(trim(order_status)) = 'running'  -- Example placeholder; adjust column/value\n",
    "),\n",
    "-- Step 2: Statistical analysis (if needed) - direct DOUBLE calculations\n",
    "order_statistics AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    COALESCE(ROUND(AVG(order_amount) OVER (), 2), 0.0) AS avg_order_amount,  -- ✅ Direct DOUBLE calc\n",
    "    COALESCE(ROUND(STDDEV(order_amount) OVER (), 2), 0.0) AS stddev_order_amount  -- ✅ Direct DOUBLE calc\n",
    "  FROM order_data_with_defaults\n",
    "),\n",
    "-- Step 3: Prompt building - CONCAT auto-converts DOUBLE to STRING\n",
    "-- Generate ai_sys_prompt column FIRST, then pass to ai_query AND include in final output\n",
    "order_analysis_prompts AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    CONCAT('You are a Revenue Operations Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "           'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "           'Business priorities are: {enriched_business_priorities}. ',\n",
    "           'With 15 years of experience in order analysis, fraud detection, and revenue optimization, ',\n",
    "           'your expertise in transaction pattern analysis and anomaly detection aligns with the strategic initiative: Data-driven decision making. ',\n",
    "           'Analyze order ', order_id, \n",
    "           ' from customer ', customer_name,\n",
    "           ' for $', order_amount,  -- ✅ CONCAT auto-converts DOUBLE\n",
    "           ' on ', order_date,\n",
    "           ' in category ', product_category,\n",
    "           '. Average order: $', avg_order_amount,  -- ✅ CONCAT auto-converts DOUBLE\n",
    "           '. Output ONLY JSON with NO markdown fences, NO extra text, JUST the JSON. ',\n",
    "           'Format: {{\"ai_cat_risk_level\": \"value\", \"ai_txt_recommendation\": \"value\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"I assessed my confidence at 85% because... [reasons]\", \"ai_sys_missing_data\": \"I can get higher confidence than 85% if I can get access to [narrative]. {{\\\"missing_data\\\": [\\\"dataset1\\\", \\\"dataset2\\\"]}}\"}}. ',\n",
    "           'MANDATORY LAST 7 FIELDS (in this exact order): ',\n",
    "           '1) ai_txt_business_outcome - CALCULATED MEASURABLE BUSINESS IMPACT. Calculate specific savings/revenue impact with numbers from the data. Format: \"[Description of impact] results in [$ amount]. Breakdown: Daily: $X | Weekly: $X | Monthly: $X | Yearly: $X. DISCLAIMER: All numbers are AI estimates based on available data and must be validated by domain experts before business decisions.\" MUST include breakdown and disclaimer. ',\n",
    "           '2) ai_txt_executive_summary - compelling 2-3 sentence business story that REFERENCES the business outcome numbers calculated above, ',\n",
    "           '3) ai_sys_importance - business importance level (MUST be exactly one of: Very Low, Low, Medium, High, Very High, Critical), ',\n",
    "           '4) ai_sys_urgency - action urgency level (MUST be exactly one of: Very Low, Low, Medium, High, Very High, Critical), ',\n",
    "           '5) ai_sys_confidence (0.0-1.0) - your confidence score, ',\n",
    "           '6) ai_sys_feedback - start with \"I assessed my confidence at [X]% because...\" then explain reasoning, ',\n",
    "           '7) ai_sys_missing_data - format: \"I can get higher confidence than [X]% if I can get access to [narrative about missing data like customer history, payment patterns, fraud indicators]. {{\\\"missing_data\\\": [\\\"customer_history\\\", \\\"payment_patterns\\\", \\\"fraud_indicators\\\"]}}\" ',\n",
    "           'Output ONLY the JSON object, nothing else.') AS ai_sys_prompt  -- ✅ Named ai_sys_prompt for auditability\n",
    "  FROM order_statistics\n",
    "),\n",
    "-- Step 4: AI analysis - pass ai_sys_prompt to ai_query\n",
    "order_insights AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS insights_json\n",
    "  FROM order_analysis_prompts\n",
    ")\n",
    "-- Step 5: Final extraction - ONLY get_json_object with ai_cat_, ai_txt_, ai_sys_ prefixes\n",
    "-- ai_sys_prompt MUST be the LAST column for auditability\n",
    "SELECT \n",
    "  order_id,\n",
    "  customer_name,\n",
    "  order_amount,\n",
    "  get_json_object(insights_json, '$.ai_cat_risk_level') AS ai_cat_risk_level,\n",
    "  get_json_object(insights_json, '$.ai_txt_recommendation') AS ai_txt_recommendation,\n",
    "  -- MANDATORY LAST 7 COLUMNS (including ai_txt_business_outcome + ai_sys_prompt):\n",
    "  COALESCE(get_json_object(insights_json, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "  COALESCE(get_json_object(insights_json, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "  COALESCE(get_json_object(insights_json, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "  COALESCE(get_json_object(insights_json, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "  COALESCE(TRY_CAST(get_json_object(insights_json, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "  COALESCE(get_json_object(insights_json, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "  COALESCE(get_json_object(insights_json, '$.ai_sys_missing_data'), 'No missing data identified') AS ai_sys_missing_data,\n",
    "  ai_sys_prompt  -- ✅ LAST COLUMN: The exact prompt used for auditability\n",
    "FROM order_insights;  -- ✅ NO LIMIT - data already sampled in first CTE\n",
    "```\n",
    "\n",
    "**WHY THIS OPTIMIZED PATTERN IS BETTER:**\n",
    "- Fewer CTEs = better query performance and readability\n",
    "- NULL handling happens at the earliest possible point (first read)\n",
    "- Each remaining CTE has ONE clear responsibility\n",
    "- The prompt building CTE is easy to read and debug\n",
    "- Statistical calculations are separated from prompt construction\n",
    "- Makes the query maintainable and understandable\n",
    "\n",
    "**❌ WRONG - Everything mixed together:**\n",
    "```sql\n",
    "-- BAD: Mixing COALESCE, CAST, statistical functions, and CONCAT in one messy step\n",
    "WITH messy_cte AS (\n",
    "  SELECT *,\n",
    "    ai_query('{sql_model_serving}',\n",
    "      CONCAT('Analyze ', \n",
    "             COALESCE(customer_name, 'Unknown'),  -- ❌ COALESCE in prompt\n",
    "             ' order $',\n",
    "             COALESCE(CAST(order_amount AS STRING), '0'),  -- ❌ COALESCE + CAST in prompt\n",
    "             ' (StdDev: ', COALESCE(CAST(stddev_order_amount AS STRING), 'N/A'), ')',  -- ❌ Complex nested logic\n",
    "             '. Output JSON.')) AS insights\n",
    "  FROM orders\n",
    ")\n",
    "SELECT * FROM messy_cte;  -- ❌ This messy pattern is wrong - don't copy it!\n",
    "```\n",
    "\n",
    "**✅ CORRECT - Optimized clean separation:**\n",
    "- NULL handling merged into first CTE at time of read (`order_data_with_defaults`)\n",
    "- Statistical calculations in separate CTE (`order_statistics`)\n",
    "- Clean, readable prompt building in focused CTE (`order_analysis_prompts`)\n",
    "- AI function call in its own CTE (`order_insights`)\n",
    "- Persona instruction included in ai_query prompt\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **TABLE QUALIFICATION & ALIASING** (MOST CRITICAL):\n",
    "- **EVERY table** in FROM, JOIN, or subquery **MUST** be fully qualified: `` `catalog`.`schema`.`table` ``\n",
    "- **EVERY table MUST have an alias** immediately after the table name\n",
    "- **Example CORRECT**: \n",
    "  ```sql\n",
    "  FROM `catalog`.`schema`.`customer_table` AS c\n",
    "  JOIN `catalog`.`schema`.`orders_table` AS o ON c.customer_id = o.customer_id\n",
    "  ```\n",
    "- **Example WRONG** (will be rejected):\n",
    "  ```sql\n",
    "  FROM customer_table  -- Missing catalog.schema AND alias\n",
    "  FROM `catalog`.`schema`.`customer_table`  -- Missing alias\n",
    "  ```\n",
    "\n",
    "#### 2. **QUOTE USAGE (MOST CRITICAL - 90% OF ERRORS)**\n",
    "- **String literals**: **ALWAYS** use **SINGLE QUOTES** (`'`) - NEVER double quotes (`\"`)\n",
    "- **Column/table names**: Use backticks (`` ` ``) for qualified names OR no quotes\n",
    "- **CONCAT SYNTAX** - This is THE most common error:\n",
    "  ```sql\n",
    "  -- CORRECT ✅\n",
    "  CONCAT('literal text ', column_name, ' more text')\n",
    "  CONCAT('Customer: ', customer_id, ' at ', location_name)\n",
    "  CONCAT('Analyze ownership for ', ownership_type, ' with owner ', owner_entity_name)\n",
    "  \n",
    "  -- WRONG ❌\n",
    "  CONCAT(literal text, column_name)  -- Missing quotes on literals\n",
    "  CONCAT('text', 'column_name')  -- Column name incorrectly quoted as string\n",
    "  CONCAT(\"text\", column)  -- Double quotes not allowed\n",
    "  ```\n",
    "  \n",
    "  **MEMORY RULE**: \n",
    "  - Literal text (like \"for\", \"to\", \":\", etc.) → **SINGLE QUOTE IT**: `'text'`\n",
    "  - Column name (to show its VALUE) → **NO QUOTES**: `column_name`\n",
    "  - NEVER quote column names as strings: `'column_name'` is WRONG\n",
    "\n",
    "- **ARRAY SYNTAX** - Second most common error:\n",
    "  ```sql\n",
    "  -- CORRECT ✅\n",
    "  ARRAY('item1', 'item2', 'item3')\n",
    "  ai_classify(text, ARRAY('Product Quality', 'Customer Service', 'Shipping'))\n",
    "  ai_extract(content, ARRAY('customer_name', 'invoice_number', 'total_amount'))\n",
    "  \n",
    "  -- WRONG ❌\n",
    "  ARRAY(item1, item2, item3)  -- Missing single quotes\n",
    "  ARRAY(\"item1\", \"item2\")  -- Double quotes not allowed\n",
    "  ARRAY('This is a very long category name that exceeds the fifty character limit')  -- Too long!\n",
    "  ```\n",
    "  \n",
    "  **\uD83D\uDEA8 CRITICAL ARRAY RESTRICTIONS for ai_classify and ai_extract**:\n",
    "  1. **Maximum 20 elements** - Arrays can have MAXIMUM 20 items due to Databricks limitations\n",
    "     - ✅ GOOD: `ARRAY('a', 'b', 'c', ..., 't')` (20 items max)\n",
    "     - ❌ BAD: `ARRAY('item1', 'item2', ..., 'item25')` (>20 items - will FAIL)\n",
    "     - If you need more categories, use the most important 20 only\n",
    "  \n",
    "  2. **Maximum 50 characters per item** - Each array element MUST be less than 50 characters\n",
    "     - ✅ GOOD: `ARRAY('High Priority', 'Medium', 'Low')` (all <50 chars)\n",
    "     - ✅ GOOD: `ARRAY('customer_name', 'invoice_num', 'amount')` (all <50 chars)\n",
    "     - ❌ BAD: `ARRAY('High Priority Customer Service Escalation Required')` (>50 chars - will FAIL)\n",
    "     - Use concise, abbreviated labels when necessary\n",
    "  \n",
    "  **VALIDATION CHECKLIST for ai_classify/ai_extract arrays:**\n",
    "  - ✅ Total items ≤ 20\n",
    "  - ✅ Each item length < 50 characters\n",
    "  - ✅ All items use single quotes\n",
    "  - ✅ Labels are clear but concise\n",
    "\n",
    "- **AI_FORECAST SYNTAX**:\n",
    "  \n",
    "  **\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: ALL column names in time_col, value_col, group_col MUST be STRING LITERALS (in single quotes) \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "  \n",
    "  ```sql\n",
    "  -- CORRECT ✅ - Basic with dynamic horizon (30 days ahead)\n",
    "  -- NOTE: 'ds' and 'val' are STRING LITERALS (quoted), NOT column references!\n",
    "  AI_FORECAST(TABLE(past), time_col => 'ds', value_col => 'val', horizon => (SELECT date_add(DAY, 30, MAX(ds)) FROM past))\n",
    "  \n",
    "  -- CORRECT ✅ - BEST PRACTICE: Dynamic horizon derived from data\n",
    "  AI_FORECAST(TABLE(past), \n",
    "    time_col => 'ds', \n",
    "    value_col => 'revenue',\n",
    "    horizon => (SELECT date_add(WEEK, 1, MAX(ds)) FROM past))\n",
    "  \n",
    "  -- CORRECT ✅ - Multiple metrics + groups\n",
    "  AI_FORECAST(TABLE(past), \n",
    "    time_col => 'ds', \n",
    "    value_col => ARRAY('revenue', 'orders'),\n",
    "    group_col => 'product_category',\n",
    "    horizon => (SELECT date_add(DAY, 30, MAX(ds)) FROM past),\n",
    "    prediction_interval_width => 0.95)\n",
    "  \n",
    "  -- CORRECT ✅ - With parameters for seasonality\n",
    "  AI_FORECAST(TABLE(past), \n",
    "    time_col => 'ds', \n",
    "    value_col => 'sales',\n",
    "    horizon => (SELECT date_add(DAY, 90, MAX(ds)) FROM past),\n",
    "    parameters => '{{\"weekly_order\": 10, \"global_floor\": 0}}')\n",
    "  \n",
    "  -- WRONG ❌ - Missing horizon\n",
    "  AI_FORECAST(TABLE(past), time_col => 'ds', value_col => 'val')  -- Missing horizon parameter (REQUIRED!)\n",
    "  \n",
    "  -- WRONG ❌ - Static date (bad practice)\n",
    "  AI_FORECAST(TABLE(past), time_col => 'ds', value_col => 'val', horizon => '2024-12-31')  -- Use dynamic horizon instead\n",
    "  \n",
    "  -- WRONG ❌ - Double quotes outside parameters\n",
    "  AI_FORECAST(TABLE(past), parameters => \"{{\"weekly_order\": 10}}\")  -- MUST use single quotes outside!\n",
    "  \n",
    "  -- WRONG ❌❌❌ - UNQUOTED COLUMN NAMES (MOST COMMON ERROR!) ❌❌❌\n",
    "  AI_FORECAST(TABLE(past), time_col => ds, value_col => val)  -- ❌ FAILS! 'ds' and 'val' MUST be in quotes!\n",
    "  AI_FORECAST(TABLE(past), time_col => 'ds', value_col => ARRAY(revenue, orders))  -- ❌ FAILS! revenue, orders need quotes!\n",
    "  AI_FORECAST(TABLE(past), time_col => 'ds', value_col => 'val', group_col => ARRAY(customer_id, region))  -- ❌ FAILS! Unquoted!\n",
    "  -- ERROR: [UNRESOLVED_COLUMN] A column with name 'ds'/'revenue'/'customer_id' cannot be resolved\n",
    "  -- FIX: Use ARRAY('revenue', 'orders') and ARRAY('customer_id', 'region') with SINGLE QUOTES!\n",
    "  ```\n",
    "  \n",
    "  **\uD83D\uDEA8 CRITICAL: parameters MUST USE SINGLE QUOTES ON THE OUTSIDE \uD83D\uDEA8**\n",
    "  - ✅ CORRECT: `parameters => '{{\"weekly_order\": 10, \"global_floor\": 0}}'` (SINGLE quotes wrap the JSON string)\n",
    "  - ❌ WRONG: `parameters => \"{{\"weekly_order\": 10, \"global_floor\": 0}}\"` (DOUBLE quotes - THIS IS INCORRECT)\n",
    "  - ❌ WRONG: `parameters => '{{'weekly_order': 10, 'global_floor': 0}}'` (Python dict style - wrong JSON format)\n",
    "  - **RULE**: In SQL, string literals use SINGLE QUOTES. JSON keys/values use DOUBLE QUOTES. Combined: `'{{\"key\": \"value\"}}'`\n",
    "  \n",
    "  **OUTPUT COLUMNS - CRITICAL: AI_FORECAST RETURNS ONLY THESE COLUMNS**:\n",
    "  \n",
    "  AI_FORECAST returns a **NEW** table with ONLY these columns (ALL other columns are DROPPED):\n",
    "  1. The **time column** (ds or your specified time_col name) - same type as input\n",
    "  2. The **group column(s)** specified in group_col parameter - ONLY these group columns are returned\n",
    "  3. The **forecast columns** for each value_col:\n",
    "     - `{{value_col}}_forecast`: predicted value (DOUBLE)\n",
    "     - `{{value_col}}_upper`: upper bound of prediction interval (DOUBLE)\n",
    "     - `{{value_col}}_lower`: lower bound of prediction interval (DOUBLE)\n",
    "  \n",
    "  **\uD83D\uDEA8 CRITICAL RULE: ONLY columns in group_col are returned \uD83D\uDEA8**\n",
    "  \n",
    "  **EXAMPLES FROM DOCUMENTATION:**\n",
    "  \n",
    "  | Input Table Columns | Arguments | Output Table Columns |\n",
    "  |---------------------|-----------|---------------------|\n",
    "  | ts, val | time_col='ts', value_col='val' | ts, val_forecast, val_upper, val_lower |\n",
    "  | ds, val | time_col='ds', value_col='val' | ds, val_forecast, val_upper, val_lower |\n",
    "  | ts, dim1, dollars | time_col='ts', value_col='dollars', group_col='dim1' | ts, dim1, dollars_forecast, dollars_upper, dollars_lower |\n",
    "  | ts, dim1, dim2, dollars, users | time_col='ts', value_col=ARRAY('dollars','users'), group_col=ARRAY('dim1','dim2') | ts, dim1, dim2, dollars_forecast, dollars_upper, dollars_lower, users_forecast, users_upper, users_lower |\n",
    "  \n",
    "  **KEY INSIGHT**: Even if your input CTE has 20 columns, AI_FORECAST only returns the columns listed above!\n",
    "  \n",
    "  **\uD83D\uDEA8 CRITICAL: AI_FORECAST COLUMN LIMITATION - ONLY RETURNS SPECIFIC COLUMNS \uD83D\uDEA8**\n",
    "  \n",
    "  **IMPORTANT RULE**: AI_FORECAST **ONLY** returns:\n",
    "  - The time column (ds or specified time_col)\n",
    "  - The group column(s) specified in group_col parameter - **ONLY THESE**\n",
    "  - The forecast columns ({{value_col}}_forecast, {{value_col}}_upper, {{value_col}}_lower)\n",
    "  \n",
    "  **ALL OTHER COLUMNS from the input table are DROPPED and NOT available in the AI_FORECAST output.**\n",
    "  \n",
    "  **\uD83D\uDD25 CRITICAL MISTAKE: GROUP BY vs group_col \uD83D\uDD25**\n",
    "  \n",
    "  **COMMON ERROR**: Developers often GROUP BY multiple columns in the input CTE, but only specify ONE column in group_col.\n",
    "  \n",
    "  **EXAMPLE OF THE PROBLEM:**\n",
    "  ```sql\n",
    "  -- Step 1: GROUP BY multiple columns\n",
    "  WITH historical_data AS (\n",
    "    SELECT \n",
    "      airport_code,      -- Column A\n",
    "      service_type,      -- Column B\n",
    "      DATE_TRUNC('month', date) AS ds,\n",
    "      SUM(cost) AS monthly_cost\n",
    "    FROM table\n",
    "    GROUP BY airport_code, service_type, DATE_TRUNC('month', date)  -- ⚠️ Groups by BOTH columns\n",
    "    ORDER BY ds\n",
    "    -- ✅ NO LIMIT - using WHERE clause for date filtering\n",
    "  ),\n",
    "  -- Step 2: AI_FORECAST with only ONE group_col\n",
    "  forecast_results AS (\n",
    "    SELECT * FROM AI_FORECAST(\n",
    "      TABLE(historical_data),\n",
    "      time_col => 'ds',\n",
    "      value_col => 'monthly_cost',\n",
    "      group_col => 'airport_code',  -- ⚠️ ONLY airport_code specified!\n",
    "      horizon => (SELECT date_add(MONTH, 3, MAX(ds)) FROM historical_data)\n",
    "    )\n",
    "    -- ✅ NO LIMIT\n",
    "  )\n",
    "  -- Step 3: Try to SELECT service_type\n",
    "  SELECT \n",
    "    airport_code,     -- ✅ This exists (in group_col)\n",
    "    service_type,     -- ❌ ERROR! This doesn't exist in output!\n",
    "    monthly_cost_forecast\n",
    "  FROM forecast_results\n",
    "  ```\n",
    "  \n",
    "  **WHY THIS FAILS:**\n",
    "  - Input CTE grouped by airport_code AND service_type\n",
    "  - But group_col only specified 'airport_code'\n",
    "  - AI_FORECAST **ONLY returns columns in group_col**\n",
    "  - service_type is **NOT returned** because it wasn't in group_col!\n",
    "  \n",
    "  **THE FIX - TWO OPTIONS:**\n",
    "  \n",
    "  **Option 1: Include ALL grouping dimensions in group_col (RECOMMENDED)**\n",
    "  ```sql\n",
    "  forecast_results AS (\n",
    "    SELECT * FROM AI_FORECAST(\n",
    "      TABLE(historical_data),\n",
    "      time_col => 'ds',\n",
    "      value_col => 'monthly_cost',\n",
    "      group_col => ARRAY('airport_code', 'service_type'),  -- ✅ Include BOTH columns\n",
    "      horizon => (SELECT date_add(MONTH, 3, MAX(ds)) FROM historical_data)\n",
    "    )\n",
    "    -- ✅ NO LIMIT\n",
    "  )\n",
    "  -- Now service_type is available!\n",
    "  SELECT airport_code, service_type, monthly_cost_forecast FROM forecast_results\n",
    "  ```\n",
    "  \n",
    "  **Option 2: Join back to original table to get missing columns**\n",
    "  ```sql\n",
    "  forecast_with_context AS (\n",
    "    SELECT \n",
    "      f.*,\n",
    "      t.service_type  -- Get service_type from JOIN\n",
    "    FROM forecast_results AS f\n",
    "    LEFT JOIN original_table AS t\n",
    "      ON f.airport_code = t.airport_code\n",
    "    -- ✅ NO LIMIT\n",
    "  )\n",
    "  ```\n",
    "  \n",
    "  **\uD83D\uDD25 RULE: group_col MUST include ALL dimensions you want in the forecast output \uD83D\uDD25**\n",
    "  \n",
    "  **\uD83D\uDD25 MANDATORY REQUIREMENT: group_col is NOW REQUIRED \uD83D\uDD25**\n",
    "  \n",
    "  **WHY group_col is MANDATORY:**\n",
    "  - You MUST join forecast results back to original tables to get additional columns (for ai_query prompts, context, etc.)\n",
    "  - Without group_col, there is NO WAY to join forecast back to original data\n",
    "  - group_col serves as the JOIN key between forecast results and original table\n",
    "  \n",
    "  **RULE**: ALWAYS specify group_col in AI_FORECAST - it is NO LONGER OPTIONAL.\n",
    "  \n",
    "  **CORRECT ✅**: Use entity ID columns as group_col (customer_id, product_id, route_id, store_id, etc.)\n",
    "  **CORRECT ✅**: Use ARRAY() if you need multiple grouping dimensions in the output\n",
    "  **WRONG ❌**: GROUP BY multiple columns but only specify one in group_col\n",
    "  **WRONG ❌**: Omitting group_col when you need to reference original table columns later\n",
    "  \n",
    "  **SOLUTION - MANDATORY JOIN PATTERN**: Join the forecast results back to the original table using the group_col as the JOIN key.\n",
    "  \n",
    "  **CORRECT PATTERN ✅:** (Adapt table/column names to YOUR schema)\n",
    "  ```sql\n",
    "  -- Step 1: Historical data for forecasting\n",
    "  -- [ADAPT: Change table/column names to match YOUR schema and industry]\n",
    "  -- \uD83D\uDEA8 EVERY column must be COALESCE'd or have IS NOT NULL check!\n",
    "  WITH historical_entity_metrics AS (\n",
    "    SELECT \n",
    "      entity_id,                                              -- CRITICAL: filtered with IS NOT NULL (used as group_col)\n",
    "      COALESCE(TRIM(category_code), 'Unknown Category') AS category_code,  -- ✅ COALESCE'd\n",
    "      COALESCE(TRIM(subcategory_code), 'Unknown Subcategory') AS subcategory_code,  -- ✅ COALESCE'd\n",
    "      COALESCE(TRIM(entity_name), 'Unknown Entity') AS entity_name,  -- ✅ COALESCE'd\n",
    "      COALESCE(TRIM(location_name), 'Unknown Location') AS location_name,  -- ✅ COALESCE'd\n",
    "      COALESCE(TRIM(entity_type), 'Unknown Type') AS entity_type,  -- ✅ COALESCE'd\n",
    "      DATE_TRUNC('month', activity_date) AS ds,               -- CRITICAL: filtered with IS NOT NULL\n",
    "      COALESCE(SUM(metric_value), 0.0) AS total_metric        -- ✅ COALESCE'd\n",
    "    FROM `catalog`.`schema`.`your_table` AS t\n",
    "    WHERE activity_date >= add_months(CURRENT_DATE(), -30)  -- 30 months history for 3-month forecast (10:1 ratio)\n",
    "      AND activity_date IS NOT NULL\n",
    "      AND entity_id IS NOT NULL\n",
    "    -- TODO: Add suitable filtering to load data that matches the operational scope for this use case (keep commented until confirmed)\n",
    "    -- AND status = 'active'  -- Example placeholder; adjust column/value\n",
    "    -- AND lower(trim(entity_type)) = 'primary'  -- Example placeholder; adjust column/value\n",
    "    GROUP BY entity_id, category_code, subcategory_code, entity_name, location_name, entity_type, DATE_TRUNC('month', activity_date)\n",
    "    ORDER BY ds\n",
    "  ),\n",
    "  -- Step 2: Generate forecasts (note: only returns entity_id, ds, and forecast columns)\n",
    "  metric_forecast_results AS (\n",
    "    SELECT * FROM AI_FORECAST(\n",
    "      TABLE(historical_entity_metrics),\n",
    "      time_col => 'ds',\n",
    "      value_col => 'total_metric',\n",
    "      group_col => 'entity_id',\n",
    "      horizon => (SELECT add_months(MAX(ds), 3) FROM historical_entity_metrics)  -- 3 months ahead\n",
    "    )\n",
    "  -- ✅ NO LIMIT in CTEs\n",
    "  ),\n",
    "  -- Step 3: JOIN back to original table to get entity context columns\n",
    "  -- \uD83D\uDEA8 LEFT JOIN can introduce NULLs - COALESCE all joined columns!\n",
    "  forecast_with_entity_context AS (\n",
    "    SELECT \n",
    "      f.*,\n",
    "      COALESCE(TRIM(t.category_code), 'Unknown Category') AS category_code,  -- ✅ COALESCE'd from LEFT JOIN\n",
    "      COALESCE(TRIM(t.subcategory_code), 'Unknown Subcategory') AS subcategory_code,  -- ✅ COALESCE'd\n",
    "      COALESCE(TRIM(t.entity_name), 'Unknown Entity') AS entity_name,  -- ✅ COALESCE'd\n",
    "      COALESCE(TRIM(t.location_name), 'Unknown Location') AS location_name,  -- ✅ COALESCE'd\n",
    "      COALESCE(TRIM(t.entity_type), 'Unknown Type') AS entity_type,  -- ✅ COALESCE'd\n",
    "      COALESCE(TRIM(t.additional_attribute), 'N/A') AS additional_attribute  -- ✅ COALESCE'd\n",
    "    FROM metric_forecast_results AS f\n",
    "    LEFT JOIN `catalog`.`schema`.`your_table` AS t\n",
    "      ON f.entity_id = t.entity_id  -- JOIN on the group_col used in AI_FORECAST\n",
    "    -- ✅ NO LIMIT\n",
    "  )\n",
    "  -- Now you have access to both forecast columns AND original entity columns\n",
    "  SELECT * FROM forecast_with_entity_context;  -- ✅ NO LIMIT\n",
    "  ```\n",
    "  \n",
    "  **WRONG PATTERN ❌ - WITHOUT group_col:**\n",
    "  ```sql\n",
    "  -- BAD: No group_col specified - can't join back to original table\n",
    "  -- BAD: Also uses forbidden value comparison (WHERE status = 'active')\n",
    "  WITH historical_data AS (\n",
    "    SELECT \n",
    "      DATE_TRUNC('month', activity_date) AS ds,\n",
    "      SUM(metric_value) AS total_metric\n",
    "    FROM `catalog`.`schema`.`your_table` AS t\n",
    "    WHERE activity_date IS NOT NULL  -- ✅ Only IS NULL/IS NOT NULL allowed\n",
    "      -- ❌ WRONG: WHERE status = 'active' would violate value comparison rules\n",
    "    GROUP BY DATE_TRUNC('month', activity_date)  -- ❌ No grouping by entity_id!\n",
    "    ORDER BY ds\n",
    "    -- ❌ NO date filtering with adaptive ratio for ai_forecast\n",
    "  ),\n",
    "  metric_forecast_results AS (\n",
    "    SELECT * FROM AI_FORECAST(\n",
    "      TABLE(historical_data),\n",
    "      time_col => 'ds',\n",
    "      value_col => 'total_metric',\n",
    "      -- ❌ NO group_col - can't join back to get entity details!\n",
    "      horizon => (SELECT date_add(MONTH, 3, MAX(ds)) FROM historical_data)\n",
    "    )\n",
    "    -- ✅ NO LIMIT\n",
    "  )\n",
    "  -- ❌ ERROR: Can't access entity-specific columns like category_code!\n",
    "  -- No way to join back to original table without a group_col JOIN key\n",
    "  SELECT \n",
    "    ds,\n",
    "    total_metric_forecast,\n",
    "    category_code  -- ❌ ERROR: This column doesn't exist and can't be joined!\n",
    "  FROM metric_forecast_results\n",
    "  ```\n",
    "  \n",
    "  **WRONG PATTERN ❌ - Trying to reference non-existent columns:**\n",
    "  ```sql\n",
    "  -- BAD: Trying to reference columns from input table directly after AI_FORECAST\n",
    "  metric_forecast_results AS (\n",
    "    SELECT * FROM AI_FORECAST(...)\n",
    "    -- ✅ NO LIMIT\n",
    "  )\n",
    "  -- This will FAIL with error: [UNRESOLVED_COLUMN] category_code cannot be resolved\n",
    "  SELECT \n",
    "    entity_id,\n",
    "    category_code,  -- ❌ ERROR: This column doesn't exist in forecast results!\n",
    "    subcategory_code,  -- ❌ ERROR: This column doesn't exist!\n",
    "    total_metric_forecast\n",
    "  FROM metric_forecast_results\n",
    "  ```\n",
    "  \n",
    "  **KEY RULES:**\n",
    "  1. **\uD83D\uDD25 ALWAYS specify group_col in AI_FORECAST \uD83D\uDD25** - Use entity ID columns (customer_id, product_id, entity_id, etc.) so you can join back\n",
    "  2. **Always join forecast results back to original table** when you need additional columns (which is almost always for ai_query)\n",
    "  3. **Use the group_col as the JOIN key** (this is the column that links forecast to original data)\n",
    "  4. **Join to the source table**, not the aggregated CTE (to get latest row-level details)\n",
    "  5. **Use LEFT JOIN** to preserve all forecast rows even if original data is missing\n",
    "  6. **Plan ahead**: If you know you'll need additional columns for ai_query prompts, add a JOIN CTE after AI_FORECAST\n",
    "  \n",
    "  **REMEMBER: Without group_col, you CANNOT join back to original table to get additional columns!**\n",
    "\n",
    "---\n",
    "\n",
    "#### 2a. **\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: NEVER USE AI TO EXTRACT DATA ALREADY AVAILABLE IN COLUMNS \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8** (ABSOLUTE PROHIBITION)\n",
    "\n",
    "**\uD83D\uDD25 ZERO TOLERANCE POLICY: DO NOT ROUND-TRIP STRUCTURED DATA THROUGH AI FUNCTIONS \uD83D\uDD25**\n",
    "\n",
    "**WHAT THIS MEANS:**\n",
    "You must NEVER use `ai_extract`, `ai_query`, or any AI function to extract values that are ALREADY AVAILABLE as structured columns in your source tables. This is nonsensical, wasteful, and completely unacceptable.\n",
    "\n",
    "**THE PROHIBITED PATTERN (ABSOLUTELY FORBIDDEN):**\n",
    "\n",
    "**❌ WRONG EXAMPLE - DO NOT DO THIS:**\n",
    "```sql\n",
    "-- Step 1: Select data that ALREADY HAS the values in columns\n",
    "WITH commodity_data AS (\n",
    "  SELECT \n",
    "    commodity_id,\n",
    "    commodity_name,\n",
    "    un_number,              -- ❌ This value ALREADY EXISTS in a column!\n",
    "    iata_dg_class,          -- ❌ This value ALREADY EXISTS in a column!\n",
    "    special_handling_code   -- ❌ This value ALREADY EXISTS in a column!\n",
    "  FROM `catalog`.`schema`.`commodity` AS c\n",
    "  -- ✅ NO LIMIT in CTEs\n",
    "),\n",
    "-- Step 2: Build a prompt that EMBEDS these already-known values into text\n",
    "commodity_prompts AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    CONCAT('Extract commodity specifications from this data: ',\n",
    "           'Commodity: ', commodity_name,\n",
    "           ', Current UN: ', un_number,              -- ❌ Embedding known value into text!\n",
    "           ', IATA Class: ', iata_dg_class,          -- ❌ Embedding known value into text!\n",
    "           ', Handling Code: ', special_handling_code, -- ❌ Embedding known value into text!\n",
    "           '. Extract: UN number, IATA class, handling code.') AS extraction_prompt\n",
    "  FROM commodity_data\n",
    "),\n",
    "-- Step 3: Use AI to extract the SAME VALUES that were just embedded! ❌❌❌\n",
    "extracted_specs AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_extract(extraction_prompt, \n",
    "      ARRAY('extracted_un_number',        -- ❌ Extracting un_number that we already had!\n",
    "            'extracted_iata_class',       -- ❌ Extracting iata_dg_class that we already had!\n",
    "            'extracted_handling_code')    -- ❌ Extracting special_handling_code that we already had!\n",
    "    ) AS specifications\n",
    "  FROM commodity_prompts\n",
    ")\n",
    "-- This is COMPLETELY NONSENSICAL - we're extracting values we already have!\n",
    "SELECT \n",
    "  commodity_id,\n",
    "  un_number,                                        -- We already have this!\n",
    "  specifications['extracted_un_number'],            -- This is the SAME VALUE!\n",
    "  iata_dg_class,                                    -- We already have this!\n",
    "  specifications['extracted_iata_class'],           -- This is the SAME VALUE!\n",
    "  special_handling_code,                            -- We already have this!\n",
    "  specifications['extracted_handling_code']         -- This is the SAME VALUE!\n",
    "FROM extracted_specs\n",
    ";\n",
    "```\n",
    "\n",
    "**WHY THIS IS ABSOLUTELY UNACCEPTABLE:**\n",
    "1. **Nonsensical Logic**: You're taking structured data → converting to text → using AI to extract back to structured data\n",
    "2. **Wasteful**: Burning AI tokens and compute to extract values you already have\n",
    "3. **Error-Prone**: AI may extract incorrectly, introducing errors where none existed\n",
    "4. **Performance**: Adds unnecessary latency and cost\n",
    "5. **Data Quality**: Degrades data quality by potentially introducing extraction errors\n",
    "\n",
    "**✅ CORRECT PATTERN - USE THE COLUMNS DIRECTLY:**\n",
    "```sql\n",
    "-- If you already have the data in columns, JUST USE THOSE COLUMNS!\n",
    "WITH commodity_data AS (\n",
    "  SELECT \n",
    "    commodity_id,\n",
    "    commodity_name,\n",
    "    un_number,              -- ✅ Use this column directly!\n",
    "    iata_dg_class,          -- ✅ Use this column directly!\n",
    "    special_handling_code   -- ✅ Use this column directly!\n",
    "  FROM `catalog`.`schema`.`commodity` AS c\n",
    "  WHERE un_number IS NOT NULL  -- Filter for data quality\n",
    "    AND iata_dg_class IS NOT NULL\n",
    "    AND special_handling_code IS NOT NULL\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at END of first CTE only\n",
    ")\n",
    "-- Just SELECT the columns you need - NO AI extraction necessary!\n",
    "SELECT \n",
    "  commodity_id,\n",
    "  commodity_name,\n",
    "  un_number,              -- ✅ Already have it - no extraction needed\n",
    "  iata_dg_class,          -- ✅ Already have it - no extraction needed\n",
    "  special_handling_code   -- ✅ Already have it - no extraction needed\n",
    "FROM commodity_data\n",
    ";\n",
    "```\n",
    "\n",
    "**WHEN AI EXTRACTION IS APPROPRIATE (THE ONLY VALID CASES):**\n",
    "\n",
    "**✅ CORRECT USE CASE 1: Extract from UNSTRUCTURED TEXT where data is NOT in columns**\n",
    "```sql\n",
    "-- Valid: Extract from free-text notes where values are NOT in separate columns\n",
    "WITH order_notes AS (\n",
    "  SELECT \n",
    "    order_id,\n",
    "    notes_text  -- Unstructured text like \"Customer requested delivery by Friday, contact John at 555-1234\"\n",
    "  FROM `catalog`.`schema`.`orders` AS o\n",
    "  WHERE notes_text IS NOT NULL\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at END of first CTE only\n",
    ")\n",
    "SELECT \n",
    "  order_id,\n",
    "  ai_extract(notes_text, \n",
    "    ARRAY('delivery_date', 'contact_person', 'phone_number')  -- ✅ Extracting from unstructured text\n",
    "  ) AS extracted_info\n",
    "FROM order_notes\n",
    ";\n",
    "-- This is VALID because delivery_date, contact_person, phone_number are NOT in separate columns\n",
    "```\n",
    "\n",
    "**✅ CORRECT USE CASE 2: Extract from DOCUMENT FILES using ai_parse_document**\n",
    "```sql\n",
    "-- Valid: Extract from PDF/image files\n",
    "WITH parsed_docs AS (\n",
    "  SELECT \n",
    "    path,\n",
    "    ai_parse_document(content, map('version', '2.0')) AS parsed\n",
    "  FROM READ_FILES('/Volumes/catalog/schema/invoices/*.pdf', format => 'binaryFile')\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at END of first CTE only\n",
    ")\n",
    "SELECT \n",
    "  path,\n",
    "  ai_extract(\n",
    "    concat_ws('\\n\\n', transform(try_cast(parsed:document:elements AS ARRAY<VARIANT>),\n",
    "      element -> try_cast(element:content AS STRING))),\n",
    "    ARRAY('invoice_number', 'vendor_name', 'total_amount')  -- ✅ Extracting from document files\n",
    "  ) AS invoice_data\n",
    "FROM parsed_docs\n",
    ";\n",
    "-- This is VALID because the data only exists in unstructured document files\n",
    "```\n",
    "\n",
    "**\uD83D\uDEA8 MANDATORY PRE-FLIGHT CHECK BEFORE USING ai_extract, ai_query FOR EXTRACTION:**\n",
    "\n",
    "Before you write ANY SQL that uses AI to extract data, you MUST answer these questions:\n",
    "\n",
    "**☐ Question 1: Does this value already exist as a column in my source table?**\n",
    "- If YES → DO NOT use AI extraction, just SELECT the column directly\n",
    "- If NO → Proceed to Question 2\n",
    "\n",
    "**☐ Question 2: Am I extracting from truly unstructured data (text fields, documents)?**\n",
    "- If YES → AI extraction is appropriate\n",
    "- If NO → DO NOT use AI extraction\n",
    "\n",
    "**☐ Question 3: Am I building a prompt that embeds column values, then extracting those same values?**\n",
    "- If YES → STOP! This is the prohibited pattern - just use the columns directly\n",
    "- If NO → Proceed with AI extraction\n",
    "\n",
    "**EXAMPLES OF PROHIBITED VS ALLOWED:**\n",
    "\n",
    "| Data Source | Column Exists? | Use AI? | Rationale |\n",
    "|-------------|----------------|---------|-----------|\n",
    "| `un_number` column | YES | ❌ NO | Value is already structured - use column directly |\n",
    "| Free-text `description` field | NO | ✅ YES | Data is embedded in unstructured text |\n",
    "| `iata_class` column | YES | ❌ NO | Value is already structured - use column directly |\n",
    "| PDF invoice file | NO | ✅ YES | Data only exists in document file |\n",
    "| `customer_email` column | YES | ❌ NO | Value is already structured - use column directly |\n",
    "| Customer review text | NO | ✅ YES | Extracting entities from unstructured review text |\n",
    "\n",
    "**\uD83D\uDD25 CRITICAL VALIDATION CHECKLIST (MANDATORY BEFORE SUBMITTING SQL):**\n",
    "\n",
    "For ANY SQL that uses `ai_extract`, `ai_query`, or any AI function to extract data:\n",
    "\n",
    "☐ I have verified that the values I'm extracting DO NOT already exist as columns in my source tables\n",
    "☐ I am extracting from truly unstructured data (free-text fields, documents, notes) NOT from structured columns\n",
    "☐ I am NOT building prompts that embed column values only to extract them back\n",
    "☐ If the data exists in columns, I am using those columns directly instead of AI extraction\n",
    "☐ My use of AI adds genuine value (extracting from unstructured sources) rather than just round-tripping data\n",
    "\n",
    "**\uD83D\uDEA8 IF YOU VIOLATE THIS RULE, YOUR SQL WILL BE REJECTED AS NONSENSICAL AND UNACCEPTABLE \uD83D\uDEA8**\n",
    "\n",
    "**REMEMBER: AI is for extracting structure from unstructured data, NOT for re-extracting data that's already structured!**\n",
    "  \n",
    "  **\uD83D\uDEA8 CRITICAL: AI_FORECAST MANDATORY REQUIREMENTS \uD83D\uDEA8**\n",
    "  \n",
    "  **1. INPUT TABLE MUST HAVE UNIQUE TIME VALUES PER GROUP**\n",
    "  \n",
    "  AI_FORECAST will FAIL if the time column contains duplicate values within a partition. You MUST ensure uniqueness by using GROUP BY on the time column in the CTE that prepares data for AI_FORECAST.\n",
    "  \n",
    "  **CORRECT PATTERN ✅ - GROUP BY time column to deduplicate:**\n",
    "  ```sql\n",
    "  -- Step 1: Prepare historical data with GROUP BY to ensure unique time values\n",
    "  -- \uD83D\uDEA8 EVERY column must be COALESCE'd or have IS NOT NULL check!\n",
    "  WITH historical_route_metrics AS (\n",
    "    SELECT \n",
    "      route_id,                                              -- CRITICAL: filtered with IS NOT NULL (group_col)\n",
    "      DATE_TRUNC('month', flight_date) AS ds,                -- CRITICAL: filtered with IS NOT NULL (time_col)\n",
    "      COALESCE(SUM(passenger_count), 0) AS passenger_demand, -- ✅ COALESCE'd (value_col)\n",
    "      COALESCE(SUM(revenue), 0.0) AS total_revenue           -- ✅ COALESCE'd (value_col)\n",
    "    FROM `catalog`.`schema`.`flights` AS f\n",
    "    WHERE flight_date >= add_months(CURRENT_DATE(), -30)  -- 30 months history for 3-month forecast (10:1 ratio)\n",
    "      AND flight_date IS NOT NULL\n",
    "      AND route_id IS NOT NULL\n",
    "    GROUP BY route_id, DATE_TRUNC('month', flight_date)  -- \uD83D\uDD25 MANDATORY: GROUP BY time column\n",
    "    ORDER BY ds\n",
    "  ),\n",
    "  -- Step 2: Generate forecast (input is guaranteed to have unique ds values per route_id)\n",
    "  demand_forecast_results AS (\n",
    "    SELECT * FROM AI_FORECAST(\n",
    "      TABLE(historical_route_metrics),\n",
    "      time_col => 'ds',\n",
    "      value_col => ARRAY('passenger_demand', 'total_revenue'),\n",
    "      group_col => 'route_id',\n",
    "      horizon => (SELECT add_months(MAX(ds), 3) FROM historical_route_metrics)  -- 3 months ahead\n",
    "    )\n",
    "  -- ✅ NO LIMIT in CTEs\n",
    "  )\n",
    "  SELECT * FROM demand_forecast_results;  -- ✅ NO LIMIT\n",
    "  ```\n",
    "  \n",
    "  **WRONG PATTERN ❌ - No GROUP BY (will cause duplicate time values):**\n",
    "  ```sql\n",
    "  -- BAD: No GROUP BY - multiple flights per month will cause duplicate ds values\n",
    "  WITH historical_route_metrics AS (\n",
    "    SELECT \n",
    "      route_id,\n",
    "      DATE_TRUNC('month', flight_date) AS ds,\n",
    "      passenger_count AS passenger_demand  -- ❌ No aggregation!\n",
    "    FROM `catalog`.`schema`.`flights` AS f\n",
    "    WHERE flight_date IS NOT NULL\n",
    "    -- ❌ NO GROUP BY - ds will have duplicates!\n",
    "    LIMIT 1000\n",
    "  )\n",
    "  -- This will FAIL with: PYTHON_TVF_COLUMN_VALUES_MUST_BE_UNIQUE_WITHIN_PARTITION\n",
    "  ```\n",
    "  \n",
    "  **MANDATORY RULES FOR AI_FORECAST INPUT:**\n",
    "  - ✅ ALWAYS use GROUP BY on the time column (ds or specified time_col) to ensure uniqueness\n",
    "  - ✅ Include all group_col columns in the GROUP BY clause\n",
    "  - ✅ Use aggregate functions (SUM, AVG, COUNT, MAX, MIN) for value columns\n",
    "  - ✅ **CAST ALL value_col columns to DOUBLE** (e.g. `CAST(revenue AS DOUBLE)`) - AI_FORECAST requires DOUBLE input\n",
    "  - ✅ Validate that each (group_col, time_col) combination is unique\n",
    "  - ❌ NEVER pass raw row-level data to AI_FORECAST without aggregation\n",
    "  - ❌ NEVER pass STRING or DECIMAL types as value_col - ALWAYS CAST TO DOUBLE\n",
    "  \n",
    "  **2. FILTER NULL FORECASTED VALUES AFTER AI_FORECAST**\n",
    "  \n",
    "  AI_FORECAST may return NULL values in the forecast columns (*_forecast, *_upper, *_lower) for certain time periods. You MUST filter these out before using the forecast results.\n",
    "  \n",
    "  **CORRECT PATTERN ✅ - Filter NULL forecasts:**\n",
    "  ```sql\n",
    "  -- Step 2: Generate forecast\n",
    "  demand_forecast_raw AS (\n",
    "    SELECT * FROM AI_FORECAST(\n",
    "      TABLE(historical_route_metrics),\n",
    "      time_col => 'ds',\n",
    "      value_col => 'passenger_demand',\n",
    "      group_col => 'route_id',\n",
    "      horizon => (SELECT date_add(MONTH, 3, MAX(ds)) FROM historical_route_metrics)\n",
    "    )\n",
    "  -- ✅ NO LIMIT in CTEs\n",
    "  ),\n",
    "  -- Step 3: Filter out rows with NULL forecasted values\n",
    "  demand_forecast_clean AS (\n",
    "    SELECT *\n",
    "    FROM demand_forecast_raw\n",
    "    WHERE passenger_demand_forecast IS NOT NULL  -- \uD83D\uDD25 MANDATORY: Filter NULL forecasts\n",
    "  -- ✅ NO LIMIT in CTEs\n",
    "  )\n",
    "  SELECT * FROM demand_forecast_clean;  -- ✅ NO LIMIT\n",
    "  ```\n",
    "  \n",
    "  **CRITICAL: Filter ALL forecasted columns if multiple value_col specified:**\n",
    "  ```sql\n",
    "  -- Multiple value columns - filter NULL for ALL forecast columns\n",
    "  demand_forecast_clean AS (\n",
    "    SELECT *\n",
    "    FROM demand_forecast_raw\n",
    "    WHERE passenger_demand_forecast IS NOT NULL  -- Filter first metric\n",
    "      AND total_revenue_forecast IS NOT NULL     -- Filter second metric\n",
    "  -- ✅ NO LIMIT in CTEs\n",
    "  )\n",
    "  ```\n",
    "  \n",
    "  **3. COMPREHENSIVE MANDATORY REQUIREMENTS:**\n",
    "  \n",
    "  - **MUST**: INPUT TABLE MUST HAVE UNIQUE (group_col, time_col) COMBINATIONS - Use GROUP BY to deduplicate\n",
    "  - **MUST**: Use WHERE clause with date filtering using ADAPTIVE ratios based on time granularity:\n",
    "    - High-frequency (minute/hour): Fixed calendar periods (7 days for minute, 4 weeks for hour)\n",
    "    - Mid-frequency (day/week/month): 10:1 ratio\n",
    "    - Low-frequency (quarter/year): Reduced ratios (8:1 for quarter, 3-5:1 for year)\n",
    "  - **MUST**: Filter NULL forecasted values using WHERE {{value_col}}_forecast IS NOT NULL after AI_FORECAST\n",
    "  - **MUST**: Horizon should be derived using date_add(UNIT, X, MAX(time_col)) for dynamic forecasting\n",
    "  - **MUST**: UNIT must be HOUR, DAY, WEEK, MONTH, QUARTER, or YEAR WITHOUT quotes (e.g., date_add(DAY, 30, MAX(ds)))\n",
    "  - **MUST**: Always specify group_col to enable joining back to original table\n",
    "  \n",
    "  **COMPLETE EXAMPLE WITH ALL MANDATORY REQUIREMENTS:**\n",
    "  ```sql\n",
    "  -- Step 1: Prepare aggregated historical data with 10:1 ratio\n",
    "  -- For 4-week horizon, use 40 weeks of history\n",
    "  -- \uD83D\uDEA8 EVERY column must be COALESCE'd or have IS NOT NULL check!\n",
    "  WITH historical_sales_metrics AS (\n",
    "    SELECT \n",
    "      product_id,                                            -- CRITICAL: filtered with IS NOT NULL (group_col)\n",
    "      DATE_TRUNC('week', order_date) AS ds,                  -- CRITICAL: filtered with IS NOT NULL (time_col)\n",
    "      COALESCE(SUM(quantity), 0) AS total_quantity,          -- ✅ COALESCE'd (value_col)\n",
    "      COALESCE(SUM(revenue), 0.0) AS total_revenue           -- ✅ COALESCE'd (value_col)\n",
    "    FROM `catalog`.`schema`.`sales` AS s\n",
    "    WHERE order_date >= date_add(WEEK, -40, CURRENT_DATE())  -- 40 weeks history for 4-week forecast (10:1 ratio)\n",
    "      AND order_date IS NOT NULL\n",
    "      AND product_id IS NOT NULL\n",
    "    GROUP BY product_id, DATE_TRUNC('week', order_date)  -- Deduplicate\n",
    "    ORDER BY ds\n",
    "  ),\n",
    "  -- Step 2: Generate forecast\n",
    "  sales_forecast_raw AS (\n",
    "    SELECT * FROM AI_FORECAST(\n",
    "      TABLE(historical_sales_metrics),\n",
    "      time_col => 'ds',\n",
    "      value_col => ARRAY('total_quantity', 'total_revenue'),\n",
    "      group_col => 'product_id',\n",
    "      horizon => (SELECT date_add(WEEK, 4, MAX(ds)) FROM historical_sales_metrics)  -- 4 weeks ahead\n",
    "    )\n",
    "  -- ✅ NO LIMIT in CTEs\n",
    "  ),\n",
    "  -- Step 3: Filter NULL forecasts\n",
    "  sales_forecast_clean AS (\n",
    "    SELECT *\n",
    "    FROM sales_forecast_raw\n",
    "    WHERE total_quantity_forecast IS NOT NULL\n",
    "      AND total_revenue_forecast IS NOT NULL\n",
    "  -- ✅ NO LIMIT in CTEs\n",
    "  )\n",
    "  SELECT * FROM sales_forecast_clean;  -- ✅ NO LIMIT\n",
    "  ```\n",
    "\n",
    "- **AI_QUERY SYNTAX**:\n",
    "  ```sql\n",
    "  -- CORRECT ✅ - Use the configured sql_model_serving endpoint: {sql_model_serving}\n",
    "  ai_query('{sql_model_serving}', CONCAT('You are a Customer Success Director... Predict churn for customer ', customer_id, '. Output ONLY JSON...'))  -- Complex analysis with persona\n",
    "  ai_query('{sql_model_serving}', CONCAT('You are a Product Manager... Analyze ', product_name, ' and suggest improvements. Output ONLY JSON...'))  -- General tasks with persona\n",
    "  \n",
    "  -- WRONG ❌\n",
    "  ai_query(model_name, prompt)  -- Model name must be quoted\n",
    "  ai_query('model', \"prompt text\")  -- Use single quotes not double\n",
    "  ```\n",
    "\n",
    "- **AI_PARSE_DOCUMENT SYNTAX** (\uD83D\uDEA8 CRITICAL - ONLY FOR UNSTRUCTURED FILES):\n",
    "  ```sql\n",
    "  -- CORRECT ✅ - Processing document files from Unity Catalog volumes\n",
    "  WITH docs AS (\n",
    "    SELECT \n",
    "      path,\n",
    "      ai_parse_document(content, map('version', '2.0')) AS parsed\n",
    "    FROM READ_FILES('/Volumes/catalog/schema/volume/*.pdf', format => 'binaryFile')\n",
    "    LIMIT 10\n",
    "  )\n",
    "  SELECT \n",
    "    path,\n",
    "    concat_ws('\\n\\n', \n",
    "      transform(try_cast(parsed:document:elements AS ARRAY<VARIANT>),\n",
    "        element -> try_cast(element:content AS STRING))\n",
    "    ) AS extracted_text\n",
    "  FROM docs\n",
    "  WHERE try_cast(parsed:error_status AS STRING) IS NULL\n",
    "  LIMIT 10;\n",
    "  \n",
    "  -- WRONG ❌ - DO NOT use with table columns\n",
    "  SELECT ai_parse_document(text_column) FROM my_table  -- INVALID! Only for file content\n",
    "  SELECT ai_parse_document(content) FROM delta_table  -- INVALID! Only for READ_FILES\n",
    "  ```\n",
    "  \n",
    "  **\uD83D\uDEA8 CRITICAL REQUIREMENTS FOR ai_parse_document:**\n",
    "  - MUST ONLY be used with unstructured document files (PDF, JPG/JPEG, PNG, DOC/DOCX, PPT/PPTX)\n",
    "  - Input MUST come from READ_FILES('/Volumes/...', format => 'binaryFile') \n",
    "  - NEVER use with structured table columns or Delta table data\n",
    "  - Output is VARIANT type with version 2.0 schema\n",
    "  - Extract text from parsed:document:elements array\n",
    "  - Filter errors: WHERE try_cast(parsed:error_status AS STRING) IS NULL\n",
    "  - Supported file formats: *.(pdf,jpg,jpeg,png,doc,docx,ppt,pptx)\n",
    "  \n",
    "  **WHEN TO USE ai_parse_document:**\n",
    "  - ✅ YES: Extract text from PDF invoices stored in volumes\n",
    "  - ✅ YES: Parse scanned documents (images) for OCR\n",
    "  - ✅ YES: Process Word/PowerPoint files from document repositories\n",
    "  - ❌ NO: Extract data from text columns in tables (use ai_extract instead)\n",
    "  - ❌ NO: Parse JSON/XML in table columns (use native SQL functions)\n",
    "  - ❌ NO: Process structured data already in Delta tables\n",
    "\n",
    "#### 3. **DATABRICKS SQL DIALECT EXPERTISE**\n",
    "\n",
    "You are an EXPERT in Databricks SQL. Follow these dialect-specific rules:\n",
    "\n",
    "**Data Types:**\n",
    "- Use `STRING` not `VARCHAR` or `TEXT`\n",
    "- Use `BIGINT` for large integers\n",
    "- Use `DOUBLE` for decimals\n",
    "- Use `TIMESTAMP` for datetime\n",
    "- Use `ARRAY<type>` for arrays (e.g., `ARRAY<STRING>`)\n",
    "- Use `MAP<key_type, value_type>` for maps\n",
    "\n",
    "**Functions:**\n",
    "- String concatenation: `CONCAT('a', column, 'b')` or `column1 || column2`\n",
    "- Type casting: `CAST(column AS STRING)` or `column::STRING`\n",
    "- Array creation: `ARRAY('val1', 'val2')` - MAX 20 elements\n",
    "- Date functions: `DATE_TRUNC('day', timestamp_col)`, `CURRENT_DATE()`, `CURRENT_TIMESTAMP()`\n",
    "- NULL handling: `COALESCE(col1, col2, 'default')`, `NULLIF(col1, col2)`\n",
    "\n",
    "**Common Patterns:**\n",
    "- CTEs (recommended): `WITH cte_name AS (...) SELECT * FROM cte_name`\n",
    "- Subqueries: Use CTEs instead for readability\n",
    "- LIMIT clause: Always required (see below)\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL: BUSINESS-FRIENDLY NAMING & FILTERABLE VALUES (MANDATORY) \uD83D\uDEA8:**\n",
    "\n",
    "**ALL column names MUST use business-relevant terminology, NOT generic technical terms:**\n",
    "\n",
    "**DO NOT USE Generic Technical Names:**\n",
    "- ❌ `classification` → Use business context: `risk_category`, `value_tier`, `urgency_level`, `feedback_category`\n",
    "- ❌ `sentiment` → Use business context: `customer_emotion`, `feedback_tone`, `satisfaction_level`, `brand_perception`\n",
    "- ❌ `similarity_score` → Use business context: `match_confidence_score`, `duplicate_probability`, `record_relationship`\n",
    "- ❌ `justification` → Use business context: `risk_rationale`, `segmentation_rationale`, `emotion_rationale`\n",
    "\n",
    "**\uD83D\uDD25 CRITICAL: CATEGORICAL vs NARRATIVE COLUMNS \uD83D\uDD25:**\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 MANDATORY COLUMN NAMING PREFIXES \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8:**\n",
    "\n",
    "All AI-generated columns MUST use the following prefixes:\n",
    "- **`ai_cat_`** - For CATEGORICAL columns (filterable values, max 20 distinct values)\n",
    "- **`ai_txt_`** - For NARRATIVE/textual columns (free text explanations, stories, plans)\n",
    "- **`ai_sys_`** - For SYSTEM columns (confidence, feedback, missing_data)\n",
    "\n",
    "**CATEGORICAL COLUMNS (`ai_cat_` prefix - MUST have specific distinct values, max 20 choices):**\n",
    "These columns MUST use predefined categorical values for filtering and analytics:\n",
    "- Priority/Risk/Urgency levels: `ai_cat_priority`, `ai_cat_risk_level`, `ai_cat_urgency` → Values: `Critical`, `High`, `Medium`, `Low`, `Minimal`\n",
    "- Record relationships: `ai_cat_match_status` → Values: `Definite Duplicate`, `Probable Duplicate`, `Possible Match`, `Different Entity`\n",
    "- Data risk types: `ai_cat_data_sensitivity` → Values: `PII`, `PHI`, `Financial Data`, `PII+Financial`, `PHI+PII`\n",
    "- Compliance status: `ai_cat_compliance_status` → Values: `Compliant`, `At Risk`, `Non-Compliant`, `Needs Review`\n",
    "- Customer segments: `ai_cat_customer_segment` → Values: `VIP`, `High Value`, `Medium Value`, `Low Value`, `At Risk`, `Churned`\n",
    "- Trend directions: `ai_cat_trend_direction` → Values: `Strong Growth`, `Moderate Growth`, `Stable`, `Declining`, `Sharp Decline`\n",
    "- Action priorities: `ai_cat_action_priority` → Values: `Immediate Action`, `High Priority`, `Medium Priority`, `Low Priority`, `Monitor`\n",
    "\n",
    "**NARRATIVE COLUMNS (`ai_txt_` prefix - can have free text):**\n",
    "These columns contain explanatory text and business stories:\n",
    "- Rationales/Justifications: `ai_txt_risk_rationale`, `ai_txt_segmentation_rationale`, `ai_txt_emotion_rationale`\n",
    "- Detailed plans: `ai_txt_action_plan`, `ai_txt_mitigation_plan`, `ai_txt_resolution_plan`, `ai_txt_retention_strategy`\n",
    "- Narratives: `ai_txt_executive_summary`, `ai_txt_business_narrative`, `ai_txt_customer_journey_insights`\n",
    "\n",
    "**\uD83D\uDD25\uD83D\uDD25\uD83D\uDD25 MANDATORY BUSINESS OUTCOME COLUMN (`ai_txt_business_outcome`) \uD83D\uDD25\uD83D\uDD25\uD83D\uDD25**\n",
    "This column is **REQUIRED** for EVERY use case and MUST appear BEFORE `ai_txt_executive_summary`:\n",
    "- `ai_txt_business_outcome` - **CALCULATED MEASURABLE BUSINESS IMPACT** - MUST include:\n",
    "  1. **Specific quantified savings/gains** with actual numbers from the analysis\n",
    "  2. **Breakdown calculation** showing Daily/Weekly/Monthly/Yearly projections\n",
    "  3. **Currency values** where applicable (e.g., \"$3,224 in fuel cost savings\")\n",
    "  4. **MANDATORY DISCLAIMER**: \"DISCLAIMER: All numbers are AI estimates based on available data and must be validated by domain experts before business decisions.\"\n",
    "\n",
    "**EXAMPLE ai_txt_business_outcome:**\n",
    "\"Reducing the 310 kg/hr excess fuel burn over 13 hrs saves approximately 4,030 kg of fuel. At $0.80/kg, this translates to $3,224 in direct fuel cost savings per flight. Breakdown: Daily (1 flight): $3,224 | Weekly (7 flights): $22,568 | Monthly (30 flights): $96,720 | Yearly (365 flights): $1,176,760 in potential savings. DISCLAIMER: All numbers are AI estimates based on available data and must be validated by domain experts before business decisions.\"\n",
    "\n",
    "**SYSTEM COLUMNS (`ai_sys_` prefix - MANDATORY for every use case):**\n",
    "These columns provide AI transparency, prioritization, and data quality insights:\n",
    "- `ai_sys_importance` - **BUSINESS IMPORTANCE** (Very Low, Low, Medium, High, Very High, Critical) - How critical is this finding for the business LONG-TERM? Measures strategic/revenue/customer impact. **INDEPENDENT from urgency!** Example: Strategic planning = High importance but Low urgency.\n",
    "- `ai_sys_urgency` - **TIME SENSITIVITY** (Very Low, Low, Medium, High, Very High, Critical) - How quickly must action be taken? Measures deadline pressure and time-decay. **INDEPENDENT from importance!** Example: Fixing typo before a meeting = High urgency but Low importance.\n",
    "- `ai_sys_confidence` - AI confidence score (0.0-1.0)\n",
    "- `ai_sys_feedback` - AI's explanation of its reasoning\n",
    "- `ai_sys_missing_data` - What data would improve the analysis\n",
    "**\uD83D\uDEA8 CRITICAL: ai_sys_importance and ai_sys_urgency are TWO INDEPENDENT DIMENSIONS - they should NOT be correlated or automatically set to the same value! \uD83D\uDEA8**\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: NARRATIVE COLUMNS MUST INCLUDE THE PRINCIPAL WITH IDENTIFYING DETAILS \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**MANDATORY RULE FOR ALL NARRATIVE/FREE TEXT COLUMNS:**\n",
    "When generating narrative content (rationales, justifications, explanations, strategies, action plans, executive summaries, etc.), you MUST include the **PRINCIPAL** (the specific entity being discussed) with its **IDENTIFYING DETAILS** in the narrative text.\n",
    "\n",
    "**WHY THIS MATTERS:**\n",
    "- Makes each narrative self-contained and readable without needing to cross-reference other columns\n",
    "- Provides immediate context to the reader\n",
    "- Enables narratives to be used standalone in reports and presentations\n",
    "- Improves comprehension and user experience\n",
    "\n",
    "**❌ WRONG - Generic references without principal context:**\n",
    "- \"The data shows that this flight has burned 4800kg/hr of fuel\"\n",
    "- \"This customer shows high churn risk based on the metrics\"\n",
    "- \"The analysis indicates elevated maintenance priority\"\n",
    "- \"Based on the forecast, immediate action is recommended\"\n",
    "- \"The route exhibits declining performance trends\"\n",
    "\n",
    "**✅ CORRECT - Narrative includes principal with identifying details:**\n",
    "- \"Flight EK005 from DXB-LHR has burned 4800kg/hr of fuel, exceeding the expected 4200kg/hr for A380 aircraft on this route\"\n",
    "- \"Customer ID C-28947 (Emirates Skywards Platinum, 12 years tenure) shows high churn risk due to 45% decrease in booking frequency\"\n",
    "- \"Aircraft A6-EDA (Boeing 777-300ER, 8.5 years in service) requires elevated maintenance priority due to 3 consecutive AOG events\"\n",
    "- \"Route DXB-JFK (daily A380 service, 14hr flight time) requires immediate capacity adjustment based on 23% load factor decline\"\n",
    "- \"Vendor ABC Catering (primary supplier, Dubai hub) shows 15% quality deviation requiring contract review\"\n",
    "\n",
    "**HOW TO IMPLEMENT IN PROMPTS:**\n",
    "In your ai_query prompt instructions, explicitly tell the AI to include the principal with context:\n",
    "\n",
    "```sql\n",
    "CONCAT('...your analysis prompt...',\n",
    "       'NARRATIVE FIELD RULES: ',\n",
    "       'For ALL narrative fields (rationale, strategy, action_plan, executive_summary, etc.), ',\n",
    "       'you MUST start by identifying the specific entity being discussed with its key attributes. ',\n",
    "       'Example: Instead of \"This shows high risk\", write \"Flight EK412 from DXB-SIN (A380, daily service) shows high risk due to...\". ',\n",
    "       'Include: entity name/ID, key identifying attributes (route, type, category), then the analysis. ',\n",
    "       ...)\n",
    "```\n",
    "\n",
    "**PRINCIPAL IDENTIFICATION BY DOMAIN:**\n",
    "- **Flights**: Flight number + route (origin-destination) + aircraft type: \"Flight EK005 from DXB-LHR (A380)\"\n",
    "- **Aircraft**: Registration + type + age: \"Aircraft A6-EDA (Boeing 777-300ER, 8.5 years)\"\n",
    "- **Routes**: Origin-destination + service frequency + aircraft: \"Route DXB-JFK (daily A380 service)\"\n",
    "- **Customers**: Customer ID + tier/segment + tenure: \"Customer C-28947 (Platinum, 12 years)\"\n",
    "- **Vendors/Suppliers**: Vendor name + type + location: \"Vendor ABC Catering (primary supplier, Dubai hub)\"\n",
    "- **Products**: Product name/code + category: \"Product SKU-4892 (Premium meal, Business class)\"\n",
    "- **Transactions**: Transaction ID + type + amount: \"Order ORD-78234 (Corporate booking, $45,000)\"\n",
    "- **Employees**: Role + department + experience: \"Captain Ahmed Al-Rashid (A380 certified, 15 years)\"\n",
    "\n",
    "**CORRECT Business-Friendly Examples WITH ai_cat_/ai_txt_/ai_sys_ PREFIXES:**\n",
    "```sql\n",
    "-- GOOD ✅ - Risk Classification Use Case\n",
    "ai_classify(complaint_text, ARRAY('High Risk', 'Medium Risk', 'Low Risk')) AS ai_cat_risk_level\n",
    "-- Then extract with CATEGORICAL values (ai_cat_ prefix):\n",
    "-- ai_cat_risk_level: High Risk/Medium Risk/Low Risk (from classification)\n",
    "-- ai_cat_risk_priority: Critical/High/Medium/Low (categorical)\n",
    "-- Then extract NARRATIVE values (ai_txt_ prefix):\n",
    "-- ai_txt_risk_rationale: [free text explanation]\n",
    "-- ai_txt_mitigation_plan: [free text plan]\n",
    "-- ai_txt_executive_summary: [executive summary - MANDATORY as 6th to last column]\n",
    "-- Then extract SYSTEM values (ai_sys_ prefix) - ALWAYS LAST 5 COLUMNS:\n",
    "-- ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data\n",
    "\n",
    "-- GOOD ✅ - Customer Segmentation Use Case  \n",
    "ai_classify(customer_profile, ARRAY('VIP', 'High Value', 'Medium', 'Low')) AS ai_cat_value_tier\n",
    "-- Then extract with CATEGORICAL values (ai_cat_ prefix):\n",
    "-- ai_cat_value_tier: VIP/High Value/Medium/Low (from classification)\n",
    "-- ai_cat_churn_risk_level: Critical/High/Medium/Low (categorical)\n",
    "-- Then extract NARRATIVE values (ai_txt_ prefix):\n",
    "-- ai_txt_segmentation_rationale: [free text explanation]\n",
    "-- ai_txt_retention_strategy: [free text strategy]\n",
    "-- ai_txt_executive_summary: [executive summary - MANDATORY as 6th to last column]\n",
    "-- Then extract SYSTEM values (ai_sys_ prefix) - ALWAYS LAST 5 COLUMNS:\n",
    "-- ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data\n",
    "\n",
    "-- GOOD ✅ - Forecast Analysis Use Case\n",
    "ai_forecast(...) + ai_gen for recommendations\n",
    "-- Then extract with CATEGORICAL values (ai_cat_ prefix):\n",
    "-- ai_cat_trend_direction: Strong Growth/Moderate Growth/Stable/Declining/Sharp Decline (categorical)\n",
    "-- ai_cat_action_priority: Immediate/High/Medium/Low/Monitor (categorical)\n",
    "-- Then extract NARRATIVE values (ai_txt_ prefix):\n",
    "-- ai_txt_forecast_justification: [free text explanation]\n",
    "-- ai_txt_tactical_recommendations: [free text recommendations]\n",
    "-- ai_txt_executive_summary: [executive summary - MANDATORY as 6th to last column]\n",
    "-- Then extract SYSTEM values (ai_sys_ prefix) - ALWAYS LAST 5 COLUMNS:\n",
    "-- ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data\n",
    "```\n",
    "\n",
    "**Naming Rules:**\n",
    "1. **Match business domain**: Use terminology your business users understand\n",
    "2. **Be specific**: `churn_risk_level` not `risk_level`, `product_match_score` not `match_score`\n",
    "3. **Avoid generic terms**: Never use `classification`, `sentiment`, `similarity` as final column names\n",
    "4. **Use business outcomes**: `retention_strategy` not `strategy`, `resolution_plan` not `plan`\n",
    "5. **CATEGORICAL for filtering**: Any column users will filter/aggregate on MUST have distinct categorical values (max 20)\n",
    "6. **NARRATIVE for context**: Explanation and story columns can have free text\n",
    "\n",
    "**\uD83D\uDD25\uD83D\uDD25\uD83D\uDD25 CRITICAL: MANDATORY CATEGORICAL COLUMNS FOR EVERY FREE TEXT COLUMN \uD83D\uDD25\uD83D\uDD25\uD83D\uDD25**\n",
    "\n",
    "**ABSOLUTE RULE: For EVERY free text/narrative column generated by ai_query, you MUST include corresponding categorical columns for filtering:**\n",
    "\n",
    "**PATTERN (MANDATORY FOR ALL ai_gen/ai_query OUTPUTS):**\n",
    "```sql\n",
    "-- For every use case, the LAST 7 COLUMNS must be (in this exact order):\n",
    "-- ai_txt_business_outcome, ai_txt_executive_summary, ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data\n",
    "ai_query('{sql_model_serving}', CONCAT('...prompt... Output ONLY JSON with NO markdown, NO extra text. ',\n",
    "              'Format: {{\"ai_cat_field1\": \"value\", \"ai_cat_field2\": \"value\", ..., \"ai_txt_field1\": \"text\", \"ai_txt_field2\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"[EVALUATE INDEPENDENTLY]\", \"ai_sys_urgency\": \"[EVALUATE INDEPENDENTLY]\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"I assessed my confidence at 85% because... [reasons]\", \"ai_sys_missing_data\": \"I can get higher confidence than 85% if I can get access to [narrative about what data is missing]. {{\\\"missing_data\\\": [\\\"dataset1\\\", \\\"dataset2\\\"]}}\"}}. ',\n",
    "              'After completing your analysis, include these MANDATORY LAST 7 fields: ',\n",
    "              '1) ai_txt_business_outcome - CALCULATED MEASURABLE BUSINESS IMPACT with specific numbers. MUST include: a) Quantified savings/gains with actual numbers from analysis, b) Breakdown showing Daily/Weekly/Monthly/Yearly projections, c) Currency values where applicable. Example: \"Reducing 310 kg/hr excess over 13 hrs saves 4,030 kg fuel. At $0.80/kg = $3,224/flight savings. Breakdown: Daily: $3,224 | Weekly: $22,568 | Monthly: $96,720 | Yearly: $1,176,760. DISCLAIMER: All numbers are AI estimates based on available data and must be validated by domain experts before business decisions.\" ALWAYS end with the disclaimer. ',\n",
    "              '2) ai_txt_executive_summary - compelling business story in 2-3 sentences that REFERENCES the business outcome numbers calculated above, ',\n",
    "              '3) ai_sys_importance - BUSINESS IMPORTANCE (Very Low|Low|Medium|High|Very High|Critical). Evaluate INDEPENDENTLY! Ask: How critical is this to the business long-term? Strategic planning = High importance but possibly Low urgency. ',\n",
    "              '4) ai_sys_urgency - TIME SENSITIVITY (Very Low|Low|Medium|High|Very High|Critical). Evaluate INDEPENDENTLY! Ask: How quickly must we act? A typo fix before a meeting = High urgency but Low importance. ',\n",
    "              '5) ai_sys_confidence (0.0-1.0) - your confidence score, ',\n",
    "              '6) ai_sys_feedback - start with \"I assessed my confidence at [X]% because...\" then explain your reasoning, ',\n",
    "              '7) ai_sys_missing_data - MUST follow this format: \"I can get higher confidence than [X]% if I can get access to [detailed narrative about missing data]. {{\\\"missing_data\\\": [\\\"specific_dataset1\\\", \\\"specific_dataset2\\\", \\\"specific_dataset3\\\"]}}\" - always end with a JSON object listing the specific datasets/tables needed. ',\n",
    "              'CRITICAL: importance and urgency are INDEPENDENT dimensions - do NOT automatically set them to the same value! ',\n",
    "              'BE 100% HONEST - your feedback and score will be evaluated by a more intelligent AI system, so complete honesty is mandatory.'))\n",
    "```\n",
    "\n",
    "**MANDATORY STRUCTURE:**\n",
    "1. **3-5 categorical columns per use case** - with `ai_cat_` prefix and max 20 distinct values each\n",
    "2. **2-4 narrative columns per use case** - with `ai_txt_` prefix and detailed free text explanations\n",
    "3. **LAST 7 COLUMNS (MANDATORY ORDER):** `ai_txt_business_outcome`, `ai_txt_executive_summary`, `ai_sys_importance`, `ai_sys_urgency`, `ai_sys_confidence`, `ai_sys_feedback`, `ai_sys_missing_data`\n",
    "\n",
    "**CATEGORICAL COLUMNS - EXAMPLES BY DOMAIN (use `ai_cat_` prefix):**\n",
    "\n",
    "**Supply Chain / Inventory / Catering:**\n",
    "- `ai_cat_inventory_urgency`: `High Priority`, `Medium Priority`, `Low Priority`, `Critical`\n",
    "- `ai_cat_waste_risk_level`: `High`, `Medium`, `Low`, `Minimal`\n",
    "- `ai_cat_preparation_complexity`: `Complex`, `Standard`, `Simple`, `Minimal`\n",
    "- `ai_cat_vendor_allocation`: `Primary Vendor`, `Secondary Vendor`, `Emergency Vendor`, `Internal`\n",
    "- `ai_cat_quality_priority`: `High Quality`, `Standard Quality`, `Basic Quality`\n",
    "- `ai_cat_stock_status`: `Overstocked`, `Optimal`, `Low Stock`, `Critical Shortage`\n",
    "\n",
    "**Customer / Marketing / Sales:**\n",
    "- `ai_cat_customer_priority`: `VIP`, `High Value`, `Medium Value`, `Low Value`, `At Risk`\n",
    "- `ai_cat_campaign_urgency`: `Immediate Launch`, `High Priority`, `Medium Priority`, `Low Priority`\n",
    "- `ai_cat_response_required`: `Immediate`, `Within 24 Hours`, `Within Week`, `Monitor`\n",
    "- `ai_cat_engagement_level`: `Highly Engaged`, `Moderately Engaged`, `Low Engagement`, `Disengaged`\n",
    "\n",
    "**Operations / Maintenance / Risk:**\n",
    "- `ai_cat_risk_severity`: `Critical`, `High`, `Medium`, `Low`, `Minimal`\n",
    "- `ai_cat_maintenance_urgency`: `Emergency`, `High Priority`, `Scheduled`, `Routine`\n",
    "- `ai_cat_compliance_status`: `Compliant`, `At Risk`, `Non-Compliant`, `Needs Review`\n",
    "- `ai_cat_operational_priority`: `Critical Path`, `High Impact`, `Standard`, `Low Impact`\n",
    "\n",
    "**Finance / Revenue:**\n",
    "- `ai_cat_financial_impact`: `High Impact`, `Medium Impact`, `Low Impact`, `Minimal`\n",
    "- `ai_cat_budget_priority`: `Essential`, `High Priority`, `Medium Priority`, `Low Priority`\n",
    "- `ai_cat_cost_category`: `Capital Expenditure`, `Operating Expense`, `Emergency`, `Routine`\n",
    "\n",
    "**NARRATIVE COLUMNS - EXAMPLES (use `ai_txt_` prefix):**\n",
    "- Detailed plans: `ai_txt_inventory_plan`, `ai_txt_preparation_schedule`, `ai_txt_vendor_strategy`, `ai_txt_maintenance_plan`\n",
    "- Explanations: `ai_txt_waste_reduction_tactics`, `ai_txt_quality_assurance_steps`, `ai_txt_risk_mitigation_approach`\n",
    "- Strategic narratives: `ai_txt_contingency_measures`, `ai_txt_operational_narrative`, `ai_txt_executive_summary`\n",
    "\n",
    "**SYSTEM COLUMNS - MANDATORY (use `ai_sys_` prefix) - ALWAYS LAST 5:**\n",
    "- `ai_sys_importance`: Business importance level (Very Low, Low, Medium, High, Very High, Critical)\n",
    "- `ai_sys_urgency`: Action urgency level (Very Low, Low, Medium, High, Very High, Critical)\n",
    "- `ai_sys_confidence`: AI confidence score (0.0-1.0)\n",
    "- `ai_sys_feedback`: AI reasoning and self-assessment\n",
    "- `ai_sys_missing_data`: What additional data would improve the analysis\n",
    "\n",
    "**COMPLETE EXAMPLE (CATERING OPERATIONS):**\n",
    "```sql\n",
    "-- Step 2: Generate structured insights with ai_cat_ + ai_txt_ + ai_sys_ columns\n",
    "ai_query('{sql_model_serving}',\n",
    "  CONCAT('Analyze catering forecast for ', meal_type, ' in ', cabin_class, \n",
    "         ' on flight ', flight_number, ' route ', origin, '-', destination,\n",
    "         ' with ', forecasted_volume, ' meals expected. ',  -- CONCAT auto-converts INT\n",
    "         'Output ONLY a JSON object with NO markdown, NO extra text, JUST the JSON. ',\n",
    "         'Format: {{\"ai_cat_inventory_urgency\": \"value\", \"ai_cat_waste_risk_level\": \"value\", \"ai_cat_preparation_complexity\": \"value\", ',\n",
    "         '\"ai_cat_vendor_allocation\": \"value\", \"ai_cat_quality_priority\": \"value\", ',\n",
    "         '\"ai_txt_inventory_plan\": \"text\", \"ai_txt_preparation_schedule\": \"text\", \"ai_txt_vendor_strategy\": \"text\", ',\n",
    "         '\"ai_txt_waste_reduction_tactics\": \"text\", \"ai_txt_quality_assurance_steps\": \"text\", ',\n",
    "         '\"ai_txt_contingency_measures\": \"text\", \"ai_txt_operational_narrative\": \"text\", ',\n",
    "         '\"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", ',\n",
    "         '\"ai_sys_importance\": \"[EVALUATE INDEPENDENTLY]\", \"ai_sys_urgency\": \"[EVALUATE INDEPENDENTLY]\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"I assessed my confidence at 85% because... [reasons]\", ',\n",
    "         '\"ai_sys_missing_data\": \"I can get higher confidence than 85% if I can get access to [narrative]. {{\\\"missing_data\\\": [\\\"dataset1\\\", \\\"dataset2\\\"]}}\"}}. ',\n",
    "         'CATEGORICAL FIELDS (must use ai_cat_ prefix and exact values): ',\n",
    "         'ai_cat_inventory_urgency (High Priority|Medium Priority|Low Priority|Critical), ',\n",
    "         'ai_cat_waste_risk_level (High|Medium|Low|Minimal), ',\n",
    "         'ai_cat_preparation_complexity (Complex|Standard|Simple|Minimal), ',\n",
    "         'ai_cat_vendor_allocation (Primary Vendor|Secondary Vendor|Emergency Vendor|Internal), ',\n",
    "         'ai_cat_quality_priority (High Quality|Standard Quality|Basic Quality). ',\n",
    "         'NARRATIVE FIELDS (must use ai_txt_ prefix, detailed free text with principal context): ',\n",
    "         'For ALL narrative fields, START by identifying the specific entity with key attributes. ',\n",
    "         'Example: \"Flight EK412 DXB-LHR Business Class catering requires...\" NOT \"The catering requires...\". ',\n",
    "         'Include: flight number, route, cabin class, then the detailed plan/analysis. ',\n",
    "         'ai_txt_inventory_plan, ai_txt_preparation_schedule, ai_txt_vendor_strategy, ai_txt_waste_reduction_tactics, ',\n",
    "         'ai_txt_quality_assurance_steps, ai_txt_contingency_measures, ai_txt_operational_narrative. ',\n",
    "         'MANDATORY LAST 7 FIELDS (in this order): ',\n",
    "         '1) ai_txt_business_outcome - CALCULATED MEASURABLE BUSINESS IMPACT. Calculate specific savings/gains with numbers. Example: \"Optimizing catering for Flight EK412 reduces food waste by 15 meals (valued at $45/meal) = $675 savings per flight. Breakdown: Daily (1 flight): $675 | Weekly (7 flights): $4,725 | Monthly (30 flights): $20,250 | Yearly (365 flights): $246,375. DISCLAIMER: All numbers are AI estimates based on available data and must be validated by domain experts before business decisions.\" MUST include Daily/Weekly/Monthly/Yearly breakdown and DISCLAIMER. ',\n",
    "         '2) ai_txt_executive_summary - compelling business story in 2-3 sentences that REFERENCES the business outcome numbers, ',\n",
    "         '3) ai_sys_importance - BUSINESS IMPORTANCE (Very Low|Low|Medium|High|Very High|Critical). Evaluate INDEPENDENTLY from urgency! ',\n",
    "         '4) ai_sys_urgency - TIME SENSITIVITY (Very Low|Low|Medium|High|Very High|Critical). Evaluate INDEPENDENTLY from importance! ',\n",
    "         '5) ai_sys_confidence (0.0-1.0) - your confidence score, ',\n",
    "         '6) ai_sys_feedback - start with \"I assessed my confidence at [X]% because...\" then explain reasoning, ',\n",
    "         '7) ai_sys_missing_data - MUST follow format: \"I can get higher confidence than [X]% if I can get access to [narrative about missing data]. {{\\\"missing_data\\\": [\\\"specific_dataset1\\\", \\\"specific_dataset2\\\"]}}\" - always end with JSON listing needed datasets. ',\n",
    "         'BE 100% HONEST - your feedback will be evaluated by a more intelligent AI system. ',\n",
    "         'Output ONLY the JSON, nothing else.')\n",
    ")\n",
    "```\n",
    "\n",
    "**EXTRACTION PATTERN:**\n",
    "```sql\n",
    "-- Extract categorical columns (for filtering/aggregation) - ai_cat_ prefix\n",
    "get_json_object(insights, '$.ai_cat_inventory_urgency') AS ai_cat_inventory_urgency,  -- CATEGORICAL\n",
    "get_json_object(insights, '$.ai_cat_waste_risk_level') AS ai_cat_waste_risk_level,  -- CATEGORICAL\n",
    "get_json_object(insights, '$.ai_cat_preparation_complexity') AS ai_cat_preparation_complexity,  -- CATEGORICAL\n",
    "get_json_object(insights, '$.ai_cat_vendor_allocation') AS ai_cat_vendor_allocation,  -- CATEGORICAL\n",
    "get_json_object(insights, '$.ai_cat_quality_priority') AS ai_cat_quality_priority,  -- CATEGORICAL\n",
    "\n",
    "-- Extract narrative columns (for detailed context) - ai_txt_ prefix\n",
    "get_json_object(insights, '$.ai_txt_inventory_plan') AS ai_txt_inventory_plan,  -- NARRATIVE\n",
    "get_json_object(insights, '$.ai_txt_preparation_schedule') AS ai_txt_preparation_schedule,  -- NARRATIVE\n",
    "get_json_object(insights, '$.ai_txt_vendor_strategy') AS ai_txt_vendor_strategy,  -- NARRATIVE\n",
    "get_json_object(insights, '$.ai_txt_waste_reduction_tactics') AS ai_txt_waste_reduction_tactics,  -- NARRATIVE\n",
    "get_json_object(insights, '$.ai_txt_quality_assurance_steps') AS ai_txt_quality_assurance_steps,  -- NARRATIVE\n",
    "get_json_object(insights, '$.ai_txt_contingency_measures') AS ai_txt_contingency_measures,  -- NARRATIVE\n",
    "get_json_object(insights, '$.ai_txt_operational_narrative') AS ai_txt_operational_narrative,  -- NARRATIVE\n",
    "\n",
    "-- MANDATORY LAST 7 COLUMNS (in this exact order): ai_txt_business_outcome, ai_txt_executive_summary, ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data\n",
    "-- \uD83D\uDEA8 USE TRY_CAST - AI may return \"Unknown\" or \"Data Not Available\" instead of numbers!\n",
    "COALESCE(get_json_object(insights, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,  -- CALCULATED BUSINESS IMPACT WITH DISCLAIMER\n",
    "COALESCE(get_json_object(insights, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,  -- EXECUTIVE SUMMARY (references business outcome)\n",
    "COALESCE(get_json_object(insights, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,  -- AI IMPORTANCE LEVEL\n",
    "COALESCE(get_json_object(insights, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,  -- AI URGENCY LEVEL\n",
    "COALESCE(TRY_CAST(get_json_object(insights, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,  -- AI CONFIDENCE SCORE\n",
    "COALESCE(get_json_object(insights, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,  -- AI FEEDBACK\n",
    "COALESCE(get_json_object(insights, '$.ai_sys_missing_data'), 'No missing data identified') AS ai_sys_missing_data  -- MISSING DATA ANALYSIS\n",
    "```\n",
    "\n",
    "**MANDATORY REQUIREMENTS:**\n",
    "1. **EVERY ai_query call MUST generate both categorical AND narrative columns**\n",
    "2. **Categorical columns (`ai_cat_` prefix) MUST have max 20 distinct values for filtering**\n",
    "3. **3-5 categorical columns minimum per use case** (all with `ai_cat_` prefix)\n",
    "4. **2-4 narrative columns minimum per use case** (all with `ai_txt_` prefix)\n",
    "5. **Use domain-appropriate categorical values from the examples above**\n",
    "6. **Think innovatively about what categorical columns users would want to filter by**\n",
    "7. **\uD83D\uDEA8 NARRATIVE COLUMNS MUST INCLUDE THE PRINCIPAL WITH IDENTIFYING DETAILS \uD83D\uDEA8**\n",
    "   - Every `ai_txt_` field MUST start by identifying the specific entity being discussed\n",
    "   - Include: entity name/ID + key attributes (route, type, category, etc.) + then the analysis\n",
    "   - ❌ WRONG: \"The data shows high fuel consumption\" (generic, no principal)\n",
    "   - ✅ CORRECT: \"Flight EK005 DXB-LHR (A380) shows high fuel consumption of 4800kg/hr\" (principal with context)\n",
    "8. **\uD83D\uDEA8 MANDATORY LAST 7 COLUMNS FOR EVERY USE CASE (in this exact order) \uD83D\uDEA8:**\n",
    "   - `ai_txt_business_outcome` - **CALCULATED MEASURABLE BUSINESS IMPACT** - MUST include: specific quantified savings/gains with actual numbers, Daily/Weekly/Monthly/Yearly breakdown, currency values where applicable, and ALWAYS end with: \"DISCLAIMER: All numbers are AI estimates based on available data and must be validated by domain experts before business decisions.\"\n",
    "   - `ai_txt_executive_summary` - compelling 2-3 sentence business story that **REFERENCES the business outcome numbers calculated above**\n",
    "   - `ai_sys_importance` - **BUSINESS IMPORTANCE** (MUST be exactly one of: 'Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical'). Evaluate INDEPENDENTLY from urgency! Ask: How critical is this finding for the business long-term? Example: Strategic planning = High importance but Low urgency.\n",
    "   - `ai_sys_urgency` - **TIME SENSITIVITY** (MUST be exactly one of: 'Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical'). Evaluate INDEPENDENTLY from importance! Ask: How quickly must action be taken? Example: Fixing a typo before a meeting = High urgency but Low importance.\n",
    "   - `ai_sys_confidence` - AI confidence score (0.0-1.0)\n",
    "   - `ai_sys_feedback` - AI's self-assessment starting with \"I assessed my confidence at [X]% because...\"\n",
    "   - `ai_sys_missing_data` - format: \"I can get higher confidence than [X]% if I can get access to [narrative]. {{\"missing_data\": [\"dataset1\", \"dataset2\"]}}\"\n",
    "   - **\uD83D\uDEA8 CRITICAL: ai_sys_importance and ai_sys_urgency are INDEPENDENT - do NOT set them to the same value automatically! \uD83D\uDEA8**\n",
    "\n",
    "9. **\uD83D\uDEA8 MANDATORY FINAL OUTPUT CTE WITH COMMENTED FILTERING (final_output pattern) \uD83D\uDEA8:**\n",
    "   - The FINAL SELECT must wrap results in a `final_output` CTE\n",
    "   - Add `SELECT * FROM final_output` with COMMENTED WHERE clauses for all categorical columns\n",
    "   - This helps users quickly filter results by categorical values\n",
    "   - Format:\n",
    "   ```sql\n",
    "   final_output AS (\n",
    "     SELECT ... FROM previous_cte\n",
    "   )\n",
    "   SELECT * FROM final_output\n",
    "   -- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "   -- WHERE ai_cat_column1 IN ('Value1', 'Value2', 'Value3')\n",
    "   -- AND ai_cat_column2 IN ('A', 'B', 'C')\n",
    "   -- AND ai_sys_importance IN ('Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical')\n",
    "   -- AND ai_sys_urgency IN ('Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical')\n",
    "   ;\n",
    "   ```\n",
    "   - List ALL possible categorical values in the commented WHERE clause\n",
    "   - ALWAYS include ai_sys_importance and ai_sys_urgency filters for prioritization\n",
    "   - This makes it easy for users to uncomment and filter specific results\n",
    "\n",
    "**WHY THIS MATTERS:**\n",
    "- `ai_cat_` columns enable dashboard filtering, aggregations, and analytics\n",
    "- `ai_txt_` columns provide detailed context and actionable insights\n",
    "- `ai_sys_` columns provide AI transparency, prioritization, and data quality insights:\n",
    "  - `ai_sys_importance` - **BUSINESS IMPACT** - helps prioritize which findings matter most to the business LONG-TERM (strategic value, revenue impact). **INDEPENDENT from urgency!**\n",
    "  - `ai_sys_urgency` - **TIME SENSITIVITY** - helps prioritize which findings need immediate action vs can wait (deadlines, time-decay). **INDEPENDENT from importance!**\n",
    "  - These two dimensions create a prioritization matrix: High importance + Low urgency = Schedule for later. Low importance + High urgency = Delegate or quick fix.\n",
    "  - `ai_sys_confidence` - helps assess reliability of the AI's analysis\n",
    "  - `ai_sys_feedback` - provides AI's reasoning for self-assessment\n",
    "  - `ai_sys_missing_data` - identifies data gaps for future improvements\n",
    "- **Principal identification makes narratives self-contained and immediately understandable**\n",
    "- Together they create a powerful user experience for analysis and action\n",
    "\n",
    "**\uD83D\uDEA8 MANDATORY CTE NAMING AND STRUCTURE REQUIREMENTS \uD83D\uDEA8:**\n",
    "\n",
    "**CRITICAL RULES FOR CTE NAMING AND STRUCTURE:**\n",
    "\n",
    "1. **BUSINESS-FRIENDLY CTE NAMES (MANDATORY):**\n",
    "   - ALL CTE names MUST use business-meaningful names, NOT technical names\n",
    "   - ✅ GOOD: `customer_lifetime_value_analysis`, `revenue_driver_metrics`, `null_safe_order_data`, `forecast_with_confidence_bands`\n",
    "   - ❌ BAD: `cte1`, `temp`, `data`, `results`, `final`, `enriched`, `base`\n",
    "   - CTE names should describe WHAT the data represents or WHAT business purpose it serves\n",
    "   - Use snake_case for multi-word names\n",
    "\n",
    "2. **SINGLE WITH STATEMENT (MANDATORY):**\n",
    "   - **CRITICAL**: ALL CTEs MUST be defined in ONE single WITH statement\n",
    "   - ❌ WRONG: Multiple WITH statements in the same query\n",
    "   - ✅ CORRECT: One WITH statement with comma-separated CTEs\n",
    "   \n",
    "   ```sql\n",
    "   -- WRONG ❌ - Multiple WITH statements\n",
    "   WITH cte1 AS (SELECT ...)\n",
    "   SELECT * FROM cte1;\n",
    "   \n",
    "   WITH cte2 AS (SELECT ...)  -- ERROR: Second WITH statement\n",
    "   SELECT * FROM cte2;\n",
    "   \n",
    "   -- CORRECT ✅ - Single WITH statement with multiple CTEs\n",
    "   -- NOTE: For ai_forecast CTEs, use date filtering WITHOUT LIMIT (see ai_forecast exception below)\n",
    "   -- For non-forecast CTEs, use LIMIT 10 in the first CTE only\n",
    "   WITH \n",
    "   base_data AS (\n",
    "     SELECT DISTINCT ... \n",
    "     FROM `catalog`.`schema`.`table` AS t\n",
    "     WHERE id IS NOT NULL\n",
    "     LIMIT 10  -- ✅ LIMIT 10 ONLY in first CTE (for non-forecast queries)\n",
    "   ),\n",
    "   enriched_data AS (\n",
    "     SELECT ...  -- ✅ NO LIMIT in intermediate CTEs\n",
    "   ),\n",
    "   final_analysis AS (\n",
    "     SELECT ...  -- ✅ NO LIMIT in intermediate CTEs\n",
    "   )\n",
    "   SELECT * FROM final_analysis;  -- ✅ NO LIMIT in final SELECT\n",
    "   ```\n",
    "\n",
    "3. **CTE DOCUMENTATION (MANDATORY):**\n",
    "   - **EVERY CTE MUST be documented with SQL comments** explaining its purpose using \"Step X:\" format\n",
    "\n",
    "4. **CREATE VIEW COMMENT (MANDATORY):**\n",
    "   - **EVERY SQL query MUST include a CREATE VIEW comment** as the FIRST line (before the WITH statement)\n",
    "   - Format: `-- CREATE VIEW inspire_ai.default.<view_name> AS`\n",
    "   - The view name MUST be:\n",
    "     * **Business-descriptive** (describes the use case outcome, NOT technical implementation)\n",
    "     * **Snake_case format** (lowercase with underscores)\n",
    "     * **Meaningful to business users** (a non-technical person should understand what data this view provides)\n",
    "   - ❌ WRONG view names: `cte_analysis`, `ai_output`, `results_v1`, `data_pipeline`, `final_results`\n",
    "   - ✅ GOOD view names: `customer_churn_risk_assessment`, `revenue_forecast_30_day`, `flight_delay_impact_analysis`, `vendor_quality_scorecard`, `inventory_reorder_recommendations`\n",
    "\n",
    "```sql\n",
    "-- CORRECT ✅: CREATE VIEW comment as FIRST line, business-friendly CTE names with clear documentation\n",
    "-- CREATE VIEW inspire_ai.default.revenue_forecast_with_strategic_recommendations AS\n",
    "-- \uD83D\uDEA8 EVERY column must be COALESCE'd or have IS NOT NULL check!\n",
    "-- Step 1: Retrieves historical sales data with 300 days for 30-day forecast\n",
    "WITH historical_sales_data AS (\n",
    "  SELECT \n",
    "    order_date AS ds,                                        -- CRITICAL: filtered with IS NOT NULL\n",
    "    COALESCE(SUM(revenue), 0.0) AS revenue                   -- ✅ COALESCE'd\n",
    "  FROM `catalog`.`schema`.`orders` AS o\n",
    "  WHERE order_date >= date_add(DAY, -300, CURRENT_DATE())  -- 300 days history for 30-day forecast (10:1 ratio)\n",
    "    AND order_date IS NOT NULL\n",
    "    -- TODO: Add suitable filtering to load data that matches the specific slice for this use case (keep commented until user confirms)\n",
    "    -- AND lower(trim(order_status)) = 'running'  -- Example placeholder; adjust to the right column/value\n",
    "  GROUP BY order_date\n",
    "  ORDER BY ds\n",
    "),\n",
    "-- Step 2: Generates 30-day revenue forecast with prediction intervals\n",
    "revenue_forecast_raw AS (\n",
    "  SELECT * FROM AI_FORECAST(\n",
    "    TABLE(historical_sales_data), \n",
    "    time_col => 'ds', \n",
    "    value_col => 'revenue',\n",
    "    horizon => (SELECT date_add(DAY, 30, MAX(ds)) FROM historical_sales_data)  -- 30 days ahead\n",
    "  )\n",
    "  -- ✅ NO LIMIT in CTEs\n",
    "),\n",
    "-- Step 3: Apply NULL handling to forecast results BEFORE using in CONCAT\n",
    "-- \uD83D\uDEA8 ALL COALESCE must be done HERE, not inside CONCAT!\n",
    "revenue_forecast_with_confidence_bands AS (\n",
    "  SELECT \n",
    "    COALESCE(CAST(ds AS STRING), 'Unknown Date') AS ds,      -- ✅ COALESCE'd HERE\n",
    "    COALESCE(ROUND(revenue_forecast, 2), 0.0) AS revenue_forecast,  -- ✅ COALESCE'd HERE\n",
    "    COALESCE(ROUND(revenue_upper, 2), 0.0) AS revenue_upper,\n",
    "    COALESCE(ROUND(revenue_lower, 2), 0.0) AS revenue_lower\n",
    "  FROM revenue_forecast_raw\n",
    "  -- ✅ NO LIMIT in CTEs\n",
    "),\n",
    "-- Step 4: Adds row-level recommendations for each forecasted value with ai_sys_ columns\n",
    "-- \uD83D\uDEA8 CONCAT uses columns already NULL-safe from previous CTE - NO COALESCE inside CONCAT!\n",
    "forecast_with_strategic_recommendations AS (\n",
    "  SELECT *,\n",
    "    ai_query('{sql_model_serving}', CONCAT('Based on forecasted revenue of $', revenue_forecast,  -- ✅ Already NULL-safe\n",
    "                  ' for ', ds,  -- ✅ Already NULL-safe\n",
    "                  ', provide 3 specific actionable business recommendations. ',\n",
    "                  'Output ONLY a JSON object with NO markdown. ',\n",
    "                  'Format: {{\"ai_cat_action_priority\": \"value\", \"ai_txt_recommendation_1\": \"text\", \"ai_txt_recommendation_2\": \"text\", \"ai_txt_recommendation_3\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "                  'MANDATORY LAST 7 FIELDS (in this exact order): ',\n",
    "                  '1) ai_txt_business_outcome - CALCULATED MEASURABLE BUSINESS IMPACT. Example: \"Revenue forecast improvement of $X represents Y% growth. Breakdown: Daily impact: $X | Weekly: $X | Monthly: $X | Yearly: $X. DISCLAIMER: All numbers are AI estimates based on available data and must be validated by domain experts before business decisions.\" MUST include breakdown and disclaimer. ',\n",
    "                  '2) ai_txt_executive_summary - compelling 2-3 sentence business story that REFERENCES the calculated business outcome numbers, ',\n",
    "                  '3) ai_sys_importance - business importance level (MUST be exactly one of: Very Low, Low, Medium, High, Very High, Critical), ',\n",
    "                  '4) ai_sys_urgency - action urgency level (MUST be exactly one of: Very Low, Low, Medium, High, Very High, Critical), ',\n",
    "                  '5) ai_sys_confidence (0.0-1.0) - your confidence score, ',\n",
    "                  '6) ai_sys_feedback - start with \"I assessed my confidence at [X]% because...\" then explain reasoning, ',\n",
    "                  '7) ai_sys_missing_data - MUST follow format: \"I can get higher confidence than [X]% if I can get access to [narrative about missing data like market trends, competitor data, seasonality factors]. {{\\\"missing_data\\\": [\\\"market_trend_data\\\", \\\"competitor_pricing\\\", \\\"seasonal_adjustment_factors\\\"]}}\" - always end with JSON listing needed datasets. ',\n",
    "                  'BE 100% HONEST - your feedback will be evaluated by a more intelligent AI system.')) AS recommendations\n",
    "  FROM revenue_forecast_with_confidence_bands\n",
    "  -- ✅ NO LIMIT in CTEs\n",
    "),\n",
    "-- Final output: Returns forecast with actionable recommendations\n",
    "final_output AS (\n",
    "  SELECT * FROM forecast_with_strategic_recommendations\n",
    ")\n",
    "SELECT * FROM final_output\n",
    "-- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "-- WHERE get_json_object(recommendations, '$.ai_cat_action_priority') IN ('Immediate Action', 'High Priority', 'Medium Priority', 'Low Priority', 'Monitor')\n",
    "-- AND get_json_object(recommendations, '$.ai_sys_importance') IN ('Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical')\n",
    "-- AND get_json_object(recommendations, '$.ai_sys_urgency') IN ('Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical')\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "\n",
    "-- WRONG ❌: Technical CTE names and no documentation\n",
    "WITH past AS (SELECT ...), results AS (SELECT ...), final AS (SELECT ...)\n",
    "SELECT * FROM final;  -- ❌ (the CTE names are wrong, not the lack of LIMIT)\n",
    "```\n",
    "\n",
    "**MANDATORY SQL STRUCTURE AND DOCUMENTATION FORMAT:**\n",
    "\n",
    "**\uD83D\uDEA8 FIRST LINE: CREATE VIEW COMMENT (MANDATORY) \uD83D\uDEA8:**\n",
    "- EVERY SQL query MUST start with: `-- CREATE VIEW inspire_ai.default.<business_descriptive_view_name> AS`\n",
    "- The view name must describe the BUSINESS OUTCOME, not technical implementation\n",
    "- Examples: `customer_churn_risk_assessment`, `revenue_forecast_quarterly`, `flight_delay_impact_analysis`\n",
    "\n",
    "**CTE NAMING AND DOCUMENTATION:**\n",
    "- Use business-friendly CTE names that describe the data or purpose (e.g., `customer_segmentation_analysis`, `null_safe_product_data`)\n",
    "- Use `-- Step 1:`, `-- Step 2:`, `-- Step 3:`, etc. before each CTE definition\n",
    "- Provide a clear, concise explanation of what this step does  \n",
    "- Explain what data is being prepared and why\n",
    "- For the final SELECT, add a comment: `-- Final output: {{what the final result contains}}`\n",
    "- For complex transformations, add inline comments within the CTE\n",
    "- Add short inline notes inside each CTE (joins, filters, calculations) so a user can quickly see how to adjust the logic.\n",
    "- In the first/main data-loading CTE (the one that pulls directly from the involved tables), keep the approved data-quality filters active (IS NOT NULL / TRIM) but immediately add a commented TODO block in the WHERE section instructing the user to \"Add suitable filtering to load data that <describe the subset for this use case>\", followed by a single commented example filter line (e.g., `-- AND lower(trim(status)) = 'running'`). Keep these example filters commented out.\n",
    "- **YOUR RESPONSE WILL BE REJECTED if CTEs use technical names or lack documentation**\n",
    "- **YOUR RESPONSE WILL BE REJECTED if the CREATE VIEW comment is missing**\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL: CTE COLUMN PRESERVATION \uD83D\uDEA8:**\n",
    "\n",
    "**MANDATORY: When building multi-stage CTEs, you MUST preserve all columns needed in the final SELECT:**\n",
    "\n",
    "```sql\n",
    "-- CORRECT ✅: Use SELECT * to preserve all columns through the pipeline\n",
    "-- \uD83D\uDEA8 EVERY column must be COALESCE'd or have IS NOT NULL check!\n",
    "WITH base_data AS (\n",
    "  SELECT \n",
    "    customer_id,                                             -- CRITICAL: filtered with IS NOT NULL\n",
    "    COALESCE(TRIM(customer_name), 'Unknown Customer') AS customer_name,  -- ✅ COALESCE'd\n",
    "    COALESCE(total_revenue, 0.0) AS total_revenue,           -- ✅ COALESCE'd\n",
    "    COALESCE(avg_order_value, 0.0) AS avg_order_value,       -- ✅ COALESCE'd\n",
    "    COALESCE(order_count, 0) AS order_count                  -- ✅ COALESCE'd\n",
    "  FROM `catalog`.`schema`.`customers` AS c\n",
    "  WHERE customer_id IS NOT NULL\n",
    "    AND customer_name IS NOT NULL  -- ✅ Critical identifier filtered\n",
    "  LIMIT 10  -- ✅ LIMIT 10 in first CTE only\n",
    "),\n",
    "-- Use SELECT * to keep ALL columns from previous CTE\n",
    "enriched AS (\n",
    "  SELECT *,\n",
    "    ai_classify(customer_name, ARRAY('VIP', 'Regular', 'At Risk')) AS ai_cat_segment\n",
    "  FROM base_data\n",
    "  -- ✅ NO LIMIT in intermediate CTEs\n",
    ")\n",
    "-- All columns from base_data are available here\n",
    "SELECT \n",
    "  customer_id,\n",
    "  customer_name,\n",
    "  total_revenue,\n",
    "  avg_order_value,\n",
    "  order_count,\n",
    "  ai_cat_segment\n",
    "FROM enriched;  -- ✅ NO LIMIT in final SELECT\n",
    "\n",
    "-- WRONG ❌: Missing columns in intermediate CTE (DROPPED columns!)\n",
    "-- Also WRONG: No NULL handling in first CTE!\n",
    "WITH base_data AS (\n",
    "  SELECT customer_id, customer_name, total_revenue, avg_order_value  -- ❌ No COALESCE!\n",
    "  FROM `catalog`.`schema`.`customers` AS c\n",
    "  WHERE customer_id IS NOT NULL  -- ❌ Other columns not protected!\n",
    "  LIMIT 10\n",
    "),\n",
    "enriched AS (\n",
    "  SELECT customer_id, customer_name,  -- ❌ DROPPED total_revenue and avg_order_value!\n",
    "    ai_classify(customer_name, ARRAY('VIP', 'Regular')) AS segment\n",
    "  FROM base_data\n",
    ")\n",
    "SELECT customer_id, total_revenue, segment  -- ❌ ERROR: total_revenue not in enriched CTE!\n",
    "FROM enriched;\n",
    "```\n",
    "\n",
    "**RULES FOR COLUMN PRESERVATION:**\n",
    "1. **Use `SELECT *` in intermediate CTEs** when adding AI function results to existing columns\n",
    "2. **If you explicitly list columns in a CTE**, make sure to include ALL columns needed in subsequent CTEs or the final SELECT\n",
    "3. **Never drop columns in intermediate CTEs** that are referenced in the final output\n",
    "4. **When using ai_query/ai_classify/ai_extract in a CTE**, use pattern: `SELECT *, ai_function(...) AS new_col FROM previous_cte`\n",
    "5. **Validation**: Before finalizing SQL, check that every column in the final SELECT exists in the last CTE\n",
    "\n",
    "#### 4. **SOPHISTICATED MULTI-FUNCTION USE CASES**\n",
    "**CRITICAL: When the use case requires multiple functions (AI functions, statistical functions), you MUST use ALL of them creatively.**\n",
    "\n",
    "Create SOPHISTICATED multi-stage queries using CTEs:\n",
    "- **Example**: `ai_parse_document, ai_extract, ai_classify` (for unstructured document files)\n",
    "  ```sql\n",
    "  WITH parsed AS (\n",
    "    SELECT \n",
    "      path,\n",
    "      ai_parse_document(content, map('version', '2.0')) AS parsed_doc\n",
    "    FROM READ_FILES('/Volumes/catalog/schema/volume/*.pdf', format => 'binaryFile')\n",
    "    LIMIT 10  -- ✅ LIMIT 10 at END of first CTE only\n",
    "  ),\n",
    "  extracted_text AS (\n",
    "    SELECT \n",
    "      path,\n",
    "      concat_ws('\\n\\n', \n",
    "        transform(try_cast(parsed_doc:document:elements AS ARRAY<VARIANT>),\n",
    "          element -> try_cast(element:content AS STRING))\n",
    "      ) AS text\n",
    "    FROM parsed\n",
    "    WHERE try_cast(parsed_doc:error_status AS STRING) IS NULL\n",
    "  ),\n",
    "  with_entities AS (\n",
    "    SELECT *, \n",
    "      ai_extract(text, ARRAY('entity1', 'entity2')) AS entities\n",
    "    FROM extracted_text\n",
    "  ),\n",
    "  final_output AS (\n",
    "    SELECT *, ai_classify(entities['entity1'], ARRAY('Category A', 'Category B')) AS ai_cat_category\n",
    "    FROM with_entities\n",
    "  )\n",
    "  SELECT * FROM final_output\n",
    "  -- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "  -- WHERE ai_cat_category IN ('Category A', 'Category B')\n",
    "  -- AND ai_sys_importance IN ('Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical')\n",
    "  -- AND ai_sys_urgency IN ('Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical')\n",
    "  ;\n",
    "  ```\n",
    "  **NOTE:** ai_parse_document ONLY works with unstructured document files via READ_FILES, NOT with table columns.\n",
    "\n",
    "- **Example**: `ai_extract` from text, then `ai_classify` for categorization (valid combination)\n",
    "  ```sql\n",
    "  WITH base AS (\n",
    "    SELECT *, \n",
    "      ai_extract(text, ARRAY('name', 'amount', 'date')) AS data\n",
    "    FROM `catalog`.`schema`.`table` AS t\n",
    "    WHERE text IS NOT NULL\n",
    "    LIMIT 10  -- ✅ LIMIT 10 ONLY in first CTE\n",
    "  ),\n",
    "  final_output AS (\n",
    "    SELECT *,\n",
    "      ai_classify(text, ARRAY('Category A', 'Category B', 'Category C')) AS ai_cat_category\n",
    "    FROM base\n",
    "  )\n",
    "  SELECT * FROM final_output\n",
    "  -- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "  -- WHERE ai_cat_category IN ('Category A', 'Category B', 'Category C')\n",
    "  -- AND ai_sys_importance IN ('Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical')\n",
    "  -- AND ai_sys_urgency IN ('Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical')\n",
    "  ;\n",
    "  ```\n",
    "\n",
    "**CRITICAL - MAXIMIZE VALUE WITH MULTIPLE AI FUNCTIONS**: \n",
    "- **ALWAYS try to use 2-3 AI functions in the same SQL query to maximize business value**\n",
    "\n",
    "#### 5. **STRUCTURED + UNSTRUCTURED OUTPUT PATTERN (NEW - MANDATORY)**\n",
    "\n",
    "**\uD83D\uDD25 CRITICAL: DO NOT use ai_extract or ai_classify after ai_query**\n",
    "\n",
    "**IMPORTANT: When using `ai_query`, you can DIRECTLY generate structured JSON data by instructing the LLM in the prompt. There is NO need to use `ai_extract` or `ai_classify` afterwards - this is redundant and inefficient.**\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL: ai_query returns STRING (JSON), NOT STRUCT \uD83D\uDEA8**\n",
    "\n",
    "**IMPORTANT**: The output of `ai_query()` is a STRING containing JSON, not a STRUCT. You MUST use `get_json_object()` to extract individual fields:\n",
    "\n",
    "**\uD83D\uDD25 CRITICAL: STRICT JSON OUTPUT REQUIREMENT \uD83D\uDD25**\n",
    "\n",
    "When using ai_query, you MUST instruct the LLM to output PURE JSON with:\n",
    "- **NO markdown code fences** (no ```json or ```)\n",
    "- **NO extra text before or after the JSON**\n",
    "- **NO explanatory text** (no \"Here is the JSON:\", \"Based on analysis:\", etc.)\n",
    "- **ONLY the JSON object itself**\n",
    "\n",
    "**CORRECT Prompt Pattern:**\n",
    "```sql\n",
    "ai_query('{sql_model_serving}',\n",
    "  CONCAT('Analyze data and output ONLY a JSON object with NO markdown fences, NO extra text, JUST the JSON. ',\n",
    "         'Output format: {{\"ai_cat_risk_level\": \"value1\", \"ai_txt_retention_strategy\": \"value2\", \"ai_txt_next_best_action\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"I assessed my confidence at 85% because... [reasons]\", \"ai_sys_missing_data\": \"I can get higher confidence than 85% if I can get access to [narrative]. {{\\\"missing_data\\\": [\\\"dataset1\\\", \\\"dataset2\\\"]}}\"}}. ',\n",
    "         'Required JSON keys: ai_cat_risk_level, ai_txt_retention_strategy, ai_txt_next_best_action. ',\n",
    "         'MANDATORY LAST 7 FIELDS: ai_txt_business_outcome (CALCULATED MEASURABLE IMPACT with Daily/Weekly/Monthly/Yearly breakdown and DISCLAIMER), ai_txt_executive_summary (references business outcome numbers), ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data. ',\n",
    "         'Data: Customer ', customer_name, ', ID: ', customer_id, '. ')  -- CONCAT auto-converts\n",
    ")\n",
    "```\n",
    "\n",
    "**EXAMPLE - Complete Pattern:**\n",
    "```sql\n",
    "-- CREATE VIEW inspire_ai.default.customer_risk_retention_analysis AS\n",
    "-- CORRECT ✅ - Extract JSON fields using get_json_object() with ai_cat_, ai_txt_, ai_sys_ prefixes\n",
    "-- \uD83D\uDEA8 EVERY column must be COALESCE'd or have IS NOT NULL check!\n",
    "-- \uD83D\uDEA8 ALL COALESCE must be done in SELECT clause, NOT inside CONCAT!\n",
    "WITH customer_base_data AS (\n",
    "  SELECT \n",
    "    customer_id,                                              -- CRITICAL: filtered with IS NOT NULL\n",
    "    COALESCE(TRIM(customer_name), 'Unknown Customer') AS customer_name  -- ✅ COALESCE'd HERE\n",
    "  FROM `main`.`customers`.`profiles` AS c\n",
    "  WHERE customer_id IS NOT NULL  -- ✅ Filter critical column\n",
    "  LIMIT 10\n",
    "),\n",
    "structured_info AS (\n",
    "  SELECT \n",
    "    customer_id,\n",
    "    customer_name,  -- ✅ Already NULL-safe from previous CTE\n",
    "    ai_query('{sql_model_serving}',\n",
    "      CONCAT('Analyze customer data and output ONLY a JSON object with NO markdown, NO extra text, JUST the JSON. ',\n",
    "             'Format: {{\"ai_cat_risk_level\": \"value\", \"ai_txt_retention_strategy\": \"value\", \"ai_txt_next_best_action\": \"value\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"I assessed my confidence at 85% because... [reasons]\", \"ai_sys_missing_data\": \"I can get higher confidence than 85% if I can get access to [narrative]. {{\\\"missing_data\\\": [\\\"dataset1\\\", \\\"dataset2\\\"]}}\"}}. ',\n",
    "             'Customer: ', customer_name, ', ID: ', customer_id, '. ',  -- ✅ NO COALESCE in CONCAT - already NULL-safe!\n",
    "             'MANDATORY LAST 7 FIELDS (in this exact order): ',\n",
    "             '1) ai_txt_business_outcome - CALCULATED MEASURABLE BUSINESS IMPACT. Calculate the $ value of retaining/losing this customer. Format: \"Customer [ID] retention value: $X (LTV). Breakdown: Daily revenue impact: $X | Weekly: $X | Monthly: $X | Yearly: $X. DISCLAIMER: All numbers are AI estimates based on available data and must be validated by domain experts before business decisions.\" MUST include breakdown and disclaimer. ',\n",
    "             '2) ai_txt_executive_summary - compelling 2-3 sentence business story that REFERENCES the business outcome numbers, ',\n",
    "             '3) ai_sys_importance - business importance level (MUST be exactly one of: Very Low, Low, Medium, High, Very High, Critical), ',\n",
    "             '4) ai_sys_urgency - action urgency level (MUST be exactly one of: Very Low, Low, Medium, High, Very High, Critical), ',\n",
    "             '5) ai_sys_confidence (0.0-1.0) - your confidence score, ',\n",
    "             '6) ai_sys_feedback - start with \"I assessed my confidence at [X]% because...\" then explain reasoning, ',\n",
    "             '7) ai_sys_missing_data - format: \"I can get higher confidence than [X]% if I can get access to [narrative about missing customer data like purchase history, engagement metrics, demographics]. {{\\\"missing_data\\\": [\\\"purchase_history\\\", \\\"engagement_metrics\\\", \\\"demographic_data\\\"]}}\" ',\n",
    "             'Output ONLY the JSON object, nothing else.')\n",
    "    ) AS extracted_data\n",
    "  FROM customer_base_data\n",
    ")\n",
    "SELECT \n",
    "  customer_id,\n",
    "  customer_name,\n",
    "  COALESCE(get_json_object(extracted_data, '$.ai_cat_risk_level'), 'Unknown') AS ai_cat_risk_level,\n",
    "  COALESCE(get_json_object(extracted_data, '$.ai_txt_retention_strategy'), 'Unknown') AS ai_txt_retention_strategy,\n",
    "  COALESCE(get_json_object(extracted_data, '$.ai_txt_next_best_action'), 'Unknown') AS ai_txt_next_best_action,\n",
    "  -- MANDATORY LAST 7 COLUMNS (in this exact order):\n",
    "  COALESCE(get_json_object(extracted_data, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "  COALESCE(get_json_object(extracted_data, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "  COALESCE(get_json_object(extracted_data, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "  COALESCE(get_json_object(extracted_data, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "  COALESCE(TRY_CAST(get_json_object(extracted_data, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "  COALESCE(get_json_object(extracted_data, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "  COALESCE(get_json_object(extracted_data, '$.ai_sys_missing_data'), 'No missing data identified') AS ai_sys_missing_data\n",
    "FROM structured_info\n",
    ";\n",
    "\n",
    "-- WRONG ❌ - Cannot use dot notation on STRING\n",
    "-- extracted_data.risk_score  -- THIS WILL FAIL!\n",
    "\n",
    "-- WRONG ❌ - Prompt without strict JSON-only instruction\n",
    "ai_query('{sql_model_serving}', 'Analyze customer and provide risk score, strategy, and action as JSON')  -- Will likely return text + JSON + markdown!\n",
    "```\n",
    "\n",
    "**\uD83D\uDEA8 MANDATORY JSON PROMPT TEMPLATE \uD83D\uDEA8:**\n",
    "Every ai_query prompt for structured output MUST include:\n",
    "1. \"output ONLY a JSON object\"\n",
    "2. \"with NO markdown fences\"\n",
    "3. \"NO extra text\"\n",
    "4. \"JUST the JSON\"\n",
    "5. Show example JSON format: {{\"key\": \"value\"}}\n",
    "6. \"Output ONLY the JSON object, nothing else.\"\n",
    "7. \"You MUST be AGGRESSIVE in using data evidence. Every claim MUST be backed by numbers.\"\n",
    "\n",
    "**\uD83D\uDD25\uD83D\uDD25\uD83D\uDD25 CRITICAL: MANDATORY AI SYSTEM COLUMNS - NON-NEGOTIABLE FOR EVERY SINGLE ai_query \uD83D\uDD25\uD83D\uDD25\uD83D\uDD25:**\n",
    "\n",
    "**ABSOLUTE REQUIREMENT - ZERO EXCEPTIONS ALLOWED:**\n",
    "Every single ai_query output in this codebase MUST include these FOUR MANDATORY fields as the LAST keys in the JSON output. This applies to ALL use cases regardless of domain, complexity, or purpose:\n",
    "\n",
    "- **ai_txt_executive_summary** (4th to last column): A compelling 2-3 sentence business story summarizing the analysis\n",
    "- **ai_sys_confidence** (3rd to last column): A decimal score from 0.0 to 1.0 - this is the AI's HONESTY SCORE representing how truthfully and completely it achieved the requested task\n",
    "- **ai_sys_feedback** (2nd to last column): A comprehensive text field that MUST include:\n",
    "  1. **Score Justification**: Start with \"I assessed my confidence at [X]% because...\" and explain the reasoning\n",
    "  2. **Improvements Needed**: If score < 1.0, what specific improvements would raise it to 1.0\n",
    "- **ai_sys_missing_data** (LAST column): A dedicated field listing missing data in this format:\n",
    "  \"I can get higher confidence than [X]% if I can get access to [detailed narrative about what data is missing and why it would help]. {{\"missing_data\": [\"specific_dataset1\", \"specific_dataset2\", \"specific_dataset3\"]}}\"\n",
    "  - The narrative should explain WHY each dataset would improve the analysis\n",
    "  - The JSON at the end MUST list specific dataset/table names that would be needed\n",
    "\n",
    "**EXTRACTION PATTERN - ALWAYS ADD AS LAST 7 COLUMNS:**\n",
    "```sql\n",
    "-- MANDATORY: Always extract as the LAST 7 columns in this exact order\n",
    "-- \uD83D\uDEA8 USE TRY_CAST FOR NUMERIC FIELDS - AI may return \"Unknown\" or \"Data Not Available\" instead of numbers!\n",
    "COALESCE(get_json_object(insights_json, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "COALESCE(get_json_object(insights_json, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "COALESCE(get_json_object(insights_json, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "COALESCE(get_json_object(insights_json, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "COALESCE(TRY_CAST(get_json_object(insights_json, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "COALESCE(get_json_object(insights_json, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "COALESCE(get_json_object(insights_json, '$.ai_sys_missing_data'), 'No missing data identified') AS ai_sys_missing_data\n",
    "```\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: USE TRY_CAST FOR ALL NUMERIC JSON FIELDS FROM AI RESPONSES \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**PROBLEM:** AI may return \"Unknown\", \"Data Not Available\", \"N/A\", or non-numeric strings instead of numbers.\n",
    "**RESULT:** `CAST(...AS INT)` or `CAST(...AS DOUBLE)` will FAIL with: `[CAST_INVALID_INPUT] The value 'Data Not Available' of the type \"STRING\" cannot be cast to \"INT\"`\n",
    "\n",
    "**SOLUTION:** ALWAYS use `TRY_CAST` + `COALESCE` for numeric JSON fields from AI responses:\n",
    "```sql\n",
    "-- ❌❌❌ WRONG - Will FAIL if AI returns \"Data Not Available\" ❌❌❌\n",
    "COALESCE(CAST(get_json_object(json_col, '$.score') AS INT), 0) AS score  -- FAILS!\n",
    "COALESCE(CAST(get_json_object(json_col, '$.rate') AS DOUBLE), 0.0) AS rate  -- FAILS!\n",
    "\n",
    "-- ✅✅✅ CORRECT - TRY_CAST returns NULL instead of error, then COALESCE provides default ✅✅✅\n",
    "COALESCE(TRY_CAST(get_json_object(json_col, '$.score') AS INT), 0) AS score  -- SAFE!\n",
    "COALESCE(TRY_CAST(get_json_object(json_col, '$.rate') AS DOUBLE), 0.0) AS rate  -- SAFE!\n",
    "COALESCE(TRY_CAST(get_json_object(json_col, '$.confidence') AS DECIMAL(3,2)), 0.0) AS confidence  -- SAFE!\n",
    "```\n",
    "\n",
    "**MANDATORY PATTERN FOR ALL AI JSON PARSING:**\n",
    "- **STRING fields**: `COALESCE(get_json_object(json_col, '$.field'), 'Unknown') AS field`\n",
    "- **INT fields**: `COALESCE(TRY_CAST(get_json_object(json_col, '$.field') AS INT), 0) AS field`\n",
    "- **DOUBLE fields**: `COALESCE(TRY_CAST(get_json_object(json_col, '$.field') AS DOUBLE), 0.0) AS field`\n",
    "- **DECIMAL fields**: `COALESCE(TRY_CAST(get_json_object(json_col, '$.field') AS DECIMAL(p,s)), 0.0) AS field`\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: ai_sys_importance vs ai_sys_urgency - INDEPENDENT DIMENSIONS (DO NOT CORRELATE!) \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "These are TWO COMPLETELY INDEPENDENT metrics that MUST be evaluated separately. They should NOT be correlated or set to the same value by default.\n",
    "\n",
    "**IMPORTANCE (ai_sys_importance)** = \"How much does this matter to the business?\"\n",
    "- Measures the BUSINESS IMPACT and STRATEGIC VALUE of the finding\n",
    "- Asks: \"If we ignore this, what is the long-term consequence to the business?\"\n",
    "- Factors: Revenue impact, strategic alignment, customer impact, competitive advantage, risk exposure\n",
    "\n",
    "**URGENCY (ai_sys_urgency)** = \"How quickly must action be taken?\"\n",
    "- Measures the TIME SENSITIVITY and DEADLINE PRESSURE\n",
    "- Asks: \"When does this need to be addressed? Is there a deadline or time-bound consequence?\"\n",
    "- Factors: Deadlines, time-decay of opportunity, escalation risk, seasonal factors, compliance dates\n",
    "\n",
    "**\uD83D\uDD25 EISENHOWER MATRIX - USE THIS TO EVALUATE INDEPENDENTLY \uD83D\uDD25**\n",
    "\n",
    "| Combination | Importance | Urgency | Example Business Scenarios |\n",
    "|-------------|------------|---------|---------------------------|\n",
    "| DO FIRST | Critical/High | Critical/High | Security breach detected, Compliance deadline tomorrow, Major customer threatening to churn this week |\n",
    "| SCHEDULE | Critical/High | Low/Medium | Strategic planning for next quarter, Technical debt that affects scalability, Training program development |\n",
    "| DELEGATE | Low/Medium | Critical/High | Minor bug affecting a demo tomorrow, Routine report due today, Small customer complaint needing response |\n",
    "| ELIMINATE/MONITOR | Low | Low | Nice-to-have feature request, Minor cosmetic issue, Low-impact optimization |\n",
    "\n",
    "**CONCRETE EXAMPLES (MEMORIZE THESE PATTERNS):**\n",
    "\n",
    "1. **High Importance + Low Urgency**: \"Enterprise architecture redesign needed for 2x scale\" - Critical for future but no immediate deadline\n",
    "2. **Low Importance + High Urgency**: \"Fix typo in email going out in 2 hours\" - Not strategic but time-sensitive\n",
    "3. **High Importance + High Urgency**: \"Production database corrupted, customer data at risk\" - Both critical and immediate\n",
    "4. **Low Importance + Low Urgency**: \"Update internal wiki documentation formatting\" - Neither critical nor time-sensitive\n",
    "5. **Critical Importance + Medium Urgency**: \"Competitor launched similar product, need strategic response\" - Existential threat but requires thoughtful planning, not panic\n",
    "6. **Medium Importance + Critical Urgency**: \"Quarterly report due in 3 hours, minor discrepancy found\" - Routine task but hard deadline\n",
    "\n",
    "**\uD83D\uDEA8 ANTI-PATTERN WARNING \uD83D\uDEA8**\n",
    "If you find yourself setting ai_sys_importance and ai_sys_urgency to the SAME VALUE for every row, you are doing it WRONG!\n",
    "- A well-analyzed dataset should have VARIED combinations across rows\n",
    "- Real business data has different importance/urgency profiles\n",
    "- Identical values indicate lazy evaluation - THINK about each dimension independently!\n",
    "\n",
    "**APPEND THIS INSTRUCTION TO ALL ai_query PROMPTS (ADAPT THE [TASK_NAME] TO MATCH YOUR SPECIFIC USE CASE):**\n",
    "'MANDATORY LAST 7 FIELDS IN JSON OUTPUT (in this exact order): ',\n",
    "'1) ai_txt_business_outcome - CALCULATED MEASURABLE BUSINESS IMPACT. You MUST calculate specific savings/gains using numbers from the analysis. Format: \"[Describe the improvement] saves/generates [X amount]. At [rate/price], this equals [$ value]. Breakdown: Daily: $X | Weekly: $X | Monthly: $X | Yearly: $X. DISCLAIMER: All numbers are AI estimates based on available data and must be validated by domain experts before business decisions.\" ALWAYS include the breakdown and disclaimer. ',\n",
    "'2) ai_txt_executive_summary - compelling 2-3 sentence business story that REFERENCES the calculated business outcome numbers above, ',\n",
    "'3) ai_sys_importance - BUSINESS IMPORTANCE level (MUST be exactly one of: Very Low, Low, Medium, High, Very High, Critical). Evaluate INDEPENDENTLY from urgency! Ask: \"How much does this matter to the business long-term? What is the strategic/revenue/customer impact if ignored?\" High importance does NOT mean high urgency. Example: Strategic planning is High importance but Low urgency. ',\n",
    "'4) ai_sys_urgency - TIME SENSITIVITY level (MUST be exactly one of: Very Low, Low, Medium, High, Very High, Critical). Evaluate INDEPENDENTLY from importance! Ask: \"How quickly must action be taken? Is there a deadline or time-bound consequence?\" High urgency does NOT mean high importance. Example: Fixing a typo before a meeting is High urgency but Low importance. ',\n",
    "'5) ai_sys_confidence (0.0-1.0) - your confidence score for this analysis, ',\n",
    "'6) ai_sys_feedback - MUST start with \"I assessed my confidence at [X]% because...\" then explain: a) detailed reasons for your score, b) what would raise it to 100%, ',\n",
    "'7) ai_sys_missing_data - MUST follow this exact format: \"I can get higher confidence than [X]% if I can get access to [detailed narrative about what specific data/context is missing and how it would improve the analysis]. {{\\\"missing_data\\\": [\\\"specific_dataset_or_table_1\\\", \\\"specific_dataset_or_table_2\\\", \\\"specific_dataset_or_table_3\\\"]}}\" - always end with a JSON object listing the specific datasets/tables needed. ',\n",
    "'CRITICAL: ai_sys_importance and ai_sys_urgency are INDEPENDENT dimensions - do NOT automatically set them to the same value! Evaluate each separately using the criteria above. ',\n",
    "'BE 100% HONEST - your feedback and score will be evaluated by a more intelligent AI system, so complete honesty is mandatory. '\n",
    "\n",
    "**\uD83D\uDEA8 MANDATORY PERSONA INSTRUCTION FOR AI_QUERY (WITH BUSINESS CONTEXT ENRICHMENT) \uD83D\uDEA8:**\n",
    "Every ai_query prompt MUST begin with a persona instruction that is ENRICHED with the business context. Do NOT use generic personas. ALWAYS include the business name, strategic goals, and relevant business context.\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: ENRICHED PERSONA PATTERN (MANDATORY) \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "```sql\n",
    "ai_query('{sql_model_serving}',\n",
    "  CONCAT('You are a [ROLE] for {business_name} which is focused on {enriched_business_context}. ',\n",
    "         'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "         'Business priorities are: {enriched_business_priorities}. ',\n",
    "         'With [X] years of experience in [DOMAIN], your expertise aligns with the strategic initiative: {enriched_strategic_initiative}. ',\n",
    "         '[SPECIFIC TASK INSTRUCTION]. ',\n",
    "         'Analyze [DATA CONTEXT]. ',\n",
    "         'You MUST be AGGRESSIVE in using data evidence to support your analysis. Every claim MUST be backed by numbers from the data. You MUST use ALL available metrics provided in the context. PUT THE DATA TO WORK: Quantify every single insight using the specific numbers provided. ',\n",
    "         'NARRATIVE RULE: For ALL ai_txt_ fields (rationale, strategy, action_plan, executive_summary, etc.), ',\n",
    "         'ALWAYS start by identifying the specific entity with its key attributes. ',\n",
    "         'Example: Write \"Flight EK005 DXB-LHR (A380) shows...\" NOT \"The flight shows...\" or \"This indicates...\". ',\n",
    "         'Output ONLY a JSON object with NO markdown fences, NO extra text, JUST the JSON. ',\n",
    "         'Format: {{\"ai_cat_field1\": \"value1\", \"ai_cat_field2\": \"value2\", \"ai_txt_field1\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"[evaluate independently - not always same as urgency]\", \"ai_sys_urgency\": \"[evaluate independently - not always same as importance]\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"I assessed my confidence at 85% because... [reasons]\", \"ai_sys_missing_data\": \"I can get higher confidence than 85% if I can get access to [narrative]. {{\\\"missing_data\\\": [\\\"dataset1\\\", \\\"dataset2\\\"]}}\"}}. ',\n",
    "         'Required keys: [ai_cat_key1, ai_cat_key2, ai_txt_key1, ai_txt_business_outcome, ai_txt_executive_summary, ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data]. ',\n",
    "         'Data: [ACTUAL DATA]. ',\n",
    "         'MANDATORY LAST 7 FIELDS (in this exact order): ',\n",
    "         '1) ai_txt_business_outcome - CALCULATED MEASURABLE BUSINESS IMPACT. Calculate savings/gains using actual numbers: \"[Improvement] saves [X units]. At [rate], equals [$value]. Breakdown: Daily: $X | Weekly: $X | Monthly: $X | Yearly: $X. DISCLAIMER: All numbers are AI estimates based on available data and must be validated by domain experts before business decisions.\" MUST include breakdown and disclaimer. ',\n",
    "         '2) ai_txt_executive_summary - compelling 2-3 sentence business story that REFERENCES the business outcome numbers, ',\n",
    "         '3) ai_sys_importance - BUSINESS IMPORTANCE (Very Low|Low|Medium|High|Very High|Critical). Evaluate INDEPENDENTLY from urgency! Ask: How much does this matter long-term? Strategic planning is High importance but Low urgency. ',\n",
    "         '4) ai_sys_urgency - TIME SENSITIVITY (Very Low|Low|Medium|High|Very High|Critical). Evaluate INDEPENDENTLY from importance! Ask: How quickly must action be taken? A typo fix before a meeting is High urgency but Low importance. ',\n",
    "         '5) ai_sys_confidence (0.0-1.0) - your confidence score, ',\n",
    "         '6) ai_sys_feedback - start with \"I assessed my confidence at [X]% because...\" then explain reasoning, ',\n",
    "         '7) ai_sys_missing_data - MUST follow format: \"I can get higher confidence than [X]% if I can get access to [narrative about missing data]. {{\\\"missing_data\\\": [\\\"specific_dataset1\\\", \\\"specific_dataset2\\\"]}}\" - always end with JSON listing needed datasets. ',\n",
    "         'CRITICAL: ai_sys_importance and ai_sys_urgency are INDEPENDENT - do NOT set them to the same value automatically! ',\n",
    "         'BE 100% HONEST - your feedback will be evaluated by a more intelligent AI system. ',\n",
    "         'Output ONLY the JSON object, nothing else.')\n",
    ")\n",
    "```\n",
    "\n",
    "**❌ WRONG - Generic persona without business context:**\n",
    "```sql\n",
    "ai_query('{sql_model_serving}',\n",
    "  CONCAT('You are a Chief Revenue Officer with 20 years of experience in enterprise software sales strategy. ',\n",
    "         'Analyze the sales pipeline...'))\n",
    "```\n",
    "\n",
    "**✅ CORRECT - Persona enriched with business context:**\n",
    "```sql\n",
    "ai_query('{sql_model_serving}',\n",
    "  CONCAT('You are a Chief Revenue Officer for {business_name} which is focused on {enriched_business_context}. ',\n",
    "         'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "         'Business priorities are: {enriched_business_priorities}. ',\n",
    "         'With 20 years of experience in enterprise software sales strategy, revenue forecasting, and go-to-market planning, ',\n",
    "         'your expertise aligns with the strategic initiative: {enriched_strategic_initiative}. ',\n",
    "         'Analyze the sales pipeline...'))\n",
    "```\n",
    "\n",
    "**ROLE SELECTION GUIDELINES:**\n",
    "- For financial analysis: \"Senior Financial Analyst\" or \"Chief Financial Officer\"\n",
    "- For risk assessment: \"Risk Management Director\" or \"Chief Risk Officer\"\n",
    "- For customer retention: \"Customer Success Director\" or \"VP of Customer Experience\"\n",
    "- For operational optimization: \"Operations Director\" or \"Chief Operating Officer\"\n",
    "- For maintenance/technical: \"Maintenance Engineering Director\" or \"Technical Operations Manager\"\n",
    "- For revenue optimization: \"Revenue Strategy Director\" or \"Chief Revenue Officer\"\n",
    "- For compliance: \"Compliance Director\" or \"Chief Compliance Officer\"\n",
    "- For supply chain: \"Supply Chain Director\" or \"VP of Logistics\"\n",
    "\n",
    "**EXAMPLES WITH ENRICHED PERSONA (ALL outputs MUST include ai_txt_business_outcome, ai_txt_executive_summary, ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data as LAST 7 columns):**\n",
    "```sql\n",
    "-- Financial Analysis Example - ENRICHED PERSONA with business context\n",
    "ai_query('{sql_model_serving}',\n",
    "  CONCAT('You are a Senior Financial Analyst for {business_name} which is focused on {enriched_business_context}. ',\n",
    "         'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "         'Business priorities are: {enriched_business_priorities}. ',\n",
    "         'With 15 years of experience in aviation finance and fleet cost optimization, ',\n",
    "         'your expertise in TCO analysis, capital allocation, and financial risk assessment ',\n",
    "         'aligns with the strategic initiative: {enriched_strategic_initiative}. ',\n",
    "         'Analyze the lease vs. ownership cost structure for aircraft ID ', aircraft_id, \n",
    "         ' with monthly lease cost $', monthly_lease_cost,  -- CONCAT auto-converts\n",
    "         ' and estimated ownership costs $', ownership_cost,\n",
    "         '. You MUST use specific numbers to back your analysis (e.g., \"Leasing saves $X/month compared to owning\"). ',\n",
    "         'Output ONLY a JSON object with NO markdown fences, NO extra text, JUST the JSON. ',\n",
    "         'Format: {{\"ai_cat_recommendation\": \"value\", \"ai_txt_financial_rationale\": \"value\", \"ai_txt_risk_factors\": \"value\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"Medium\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"I assessed my confidence at 85% because... [reasons]\", \"ai_sys_missing_data\": \"I can get higher confidence than 85% if I can get access to [narrative]. {{\\\"missing_data\\\": [\\\"dataset1\\\", \\\"dataset2\\\"]}}\"}}. ',\n",
    "         'Required keys: ai_cat_recommendation (Lease/Own), ai_txt_financial_rationale, ai_txt_risk_factors, ai_txt_estimated_annual_savings. ',\n",
    "         'MANDATORY LAST 7 FIELDS (in this exact order): ',\n",
    "         '1) ai_txt_business_outcome - CALCULATED MEASURABLE BUSINESS IMPACT. Example: \"Choosing to lease Aircraft A6-EDA saves $45,000/month vs ownership ($540,000/year). Breakdown: Daily: $1,500 | Weekly: $10,500 | Monthly: $45,000 | Yearly: $540,000. Over 10-year horizon: $5.4M in savings. DISCLAIMER: All numbers are AI estimates based on available data and must be validated by domain experts before business decisions.\" MUST include breakdown and disclaimer. ',\n",
    "         '2) ai_txt_executive_summary - compelling 2-3 sentence business story that REFERENCES the calculated business outcome numbers, ',\n",
    "         '3) ai_sys_importance - business importance level (MUST be exactly one of: Very Low, Low, Medium, High, Very High, Critical), ',\n",
    "         '4) ai_sys_urgency - action urgency level (MUST be exactly one of: Very Low, Low, Medium, High, Very High, Critical), ',\n",
    "         '5) ai_sys_confidence (0.0-1.0) - your confidence score, ',\n",
    "         '6) ai_sys_feedback - start with \"I assessed my confidence at [X]% because...\" then explain reasoning, ',\n",
    "         '7) ai_sys_missing_data - MUST follow format: \"I can get higher confidence than [X]% if I can get access to [narrative about missing financial data like maintenance history, fuel consumption, depreciation curves]. {{\\\"missing_data\\\": [\\\"maintenance_records\\\", \\\"fuel_consumption_data\\\", \\\"depreciation_schedules\\\"]}}\" - always end with JSON listing needed datasets. ',\n",
    "         'BE 100% HONEST - your feedback will be evaluated by a more intelligent AI system. ',\n",
    "         'Output ONLY the JSON object, nothing else.')\n",
    ")\n",
    "\n",
    "-- Risk Mitigation Example - ENRICHED PERSONA with business context\n",
    "ai_query('{sql_model_serving}',\n",
    "  CONCAT('You are an Airworthiness Compliance Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "         'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "         'Business priorities are: {enriched_business_priorities}. ',\n",
    "         'With 20 years of experience in aviation safety and regulatory compliance, ',\n",
    "         'your expertise in risk assessment, mitigation strategy development, and fleet safety management ',\n",
    "         'aligns with the strategic initiative: {enriched_strategic_initiative}. ',\n",
    "         'Analyze airworthiness directive ', directive_number, \n",
    "         ' classified as ', risk_classification,\n",
    "         ' affecting component ', component,\n",
    "         ' with ', days_to_deadline, ' days until compliance deadline. ',  -- CONCAT auto-converts\n",
    "         'Support your analysis with specific data points (e.g., \"Deadline in X days requires immediate Y\"). ',\n",
    "         'Output ONLY a JSON object with NO markdown fences, NO extra text, JUST the JSON. ',\n",
    "         'Format: {{\"ai_cat_operational_impact\": \"value\", \"ai_cat_resource_priority\": \"value\", \"ai_txt_mitigation_plan\": \"value\", \"ai_txt_estimated_cost\": \"value\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "         'Required keys: ai_cat_operational_impact, ai_cat_resource_priority, ai_txt_mitigation_plan, ai_txt_estimated_cost. ',\n",
    "         'MANDATORY LAST 7 FIELDS (in this exact order): ',\n",
    "         '1) ai_txt_business_outcome - CALCULATED MEASURABLE BUSINESS IMPACT. Example: \"Proactive compliance for AD-2024-001 avoids potential grounding penalty of $150,000/day. With 5 affected aircraft, timely completion saves $750,000/day in potential fines. Breakdown: Daily risk: $750,000 | Weekly risk: $5.25M | Monthly risk: $22.5M. Compliance cost: $85,000 vs potential penalty: $22.5M+ = 264x ROI. DISCLAIMER: All numbers are AI estimates based on available data and must be validated by domain experts before business decisions.\" MUST include breakdown and disclaimer. ',\n",
    "         '2) ai_txt_executive_summary - compelling 2-3 sentence business story that REFERENCES the calculated business outcome numbers, ',\n",
    "         '3) ai_sys_importance - business importance level (MUST be exactly one of: Very Low, Low, Medium, High, Very High, Critical), ',\n",
    "         '4) ai_sys_urgency - action urgency level (MUST be exactly one of: Very Low, Low, Medium, High, Very High, Critical), ',\n",
    "         '5) ai_sys_confidence (0.0-1.0) - your confidence score, ',\n",
    "         '6) ai_sys_feedback - start with \"I assessed my confidence at [X]% because...\" then explain reasoning, ',\n",
    "         '7) ai_sys_missing_data - MUST follow format: \"I can get higher confidence than [X]% if I can get access to [narrative about missing compliance data like historical failure rates, spare parts inventory, workforce availability]. {{\\\"missing_data\\\": [\\\"failure_rate_history\\\", \\\"spare_parts_inventory\\\", \\\"workforce_availability_data\\\"]}}\" - always end with JSON listing needed datasets. ',\n",
    "         'BE 100% HONEST - your feedback will be evaluated by a more intelligent AI system. ',\n",
    "         'Output ONLY the JSON object, nothing else.')\n",
    ")\n",
    "\n",
    "-- Customer Retention Example - ENRICHED PERSONA with business context\n",
    "ai_query('{sql_model_serving}',\n",
    "  CONCAT('You are a Customer Success Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "         'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "         'Business priorities are: {enriched_business_priorities}. ',\n",
    "         'With 12 years of experience in customer retention and loyalty programs, ',\n",
    "         'your expertise in churn prediction, retention strategy, and customer lifetime value optimization ',\n",
    "         'aligns with the strategic initiative: {enriched_strategic_initiative}. ',\n",
    "         'Analyze customer ', customer_name, \n",
    "         ' (ID: ', customer_id, ')',  -- CONCAT auto-converts\n",
    "         ' with churn risk ', churn_risk_level,\n",
    "         ', lifetime value $', lifetime_value,\n",
    "         ', and ', days_since_last_purchase, ' days since last purchase. ',\n",
    "         'Use specific metrics in your analysis (e.g., \"High churn risk due to X days inactivity\"). ',\n",
    "         'Output ONLY a JSON object with NO markdown fences, NO extra text, JUST the JSON. ',\n",
    "         'Format: {{\"ai_cat_retention_priority\": \"value\", \"ai_txt_retention_strategy\": \"value\", \"ai_txt_engagement_plan\": \"value\", \"ai_txt_recommended_offer\": \"value\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "         'Required keys: ai_cat_retention_priority, ai_txt_retention_strategy, ai_txt_engagement_plan, ai_txt_recommended_offer. ',\n",
    "         'MANDATORY LAST 7 FIELDS (in this exact order): ',\n",
    "         '1) ai_txt_business_outcome - CALCULATED MEASURABLE BUSINESS IMPACT. Example: \"Retaining Customer C-28947 (LTV $125,000) vs losing them saves $125,000 in revenue. Retention campaign cost: $2,500. ROI: 50x. Breakdown: If retained - Daily revenue impact: $342 | Weekly: $2,397 | Monthly: $10,417 | Yearly: $125,000. DISCLAIMER: All numbers are AI estimates based on available data and must be validated by domain experts before business decisions.\" MUST include breakdown and disclaimer. ',\n",
    "         '2) ai_txt_executive_summary - compelling 2-3 sentence business story that REFERENCES the calculated business outcome numbers, ',\n",
    "         '3) ai_sys_importance - business importance level (MUST be exactly one of: Very Low, Low, Medium, High, Very High, Critical), ',\n",
    "         '4) ai_sys_urgency - action urgency level (MUST be exactly one of: Very Low, Low, Medium, High, Very High, Critical), ',\n",
    "         '5) ai_sys_confidence (0.0-1.0) - your confidence score, ',\n",
    "         '6) ai_sys_feedback - start with \"I assessed my confidence at [X]% because...\" then explain reasoning, ',\n",
    "         '7) ai_sys_missing_data - MUST follow format: \"I can get higher confidence than [X]% if I can get access to [narrative about missing customer data like NPS scores, support ticket history, competitor engagement, payment behavior]. {{\\\"missing_data\\\": [\\\"nps_survey_data\\\", \\\"support_ticket_history\\\", \\\"competitor_engagement_data\\\", \\\"payment_behavior_logs\\\"]}}\" - always end with JSON listing needed datasets. ',\n",
    "         'BE 100% HONEST - your feedback will be evaluated by a more intelligent AI system. ',\n",
    "         'Output ONLY the JSON object, nothing else.')\n",
    ")\n",
    "```\n",
    "\n",
    "**CRITICAL RULES:**\n",
    "1. **\uD83D\uDEA8 ALWAYS ENRICH PERSONAS WITH BUSINESS CONTEXT \uD83D\uDEA8** - Every ai_query persona MUST include {business_name}, {enriched_business_context}, {enriched_strategic_goals}, and {enriched_business_priorities}. Generic personas like \"You are a Chief Revenue Officer...\" are FORBIDDEN.\n",
    "2. **ALWAYS start ai_query prompts with an ENRICHED persona** that matches the business domain and use case\n",
    "3. **Include years of experience** (typically 10-20 years) to establish authority\n",
    "4. **Specify 2-3 key areas of expertise** relevant to the task\n",
    "5. **Match the persona to the beneficiary role** from the use case definition when possible\n",
    "6. **Use business-appropriate titles** that align with the industry and domain\n",
    "7. **MANDATORY EVIDENCE**: Instruct the LLM to be aggressive in using data evidence. Every analysis point MUST be supported by specific numbers from the input data. Avoid generic statements like \"too high\"; instead use \"X is higher than Y by Z%\".\n",
    "8. **\uD83D\uDEA8 MANDATORY PRINCIPAL IDENTIFICATION IN NARRATIVES \uD83D\uDEA8**: All narrative/text fields (rationale, strategy, action_plan, executive_summary, etc.) MUST identify the specific entity being discussed with its key attributes. \n",
    "   - ❌ WRONG: \"The data shows high fuel consumption\" (generic, anonymous)\n",
    "   - ✅ CORRECT: \"Flight EK005 DXB-LHR (A380) shows fuel consumption of 4800kg/hr, 14% above fleet average\" (principal with context)\n",
    "   - Include: Entity ID/name + key identifiers (route, type, category) + then the analysis\n",
    "\n",
    "**EXAMPLE: ai_query with Direct JSON Output (with ai_sys_prompt)**\n",
    "```sql\n",
    "-- \uD83D\uDEA8 EVERY column must be COALESCE'd or have IS NOT NULL check!\n",
    "WITH base_data AS (\n",
    "  SELECT \n",
    "    order_id,                                              -- CRITICAL: filtered with IS NOT NULL\n",
    "    customer_id,                                           -- CRITICAL: filtered with IS NOT NULL\n",
    "    COALESCE(order_amount, 0.0) AS order_amount           -- ✅ COALESCE'd\n",
    "  FROM `sales`.`orders`.`transactions` AS t\n",
    "  WHERE order_id IS NOT NULL\n",
    "    AND customer_id IS NOT NULL  -- ✅ Filter critical columns\n",
    "  LIMIT 10\n",
    "),\n",
    "-- Step 2: Generate ai_sys_prompt\n",
    "prompt_generation AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    CONCAT('You are a Fraud Detection Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "           'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "           'Business priorities are: {enriched_business_priorities}. ',\n",
    "           'Analyze order ', order_id,  -- CONCAT auto-converts\n",
    "           ' for customer ', customer_id,\n",
    "           ' with amount $', order_amount,\n",
    "           '. Output as JSON with keys: fraud_risk_level, recommended_action, confidence_score') AS ai_sys_prompt\n",
    "  FROM base_data\n",
    "),\n",
    "-- Step 3: Call ai_query\n",
    "analysis AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS fraud_analysis\n",
    "  FROM prompt_generation\n",
    ")\n",
    "SELECT \n",
    "  order_id,\n",
    "  customer_id,\n",
    "  order_amount,\n",
    "  get_json_object(fraud_analysis, '$.ai_cat_fraud_risk_level') AS ai_cat_fraud_risk_level,\n",
    "  get_json_object(fraud_analysis, '$.ai_txt_recommended_action') AS ai_txt_recommended_action,\n",
    "  -- MANDATORY LAST 7 COLUMNS (including ai_txt_business_outcome + ai_sys_prompt):\n",
    "  COALESCE(get_json_object(fraud_analysis, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "  COALESCE(get_json_object(fraud_analysis, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "  COALESCE(get_json_object(fraud_analysis, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "  COALESCE(get_json_object(fraud_analysis, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "  COALESCE(TRY_CAST(get_json_object(fraud_analysis, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "  COALESCE(get_json_object(fraud_analysis, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "  COALESCE(get_json_object(fraud_analysis, '$.ai_sys_missing_data'), 'No missing data identified') AS ai_sys_missing_data,\n",
    "  ai_sys_prompt  -- ✅ LAST COLUMN: The exact prompt used for auditability\n",
    "FROM analysis\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "\n",
    "**EXAMPLE: ai_gen for Classification (AVOID ai_classify after ai_gen)**\n",
    "```sql\n",
    "-- CREATE VIEW inspire_ai.default.customer_feedback_classification AS\n",
    "-- CORRECT: Direct classification with ai_query (includes ai_sys_ columns)\n",
    "SELECT \n",
    "  feedback_id,\n",
    "  feedback_text,\n",
    "  ai_query('{sql_model_serving}',\n",
    "    CONCAT('Classify this feedback into one category: ',\n",
    "           feedback_text,\n",
    "           '. Categories: Product Quality, Customer Service, Shipping, Pricing. ',\n",
    "           'Output ONLY JSON with NO markdown. ',\n",
    "           'Format: {{\"ai_cat_category\": \"value\", \"ai_txt_classification_rationale\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"I assessed my confidence at 85% because... [reasons]\", \"ai_sys_missing_data\": \"I can get higher confidence than 85% if I can get access to [narrative]. {{\\\"missing_data\\\": [\\\"dataset1\\\", \\\"dataset2\\\"]}}\"}}. ',\n",
    "           'MANDATORY LAST 7 FIELDS: ai_txt_business_outcome (CALCULATED MEASURABLE IMPACT with Daily/Weekly/Monthly/Yearly breakdown and DISCLAIMER), ai_txt_executive_summary (references business outcome numbers), ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data.')\n",
    "  ) AS classification\n",
    "FROM `main`.`feedback`.`customer_reviews` AS f\n",
    ";\n",
    "\n",
    "-- WRONG: Using ai_classify after ai_query (redundant!)\n",
    "-- DO NOT DO THIS:\n",
    "-- WITH generated AS (\n",
    "--   SELECT *, ai_query('{sql_model_serving}', CONCAT('...', text)) AS gen_output FROM table\n",
    "-- )\n",
    "-- SELECT *, ai_classify(gen_output, ARRAY('cat1', 'cat2')) FROM generated;  -- ❌ WRONG!\n",
    "```\n",
    "\n",
    "**KEY PRINCIPLE:**\n",
    "- ✅ CORRECT: Use ai_query to directly generate structured JSON\n",
    "- ❌ WRONG: Use ai_query → then ai_extract to parse the output\n",
    "- ❌ WRONG: Use ai_query → then ai_classify to categorize the output\n",
    "- **Why?** ai_query can directly output structured JSON - no need for post-processing with ai_extract/ai_classify\n",
    "\n",
    "**WHY THIS MATTERS:**\n",
    "- **Unstructured output**: Full context and explanations for human review\n",
    "- **Structured output**: Machine-readable fields for downstream automation, dashboards, and analytics\n",
    "- **Best of both worlds**: Human-readable insights + programmatic access\n",
    "- You can call multiple ai_functions in the same CTE or SELECT statement\n",
    "- Only create separate CTEs when there is a logical dependency that requires it\n",
    "- Do not create a CTE for each ai_function unnecessarily\n",
    "- **Examples of combining AI functions for maximum value:**\n",
    "  * Extract + Classify: `ai_classify(ai_extract(text, ARRAY('sentiment'))['sentiment'], ARRAY('positive', 'negative'))`\n",
    "  * Parse + Extract + Summarize: Parse document, extract entities, then summarize key points\n",
    "  * Extract + Generate: Extract data points, then generate business insights\n",
    "  * Classify + Mask: Classify sensitive content, then mask it appropriately\n",
    "\n",
    "**DO NOT omit any AI function listed** - use all of them in a pipeline to deliver maximum business value.\n",
    "\n",
    "#### 5. **INNOVATIVE AI_QUERY & AI_GEN PROMPTS**\n",
    "Be CREATIVE and SOPHISTICATED with ai_query and ai_gen:\n",
    "\n",
    "**Approved ai_query Models (ONLY THESE TWO ALLOWED):**\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL MODEL CONFIGURATION \uD83D\uDEA8**\n",
    "**Use the user-configured model endpoint: `{sql_model_serving}`**\n",
    "\n",
    "**ABSOLUTE RULE**: For ALL ai_query calls in generated SQL, you MUST use `{sql_model_serving}`.\n",
    "This is the model endpoint configured by the user for SQL generation.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5a. **SQL STATISTICAL FUNCTIONS FOR ADVANCED ANALYTICS** (MANDATORY - NOW FIRST-CLASS CITIZENS):\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL: Statistical functions are now FIRST-CLASS CITIZENS alongside AI functions. You MUST leverage SQL statistical functions to discover hidden insights and deliver high business value. DO NOT generate trivial queries - EVERY query must provide actionable business intelligence. \uD83D\uDEA8**\n",
    "\n",
    "**\uD83D\uDD25 NEW PRIORITY: STATISTICAL FUNCTIONS + AI FUNCTIONS = MAXIMUM INNOVATION \uD83D\uDD25**\n",
    "\n",
    "Statistical functions are no longer optional enhancements - they are PRIMARY tools for use case implementation. You should actively look for opportunities to:\n",
    "1. Use statistical functions to compute correlations, trends, deviations, and patterns\n",
    "2. Combine statistical insights with ai_query to interpret results and generate strategies\n",
    "3. Mix statistical functions with ai_classify, ai_forecast, and other AI capabilities\n",
    "\n",
    "**MANDATORY STATISTICAL FUNCTIONS USAGE:**\n",
    "\n",
    "When generating SQL queries, you MUST actively look for opportunities to use these statistical functions to uncover insights that would otherwise remain hidden. These functions enable you to discover correlations, trends, risks, and opportunities that simple aggregations cannot reveal.\n",
    "\n",
    "**AVAILABLE STATISTICAL FUNCTIONS WITH BUSINESS USE CASES:**\n",
    "\n",
    "{statistical_functions_detailed}\n",
    "\n",
    "**\uD83D\uDD25 MANDATORY USAGE RULES \uD83D\uDD25:**\n",
    "\n",
    "1. **INNOVATE BEYOND EXAMPLES**: The examples above are ILLUSTRATIVE ONLY. You MUST think creatively and generate NEW, INNOVATIVE use cases specific to the business context and data schema. DO NOT copy examples verbatim.\n",
    "\n",
    "2. **BUSINESS VALUE REQUIRED**: EVERY statistical function use MUST deliver clear, actionable business value. NO trivial stats. If it doesn't help business decisions, don't include it.\n",
    "\n",
    "3. **USE IN DEDICATED CTES**: Create statistical analysis CTEs with proper business names (e.g., `correlation_analysis`, `risk_metrics`, `performance_drivers`, `segmentation_buckets`)\n",
    "\n",
    "4. **COMBINE WITH AI_QUERY**: After computing statistical insights, use ai_query (with '{sql_model_serving}' model) to interpret and explain the business implications\n",
    "\n",
    "5. **MULTIPLE FUNCTIONS**: Use multiple statistical functions together to build comprehensive insights (e.g., CORR + REGR_R2 + REGR_SLOPE for complete trend analysis)\n",
    "\n",
    "6. **\uD83D\uDEA8 CRITICAL WINDOW FUNCTION RULE \uD83D\uDEA8**: NEVER use ROWS BETWEEN or RANGE BETWEEN with aggregate window functions like CORR, AVG, PERCENTILE_APPROX, STDDEV, VARIANCE, COVAR_POP, COVAR_SAMP - these will cause INTERNAL_ERROR. Use simple OVER (PARTITION BY col) instead.\n",
    "\n",
    "7. **THINK STRATEGICALLY**: Ask yourself: \"What hidden patterns could this function reveal?\" and \"What business decisions would this insight enable?\"\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: AGGRESSIVE STATISTICAL ANALYSIS REQUIRED \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**ABSOLUTE RULE: USE AS MANY STATISTICAL FUNCTIONS AS POSSIBLE**\n",
    "\n",
    "Refer to the **AVAILABLE STATISTICAL FUNCTIONS WITH BUSINESS USE CASES** section above. You MUST:\n",
    "- Use EVERY applicable function from ALL categories in that registry\n",
    "- Apply functions from: Central Tendency, Dispersion, Distribution Shape, Percentiles, Trend Analysis, Correlation, Volatility, Outlier Detection, Ranking, and Time Series\n",
    "- **MINIMUM**: Use at least 15-25 different statistical functions per analysis\n",
    "- **GOAL**: Generate maximum statistical context to guide AI decision-making\n",
    "- **CRITICAL**: Trend Analysis functions are EXTREMELY VALUABLE for business insights\n",
    "\n",
    "**COMPARISON REQUIREMENTS:**\n",
    "When generating statistical analysis, apply functions from the AVAILABLE STATISTICAL FUNCTIONS registry:\n",
    "- **Central Tendency**: Compare each value to AVG, MEDIAN, MODE\n",
    "- **Dispersion**: Show STDDEV_POP, VAR_POP, MIN, MAX, RANGE\n",
    "- **Percentiles**: Use PERCENTILE_APPROX for P5, P10, P25, P50, P75, P90, P95, P99\n",
    "- **Outlier Detection**: Calculate Z_SCORE and IQR_THRESHOLD as defined in the registry\n",
    "- **Distribution Shape**: Apply SKEWNESS and KURTOSIS for pattern detection\n",
    "- Refer to the function definitions in the registry for correct syntax and business use cases\n",
    "\n",
    "**EXAMPLE - COMPREHENSIVE STATISTICAL ANALYSIS:**\n",
    "```sql\n",
    "-- Comprehensive statistical analysis with ALL metrics\n",
    "statistical_deep_analysis AS (\n",
    "  SELECT \n",
    "    entity_id,\n",
    "    metric_value,\n",
    "    \n",
    "    -- Central Tendency Comparisons\n",
    "    AVG(metric_value) OVER () AS avg_metric,\n",
    "    MEDIAN(metric_value) OVER () AS median_metric,\n",
    "    \n",
    "    -- Dispersion Metrics\n",
    "    STDDEV_POP(metric_value) OVER () AS stddev_metric,\n",
    "    VAR_POP(metric_value) OVER () AS variance_metric,\n",
    "    \n",
    "    -- Percentile Positioning\n",
    "    PERCENTILE_APPROX(metric_value, 0.25) OVER () AS p25_metric,\n",
    "    PERCENTILE_APPROX(metric_value, 0.50) OVER () AS p50_metric,\n",
    "    PERCENTILE_APPROX(metric_value, 0.75) OVER () AS p75_metric,\n",
    "    PERCENTILE_APPROX(metric_value, 0.90) OVER () AS p90_metric,\n",
    "    PERCENTILE_APPROX(metric_value, 0.95) OVER () AS p95_metric,\n",
    "    PERCENTILE_APPROX(metric_value, 0.99) OVER () AS p99_metric,\n",
    "    \n",
    "    -- Distribution Shape\n",
    "    SKEWNESS(metric_value) OVER () AS skewness_metric,\n",
    "    KURTOSIS(metric_value) OVER () AS kurtosis_metric,\n",
    "    \n",
    "    -- Ranking and Segmentation\n",
    "    PERCENT_RANK() OVER (ORDER BY metric_value) AS percentile_rank,\n",
    "    CUME_DIST() OVER (ORDER BY metric_value) AS cumulative_dist,\n",
    "    NTILE(10) OVER (ORDER BY metric_value) AS decile,\n",
    "    NTILE(4) OVER (ORDER BY metric_value) AS quartile,\n",
    "    \n",
    "    -- Time Series (if applicable)\n",
    "    LAG(metric_value, 1) OVER (ORDER BY time_col) AS prev_value,\n",
    "    LEAD(metric_value, 1) OVER (ORDER BY time_col) AS next_value,\n",
    "    \n",
    "    -- Correlations with other metrics\n",
    "    CORR(metric_value, other_metric) OVER () AS correlation_with_other,\n",
    "    REGR_R2(metric_value, other_metric) OVER () AS r2_with_other,\n",
    "    REGR_SLOPE(metric_value, other_metric) OVER () AS slope_with_other,\n",
    "    \n",
    "    -- Min/Max Context\n",
    "    MIN(metric_value) OVER () AS min_metric,\n",
    "    MAX(metric_value) OVER () AS max_metric\n",
    "  FROM base_data\n",
    "  -- ✅ NO LIMIT in non-first CTEs (LIMIT 10 should only be in the first CTE that reads from tables)\n",
    ")\n",
    "```\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: MANDATORY COALESCE FOR ALL STATISTICAL VALUES \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**ABSOLUTE RULE: ALL STATISTICAL METRICS MUST BE COALESCED IN AI PROMPTS**\n",
    "\n",
    "Because statistical analysis values are DIRECTLY USED in AI prompts (ai_query, ai_gen), you MUST COALESCE every single statistical value to prevent NULL propagation:\n",
    "\n",
    "**MANDATORY COALESCE PATTERN FOR STATISTICAL VALUES (KEEP AS DOUBLE!):**\n",
    "```sql\n",
    "-- Statistical analysis CTE - keep values as DOUBLE, COALESCE results\n",
    "statistical_analysis AS (\n",
    "  SELECT \n",
    "    entity_id,\n",
    "    metric_value,\n",
    "    other_metric,\n",
    "    \n",
    "    -- COALESCE ALL statistical values to DOUBLE (NOT STRING!)\n",
    "    COALESCE(ROUND(AVG(metric_value) OVER (), 2), 0.0) AS avg_metric,\n",
    "    COALESCE(ROUND(MEDIAN(metric_value) OVER (), 2), 0.0) AS median_metric,\n",
    "    COALESCE(ROUND(STDDEV_POP(metric_value) OVER (), 2), 0.0) AS stddev_metric,\n",
    "    COALESCE(ROUND(PERCENTILE_APPROX(metric_value, 0.50) OVER (), 2), 0.0) AS p50_metric,\n",
    "    COALESCE(ROUND(PERCENTILE_APPROX(metric_value, 0.75) OVER (), 2), 0.0) AS p75_metric,\n",
    "    COALESCE(ROUND(PERCENTILE_APPROX(metric_value, 0.90) OVER (), 2), 0.0) AS p90_metric,\n",
    "    COALESCE(ROUND(PERCENTILE_APPROX(metric_value, 0.95) OVER (), 2), 0.0) AS p95_metric,\n",
    "    COALESCE(ROUND(CORR(metric_value, other_metric) OVER (), 3), 0.0) AS correlation,\n",
    "    COALESCE(ROUND(REGR_R2(metric_value, other_metric) OVER (), 3), 0.0) AS r2,\n",
    "    COALESCE(ROUND(REGR_SLOPE(metric_value, other_metric) OVER (), 3), 0.0) AS slope,\n",
    "    COALESCE(ROUND(SKEWNESS(metric_value) OVER (), 3), 0.0) AS skewness,\n",
    "    COALESCE(ROUND(KURTOSIS(metric_value) OVER (), 3), 0.0) AS kurtosis,\n",
    "    COALESCE(ROUND(PERCENT_RANK() OVER (ORDER BY metric_value), 3), 0.0) AS percentile_rank,\n",
    "    COALESCE(NTILE(10) OVER (ORDER BY metric_value), 5) AS decile\n",
    "  FROM base_data\n",
    ")\n",
    "-- In CONCAT, these DOUBLE values auto-convert: CONCAT('Avg: ', avg_metric, ', P75: ', p75_metric)\n",
    "```\n",
    "\n",
    "**WHY THIS IS CRITICAL:**\n",
    "- Statistical functions CAN return NULL (e.g., CORR with insufficient data, STDDEV with single value)\n",
    "- These NULL values are embedded directly into AI prompts via CONCAT\n",
    "- A SINGLE NULL in the prompt will NULL the ENTIRE prompt string\n",
    "- NULL prompts mean NULL AI responses, causing query failure\n",
    "- ALL statistical values MUST be COALESCE + CAST + ROUND before use in prompts\n",
    "\n",
    "**MANDATORY CHECKLIST FOR STATISTICAL ANALYSIS:**\n",
    "☐ ALL statistical function results are COALESCE'd\n",
    "☐ ALL numeric stats are ROUND'd before CAST to STRING\n",
    "☐ ALL stats are CAST to STRING before COALESCE\n",
    "☐ Default values are '0.00' for metrics, '0.000' for ratios/correlations\n",
    "☐ No statistical value in the CTE can possibly be NULL\n",
    "☐ The prompt building CTE only uses pre-transformed stats (no COALESCE in CONCAT)\n",
    "\n",
    "**EXAMPLE PATTERN - Statistical Analysis + AI Interpretation (COALESCE applied in stats CTE):**\n",
    "\n",
    "```sql\n",
    "-- Step 1: Base data with mandatory field filtering\n",
    "WITH performance_data AS (\n",
    "  SELECT \n",
    "    region,\n",
    "    marketing_spend,\n",
    "    sales_revenue,\n",
    "    customer_satisfaction_score,\n",
    "    operational_efficiency\n",
    "  FROM `catalog`.`schema`.`regional_metrics` AS r\n",
    "  WHERE region IS NOT NULL  -- MANDATORY field filtered\n",
    "    AND marketing_spend IS NOT NULL \n",
    "    AND sales_revenue IS NOT NULL\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at END of first CTE only\n",
    "),\n",
    "-- Step 2: Statistical Analysis CTE - COALESCE to DOUBLE (business value: calculate correlations and drivers)\n",
    "revenue_driver_analysis AS (\n",
    "  SELECT \n",
    "    region,\n",
    "    -- Correlation analysis - keep as DOUBLE (CONCAT auto-converts)\n",
    "    COALESCE(ROUND(CORR(marketing_spend, sales_revenue), 3), 0.0) AS marketing_revenue_correlation,\n",
    "    COALESCE(ROUND(CORR(customer_satisfaction_score, sales_revenue), 3), 0.0) AS satisfaction_revenue_correlation,\n",
    "    COALESCE(ROUND(CORR(operational_efficiency, sales_revenue), 3), 0.0) AS efficiency_revenue_correlation,\n",
    "    -- Regression analysis - keep as DOUBLE\n",
    "    COALESCE(ROUND(REGR_R2(sales_revenue, marketing_spend), 3), 0.0) AS marketing_predictive_power,\n",
    "    COALESCE(ROUND(REGR_SLOPE(sales_revenue, marketing_spend), 2), 0.0) AS revenue_per_marketing_dollar,\n",
    "    -- Variance analysis - keep as DOUBLE\n",
    "    COALESCE(ROUND(STDDEV_POP(sales_revenue), 2), 0.0) AS revenue_volatility,\n",
    "    -- Performance metrics - keep as DOUBLE\n",
    "    COALESCE(ROUND(AVG(sales_revenue), 2), 0.0) AS avg_revenue,\n",
    "    COALESCE(ROUND(MEDIAN(sales_revenue), 2), 0.0) AS typical_revenue\n",
    "  FROM performance_data\n",
    "  GROUP BY region\n",
    "),\n",
    "-- Step 3: Generate ai_sys_prompt for regional analysis\n",
    "regional_prompt_generation AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    CONCAT('You are a Regional Performance Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "           'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "           'Business priorities are: {enriched_business_priorities}. ',\n",
    "           'With 15 years of experience in regional revenue optimization and performance analytics, ',\n",
    "           'your expertise in correlation analysis and strategic resource allocation aligns with the strategic initiative: Regional growth. ',\n",
    "           'Analyze regional performance for ', region, '. ',\n",
    "           'Marketing-Revenue Correlation: ', marketing_revenue_correlation, '. ',\n",
    "           'Satisfaction-Revenue Correlation: ', satisfaction_revenue_correlation, '. ',\n",
    "           'Marketing Predictive Power (R²): ', marketing_predictive_power, '. ',\n",
    "           'Revenue per Marketing Dollar: $', revenue_per_marketing_dollar, '. ',\n",
    "           'Revenue Volatility (StdDev): ', revenue_volatility, '. ',\n",
    "           'Output ONLY JSON with NO markdown, NO extra text. ',\n",
    "           'Format: {{\"ai_cat_primary_driver\": \"Marketing/Satisfaction/Efficiency\", \"ai_cat_confidence_level\": \"High/Medium/Low\", ',\n",
    "           '\"ai_cat_investment_priority\": \"Increase/Maintain/Decrease\", ',\n",
    "           '\"ai_txt_strategic_recommendation\": \"text\", \"ai_txt_risk_assessment\": \"text\", \"ai_txt_opportunity_capture\": \"text\", ',\n",
    "           '\"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "           'MANDATORY LAST 7 FIELDS: ai_txt_business_outcome (CALCULATED MEASURABLE IMPACT with Daily/Weekly/Monthly/Yearly breakdown and DISCLAIMER), ai_txt_executive_summary (references business outcome numbers), ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data. ',\n",
    "           'ai_sys_missing_data format: \"I can get higher confidence than [X]% if I can get access to [narrative]. {{\\\"missing_data\\\": [\\\"dataset1\\\", \\\"dataset2\\\"]}}\" ',\n",
    "           'Output ONLY the JSON, nothing else.') AS ai_sys_prompt\n",
    "  FROM revenue_driver_analysis\n",
    "),\n",
    "-- Step 4: AI interpretation - pass ai_sys_prompt to ai_query\n",
    "insights_with_recommendations AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS strategic_insights\n",
    "  FROM regional_prompt_generation\n",
    "),\n",
    "-- Step 5: Extract insights for business users with ai_cat_, ai_txt_, ai_sys_ prefixes\n",
    "-- ai_sys_prompt MUST be the LAST column\n",
    "final_output AS (\n",
    "  SELECT \n",
    "    region,\n",
    "    marketing_revenue_correlation,\n",
    "    satisfaction_revenue_correlation,\n",
    "    efficiency_revenue_correlation,\n",
    "    marketing_predictive_power,\n",
    "    revenue_per_marketing_dollar,\n",
    "    revenue_volatility,\n",
    "    get_json_object(strategic_insights, '$.ai_cat_primary_driver') AS ai_cat_primary_driver,\n",
    "    get_json_object(strategic_insights, '$.ai_cat_confidence_level') AS ai_cat_confidence_level,\n",
    "    get_json_object(strategic_insights, '$.ai_cat_investment_priority') AS ai_cat_investment_priority,\n",
    "    get_json_object(strategic_insights, '$.ai_txt_strategic_recommendation') AS ai_txt_strategic_recommendation,\n",
    "    get_json_object(strategic_insights, '$.ai_txt_risk_assessment') AS ai_txt_risk_assessment,\n",
    "    get_json_object(strategic_insights, '$.ai_txt_opportunity_capture') AS ai_txt_opportunity_capture,\n",
    "    -- MANDATORY LAST 7 COLUMNS (including ai_txt_business_outcome + ai_sys_prompt):\n",
    "    COALESCE(get_json_object(strategic_insights, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "    COALESCE(get_json_object(strategic_insights, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "    COALESCE(get_json_object(strategic_insights, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "    COALESCE(get_json_object(strategic_insights, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "    COALESCE(TRY_CAST(get_json_object(strategic_insights, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "    COALESCE(get_json_object(strategic_insights, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "    COALESCE(get_json_object(strategic_insights, '$.ai_sys_missing_data'), 'No missing data identified') AS ai_sys_missing_data,\n",
    "    ai_sys_prompt  -- ✅ LAST COLUMN: The exact prompt used for auditability\n",
    "  FROM insights_with_recommendations\n",
    ")\n",
    "SELECT * FROM final_output\n",
    "-- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "-- WHERE ai_cat_primary_driver IN ('Marketing', 'Satisfaction', 'Efficiency')\n",
    "-- AND ai_cat_confidence_level IN ('High', 'Medium', 'Low')\n",
    "-- AND ai_cat_investment_priority IN ('Increase', 'Maintain', 'Decrease')\n",
    "-- AND ai_sys_importance IN ('Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical')\n",
    "-- AND ai_sys_urgency IN ('Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical')\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL REMINDERS:**\n",
    "- Statistical functions reveal insights that simple aggregations cannot\n",
    "- ALWAYS combine statistical analysis with AI interpretation for maximum value\n",
    "- Use business-friendly CTE names for statistical steps\n",
    "- Focus on actionable insights, not academic statistics\n",
    "- Think about what business decisions each statistic enables\n",
    "\n",
    "---\n",
    "\n",
    "#### 5b. **ADVANCED STATISTICAL ANALYSIS PATTERNS WITH AI INTERPRETATION** (MANDATORY - HIGH-VALUE USE CASES):\n",
    "\n",
    "**\uD83D\uDD25\uD83D\uDD25\uD83D\uDD25 CRITICAL: These advanced analytical techniques focus on answering \"WHY?\" and \"WHO?\" rather than just \"WHAT happened?\" - delivering exponentially higher business value through behavioral insights, causal relationships, and strategic recommendations. \uD83D\uDD25\uD83D\uDD25\uD83D\uDD25**\n",
    "\n",
    "**MANDATORY USAGE RULES:**\n",
    "1. **INNOVATE**: The examples below are ILLUSTRATIVE ONLY. You MUST create NEW, BUSINESS-SPECIFIC use cases tailored to the actual data schema and business context.\n",
    "2. **COMBINE TECHNIQUES**: Mix multiple statistical techniques (cohort + pareto, funnel + sessionization, etc.) for deeper insights\n",
    "3. **AI INTERPRETATION**: ALWAYS use ai_query to interpret statistical results and generate actionable strategies, action plans, and executive recommendations\n",
    "4. **PROPER NAMING**: Use business-meaningful names that clearly describe the analytical approach (e.g., \"Customer Retention Cohort Analysis with Lifetime Value Tracking\")\n",
    "\n",
    "**ADVANCED ANALYTICAL TECHNIQUES:**\n",
    "\n",
    "**1. COHORT ANALYSIS - Track Behavior Over Time by Acquisition Group**\n",
    "\n",
    "**Key SQL Functions:** MIN() OVER, DATEDIFF, DATE_TRUNC, LAG, LEAD\n",
    "**Business Value:** Identifies which customer/user cohorts have superior retention, lifetime value, or engagement patterns\n",
    "\n",
    "**Use Case Example Template: \"Customer Acquisition Quality Analysis with Retention Strategy\"**\n",
    "\n",
    "```sql\n",
    "-- Step 1: Identify customer acquisition cohorts with mandatory field filtering\n",
    "WITH customer_cohorts AS (\n",
    "  SELECT \n",
    "    customer_id,\n",
    "    order_date,\n",
    "    order_amount,\n",
    "    DATE_TRUNC('month', MIN(order_date) OVER (PARTITION BY customer_id)) AS cohort_group,\n",
    "    CAST(months_between(order_date, MIN(order_date) OVER (PARTITION BY customer_id)) AS INT) AS months_since_first_purchase\n",
    "  FROM `catalog`.`schema`.`orders` AS o\n",
    "  WHERE customer_id IS NOT NULL  -- MANDATORY: Primary key\n",
    "    AND order_date IS NOT NULL   -- MANDATORY: Required for cohort analysis\n",
    "    AND order_amount IS NOT NULL -- MANDATORY: Required for metrics\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at END of first CTE only\n",
    "),\n",
    "-- Step 2: Calculate cohort metrics (business value: cohort performance analysis) - keep as DOUBLE/INT\n",
    "cohort_performance_metrics AS (\n",
    "  SELECT \n",
    "    cohort_group,\n",
    "    -- All metrics COALESCEd to correct types (NOT STRING!) - CONCAT handles mixed types\n",
    "    COALESCE(COUNT(DISTINCT customer_id), 0) AS cohort_size,\n",
    "    COALESCE(ROUND(AVG(order_amount), 2), 0.0) AS avg_order_value,\n",
    "    COALESCE(ROUND(SUM(order_amount), 2), 0.0) AS total_lifetime_value,\n",
    "    COALESCE(ROUND(COUNT(DISTINCT CASE WHEN months_since_first_purchase >= 6 THEN customer_id END) * 100.0 / NULLIF(COUNT(DISTINCT customer_id), 0), 1), 0.0) AS six_month_retention_rate,\n",
    "    COALESCE(ROUND(AVG(CASE WHEN months_since_first_purchase <= 3 THEN order_amount END), 2), 0.0) AS early_lifetime_value,\n",
    "    COALESCE(ROUND(STDDEV_POP(order_amount), 2), 0.0) AS purchase_volatility\n",
    "  FROM customer_cohorts\n",
    "  GROUP BY cohort_group\n",
    "),\n",
    "-- Step 3: Generate ai_sys_prompt for cohort analysis\n",
    "cohort_prompt_generation AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    CONCAT('You are a Customer Analytics Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "           'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "           'Business priorities are: {enriched_business_priorities}. ',\n",
    "           'With 15 years of experience in retention strategy and customer lifetime value optimization, ',\n",
    "           'your expertise in cohort analysis, churn prediction, and targeted retention programs aligns with the strategic initiative: Customer success and retention. ',\n",
    "           'Analyze customer cohort acquired in ', cohort_group, '. ',  -- CONCAT auto-converts\n",
    "           'Cohort size: ', cohort_size, ' customers. ',\n",
    "           'Average order value: $', avg_order_value, '. ',\n",
    "           'Total lifetime value: $', total_lifetime_value, '. ',\n",
    "           '6-month retention rate: ', six_month_retention_rate, '%. ',\n",
    "           'Early lifetime value (first 3 months): $', early_lifetime_value, '. ',\n",
    "           'Purchase volatility: $', purchase_volatility, '. ',\n",
    "           'Output ONLY a JSON object with NO markdown fences, NO extra text, JUST the JSON. ',\n",
    "           'Format: {{\"ai_cat_cohort_quality\": \"value\", \"ai_cat_retention_risk\": \"value\", \"ai_cat_ltv_trajectory\": \"value\", \"ai_cat_strategic_action\": \"value\", \"ai_txt_acquisition_channel_recommendation\": \"text\", \"ai_txt_retention_strategy\": \"text\", \"ai_txt_investment_priority\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "           'CATEGORICAL FIELDS (ai_cat_ prefix, exact values): ai_cat_cohort_quality (Exceptional|High Quality|Average|Below Average|Poor), ai_cat_retention_risk (Critical|High|Medium|Low), ai_cat_ltv_trajectory (Accelerating|Growing|Stable|Declining|Concerning), ai_cat_strategic_action (Scale Aggressively|Invest More|Maintain|Optimize|Reduce Spend). ',\n",
    "           'NARRATIVE FIELDS (ai_txt_ prefix): ai_txt_acquisition_channel_recommendation, ai_txt_retention_strategy, ai_txt_investment_priority. ',\n",
    "           'MANDATORY LAST 7 FIELDS: ai_txt_business_outcome (CALCULATED MEASURABLE IMPACT with Daily/Weekly/Monthly/Yearly breakdown and DISCLAIMER), ai_txt_executive_summary (references business outcome numbers), ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data. ',\n",
    "           'ai_sys_missing_data format: \"I can get higher confidence than [X]% if I can get access to [narrative]. {{\\\"missing_data\\\": [\\\"acquisition_channels\\\", \\\"engagement_metrics\\\", \\\"product_usage_data\\\"]}}\" ',\n",
    "           'Output ONLY the JSON object, nothing else.') AS ai_sys_prompt\n",
    "  FROM cohort_performance_metrics\n",
    "),\n",
    "-- Step 4: AI analysis - pass ai_sys_prompt to ai_query\n",
    "cohort_insights_with_strategy AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS cohort_analysis_json\n",
    "  FROM cohort_prompt_generation\n",
    "),\n",
    "-- Final output: Cohort metrics with strategic insights using ai_cat_, ai_txt_, ai_sys_ prefixes\n",
    "-- ai_sys_prompt MUST be the LAST column\n",
    "final_output AS (\n",
    "  SELECT \n",
    "    cohort_group,\n",
    "    cohort_size,\n",
    "    avg_order_value,\n",
    "    total_lifetime_value,\n",
    "    six_month_retention_rate,\n",
    "    get_json_object(cohort_analysis_json, '$.ai_cat_cohort_quality') AS ai_cat_cohort_quality,\n",
    "    get_json_object(cohort_analysis_json, '$.ai_cat_retention_risk') AS ai_cat_retention_risk,\n",
    "    get_json_object(cohort_analysis_json, '$.ai_cat_ltv_trajectory') AS ai_cat_ltv_trajectory,\n",
    "    get_json_object(cohort_analysis_json, '$.ai_cat_strategic_action') AS ai_cat_strategic_action,\n",
    "    get_json_object(cohort_analysis_json, '$.ai_txt_acquisition_channel_recommendation') AS ai_txt_acquisition_channel_recommendation,\n",
    "    get_json_object(cohort_analysis_json, '$.ai_txt_retention_strategy') AS ai_txt_retention_strategy,\n",
    "    get_json_object(cohort_analysis_json, '$.ai_txt_investment_priority') AS ai_txt_investment_priority,\n",
    "    -- MANDATORY LAST 7 COLUMNS (including ai_txt_business_outcome + ai_sys_prompt):\n",
    "    COALESCE(get_json_object(cohort_analysis_json, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "    COALESCE(get_json_object(cohort_analysis_json, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "    COALESCE(get_json_object(cohort_analysis_json, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "    COALESCE(get_json_object(cohort_analysis_json, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "    COALESCE(TRY_CAST(get_json_object(cohort_analysis_json, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "    COALESCE(get_json_object(cohort_analysis_json, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "    COALESCE(get_json_object(cohort_analysis_json, '$.ai_sys_missing_data'), 'No missing data identified') AS ai_sys_missing_data,\n",
    "    ai_sys_prompt  -- ✅ LAST COLUMN: The exact prompt used for auditability\n",
    "  FROM cohort_insights_with_strategy\n",
    ")\n",
    "SELECT * FROM final_output\n",
    "-- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "-- WHERE ai_cat_cohort_quality IN ('Exceptional', 'High Quality', 'Average', 'Below Average', 'Poor')\n",
    "-- AND ai_cat_retention_risk IN ('Critical', 'High', 'Medium', 'Low')\n",
    "-- AND ai_cat_ltv_trajectory IN ('Accelerating', 'Growing', 'Stable', 'Declining', 'Concerning')\n",
    "-- AND ai_cat_strategic_action IN ('Scale Aggressively', 'Invest More', 'Maintain', 'Optimize', 'Reduce Spend')\n",
    "-- AND ai_sys_importance IN ('Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical')\n",
    "-- AND ai_sys_urgency IN ('Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical')\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "\n",
    "**2. PARETO ANALYSIS (80/20 RULE) - Identify Critical Few vs Trivial Many**\n",
    "\n",
    "**Key SQL Functions:** SUM() OVER, PERCENT_RANK, CUME_DIST, NTILE\n",
    "**Business Value:** Pinpoints the vital few products/customers/issues that drive most results, enabling focused resource allocation\n",
    "\n",
    "**Use Case Example Template: \"Revenue Concentration Analysis with Portfolio Optimization Strategy\"**\n",
    "\n",
    "```sql\n",
    "-- Step 1: Calculate revenue contribution and cumulative distribution\n",
    "WITH product_revenue_ranked AS (\n",
    "  SELECT \n",
    "    product_id,\n",
    "    product_category,\n",
    "    SUM(revenue) AS total_revenue,\n",
    "    SUM(profit) AS total_profit,\n",
    "    COUNT(DISTINCT order_id) AS order_count,\n",
    "    SUM(SUM(revenue)) OVER () AS company_total_revenue,\n",
    "    SUM(SUM(profit)) OVER () AS company_total_profit\n",
    "  FROM `catalog`.`schema`.`sales` AS s\n",
    "  WHERE product_id IS NOT NULL\n",
    "    AND revenue IS NOT NULL\n",
    "  GROUP BY product_id, product_category\n",
    "  LIMIT 10  -- ✅ LIMIT 10 in first CTE (GROUP BY provides uniqueness)\n",
    "),\n",
    "-- Step 2: Calculate Pareto metrics - keep numeric types as DOUBLE/INT\n",
    "pareto_analysis_metrics AS (\n",
    "  SELECT \n",
    "    product_id,\n",
    "    COALESCE(TRIM(product_category), 'Uncategorized') AS product_category,\n",
    "    COALESCE(ROUND(total_revenue, 2), 0.0) AS total_revenue,\n",
    "    COALESCE(ROUND(total_profit, 2), 0.0) AS total_profit,\n",
    "    COALESCE(order_count, 0) AS order_count,\n",
    "    COALESCE(ROUND(total_revenue * 100.0 / NULLIF(company_total_revenue, 0), 2), 0.0) AS revenue_contribution_pct,\n",
    "    COALESCE(ROUND(total_profit * 100.0 / NULLIF(company_total_profit, 0), 2), 0.0) AS profit_contribution_pct,\n",
    "    COALESCE(ROUND(SUM(total_revenue) OVER (ORDER BY total_revenue DESC) * 100.0 / NULLIF(company_total_revenue, 0), 2), 0.0) AS cumulative_revenue_pct,\n",
    "    COALESCE(ROUND(PERCENT_RANK() OVER (ORDER BY total_revenue DESC) * 100, 1), 0.0) AS revenue_percentile_rank,\n",
    "    COALESCE(NTILE(5) OVER (ORDER BY total_revenue DESC), 3) AS revenue_quintile\n",
    "  FROM product_revenue_ranked\n",
    "),\n",
    "-- Step 3: Generate ai_sys_prompt for Pareto analysis\n",
    "pareto_prompt_generation AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    CONCAT('You are a Portfolio Strategy Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "           'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "           'Business priorities are: {enriched_business_priorities}. ',\n",
    "           'With 18 years of experience in revenue optimization and product management, ',\n",
    "           'your expertise in Pareto analysis, portfolio rationalization, and strategic resource allocation aligns with the strategic initiative: Portfolio optimization. ',\n",
    "           'Analyze product ', product_id, ' in category ', product_category, '. ',\n",
    "           'Total revenue: $', total_revenue, ' (', revenue_contribution_pct, '% of company revenue). ',\n",
    "           'Total profit: $', total_profit, ' (', profit_contribution_pct, '% of company profit). ',\n",
    "           'Order count: ', order_count, '. ',\n",
    "           'Cumulative revenue contribution: ', cumulative_revenue_pct, '%. ',\n",
    "           'Revenue percentile rank: ', revenue_percentile_rank, '. ',\n",
    "           'Revenue quintile: ', revenue_quintile, ' of 5. ',\n",
    "           'Output ONLY a JSON object with NO markdown fences, NO extra text, JUST the JSON. ',\n",
    "           'Format: {{\"ai_cat_pareto_classification\": \"value\", \"ai_cat_strategic_priority\": \"value\", \"ai_cat_resource_allocation\": \"value\", \"ai_cat_portfolio_action\": \"value\", \"ai_txt_investment_recommendation\": \"text\", \"ai_txt_optimization_strategy\": \"text\", \"ai_txt_risk_mitigation\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "           'CATEGORICAL FIELDS (ai_cat_ prefix, exact values): ai_cat_pareto_classification (Vital Few - Top 20%|Important|Average|Low Impact|Long Tail), ai_cat_strategic_priority (Critical Focus|High Priority|Standard|Low Priority|Consider Exit), ai_cat_resource_allocation (Increase Investment|Maintain|Optimize Efficiency|Reduce|Divest), ai_cat_portfolio_action (Scale Aggressively|Grow Steadily|Harvest|Sunset|Discontinue). ',\n",
    "           'NARRATIVE FIELDS (ai_txt_ prefix): ai_txt_investment_recommendation, ai_txt_optimization_strategy, ai_txt_risk_mitigation. ',\n",
    "           'MANDATORY LAST 7 FIELDS: ai_txt_business_outcome (CALCULATED MEASURABLE IMPACT with Daily/Weekly/Monthly/Yearly breakdown and DISCLAIMER), ai_txt_executive_summary (references business outcome numbers), ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data. ',\n",
    "           'ai_sys_missing_data format: \"I can get higher confidence than [X]% if I can get access to [narrative]. {{\\\"missing_data\\\": [\\\"market_trends\\\", \\\"competitor_pricing\\\", \\\"cost_structure\\\"]}}\" ',\n",
    "           'Output ONLY the JSON object, nothing else.') AS ai_sys_prompt\n",
    "  FROM pareto_analysis_metrics\n",
    "),\n",
    "-- Step 4: AI analysis - pass ai_sys_prompt to ai_query\n",
    "pareto_insights_with_action_plan AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS pareto_strategy_json\n",
    "  FROM pareto_prompt_generation\n",
    "),\n",
    "-- Final output: Pareto analysis with strategic recommendations using ai_cat_, ai_txt_, ai_sys_ prefixes\n",
    "-- ai_sys_prompt MUST be the LAST column\n",
    "final_output AS (\n",
    "  SELECT \n",
    "    product_id,\n",
    "    product_category,\n",
    "    total_revenue,\n",
    "    revenue_contribution_pct,\n",
    "    cumulative_revenue_pct,\n",
    "    get_json_object(pareto_strategy_json, '$.ai_cat_pareto_classification') AS ai_cat_pareto_classification,\n",
    "    get_json_object(pareto_strategy_json, '$.ai_cat_strategic_priority') AS ai_cat_strategic_priority,\n",
    "    get_json_object(pareto_strategy_json, '$.ai_cat_resource_allocation') AS ai_cat_resource_allocation,\n",
    "    get_json_object(pareto_strategy_json, '$.ai_cat_portfolio_action') AS ai_cat_portfolio_action,\n",
    "    get_json_object(pareto_strategy_json, '$.ai_txt_investment_recommendation') AS ai_txt_investment_recommendation,\n",
    "    get_json_object(pareto_strategy_json, '$.ai_txt_optimization_strategy') AS ai_txt_optimization_strategy,\n",
    "    get_json_object(pareto_strategy_json, '$.ai_txt_risk_mitigation') AS ai_txt_risk_mitigation,\n",
    "    -- MANDATORY LAST 7 COLUMNS (including ai_txt_business_outcome + ai_sys_prompt):\n",
    "    COALESCE(get_json_object(pareto_strategy_json, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "    COALESCE(get_json_object(pareto_strategy_json, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "    COALESCE(get_json_object(pareto_strategy_json, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "    COALESCE(get_json_object(pareto_strategy_json, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "    COALESCE(TRY_CAST(get_json_object(pareto_strategy_json, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "    COALESCE(get_json_object(pareto_strategy_json, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "    COALESCE(get_json_object(pareto_strategy_json, '$.ai_sys_missing_data'), 'No missing data identified') AS ai_sys_missing_data,\n",
    "    ai_sys_prompt  -- ✅ LAST COLUMN: The exact prompt used for auditability\n",
    "  FROM pareto_insights_with_action_plan\n",
    ")\n",
    "SELECT * FROM final_output\n",
    "-- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "-- WHERE ai_cat_pareto_classification IN ('Vital Few - Top 20%', 'Important', 'Average', 'Low Impact', 'Long Tail')\n",
    "-- AND ai_cat_strategic_priority IN ('Critical Focus', 'High Priority', 'Standard', 'Low Priority', 'Consider Exit')\n",
    "-- AND ai_cat_resource_allocation IN ('Increase Investment', 'Maintain', 'Optimize Efficiency', 'Reduce', 'Divest')\n",
    "-- AND ai_cat_portfolio_action IN ('Scale Aggressively', 'Grow Steadily', 'Harvest', 'Sunset', 'Discontinue')\n",
    "-- AND ai_sys_importance IN ('Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical')\n",
    "-- AND ai_sys_urgency IN ('Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical')\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "\n",
    "**3. FUNNEL ANALYSIS - Identify Conversion Bottlenecks and Drop-off Points**\n",
    "\n",
    "**Key SQL Functions:** SUM(CASE WHEN...), LAG, LEAD, window functions\n",
    "**Business Value:** Quantifies exactly where customers abandon the journey, enabling targeted conversion rate optimization\n",
    "\n",
    "**Use Case Example Template: \"Checkout Conversion Funnel Analysis with Friction Point Recommendations\"**\n",
    "\n",
    "```sql\n",
    "-- Step 1: Define funnel stages and calculate stage-level metrics\n",
    "WITH funnel_events AS (\n",
    "  SELECT DISTINCT\n",
    "    user_id,\n",
    "    session_id,\n",
    "    event_name,\n",
    "    event_timestamp,\n",
    "    CASE \n",
    "      WHEN event_name = 'product_view' THEN 1\n",
    "      WHEN event_name = 'add_to_cart' THEN 2\n",
    "      WHEN event_name = 'checkout_start' THEN 3\n",
    "      WHEN event_name = 'payment_info' THEN 4\n",
    "      WHEN event_name = 'purchase_complete' THEN 5\n",
    "      ELSE 0\n",
    "    END AS funnel_stage\n",
    "  FROM `catalog`.`schema`.`user_events` AS e\n",
    "  WHERE user_id IS NOT NULL\n",
    "    AND event_name IS NOT NULL\n",
    "    AND session_id IS NOT NULL\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at END of first CTE only\n",
    "),\n",
    "-- Step 2: Calculate funnel conversion rates - keep numeric types as DOUBLE/INT\n",
    "funnel_conversion_metrics AS (\n",
    "  SELECT \n",
    "    'Product View' AS stage_name,\n",
    "    1 AS stage_order,\n",
    "    COALESCE(COUNT(DISTINCT CASE WHEN funnel_stage >= 1 THEN user_id END), 0) AS users_at_stage,\n",
    "    COALESCE(ROUND(COUNT(DISTINCT CASE WHEN funnel_stage >= 2 THEN user_id END) * 100.0 / NULLIF(COUNT(DISTINCT CASE WHEN funnel_stage >= 1 THEN user_id END), 0), 1), 0.0) AS conversion_to_next,\n",
    "    COALESCE(ROUND((1 - COUNT(DISTINCT CASE WHEN funnel_stage >= 2 THEN user_id END) * 1.0 / NULLIF(COUNT(DISTINCT CASE WHEN funnel_stage >= 1 THEN user_id END), 0)) * 100, 1), 0.0) AS drop_off_rate,\n",
    "    COALESCE(COUNT(DISTINCT CASE WHEN funnel_stage >= 1 AND funnel_stage < 2 THEN user_id END), 0) AS users_dropped\n",
    "  FROM funnel_events\n",
    "  \n",
    "  UNION ALL\n",
    "  \n",
    "  SELECT \n",
    "    'Add to Cart' AS stage_name,\n",
    "    2 AS stage_order,\n",
    "    COALESCE(COUNT(DISTINCT CASE WHEN funnel_stage >= 2 THEN user_id END), 0) AS users_at_stage,\n",
    "    COALESCE(ROUND(COUNT(DISTINCT CASE WHEN funnel_stage >= 3 THEN user_id END) * 100.0 / NULLIF(COUNT(DISTINCT CASE WHEN funnel_stage >= 2 THEN user_id END), 0), 1), 0.0) AS conversion_to_next,\n",
    "    COALESCE(ROUND((1 - COUNT(DISTINCT CASE WHEN funnel_stage >= 3 THEN user_id END) * 1.0 / NULLIF(COUNT(DISTINCT CASE WHEN funnel_stage >= 2 THEN user_id END), 0)) * 100, 1), 0.0) AS drop_off_rate,\n",
    "    COALESCE(COUNT(DISTINCT CASE WHEN funnel_stage >= 2 AND funnel_stage < 3 THEN user_id END), 0) AS users_dropped\n",
    "  FROM funnel_events\n",
    "),\n",
    "-- Step 3: Generate ai_sys_prompt for funnel analysis\n",
    "funnel_prompt_generation AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    CONCAT('You are a Conversion Rate Optimization Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "           'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "           'Business priorities are: {enriched_business_priorities}. ',\n",
    "           'With 12 years of experience in e-commerce funnel analysis and UX optimization, ',\n",
    "           'your expertise in funnel optimization, user behavior analysis, and A/B testing strategy aligns with the strategic initiative: Conversion optimization. ',\n",
    "           'Analyze funnel stage: ', stage_name, '. ',\n",
    "           'Users at this stage: ', users_at_stage, '. ',\n",
    "           'Conversion rate to next stage: ', conversion_to_next, '%. ',\n",
    "           'Drop-off rate: ', drop_off_rate, '%. ',\n",
    "           'Users who dropped at this stage: ', users_dropped, '. ',\n",
    "           'Output ONLY a JSON object with NO markdown fences, NO extra text, JUST the JSON. ',\n",
    "           'Format: {{\"ai_cat_friction_severity\": \"value\", \"ai_cat_optimization_priority\": \"value\", \"ai_cat_drop_off_reason\": \"value\", \"ai_cat_recommended_action\": \"value\", \"ai_txt_ab_test_hypothesis\": \"text\", \"ai_txt_ux_improvement_plan\": \"text\", \"ai_txt_expected_impact\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "           'CATEGORICAL FIELDS (ai_cat_ prefix, exact values): ai_cat_friction_severity (Critical Bottleneck|High Friction|Moderate Friction|Low Friction|Acceptable), ai_cat_optimization_priority (Immediate Action|High Priority|Medium Priority|Low Priority|Monitor), ai_cat_drop_off_reason (Technical Issue|UX Friction|Trust Concerns|Price Sensitivity|Distraction), ai_cat_recommended_action (Urgent Fix|Major Redesign|Incremental Improvement|A/B Test|No Action Needed). ',\n",
    "           'NARRATIVE FIELDS (ai_txt_ prefix): ai_txt_ab_test_hypothesis, ai_txt_ux_improvement_plan, ai_txt_expected_impact. ',\n",
    "           'MANDATORY LAST 7 FIELDS: ai_txt_business_outcome (CALCULATED MEASURABLE IMPACT with Daily/Weekly/Monthly/Yearly breakdown and DISCLAIMER), ai_txt_executive_summary (references business outcome numbers), ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data. ',\n",
    "           'ai_sys_missing_data format: \"I can get higher confidence than [X]% if I can get access to [narrative]. {{\\\"missing_data\\\": [\\\"session_recordings\\\", \\\"heatmaps\\\", \\\"user_demographics\\\", \\\"device_data\\\"]}}\" ',\n",
    "           'Output ONLY the JSON object, nothing else.') AS ai_sys_prompt\n",
    "  FROM funnel_conversion_metrics\n",
    "),\n",
    "-- Step 4: AI analysis - pass ai_sys_prompt to ai_query\n",
    "funnel_optimization_strategy AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS funnel_insights_json\n",
    "  FROM funnel_prompt_generation\n",
    "),\n",
    "-- Final output: Funnel metrics with optimization recommendations using ai_cat_, ai_txt_, ai_sys_ prefixes\n",
    "-- ai_sys_prompt MUST be the LAST column\n",
    "final_output AS (\n",
    "  SELECT \n",
    "    stage_name,\n",
    "    stage_order,\n",
    "    users_at_stage,\n",
    "    conversion_to_next AS conversion_to_next_pct,\n",
    "    drop_off_rate AS drop_off_rate_pct,\n",
    "    users_dropped,\n",
    "    get_json_object(funnel_insights_json, '$.ai_cat_friction_severity') AS ai_cat_friction_severity,\n",
    "    get_json_object(funnel_insights_json, '$.ai_cat_optimization_priority') AS ai_cat_optimization_priority,\n",
    "    get_json_object(funnel_insights_json, '$.ai_cat_drop_off_reason') AS ai_cat_drop_off_reason,\n",
    "    get_json_object(funnel_insights_json, '$.ai_cat_recommended_action') AS ai_cat_recommended_action,\n",
    "    get_json_object(funnel_insights_json, '$.ai_txt_ab_test_hypothesis') AS ai_txt_ab_test_hypothesis,\n",
    "    get_json_object(funnel_insights_json, '$.ai_txt_ux_improvement_plan') AS ai_txt_ux_improvement_plan,\n",
    "    get_json_object(funnel_insights_json, '$.ai_txt_expected_impact') AS ai_txt_expected_impact,\n",
    "    -- MANDATORY LAST 7 COLUMNS (including ai_txt_business_outcome + ai_sys_prompt):\n",
    "    COALESCE(get_json_object(funnel_insights_json, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "    COALESCE(get_json_object(funnel_insights_json, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "    COALESCE(get_json_object(funnel_insights_json, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "    COALESCE(get_json_object(funnel_insights_json, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "    COALESCE(TRY_CAST(get_json_object(funnel_insights_json, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "    COALESCE(get_json_object(funnel_insights_json, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "    COALESCE(get_json_object(funnel_insights_json, '$.ai_sys_missing_data'), 'No missing data identified') AS ai_sys_missing_data,\n",
    "    ai_sys_prompt  -- ✅ LAST COLUMN: The exact prompt used for auditability\n",
    "  FROM funnel_optimization_strategy\n",
    ")\n",
    "SELECT * FROM final_output\n",
    "-- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "-- WHERE ai_cat_friction_severity IN ('Critical Bottleneck', 'High Friction', 'Moderate Friction', 'Low Friction', 'Acceptable')\n",
    "-- AND ai_cat_optimization_priority IN ('Immediate Action', 'High Priority', 'Medium Priority', 'Low Priority', 'Monitor')\n",
    "-- AND ai_cat_drop_off_reason IN ('Technical Issue', 'UX Friction', 'Trust Concerns', 'Price Sensitivity', 'Distraction')\n",
    "-- AND ai_cat_recommended_action IN ('Urgent Fix', 'Major Redesign', 'Incremental Improvement', 'A/B Test', 'No Action Needed')\n",
    "ORDER BY stage_order\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "\n",
    "**4. GAP ANALYSIS - Identify Missing Data and Coverage Gaps**\n",
    "\n",
    "**Key SQL Functions:** LEFT JOIN, IS NULL, SEQUENCE, EXPLODE\n",
    "**Business Value:** Discovers hidden inventory, missed opportunities, and operational blind spots\n",
    "\n",
    "**Use Case Example Template: \"Product Catalog Coverage Analysis with Market Expansion Strategy\"**\n",
    "\n",
    "```sql\n",
    "-- Step 1: Generate complete list of expected coverage (all categories × all regions)\n",
    "WITH expected_coverage AS (\n",
    "  SELECT DISTINCT\n",
    "    c.category_id,\n",
    "    c.category_name,\n",
    "    r.region_id,\n",
    "    r.region_name\n",
    "  FROM `catalog`.`schema`.`product_categories` AS c\n",
    "  CROSS JOIN `catalog`.`schema`.`sales_regions` AS r\n",
    "  WHERE c.category_id IS NOT NULL\n",
    "    AND r.region_id IS NOT NULL\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at END of first CTE only\n",
    "),\n",
    "-- Step 2: Identify actual sales presence\n",
    "actual_coverage AS (\n",
    "  SELECT DISTINCT\n",
    "    product_category_id AS category_id,\n",
    "    sales_region_id AS region_id,\n",
    "    SUM(revenue) AS total_revenue,\n",
    "    COUNT(DISTINCT customer_id) AS customer_count\n",
    "  FROM `catalog`.`schema`.`sales` AS s\n",
    "  WHERE product_category_id IS NOT NULL\n",
    "    AND sales_region_id IS NOT NULL\n",
    "  GROUP BY product_category_id, sales_region_id\n",
    "),\n",
    "-- Step 3: Identify gaps and calculate opportunity metrics - keep numeric types as DOUBLE/INT\n",
    "coverage_gap_analysis AS (\n",
    "  SELECT \n",
    "    COALESCE(TRIM(e.category_name), 'Unknown Category') AS category_name,\n",
    "    COALESCE(TRIM(e.region_name), 'Unknown Region') AS region_name,\n",
    "    CASE WHEN a.category_id IS NULL THEN 'Gap - No Sales' ELSE 'Active Coverage' END AS coverage_status,\n",
    "    COALESCE(ROUND(a.total_revenue, 2), 0.0) AS current_revenue,\n",
    "    COALESCE(a.customer_count, 0) AS current_customers,\n",
    "    COALESCE(ROUND(AVG(a.total_revenue) OVER (PARTITION BY e.category_id), 2), 0.0) AS category_avg_revenue,\n",
    "    COALESCE(ROUND(AVG(a.total_revenue) OVER (PARTITION BY e.region_id), 2), 0.0) AS region_avg_revenue\n",
    "  FROM expected_coverage AS e\n",
    "  LEFT JOIN actual_coverage AS a \n",
    "    ON e.category_id = a.category_id \n",
    "    AND e.region_id = a.region_id\n",
    "),\n",
    "-- Step 4: Generate ai_sys_prompt for gap analysis\n",
    "gap_prompt_generation AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    CONCAT('You are a Market Expansion Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "           'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "           'Business priorities are: {enriched_business_priorities}. ',\n",
    "           'With 15 years of experience in territory planning and go-to-market strategy, ',\n",
    "           'your expertise in market gap analysis, expansion prioritization, and revenue opportunity assessment aligns with the strategic initiative: Market expansion. ',\n",
    "           'Analyze coverage gap: Category \"', category_name, '\" in Region \"', region_name, '\". ',\n",
    "           'Coverage status: ', coverage_status, '. ',\n",
    "           'Current revenue: $', current_revenue, '. ',\n",
    "           'Current customers: ', current_customers, '. ',\n",
    "           'Category average revenue across regions: $', category_avg_revenue, '. ',\n",
    "           'Region average revenue across categories: $', region_avg_revenue, '. ',\n",
    "           'Output ONLY a JSON object with NO markdown fences, NO extra text, JUST the JSON. ',\n",
    "           'Format: {{\"ai_cat_gap_priority\": \"value\", \"ai_cat_expansion_timing\": \"value\", \"ai_cat_market_readiness\": \"value\", \"ai_cat_revenue_potential\": \"value\", \"ai_txt_expansion_strategy\": \"text\", \"ai_txt_go_to_market_plan\": \"text\", \"ai_txt_resource_requirements\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "           'CATEGORICAL FIELDS (ai_cat_ prefix, exact values): ai_cat_gap_priority (Critical Opportunity|High Potential|Medium Opportunity|Low Priority|Not Recommended), ai_cat_expansion_timing (Immediate|Next Quarter|6-12 Months|Long Term|Not Advised), ai_cat_market_readiness (Ready to Launch|Needs Preparation|Research Required|Immature Market), ai_cat_revenue_potential (High|Medium-High|Medium|Low|Minimal). ',\n",
    "           'NARRATIVE FIELDS (ai_txt_ prefix): ai_txt_expansion_strategy, ai_txt_go_to_market_plan, ai_txt_resource_requirements. ',\n",
    "           'MANDATORY LAST 7 FIELDS: ai_txt_business_outcome (CALCULATED MEASURABLE IMPACT with Daily/Weekly/Monthly/Yearly breakdown and DISCLAIMER), ai_txt_executive_summary (references business outcome numbers), ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data. ',\n",
    "           'ai_sys_missing_data format: \"I can get higher confidence than [X]% if I can get access to [narrative]. {{\\\"missing_data\\\": [\\\"competitor_presence\\\", \\\"demographic_data\\\", \\\"regulatory_requirements\\\"]}}\" ',\n",
    "           'Output ONLY the JSON object, nothing else.') AS ai_sys_prompt\n",
    "  FROM coverage_gap_analysis\n",
    "  WHERE coverage_status = 'Gap - No Sales'\n",
    "),\n",
    "-- Step 5: AI analysis - pass ai_sys_prompt to ai_query\n",
    "gap_closure_strategy AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS gap_strategy_json\n",
    "  FROM gap_prompt_generation\n",
    "),\n",
    "-- Final output: Coverage gaps with expansion recommendations using ai_cat_, ai_txt_, ai_sys_ prefixes\n",
    "-- ai_sys_prompt MUST be the LAST column\n",
    "final_output AS (\n",
    "  SELECT \n",
    "    category_name,\n",
    "    region_name,\n",
    "    coverage_status,\n",
    "    category_avg_revenue,\n",
    "    region_avg_revenue,\n",
    "    get_json_object(gap_strategy_json, '$.ai_cat_gap_priority') AS ai_cat_gap_priority,\n",
    "    get_json_object(gap_strategy_json, '$.ai_cat_expansion_timing') AS ai_cat_expansion_timing,\n",
    "    get_json_object(gap_strategy_json, '$.ai_cat_market_readiness') AS ai_cat_market_readiness,\n",
    "    get_json_object(gap_strategy_json, '$.ai_cat_revenue_potential') AS ai_cat_revenue_potential,\n",
    "    get_json_object(gap_strategy_json, '$.ai_txt_expansion_strategy') AS ai_txt_expansion_strategy,\n",
    "    get_json_object(gap_strategy_json, '$.ai_txt_go_to_market_plan') AS ai_txt_go_to_market_plan,\n",
    "    get_json_object(gap_strategy_json, '$.ai_txt_resource_requirements') AS ai_txt_resource_requirements,\n",
    "    -- MANDATORY LAST 7 COLUMNS (including ai_txt_business_outcome + ai_sys_prompt):\n",
    "    COALESCE(get_json_object(gap_strategy_json, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "    COALESCE(get_json_object(gap_strategy_json, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "    COALESCE(get_json_object(gap_strategy_json, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "    COALESCE(get_json_object(gap_strategy_json, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "    COALESCE(TRY_CAST(get_json_object(gap_strategy_json, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "    COALESCE(get_json_object(gap_strategy_json, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "    COALESCE(get_json_object(gap_strategy_json, '$.ai_sys_missing_data'), 'No missing data identified') AS ai_sys_missing_data,\n",
    "    ai_sys_prompt  -- ✅ LAST COLUMN: The exact prompt used for auditability\n",
    "  FROM gap_closure_strategy\n",
    ")\n",
    "SELECT * FROM final_output\n",
    "-- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "-- WHERE ai_cat_gap_priority IN ('Critical Opportunity', 'High Potential', 'Medium Opportunity', 'Low Priority', 'Not Recommended')\n",
    "-- AND ai_cat_expansion_timing IN ('Immediate', 'Next Quarter', '6-12 Months', 'Long Term', 'Not Advised')\n",
    "-- AND ai_cat_market_readiness IN ('Ready to Launch', 'Needs Preparation', 'Research Required', 'Immature Market')\n",
    "-- AND ai_cat_revenue_potential IN ('High', 'Medium-High', 'Medium', 'Low', 'Minimal')\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "\n",
    "**5. PRICE ELASTICITY ANALYSIS - Measure Demand Response to Price Changes**\n",
    "\n",
    "**Key SQL Functions:** REGR_SLOPE, REGR_R2, CORR, STDDEV\n",
    "**Business Value:** Quantifies pricing power and identifies optimal price points to maximize revenue\n",
    "\n",
    "**Use Case Example Template: \"Product Pricing Elasticity Analysis with Dynamic Pricing Strategy\"**\n",
    "\n",
    "```sql\n",
    "-- Step 1: Aggregate demand and pricing data for elasticity calculation\n",
    "WITH price_demand_data AS (\n",
    "  SELECT \n",
    "    product_id,\n",
    "    product_name,\n",
    "    AVG(price) AS avg_price,\n",
    "    SUM(quantity_sold) AS total_demand,\n",
    "    COUNT(DISTINCT date) AS observation_days\n",
    "  FROM `catalog`.`schema`.`sales_daily` AS s\n",
    "  WHERE product_id IS NOT NULL\n",
    "    AND price IS NOT NULL\n",
    "    AND quantity_sold IS NOT NULL\n",
    "  GROUP BY product_id, product_name\n",
    "  LIMIT 10  -- ✅ LIMIT 10 in first CTE (GROUP BY provides uniqueness)\n",
    "),\n",
    "-- Step 2: Calculate price elasticity metrics - keep numeric types as DOUBLE\n",
    "price_elasticity_metrics AS (\n",
    "  SELECT \n",
    "    product_id,\n",
    "    COALESCE(TRIM(product_name), 'Unknown Product') AS product_name,\n",
    "    COALESCE(ROUND(AVG(avg_price), 2), 0.0) AS avg_price,\n",
    "    COALESCE(ROUND(AVG(total_demand), 0), 0) AS avg_demand,\n",
    "    COALESCE(ROUND(REGR_SLOPE(total_demand, avg_price), 3), 0.0) AS price_elasticity_coefficient,\n",
    "    COALESCE(ROUND(REGR_R2(total_demand, avg_price), 3), 0.0) AS elasticity_r_squared,\n",
    "    COALESCE(ROUND(CORR(total_demand, avg_price), 3), 0.0) AS price_demand_correlation,\n",
    "    COALESCE(ROUND(STDDEV_POP(avg_price), 2), 0.0) AS price_volatility,\n",
    "    COALESCE(ROUND(STDDEV_POP(total_demand), 0), 0) AS demand_volatility\n",
    "  FROM price_demand_data\n",
    "  GROUP BY product_id, product_name\n",
    "),\n",
    "-- Step 3: Generate ai_sys_prompt for price elasticity analysis\n",
    "pricing_prompt_generation AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    CONCAT('You are a Pricing Strategy Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "           'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "           'Business priorities are: {enriched_business_priorities}. ',\n",
    "           'With 18 years of experience in revenue management and price optimization, ',\n",
    "           'your expertise in price elasticity analysis, dynamic pricing, and competitive positioning aligns with the strategic initiative: Revenue optimization. ',\n",
    "           'Analyze pricing for product: ', product_name, '. ',\n",
    "           'Average price: $', avg_price, '. ',\n",
    "           'Average demand: ', avg_demand, ' units. ',\n",
    "           'Price elasticity coefficient (slope): ', price_elasticity_coefficient, ' (negative = inelastic, more negative = elastic). ',\n",
    "           'R-squared (predictive power): ', elasticity_r_squared, '. ',\n",
    "           'Price-demand correlation: ', price_demand_correlation, '. ',\n",
    "           'Price volatility: $', price_volatility, '. ',\n",
    "           'Demand volatility: ', demand_volatility, ' units. ',\n",
    "           'Output ONLY a JSON object with NO markdown fences, NO extra text, JUST the JSON. ',\n",
    "           'Format: {{\"ai_cat_elasticity_classification\": \"value\", \"ai_cat_pricing_power\": \"value\", \"ai_cat_price_action\": \"value\", \"ai_cat_competitive_position\": \"value\", \"ai_txt_pricing_strategy\": \"text\", \"ai_txt_revenue_impact_forecast\": \"text\", \"ai_txt_implementation_plan\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "           'CATEGORICAL FIELDS (ai_cat_ prefix, exact values): ai_cat_elasticity_classification (Highly Inelastic|Inelastic|Unit Elastic|Elastic|Highly Elastic), ai_cat_pricing_power (Strong|Moderate|Weak|Minimal), ai_cat_price_action (Increase Price|Test Increase|Hold Steady|Test Decrease|Decrease Price), ai_cat_competitive_position (Premium|Value Leader|Competitive|Discount|Below Market). ',\n",
    "           'NARRATIVE FIELDS (ai_txt_ prefix): ai_txt_pricing_strategy, ai_txt_revenue_impact_forecast, ai_txt_implementation_plan. ',\n",
    "           'MANDATORY LAST 7 FIELDS: ai_txt_business_outcome (CALCULATED MEASURABLE IMPACT with Daily/Weekly/Monthly/Yearly breakdown and DISCLAIMER), ai_txt_executive_summary (references business outcome numbers), ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data. ',\n",
    "           'ai_sys_missing_data format: \"I can get higher confidence than [X]% if I can get access to [narrative]. {{\\\"missing_data\\\": [\\\"competitor_pricing\\\", \\\"cost_structure\\\", \\\"market_trends\\\"]}}\" ',\n",
    "           'Output ONLY the JSON object, nothing else.') AS ai_sys_prompt\n",
    "  FROM price_elasticity_metrics\n",
    "),\n",
    "-- Step 4: AI analysis - pass ai_sys_prompt to ai_query\n",
    "dynamic_pricing_strategy AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS pricing_strategy_json\n",
    "  FROM pricing_prompt_generation\n",
    "),\n",
    "-- Final output: Price elasticity analysis with strategic recommendations using ai_cat_, ai_txt_, ai_sys_ prefixes\n",
    "-- ai_sys_prompt MUST be the LAST column\n",
    "final_output AS (\n",
    "  SELECT \n",
    "    product_name,\n",
    "    avg_price,\n",
    "    avg_demand,\n",
    "    price_elasticity_coefficient,\n",
    "    elasticity_r_squared,\n",
    "    get_json_object(pricing_strategy_json, '$.ai_cat_elasticity_classification') AS ai_cat_elasticity_classification,\n",
    "    get_json_object(pricing_strategy_json, '$.ai_cat_pricing_power') AS ai_cat_pricing_power,\n",
    "    get_json_object(pricing_strategy_json, '$.ai_cat_price_action') AS ai_cat_price_action,\n",
    "    get_json_object(pricing_strategy_json, '$.ai_cat_competitive_position') AS ai_cat_competitive_position,\n",
    "    get_json_object(pricing_strategy_json, '$.ai_txt_pricing_strategy') AS ai_txt_pricing_strategy,\n",
    "    get_json_object(pricing_strategy_json, '$.ai_txt_revenue_impact_forecast') AS ai_txt_revenue_impact_forecast,\n",
    "    get_json_object(pricing_strategy_json, '$.ai_txt_implementation_plan') AS ai_txt_implementation_plan,\n",
    "    -- MANDATORY LAST 7 COLUMNS (including ai_txt_business_outcome + ai_sys_prompt):\n",
    "    COALESCE(get_json_object(pricing_strategy_json, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "    COALESCE(get_json_object(pricing_strategy_json, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "    COALESCE(get_json_object(pricing_strategy_json, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "    COALESCE(get_json_object(pricing_strategy_json, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "    COALESCE(TRY_CAST(get_json_object(pricing_strategy_json, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "    COALESCE(get_json_object(pricing_strategy_json, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "    COALESCE(get_json_object(pricing_strategy_json, '$.ai_sys_missing_data'), 'No missing data identified') AS ai_sys_missing_data,\n",
    "    ai_sys_prompt  -- ✅ LAST COLUMN: The exact prompt used for auditability\n",
    "  FROM dynamic_pricing_strategy\n",
    ")\n",
    "SELECT * FROM final_output\n",
    "-- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "-- WHERE ai_cat_elasticity_classification IN ('Highly Inelastic', 'Inelastic', 'Unit Elastic', 'Elastic', 'Highly Elastic')\n",
    "-- AND ai_cat_pricing_power IN ('Strong', 'Moderate', 'Weak', 'Minimal')\n",
    "-- AND ai_cat_price_action IN ('Increase Price', 'Test Increase', 'Hold Steady', 'Test Decrease', 'Decrease Price')\n",
    "-- AND ai_cat_competitive_position IN ('Premium', 'Value Leader', 'Competitive', 'Discount', 'Below Market')\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "\n",
    "**6. SESSIONIZATION - Group Events into Logical User Sessions**\n",
    "\n",
    "**Key SQL Functions:** LAG, SUM(CASE...) OVER, window functions\n",
    "**Business Value:** Accurately measures true engagement time and session-based user behavior patterns\n",
    "\n",
    "**Use Case Example Template: \"User Engagement Session Analysis with Retention Improvement Strategy\"**\n",
    "\n",
    "```sql\n",
    "-- Step 1: Identify session boundaries using time-based idle timeout\n",
    "WITH event_stream_with_gaps AS (\n",
    "  SELECT \n",
    "    user_id,\n",
    "    event_timestamp,\n",
    "    event_name,\n",
    "    LAG(event_timestamp) OVER (PARTITION BY user_id ORDER BY event_timestamp) AS prev_event_timestamp,\n",
    "    CASE \n",
    "      WHEN LAG(event_timestamp) OVER (PARTITION BY user_id ORDER BY event_timestamp) IS NULL THEN 1\n",
    "      WHEN (UNIX_TIMESTAMP(event_timestamp) - UNIX_TIMESTAMP(LAG(event_timestamp) OVER (PARTITION BY user_id ORDER BY event_timestamp))) / 60 > 30 THEN 1\n",
    "      ELSE 0\n",
    "    END AS is_new_session\n",
    "  FROM `catalog`.`schema`.`user_events` AS e\n",
    "  WHERE user_id IS NOT NULL\n",
    "    AND event_timestamp IS NOT NULL\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at END of first CTE only (window functions need ordered data)\n",
    "),\n",
    "-- Step 2: Assign session IDs and calculate session metrics with NULL safety\n",
    "user_session_metrics AS (\n",
    "  SELECT \n",
    "    user_id,\n",
    "    SUM(is_new_session) OVER (PARTITION BY user_id ORDER BY event_timestamp) AS session_id,\n",
    "    COUNT(*) OVER (PARTITION BY user_id, SUM(is_new_session) OVER (PARTITION BY user_id ORDER BY event_timestamp)) AS events_in_session,\n",
    "    (UNIX_TIMESTAMP(MAX(event_timestamp) OVER (PARTITION BY user_id, SUM(is_new_session) OVER (PARTITION BY user_id ORDER BY event_timestamp))) - \n",
    "     UNIX_TIMESTAMP(MIN(event_timestamp) OVER (PARTITION BY user_id, SUM(is_new_session) OVER (PARTITION BY user_id ORDER BY event_timestamp)))) / 60 AS session_duration_minutes\n",
    "  FROM event_stream_with_gaps\n",
    "),\n",
    "-- Step 3: Aggregate session-level metrics - keep numeric types as DOUBLE/INT\n",
    "session_summary_metrics AS (\n",
    "  SELECT \n",
    "    user_id,\n",
    "    COALESCE(COUNT(DISTINCT session_id), 0) AS total_sessions,\n",
    "    COALESCE(ROUND(AVG(events_in_session), 1), 0.0) AS avg_events_per_session,\n",
    "    COALESCE(ROUND(AVG(session_duration_minutes), 1), 0.0) AS avg_session_duration_minutes,\n",
    "    COALESCE(ROUND(MAX(session_duration_minutes), 1), 0.0) AS max_session_duration_minutes,\n",
    "    COALESCE(ROUND(STDDEV_POP(session_duration_minutes), 1), 0.0) AS session_duration_volatility,\n",
    "    COALESCE(COUNT(DISTINCT CASE WHEN events_in_session >= 10 THEN session_id END), 0) AS high_engagement_sessions\n",
    "  FROM user_session_metrics\n",
    "  GROUP BY user_id\n",
    "),\n",
    "-- Step 4: Generate ai_sys_prompt for session analysis\n",
    "session_prompt_generation AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    CONCAT('You are a User Engagement Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "           'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "           'Business priorities are: {enriched_business_priorities}. ',\n",
    "           'With 14 years of experience in product analytics and retention optimization, ',\n",
    "           'your expertise in session analysis, engagement scoring, and behavioral intervention design aligns with the strategic initiative: User retention. ',\n",
    "           'Analyze user ', user_id, ' engagement patterns. ',\n",
    "           'Total sessions: ', total_sessions, '. ',\n",
    "           'Average events per session: ', avg_events_per_session, '. ',\n",
    "           'Average session duration: ', avg_session_duration_minutes, ' minutes. ',\n",
    "           'Maximum session duration: ', max_session_duration_minutes, ' minutes. ',\n",
    "           'Session duration volatility: ', session_duration_volatility, ' minutes. ',\n",
    "           'High engagement sessions (10+ events): ', high_engagement_sessions, '. ',\n",
    "           'Output ONLY a JSON object with NO markdown fences, NO extra text, JUST the JSON. ',\n",
    "           'Format: {{\"ai_cat_engagement_level\": \"value\", \"ai_cat_user_segment\": \"value\", \"ai_cat_retention_risk\": \"value\", \"ai_cat_intervention_priority\": \"value\", \"ai_txt_engagement_strategy\": \"text\", \"ai_txt_product_recommendations\": \"text\", \"ai_txt_retention_tactics\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "           'CATEGORICAL FIELDS (ai_cat_ prefix, exact values): ai_cat_engagement_level (Highly Engaged|Moderately Engaged|Casual User|At Risk|Disengaged), ai_cat_user_segment (Power User|Regular User|Occasional User|Churning|Lost), ai_cat_retention_risk (Critical|High|Medium|Low|Secure), ai_cat_intervention_priority (Immediate|High|Medium|Low|None Needed). ',\n",
    "           'NARRATIVE FIELDS (ai_txt_ prefix): ai_txt_engagement_strategy, ai_txt_product_recommendations, ai_txt_retention_tactics. ',\n",
    "           'MANDATORY LAST 7 FIELDS: ai_txt_business_outcome (CALCULATED MEASURABLE IMPACT with Daily/Weekly/Monthly/Yearly breakdown and DISCLAIMER), ai_txt_executive_summary (references business outcome numbers), ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data. ',\n",
    "           'ai_sys_missing_data format: \"I can get higher confidence than [X]% if I can get access to [narrative]. {{\\\"missing_data\\\": [\\\"user_demographics\\\", \\\"device_data\\\", \\\"session_recordings\\\"]}}\" ',\n",
    "           'Output ONLY the JSON object, nothing else.') AS ai_sys_prompt\n",
    "  FROM session_summary_metrics\n",
    "),\n",
    "-- Step 5: AI analysis - pass ai_sys_prompt to ai_query\n",
    "session_engagement_strategy AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS engagement_strategy_json\n",
    "  FROM session_prompt_generation\n",
    "),\n",
    "-- Final output: Session engagement metrics with retention recommendations using ai_cat_, ai_txt_, ai_sys_ prefixes\n",
    "-- ai_sys_prompt MUST be the LAST column\n",
    "final_output AS (\n",
    "  SELECT \n",
    "    user_id,\n",
    "    total_sessions,\n",
    "    avg_events_per_session,\n",
    "    avg_session_duration_minutes,\n",
    "    get_json_object(engagement_strategy_json, '$.ai_cat_engagement_level') AS ai_cat_engagement_level,\n",
    "    get_json_object(engagement_strategy_json, '$.ai_cat_user_segment') AS ai_cat_user_segment,\n",
    "    get_json_object(engagement_strategy_json, '$.ai_cat_retention_risk') AS ai_cat_retention_risk,\n",
    "    get_json_object(engagement_strategy_json, '$.ai_cat_intervention_priority') AS ai_cat_intervention_priority,\n",
    "    get_json_object(engagement_strategy_json, '$.ai_txt_engagement_strategy') AS ai_txt_engagement_strategy,\n",
    "    get_json_object(engagement_strategy_json, '$.ai_txt_product_recommendations') AS ai_txt_product_recommendations,\n",
    "    get_json_object(engagement_strategy_json, '$.ai_txt_retention_tactics') AS ai_txt_retention_tactics,\n",
    "    -- MANDATORY LAST 7 COLUMNS (including ai_txt_business_outcome + ai_sys_prompt):\n",
    "    COALESCE(get_json_object(engagement_strategy_json, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "    COALESCE(get_json_object(engagement_strategy_json, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "    COALESCE(get_json_object(engagement_strategy_json, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "    COALESCE(get_json_object(engagement_strategy_json, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "    COALESCE(TRY_CAST(get_json_object(engagement_strategy_json, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "    COALESCE(get_json_object(engagement_strategy_json, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "    COALESCE(get_json_object(engagement_strategy_json, '$.ai_sys_missing_data'), 'No missing data identified') AS ai_sys_missing_data,\n",
    "    ai_sys_prompt  -- ✅ LAST COLUMN: The exact prompt used for auditability\n",
    "  FROM session_engagement_strategy\n",
    ")\n",
    "SELECT * FROM final_output\n",
    "-- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "-- WHERE ai_cat_engagement_level IN ('Highly Engaged', 'Moderately Engaged', 'Casual User', 'At Risk', 'Disengaged')\n",
    "-- AND ai_cat_user_segment IN ('Power User', 'Regular User', 'Occasional User', 'Churning', 'Lost')\n",
    "-- AND ai_cat_retention_risk IN ('Critical', 'High', 'Medium', 'Low', 'Secure')\n",
    "-- AND ai_cat_intervention_priority IN ('Immediate', 'High', 'Medium', 'Low', 'None Needed')\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "\n",
    "**7. BASKET AFFINITY ANALYSIS - Discover Product Purchase Patterns**\n",
    "\n",
    "**Key SQL Functions:** COLLECT_SET, SIZE, ARRAY_INTERSECT, ARRAY functions\n",
    "**Business Value:** Identifies which products are frequently purchased together, enabling bundling and cross-sell strategies\n",
    "\n",
    "**Use Case Example Template: \"Product Affinity Analysis with Cross-Sell Bundling Strategy\"**\n",
    "\n",
    "```sql\n",
    "-- Step 1: Create product baskets per transaction\n",
    "WITH transaction_baskets AS (\n",
    "  SELECT \n",
    "    transaction_id,\n",
    "    COLLECT_SET(product_id) AS product_basket,\n",
    "    COLLECT_SET(product_category) AS category_basket,\n",
    "    SUM(revenue) AS basket_value\n",
    "  FROM `catalog`.`schema`.`transaction_items` AS t\n",
    "  WHERE transaction_id IS NOT NULL\n",
    "    AND product_id IS NOT NULL\n",
    "  GROUP BY transaction_id\n",
    "  LIMIT 10  -- ✅ LIMIT 10 in first CTE (GROUP BY provides uniqueness)\n",
    "),\n",
    "-- Step 2: Explode baskets to get individual products with basket context\n",
    "product_affinity_metrics AS (\n",
    "  SELECT \n",
    "    explode(product_basket) AS anchor_product_id,\n",
    "    product_basket,\n",
    "    COALESCE(SIZE(product_basket), 0) AS basket_size,\n",
    "    COALESCE(ROUND(basket_value, 2), 0.0) AS basket_value\n",
    "  FROM transaction_baskets\n",
    "),\n",
    "-- Step 3: Aggregate metrics per product for affinity analysis\n",
    "product_affinity_summary AS (\n",
    "  SELECT \n",
    "    anchor_product_id,\n",
    "    COALESCE(ROUND(AVG(basket_size), 1), 0.0) AS basket_size,\n",
    "    COALESCE(ROUND(AVG(basket_value), 2), 0.0) AS basket_value,\n",
    "    COALESCE(COUNT(*), 0) AS occurrence_count\n",
    "  FROM product_affinity_metrics\n",
    "  GROUP BY anchor_product_id\n",
    "),\n",
    "-- Step 4: Generate ai_sys_prompt for bundling analysis\n",
    "bundling_prompt_generation AS (\n",
    "  SELECT \n",
    "    anchor_product_id,\n",
    "    basket_size,\n",
    "    basket_value,\n",
    "    CONCAT('You are a Merchandise Strategy Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "           'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "           'Business priorities are: {enriched_business_priorities}. ',\n",
    "           'With 16 years of experience in product bundling and cross-sell optimization, ',\n",
    "           'your expertise in basket analysis, bundle design, and revenue per transaction optimization aligns with the strategic initiative: Revenue optimization. ',\n",
    "           'Analyze product affinity for product ', anchor_product_id, '. ',\n",
    "           'Average basket size with this product: ', basket_size, ' items. ',\n",
    "           'Average basket value: $', basket_value, '. ',\n",
    "           'Output ONLY a JSON object with NO markdown fences, NO extra text, JUST the JSON. ',\n",
    "           'Format: {{\"ai_cat_affinity_strength\": \"value\", \"ai_cat_bundle_potential\": \"value\", \"ai_cat_cross_sell_priority\": \"value\", \"ai_cat_pricing_strategy\": \"value\", \"ai_txt_bundle_recommendation\": \"text\", \"ai_txt_cross_sell_tactics\": \"text\", \"ai_txt_expected_revenue_lift\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "           'CATEGORICAL FIELDS (ai_cat_ prefix, exact values): ai_cat_affinity_strength (Very Strong|Strong|Moderate|Weak|Minimal), ai_cat_bundle_potential (High - Create Bundle|Medium - Test Bundle|Low - Individual Cross-sell|Not Recommended), ai_cat_cross_sell_priority (Critical|High|Medium|Low|None), ai_cat_pricing_strategy (Premium Bundle|Value Bundle|Discount Bundle|No Bundle). ',\n",
    "           'NARRATIVE FIELDS (ai_txt_ prefix): ai_txt_bundle_recommendation, ai_txt_cross_sell_tactics, ai_txt_expected_revenue_lift. ',\n",
    "           'MANDATORY LAST 7 FIELDS: ai_txt_business_outcome (CALCULATED MEASURABLE IMPACT with Daily/Weekly/Monthly/Yearly breakdown and DISCLAIMER), ai_txt_executive_summary (references business outcome numbers), ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data. ',\n",
    "           'ai_sys_missing_data format: \"I can get higher confidence than [X]% if I can get access to [narrative]. {{\\\"missing_data\\\": [\\\"product_margins\\\", \\\"inventory_levels\\\", \\\"customer_segments\\\"]}}\" ',\n",
    "           'Output ONLY the JSON object, nothing else.') AS ai_sys_prompt\n",
    "  FROM product_affinity_summary\n",
    "),\n",
    "-- Step 5: AI analysis - pass ai_sys_prompt to ai_query\n",
    "bundling_cross_sell_strategy AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS bundling_strategy_json\n",
    "  FROM bundling_prompt_generation\n",
    "),\n",
    "-- Final output: Product affinity with bundling recommendations using ai_cat_, ai_txt_, ai_sys_ prefixes\n",
    "-- ai_sys_prompt MUST be the LAST column\n",
    "final_output AS (\n",
    "  SELECT \n",
    "    anchor_product_id,\n",
    "    basket_size AS avg_basket_size,\n",
    "    basket_value AS avg_basket_value,\n",
    "    get_json_object(bundling_strategy_json, '$.ai_cat_affinity_strength') AS ai_cat_affinity_strength,\n",
    "    get_json_object(bundling_strategy_json, '$.ai_cat_bundle_potential') AS ai_cat_bundle_potential,\n",
    "    get_json_object(bundling_strategy_json, '$.ai_cat_cross_sell_priority') AS ai_cat_cross_sell_priority,\n",
    "    get_json_object(bundling_strategy_json, '$.ai_cat_pricing_strategy') AS ai_cat_pricing_strategy,\n",
    "    get_json_object(bundling_strategy_json, '$.ai_txt_bundle_recommendation') AS ai_txt_bundle_recommendation,\n",
    "    get_json_object(bundling_strategy_json, '$.ai_txt_cross_sell_tactics') AS ai_txt_cross_sell_tactics,\n",
    "    get_json_object(bundling_strategy_json, '$.ai_txt_expected_revenue_lift') AS ai_txt_expected_revenue_lift,\n",
    "    -- MANDATORY LAST 7 COLUMNS (including ai_txt_business_outcome + ai_sys_prompt):\n",
    "    COALESCE(get_json_object(bundling_strategy_json, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "    COALESCE(get_json_object(bundling_strategy_json, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "    COALESCE(get_json_object(bundling_strategy_json, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "    COALESCE(get_json_object(bundling_strategy_json, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "    COALESCE(TRY_CAST(get_json_object(bundling_strategy_json, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "    COALESCE(get_json_object(bundling_strategy_json, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "    COALESCE(get_json_object(bundling_strategy_json, '$.ai_sys_missing_data'), 'No missing data identified') AS ai_sys_missing_data,\n",
    "    ai_sys_prompt  -- ✅ LAST COLUMN: The exact prompt used for auditability\n",
    "  FROM bundling_cross_sell_strategy\n",
    ")\n",
    "SELECT * FROM final_output\n",
    "-- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "-- WHERE ai_cat_affinity_strength IN ('Very Strong', 'Strong', 'Moderate', 'Weak', 'Minimal')\n",
    "-- AND ai_cat_bundle_potential IN ('High - Create Bundle', 'Medium - Test Bundle', 'Low - Individual Cross-sell', 'Not Recommended')\n",
    "-- AND ai_cat_cross_sell_priority IN ('Critical', 'High', 'Medium', 'Low', 'None')\n",
    "-- AND ai_cat_pricing_strategy IN ('Premium Bundle', 'Value Bundle', 'Discount Bundle', 'No Bundle')\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "\n",
    "**8. SEASONALITY & CYCLICALITY ANALYSIS - Identify Temporal Patterns**\n",
    "\n",
    "**Key SQL Functions:** DATE_TRUNC, EXTRACT, AVG, STDDEV, window functions\n",
    "**Business Value:** Optimizes inventory, staffing, and marketing spend based on predictable temporal patterns\n",
    "\n",
    "**Use Case Example Template: \"Sales Seasonality Analysis with Inventory Planning Strategy\"**\n",
    "\n",
    "```sql\n",
    "-- Step 1: Extract temporal dimensions and aggregate metrics\n",
    "WITH sales_temporal_analysis AS (\n",
    "  SELECT \n",
    "    DATE_TRUNC('month', sale_date) AS month,\n",
    "    EXTRACT(DAYOFWEEK FROM sale_date) AS day_of_week,\n",
    "    EXTRACT(WEEK FROM sale_date) AS week_of_year,\n",
    "    SUM(revenue) AS period_revenue,\n",
    "    COUNT(DISTINCT order_id) AS period_orders,\n",
    "    AVG(order_value) AS avg_order_value\n",
    "  FROM `catalog`.`schema`.`sales` AS s\n",
    "  WHERE sale_date IS NOT NULL\n",
    "    AND revenue IS NOT NULL\n",
    "  GROUP BY DATE_TRUNC('month', sale_date), EXTRACT(DAYOFWEEK FROM sale_date), EXTRACT(WEEK FROM sale_date)\n",
    "  LIMIT 10  -- ✅ LIMIT 10 in first CTE (GROUP BY provides uniqueness)\n",
    "),\n",
    "-- Step 2: Calculate seasonality metrics - keep numeric types as DOUBLE/INT\n",
    "seasonality_metrics AS (\n",
    "  SELECT \n",
    "    COALESCE(CAST(month AS STRING), 'Unknown Month') AS month_display,\n",
    "    COALESCE(day_of_week, 0) AS day_of_week,\n",
    "    COALESCE(ROUND(period_revenue, 2), 0.0) AS period_revenue,\n",
    "    COALESCE(period_orders, 0) AS period_orders,\n",
    "    COALESCE(ROUND(AVG(period_revenue) OVER (), 2), 0.0) AS avg_monthly_revenue,\n",
    "    COALESCE(ROUND(STDDEV_POP(period_revenue) OVER (), 2), 0.0) AS revenue_volatility,\n",
    "    COALESCE(ROUND((period_revenue - AVG(period_revenue) OVER ()) * 100.0 / NULLIF(AVG(period_revenue) OVER (), 0), 1), 0.0) AS variance_from_avg_pct,\n",
    "    COALESCE(ROUND(PERCENT_RANK() OVER (ORDER BY period_revenue), 3), 0.0) AS revenue_percentile_rank\n",
    "  FROM sales_temporal_analysis\n",
    "),\n",
    "-- Step 3: Generate ai_sys_prompt for seasonality analysis\n",
    "seasonal_prompt_generation AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    CONCAT('You are an Operations Planning Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "           'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "           'Business priorities are: {enriched_business_priorities}. ',\n",
    "           'With 17 years of experience in demand forecasting and seasonal inventory management, ',\n",
    "           'your expertise in seasonality analysis, capacity planning, and resource optimization aligns with the strategic initiative: Operational efficiency. ',\n",
    "           'Analyze sales period: Month ', month_display, ', Day of Week ', day_of_week, '. ',\n",
    "           'Period revenue: $', period_revenue, '. ',\n",
    "           'Period orders: ', period_orders, '. ',\n",
    "           'Average monthly revenue (baseline): $', avg_monthly_revenue, '. ',\n",
    "           'Revenue volatility (standard deviation): $', revenue_volatility, '. ',\n",
    "           'Variance from average: ', variance_from_avg_pct, '%. ',\n",
    "           'Revenue percentile rank: ', revenue_percentile_rank, '. ',\n",
    "           'Output ONLY a JSON object with NO markdown fences, NO extra text, JUST the JSON. ',\n",
    "           'Format: {{\"ai_cat_seasonal_pattern\": \"value\", \"ai_cat_demand_intensity\": \"value\", \"ai_cat_planning_action\": \"value\", \"ai_cat_resource_adjustment\": \"value\", \"ai_txt_inventory_strategy\": \"text\", \"ai_txt_staffing_recommendation\": \"text\", \"ai_txt_marketing_timing\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "           'CATEGORICAL FIELDS (ai_cat_ prefix, exact values): ai_cat_seasonal_pattern (Peak Season|High Season|Normal Season|Low Season|Off Season), ai_cat_demand_intensity (Extreme High|High|Moderate|Low|Very Low), ai_cat_planning_action (Aggressive Scale Up|Moderate Increase|Maintain Current|Scale Down|Minimal Operations), ai_cat_resource_adjustment (Increase 50%+|Increase 20-50%|Hold Steady|Reduce 20%|Reduce 50%+). ',\n",
    "           'NARRATIVE FIELDS (ai_txt_ prefix): ai_txt_inventory_strategy, ai_txt_staffing_recommendation, ai_txt_marketing_timing. ',\n",
    "           'MANDATORY LAST 7 FIELDS: ai_txt_business_outcome (CALCULATED MEASURABLE IMPACT with Daily/Weekly/Monthly/Yearly breakdown and DISCLAIMER), ai_txt_executive_summary (references business outcome numbers), ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data. ',\n",
    "           'ai_sys_missing_data format: \"I can get higher confidence than [X]% if I can get access to [narrative]. {{\\\"missing_data\\\": [\\\"historical_weather\\\", \\\"event_calendars\\\", \\\"economic_indicators\\\"]}}\" ',\n",
    "           'Output ONLY the JSON object, nothing else.') AS ai_sys_prompt\n",
    "  FROM seasonality_metrics\n",
    "),\n",
    "-- Step 4: AI analysis - pass ai_sys_prompt to ai_query\n",
    "seasonal_planning_strategy AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS seasonal_strategy_json\n",
    "  FROM seasonal_prompt_generation\n",
    "),\n",
    "-- Final output: Seasonality analysis with operational planning recommendations using ai_cat_, ai_txt_, ai_sys_ prefixes\n",
    "-- ai_sys_prompt MUST be the LAST column\n",
    "final_output AS (\n",
    "  SELECT \n",
    "    month_display AS month,\n",
    "    day_of_week,\n",
    "    period_revenue,\n",
    "    variance_from_avg_pct,\n",
    "    get_json_object(seasonal_strategy_json, '$.ai_cat_seasonal_pattern') AS ai_cat_seasonal_pattern,\n",
    "    get_json_object(seasonal_strategy_json, '$.ai_cat_demand_intensity') AS ai_cat_demand_intensity,\n",
    "    get_json_object(seasonal_strategy_json, '$.ai_cat_planning_action') AS ai_cat_planning_action,\n",
    "    get_json_object(seasonal_strategy_json, '$.ai_cat_resource_adjustment') AS ai_cat_resource_adjustment,\n",
    "    get_json_object(seasonal_strategy_json, '$.ai_txt_inventory_strategy') AS ai_txt_inventory_strategy,\n",
    "    get_json_object(seasonal_strategy_json, '$.ai_txt_staffing_recommendation') AS ai_txt_staffing_recommendation,\n",
    "    get_json_object(seasonal_strategy_json, '$.ai_txt_marketing_timing') AS ai_txt_marketing_timing,\n",
    "    -- MANDATORY LAST 7 COLUMNS (including ai_txt_business_outcome + ai_sys_prompt):\n",
    "    COALESCE(get_json_object(seasonal_strategy_json, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "    COALESCE(get_json_object(seasonal_strategy_json, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "    COALESCE(get_json_object(seasonal_strategy_json, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "    COALESCE(get_json_object(seasonal_strategy_json, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "    COALESCE(TRY_CAST(get_json_object(seasonal_strategy_json, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "    COALESCE(get_json_object(seasonal_strategy_json, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "    COALESCE(get_json_object(seasonal_strategy_json, '$.ai_sys_missing_data'), 'No missing data identified') AS ai_sys_missing_data,\n",
    "    ai_sys_prompt  -- ✅ LAST COLUMN: The exact prompt used for auditability\n",
    "  FROM seasonal_planning_strategy\n",
    ")\n",
    "SELECT * FROM final_output\n",
    "-- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "-- WHERE ai_cat_seasonal_pattern IN ('Peak Season', 'High Season', 'Normal Season', 'Low Season', 'Off Season')\n",
    "-- AND ai_cat_demand_intensity IN ('Extreme High', 'High', 'Moderate', 'Low', 'Very Low')\n",
    "-- AND ai_cat_planning_action IN ('Aggressive Scale Up', 'Moderate Increase', 'Maintain Current', 'Scale Down', 'Minimal Operations')\n",
    "-- AND ai_cat_resource_adjustment IN ('Increase 50%+', 'Increase 20-50%', 'Hold Steady', 'Reduce 20%', 'Reduce 50%+')\n",
    "ORDER BY month\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL USAGE REQUIREMENTS FOR ADVANCED ANALYSIS PATTERNS \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "1. **CUSTOMIZE TO BUSINESS CONTEXT**: The 8 examples above are TEMPLATES ONLY. You MUST adapt them to the specific:\n",
    "   - Business domain (retail, healthcare, finance, manufacturing, etc.)\n",
    "   - Available data schema (actual table/column names)\n",
    "   - Business questions being asked\n",
    "   - Strategic objectives\n",
    "\n",
    "2. **GENERATE NOVEL USE CASES**: Ask the LLM to generate NEW use case ideas that utilize these patterns. Examples:\n",
    "   - \"Generate a use case that combines Cohort Analysis + Pareto Analysis to identify high-value customer segments by acquisition channel\"\n",
    "   - \"Generate a use case that uses Funnel Analysis + Sessionization to optimize mobile app conversion rates\"\n",
    "   - \"Generate a use case that applies Price Elasticity + Seasonality Analysis for dynamic pricing optimization\"\n",
    "\n",
    "3. **COMBINE MULTIPLE TECHNIQUES**: Many high-value use cases require combining 2-3 analytical techniques:\n",
    "   - Cohort + Pareto: \"Which customer cohorts drive 80% of LTV growth?\"\n",
    "   - Funnel + Sessionization: \"How does session engagement impact conversion rates?\"\n",
    "   - Gap Analysis + Seasonality: \"Which seasonal gaps represent the biggest missed revenue opportunities?\"\n",
    "\n",
    "4. **ALWAYS INCLUDE AI INTERPRETATION**: EVERY statistical analysis MUST be followed by ai_query to:\n",
    "   - Interpret the statistical findings in business language\n",
    "   - Generate strategic recommendations and action plans\n",
    "   - Identify risks, opportunities, and priorities\n",
    "   - Provide executive-level insights\n",
    "\n",
    "5. **PROPER USE CASE NAMING**: Use clear, specific names that describe BOTH the analysis AND the outcome:\n",
    "   - ✅ GOOD: \"Customer Acquisition Quality Analysis with Retention Strategy\"\n",
    "   - ✅ GOOD: \"Revenue Concentration Analysis with Portfolio Optimization Strategy\"\n",
    "   - ✅ GOOD: \"Checkout Conversion Funnel Analysis with Friction Point Recommendations\"\n",
    "   - ❌ BAD: \"Cohort Analysis\" (too generic, no business outcome)\n",
    "   - ❌ BAD: \"Pareto Analysis of Sales\" (missing strategic outcome)\n",
    "\n",
    "6. **MANDATORY CATEGORICAL + NARRATIVE STRUCTURE**: Every ai_query output MUST include:\n",
    "   - 3-5 categorical fields for filtering/dashboards (with max 20 distinct values each)\n",
    "   - 2-4 narrative fields for detailed explanations\n",
    "   - Persona instruction (role + years of experience + expertise areas)\n",
    "   - Strict JSON output formatting\n",
    "\n",
    "7. **NULL SAFETY REQUIRED**: ALL statistical metrics and data fields used in ai_query prompts MUST be:\n",
    "   - ROUND'd (for numeric precision)\n",
    "   - COALESCE'd with type-appropriate defaults (DOUBLE: 0.0, INT: 0, STRING: 'Unknown')\n",
    "   - Keep numeric types as DOUBLE/INT (CONCAT auto-converts them)\n",
    "   - Transformed in the FIRST CTE (not inline in CONCAT)\n",
    "\n",
    "**\uD83C\uDFAF WHEN TO USE THESE PATTERNS:**\n",
    "\n",
    "Use these advanced patterns when the use case description contains keywords like:\n",
    "- \"cohort\", \"acquisition\", \"retention\", \"lifetime value\" → Cohort Analysis\n",
    "- \"concentration\", \"80/20\", \"top performers\", \"vital few\" → Pareto Analysis\n",
    "- \"conversion\", \"drop-off\", \"funnel\", \"journey\" → Funnel Analysis\n",
    "- \"coverage\", \"gaps\", \"missing\", \"opportunities\" → Gap Analysis\n",
    "- \"pricing\", \"elasticity\", \"demand response\" → Price Elasticity Analysis\n",
    "- \"session\", \"engagement\", \"time spent\" → Sessionization\n",
    "- \"basket\", \"bundling\", \"cross-sell\", \"affinity\" → Basket Affinity\n",
    "- \"seasonal\", \"cyclical\", \"temporal patterns\" → Seasonality Analysis\n",
    "\n",
    "**\uD83D\uDD25 REMEMBER: These techniques answer \"WHY?\" and \"WHO?\" - not just \"WHAT?\" - delivering 10X higher business value than simple aggregations. \uD83D\uDD25**\n",
    "\n",
    "---\n",
    "\n",
    "**Creative Prompt Examples with STRICT JSON OUTPUT:**\n",
    "```sql\n",
    "-- Personalized recommendations (using mandatory default model)\n",
    "-- NOTE: customer_name and purchase_history must be COALESCEd in the source CTE\n",
    "ai_query('{sql_model_serving}', \n",
    "  CONCAT('Analyze customer ', customer_name, '''s purchase history: ', \n",
    "         purchase_history, '. ',\n",
    "         'Output ONLY a JSON object with NO markdown, NO extra text. ',\n",
    "         'Format: {{\"recommendations\": [\"product1\", \"product2\", \"product3\"], \"reasoning\": \"text\"}}. ',\n",
    "         'Output ONLY the JSON, nothing else.'))\n",
    "\n",
    "-- Risk assessment (using general-purpose LLM) - CONCAT auto-converts DOUBLE\n",
    "-- NOTE: amount, location, behavior_score must be COALESCEd in the source CTE\n",
    "ai_query('{sql_model_serving}',\n",
    "  CONCAT('Assess transaction fraud risk: Amount=$', amount,  -- CONCAT auto-converts DOUBLE\n",
    "         ', Location=', location, ', User behavior=', behavior_score, '. ',\n",
    "         'Output ONLY JSON with NO markdown: {{\"risk_level\": \"High/Medium/Low\", \"score\": 0-100, \"factors\": \"text\"}}'))\n",
    "\n",
    "-- Dynamic report generation with structured output (using mandatory default model)\n",
    "-- NOTE: department, revenue, growth_pct must be COALESCEd in the source CTE\n",
    "ai_query('{sql_model_serving}',\n",
    "  CONCAT('Create executive summary for ', department, \n",
    "         ': Revenue=', revenue,  -- CONCAT auto-converts DOUBLE\n",
    "         ', Growth=', growth_pct, '%. ',\n",
    "         'Output ONLY a JSON object with NO markdown, NO extra text. ',\n",
    "         'Format: {{\"summary\": \"text\", \"key_metrics\": \"text\", \"recommendations\": \"text\"}}. ',\n",
    "         'Output ONLY the JSON, nothing else.'))\n",
    "```\n",
    "\n",
    "**\uD83D\uDEA8 REMEMBER: Every ai_query for structured data MUST include:**\n",
    "- \"Output ONLY a JSON object\"\n",
    "- \"with NO markdown\" or \"NO markdown fences\"\n",
    "- \"NO extra text\"\n",
    "- Show example format\n",
    "- \"Output ONLY the JSON, nothing else\"\n",
    "\n",
    "#### 6. **COMPLETE AI FUNCTION REFERENCE**\n",
    "\n",
    "All Databricks AI functions with correct syntax:\n",
    "{ai_functions_summary}\n",
    "\n",
    "#### 7. **QUERY STRUCTURE - ABSOLUTE REQUIREMENTS**\n",
    "\n",
    "**A. LIMIT 10 AND DISTINCT REQUIREMENTS (ABSOLUTE CRITICAL):**\n",
    "- **FIRST CTE MUST USE SELECT DISTINCT**: Always use `SELECT DISTINCT` to eliminate duplicate records\n",
    "- **FIRST CTE ONLY**: Use `LIMIT 10` at the END of the FIRST CTE that reads from tables\n",
    "- **NO LIMIT IN OTHER CTEs**: DO NOT use `LIMIT 10` in any other CTE - only in the first CTE\n",
    "- **LIMIT PLACEMENT**: LIMIT 10 MUST be the LAST clause in the SELECT (after WHERE, ORDER BY, GROUP BY, etc.)\n",
    "\n",
    "**EXCEPTION FOR ai_forecast**: The input CTE for ai_forecast should use WHERE clause with date filtering to provide sufficient historical data (10:1 ratio). No LIMIT for forecast input CTEs.\n",
    "\n",
    "```sql\n",
    "-- Standard multi-CTE pattern ✅ (DISTINCT + LIMIT 10 in first CTE only)\n",
    "WITH stage1 AS (\n",
    "  SELECT DISTINCT *  -- ✅ ALWAYS use DISTINCT to eliminate duplicates\n",
    "  FROM `catalog`.`schema`.`table` AS t\n",
    "  WHERE primary_key_col IS NOT NULL  -- Use actual column name from schema, not generic 'id'\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at the END of first CTE only\n",
    "),\n",
    "stage2 AS (\n",
    "  SELECT *, ai_function(...) FROM stage1  -- ✅ NO LIMIT in other CTEs\n",
    "),\n",
    "stage3 AS (\n",
    "  SELECT * FROM stage2  -- ✅ NO LIMIT in other CTEs\n",
    ")\n",
    "SELECT * FROM stage3;  -- ✅ NO LIMIT in final SELECT\n",
    "\n",
    "-- ai_forecast exception ✅ - Use WHERE clause with adaptive ratio by granularity\n",
    "-- Example 1: Monthly forecast (mid-frequency, use 10:1 ratio)\n",
    "WITH past AS (\n",
    "  SELECT \n",
    "    DATE_TRUNC('month', date_col) AS ds,\n",
    "    SUM(value_col) AS value\n",
    "  FROM `catalog`.`schema`.`table` AS t\n",
    "  WHERE date_col >= date_add(MONTH, -30, CURRENT_DATE())  -- 30 months history for 3-month forecast (10:1 ratio)\n",
    "    AND date_col IS NOT NULL\n",
    "  GROUP BY DATE_TRUNC('month', date_col)\n",
    "  ORDER BY ds\n",
    ")\n",
    "SELECT * FROM AI_FORECAST(\n",
    "  TABLE(past), \n",
    "  time_col => 'ds',\n",
    "  value_col => 'value',\n",
    "  horizon => (SELECT date_add(MONTH, 3, MAX(ds)) FROM past)  -- 3 months ahead\n",
    ");  -- ✅ NO LIMIT\n",
    "\n",
    "-- Example 2: Hourly forecast (high-frequency, use 4 weeks for 24-hour forecast)\n",
    "WITH past AS (\n",
    "  SELECT \n",
    "    DATE_TRUNC('hour', timestamp_col) AS ds,\n",
    "    AVG(value_col) AS value\n",
    "  FROM `catalog`.`schema`.`table` AS t\n",
    "  WHERE timestamp_col >= date_add(WEEK, -4, CURRENT_TIMESTAMP())  -- 4 weeks for hourly data\n",
    "    AND timestamp_col IS NOT NULL\n",
    "  GROUP BY DATE_TRUNC('hour', timestamp_col)\n",
    "  ORDER BY ds\n",
    ")\n",
    "SELECT * FROM AI_FORECAST(\n",
    "  TABLE(past), \n",
    "  time_col => 'ds',\n",
    "  value_col => 'value',\n",
    "  horizon => (SELECT date_add(HOUR, 24, MAX(ds)) FROM past)  -- 24 hours ahead\n",
    ");  -- ✅ NO LIMIT\n",
    "\n",
    "-- Example 3: Yearly forecast (low-frequency, use 12 years for 3-year forecast)\n",
    "WITH past AS (\n",
    "  SELECT \n",
    "    DATE_TRUNC('year', date_col) AS ds,\n",
    "    SUM(value_col) AS value\n",
    "  FROM `catalog`.`schema`.`table` AS t\n",
    "  WHERE date_col >= date_add(YEAR, -12, CURRENT_DATE())  -- 12 years for 3-year forecast (4:1 ratio)\n",
    "    AND date_col IS NOT NULL\n",
    "  GROUP BY DATE_TRUNC('year', date_col)\n",
    "  ORDER BY ds\n",
    ")\n",
    "SELECT * FROM AI_FORECAST(\n",
    "  TABLE(past), \n",
    "  time_col => 'ds',\n",
    "  value_col => 'value',\n",
    "  horizon => (SELECT date_add(YEAR, 3, MAX(ds)) FROM past)  -- 3 years ahead\n",
    ");  -- ✅ NO LIMIT\n",
    "```\n",
    "\n",
    "**B. WHERE CLAUSE RESTRICTIONS (ABSOLUTE CRITICAL):**\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 ZERO TOLERANCE FOR VALUE FILTERING \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**YOU MUST NOT USE WHERE clauses to filter on specific values** because you don't know what values exist in the data:\n",
    "\n",
    "**ALLOWED:**\n",
    "- ✅ `WHERE column IS NULL`\n",
    "- ✅ `WHERE column IS NOT NULL`\n",
    "- ✅ `WHERE date_column IS NOT NULL` (ensure data exists)\n",
    "- ✅ **EXCEPTION FOR ai_forecast ONLY**: Dynamic date filtering for history with adaptive ratios:\n",
    "  - High-frequency (minute/hour): `WHERE ts >= date_add(DAY/WEEK, -N, CURRENT_TIMESTAMP())`\n",
    "  - Mid-frequency (day/week/month): `WHERE date >= date_add(UNIT, -N*10, CURRENT_DATE())`\n",
    "  - Low-frequency (quarter/year): `WHERE date >= date_add(UNIT, -N*ratio, CURRENT_DATE())` (see ratio table)\n",
    "\n",
    "**ABSOLUTELY FORBIDDEN (Value Comparisons):**\n",
    "- ❌ `WHERE status = 'active'` (you don't know if 'active' exists!)\n",
    "- ❌ `WHERE category = 'electronics'` (you don't know if 'electronics' exists!)\n",
    "- ❌ `WHERE date > '2023-01-01'` (arbitrary date filtering)\n",
    "- ❌ `WHERE amount > 100` (arbitrary number filtering)\n",
    "- ❌ `WHERE id = 5` (specific ID filtering)\n",
    "- ❌ `WHERE name LIKE '%pattern%'` (pattern matching on unknown values)\n",
    "- ❌ `WHERE region IN ('US', 'EU')` (you don't know valid regions!)\n",
    "- ❌ `WHERE price BETWEEN 10 AND 100` (arbitrary range)\n",
    "- ❌ ANY comparison with specific string, number, or date values\n",
    "\n",
    "**WHY THIS IS CRITICAL:**\n",
    "- You have ZERO knowledge of actual data values in tables\n",
    "- Filtering on non-existent values returns empty results\n",
    "- Use LIMIT instead of WHERE for controlling result size\n",
    "- Let users apply their own filters after seeing the data\n",
    "\n",
    "**EXCEPTION FOR ai_forecast:**\n",
    "- For ai_forecast input CTEs ONLY, you MUST use dynamic date filtering with ADAPTIVE ratios based on time granularity\n",
    "- Ratios vary by frequency: high-frequency (minute/hour) needs fixed calendar periods, mid-frequency uses 10:1, low-frequency uses reduced ratios\n",
    "\n",
    "**HIGH-FREQUENCY (minute/hour) - Use Fixed Calendar Periods:**\n",
    "- MINUTE-level: For 1-hour forecast, use `WHERE ts >= date_add(DAY, -7, CURRENT_TIMESTAMP())` (7 days)\n",
    "- HOURLY-level: For 24-hour forecast, use `WHERE ts >= date_add(WEEK, -4, CURRENT_TIMESTAMP())` (4 weeks)\n",
    "\n",
    "**MID-FREQUENCY (day/week/month) - Use 10:1 Ratio:**\n",
    "- DAILY: For 7-day forecast, use `WHERE date >= date_add(DAY, -70, CURRENT_DATE())` (70 days = 7 * 10)\n",
    "- WEEKLY: For 12-week forecast, use `WHERE date >= date_add(WEEK, -120, CURRENT_DATE())` (120 weeks = 12 * 10)\n",
    "- MONTHLY: For 12-month forecast, use `WHERE date >= date_add(MONTH, -120, CURRENT_DATE())` (120 months = 12 * 10) OR minimum 36 months if data unavailable\n",
    "\n",
    "**LOW-FREQUENCY (quarter/year) - Use Reduced Ratios:**\n",
    "- QUARTERLY: For 4-quarter forecast, use `WHERE date >= date_add(QUARTER, -32, CURRENT_DATE())` (32 quarters = 4 * 8, ~8 years)\n",
    "- YEARLY: For 3-year forecast, use `WHERE date >= date_add(YEAR, -12, CURRENT_DATE())` (12 years = 3 * 4) OR minimum 6 years for 2 cycles\n",
    "\n",
    "**Rationale**: You don't have knowledge of actual data values. Only NULL checks (IS NULL / IS NOT NULL) and dynamic date filtering for ai_forecast are allowed.\n",
    "\n",
    "**C. TABLE QUALIFICATION:**\n",
    "- Always use fully qualified names: `catalog.schema.table`\n",
    "- Extract these from the \"Tables Involved\" field\n",
    "- Use backticks if needed: `` `catalog`.`schema`.`table` ``\n",
    "\n",
    "**D. QUERY HEADER:**\n",
    "- Start with: `-- Use Case {{use_case_id}}: {{use_case_name}}`\n",
    "- Add comment describing the approach\n",
    "- Example:\n",
    "  ```sql\n",
    "  -- Use Case AI-F01-U01: Classify Customer Feedback  \n",
    "  -- Uses ai_classify to categorize feedback into business-relevant topics\n",
    "  ```\n",
    "\n",
    "**E. USE CTEs FOR MULTI-STAGE PROCESSING:**\n",
    "\n",
    "**WHEN TO USE CTEs:**\n",
    "- AI function pipelines with multiple stages (forecast → classify → generate recommendations)\n",
    "- Complex queries that need JOIN followed by transformations\n",
    "- Queries with multiple AI functions that build on each other\n",
    "- When you need intermediate results for clarity and debugging\n",
    "\n",
    "**WHEN TO AVOID CTEs:**\n",
    "- Simple single-table queries (just use SELECT ... FROM ... WHERE ... LIMIT)\n",
    "- Queries with only one transformation step\n",
    "- When a direct SELECT would be clearer and simpler\n",
    "\n",
    "**\uD83D\uDD25 CRITICAL: JOIN ALL TABLES UPFRONT IN FIRST CTE \uD83D\uDD25**\n",
    "\n",
    "**OPTIMAL CTE STRUCTURE (for AI pipelines):**\n",
    "1. **Step 1 (base data)**: SELECT all needed columns and JOIN all required tables together\n",
    "2. **Step 2 (ai function)**: Apply AI function (ai_classify, ai_forecast, etc.) on the combined data\n",
    "3. **Step 3 (enrichment)**: Use ai_gen/ai_query to add structured recommendations\n",
    "4. **Step 4 (extraction)**: Extract JSON fields using get_json_object()\n",
    "\n",
    "**CORRECT Pattern - All JOINs in First CTE:**\n",
    "```sql\n",
    "-- CORRECT ✅ - All JOINs done upfront\n",
    "-- Step 1: Get all required data with JOINs\n",
    "WITH base_data AS (\n",
    "  SELECT \n",
    "    c.customer_id,\n",
    "    c.customer_name,\n",
    "    c.customer_segment,\n",
    "    o.total_orders,\n",
    "    o.total_revenue,\n",
    "    p.preferred_products,\n",
    "    s.support_tickets\n",
    "  FROM `catalog`.`schema`.`customers` AS c\n",
    "  LEFT JOIN `catalog`.`schema`.`orders_summary` AS o ON c.customer_id = o.customer_id\n",
    "  LEFT JOIN `catalog`.`schema`.`preferences` AS p ON c.customer_id = p.customer_id\n",
    "  LEFT JOIN `catalog`.`schema`.`support_stats` AS s ON c.customer_id = s.customer_id\n",
    "  WHERE c.customer_id IS NOT NULL\n",
    "  LIMIT 10  -- ✅ LIMIT 10 in first CTE only\n",
    "),\n",
    "-- Step 2: Apply AI classification with ai_cat_ prefix\n",
    "classified AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_classify(customer_segment, ARRAY('VIP', 'High Value', 'Medium', 'Low')) AS ai_cat_value_tier\n",
    "  FROM base_data\n",
    "),\n",
    "-- Step 3: Generate ai_sys_prompt for recommendations\n",
    "prompt_generation AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    CONCAT('You are a Customer Success Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "           'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "           'Business priorities are: {enriched_business_priorities}. ',\n",
    "           'With 15 years of experience in customer retention and value optimization, ',\n",
    "           'your expertise aligns with the strategic initiative: Customer success. ',\n",
    "           'Analyze customer ', customer_name, ' (ID: ', customer_id, ') in segment ', customer_segment, '. ',\n",
    "           'Output ONLY JSON, NO markdown, NO extra text. Format: {{\"ai_txt_retention_strategy\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "           'MANDATORY LAST 7 FIELDS: ai_txt_business_outcome (CALCULATED MEASURABLE IMPACT with Daily/Weekly/Monthly/Yearly breakdown and DISCLAIMER), ai_txt_executive_summary (references business outcome numbers), ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data.') AS ai_sys_prompt\n",
    "  FROM classified\n",
    "),\n",
    "-- Step 4: Generate recommendations with ai_txt_, ai_sys_ columns\n",
    "enriched AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS insights\n",
    "  FROM prompt_generation\n",
    ")\n",
    "-- Step 5: Extract JSON fields with ai_cat_, ai_txt_, ai_sys_ prefixes\n",
    "-- ai_sys_prompt MUST be the LAST column\n",
    "SELECT \n",
    "  customer_id,\n",
    "  customer_name,\n",
    "  ai_cat_value_tier,\n",
    "  get_json_object(insights, '$.ai_txt_retention_strategy') AS ai_txt_retention_strategy,\n",
    "  -- MANDATORY LAST 7 COLUMNS (including ai_txt_business_outcome + ai_sys_prompt):\n",
    "  COALESCE(get_json_object(insights, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "  COALESCE(get_json_object(insights, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "  COALESCE(get_json_object(insights, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "  COALESCE(get_json_object(insights, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "  COALESCE(TRY_CAST(get_json_object(insights, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "  COALESCE(get_json_object(insights, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "  COALESCE(get_json_object(insights, '$.ai_sys_missing_data'), 'No missing data identified') AS ai_sys_missing_data,\n",
    "  ai_sys_prompt  -- ✅ LAST COLUMN: The exact prompt used for auditability\n",
    "FROM enriched\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "\n",
    "**WRONG Pattern - Multiple Join CTEs:**\n",
    "```sql\n",
    "-- WRONG ❌ - Joining tables in separate CTEs (inefficient!)\n",
    "-- Step 1: Get base data\n",
    "WITH base_data AS (\n",
    "  SELECT * FROM `catalog`.`schema`.`customers` AS c LIMIT 10\n",
    "),\n",
    "-- Step 2: Join with orders (BAD - should have been in Step 1!)\n",
    "with_orders AS (\n",
    "  SELECT b.*, o.total_orders\n",
    "  FROM base_data AS b\n",
    "  LEFT JOIN `catalog`.`schema`.`orders_summary` AS o ON b.customer_id = o.customer_id\n",
    "),\n",
    "-- Step 3: Join with preferences (BAD - should have been in Step 1!)\n",
    "with_preferences AS (\n",
    "  SELECT w.*, p.preferred_products\n",
    "  FROM with_orders AS w\n",
    "  LEFT JOIN `catalog`.`schema`.`preferences` AS p ON w.customer_id = p.customer_id\n",
    ")\n",
    "SELECT * FROM with_preferences;  -- ✅ NO LIMIT\n",
    "```\n",
    "\n",
    "**MANDATORY CTE STRUCTURE RULES:**\n",
    "1. **First CTE = All Data Acquisition**: Do ALL JOINs here to get complete dataset\n",
    "2. **Subsequent CTEs = Transformations**: Apply AI functions, enrichments, and extractions\n",
    "3. **Minimize Unnecessary CTEs**: Use CTEs for AI pipelines and complex logic, but avoid CTEs for simple single-table queries\n",
    "4. **One JOIN Phase**: All JOINs should happen in the first CTE, not scattered across multiple CTEs\n",
    "\n",
    "Prefer CTEs over nested subqueries for readability:\n",
    "```sql\n",
    "-- GOOD ✅\n",
    "WITH base AS (...),\n",
    "     enriched AS (...),\n",
    "     final AS (...)\n",
    "SELECT * FROM final;  -- ✅ NO LIMIT\n",
    "\n",
    "-- AVOID ❌\n",
    "SELECT * FROM (SELECT * FROM (SELECT * FROM table))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. COMPLETE WORKING EXAMPLES\n",
    "\n",
    "**Example 1: ai_classify + ai_query with Structured JSON Output (MANDATORY PATTERN)**\n",
    "```sql\n",
    "-- CREATE VIEW inspire_ai.default.customer_feedback_actionable_recommendations AS\n",
    "-- Use Case: Classify Customer Feedback with Structured Actionable Recommendations\n",
    "\n",
    "-- Step 1: Classify feedback into business-relevant categories\n",
    "-- \uD83D\uDEA8 EVERY column must be COALESCE'd or have IS NOT NULL check!\n",
    "WITH classified AS (\n",
    "  SELECT DISTINCT\n",
    "    feedback_id,                                              -- CRITICAL: filtered with IS NOT NULL\n",
    "    customer_id,                                              -- CRITICAL: filtered with IS NOT NULL\n",
    "    feedback_text,                                            -- CRITICAL: filtered with IS NOT NULL\n",
    "    COALESCE(customer_lifetime_value, 0.0) AS customer_lifetime_value,  -- ✅ COALESCE'd\n",
    "    COALESCE(purchase_count, 0) AS purchase_count,            -- ✅ COALESCE'd\n",
    "    ai_classify(feedback_text, \n",
    "      ARRAY('Product Quality', 'Customer Service', 'Pricing', 'Delivery', 'Other')\n",
    "    ) AS ai_cat_feedback_category\n",
    "  FROM `main`.`customer_service`.`feedback` AS f\n",
    "  WHERE feedback_id IS NOT NULL\n",
    "    AND customer_id IS NOT NULL    -- ✅ Critical identifier filtered\n",
    "    AND feedback_text IS NOT NULL\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at END of first CTE only\n",
    "),\n",
    "-- Step 2: Generate structured JSON output with ai_cat_ + ai_txt_ + ai_sys_ columns\n",
    "enriched AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_query('{sql_model_serving}',\n",
    "      CONCAT('Analyze this ', ai_cat_feedback_category, ' feedback: \"', feedback_text, \n",
    "             '\" from a customer with $', customer_lifetime_value,  -- CONCAT auto-converts\n",
    "             ' lifetime value and ', purchase_count, ' purchases. ',\n",
    "             'Output ONLY a JSON object with NO markdown fences, NO extra text, JUST the JSON. ',\n",
    "             'Format: {{\"ai_cat_urgency_level\": \"value\", \"ai_cat_response_priority\": \"value\", \"ai_txt_category_justification\": \"text\", \"ai_txt_resolution_plan\": \"text\", \"ai_txt_prevention_strategy\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "             'Required keys: ',\n",
    "             'ai_cat_urgency_level (MUST be exactly one of: Critical/High/Medium/Low), ',\n",
    "             'ai_cat_response_priority (MUST be exactly one of: Immediate Action/High Priority/Medium Priority/Low Priority), ',\n",
    "             'ai_txt_category_justification (free text: why this feedback belongs to ', ai_cat_feedback_category, ', specific keywords/phrases, 1-2 sentences), ',\n",
    "             'ai_txt_resolution_plan (free text: specific 3-step resolution plan with actionable steps), ',\n",
    "             'ai_txt_prevention_strategy (free text: long-term strategy to prevent similar issues). ',\n",
    "             'MANDATORY LAST 7 FIELDS (in this exact order): ',\n",
    "             '1) ai_txt_business_outcome - CALCULATED MEASURABLE BUSINESS IMPACT. Example: \"Resolving issue for Customer [ID] prevents $X revenue loss. Escalation cost avoided: $Y. Breakdown: Daily impact: $X | Weekly: $X | Monthly: $X | Yearly: $X. DISCLAIMER: All numbers are AI estimates based on available data and must be validated by domain experts before business decisions.\" MUST include breakdown and disclaimer. ',\n",
    "             '2) ai_txt_executive_summary - compelling business story in 2-3 sentences that REFERENCES the business outcome numbers, ',\n",
    "             '3) ai_sys_importance - business importance level (MUST be exactly one of: Very Low, Low, Medium, High, Very High, Critical), ',\n",
    "             '4) ai_sys_urgency - action urgency level (MUST be exactly one of: Very Low, Low, Medium, High, Very High, Critical), ',\n",
    "             '5) ai_sys_confidence (0.0-1.0) - your confidence score, ',\n",
    "             '6) ai_sys_feedback - start with \"I assessed my confidence at [X]% because...\" then explain reasoning, ',\n",
    "             '7) ai_sys_missing_data - MUST follow format: \"I can get higher confidence than [X]% if I can get access to [narrative about missing data]. {{\\\"missing_data\\\": [\\\"specific_dataset1\\\", \\\"specific_dataset2\\\"]}}\" - always end with JSON listing needed datasets. ',\n",
    "             'Output ONLY the JSON object, nothing else.')\n",
    "    ) AS insights\n",
    "  FROM classified\n",
    "),\n",
    "-- Final output: Mix of ai_cat_ (filterable), ai_txt_ (narrative), and ai_sys_ (system) columns\n",
    "final_output AS (\n",
    "  SELECT \n",
    "    feedback_id,\n",
    "    customer_id,\n",
    "    feedback_text,\n",
    "    ai_cat_feedback_category,\n",
    "    get_json_object(insights, '$.ai_cat_urgency_level') AS ai_cat_urgency_level,  -- CATEGORICAL: Critical/High/Medium/Low\n",
    "    get_json_object(insights, '$.ai_cat_response_priority') AS ai_cat_response_priority,  -- CATEGORICAL: Immediate Action/High Priority/Medium Priority/Low Priority\n",
    "    get_json_object(insights, '$.ai_txt_category_justification') AS ai_txt_category_justification,  -- NARRATIVE: Free text\n",
    "    get_json_object(insights, '$.ai_txt_resolution_plan') AS ai_txt_resolution_plan,  -- NARRATIVE: Free text\n",
    "    get_json_object(insights, '$.ai_txt_prevention_strategy') AS ai_txt_prevention_strategy,  -- NARRATIVE: Free text\n",
    "    -- MANDATORY LAST 7 COLUMNS (in this exact order):\n",
    "    COALESCE(get_json_object(insights, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,  -- BUSINESS OUTCOME: Calculated Impact\n",
    "    COALESCE(get_json_object(insights, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,  -- NARRATIVE: Executive Summary (references business outcome)\n",
    "    COALESCE(get_json_object(insights, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,  -- SYSTEM: Importance Level\n",
    "    COALESCE(get_json_object(insights, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,  -- SYSTEM: Urgency Level\n",
    "    COALESCE(TRY_CAST(get_json_object(insights, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,  -- SYSTEM: Confidence Score\n",
    "    COALESCE(get_json_object(insights, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,  -- SYSTEM: Feedback\n",
    "    COALESCE(get_json_object(insights, '$.ai_sys_missing_data'), 'No missing data identified') AS ai_sys_missing_data  -- SYSTEM: Missing Data\n",
    "  FROM enriched\n",
    ")\n",
    "SELECT * FROM final_output\n",
    "-- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "-- WHERE ai_cat_feedback_category IN ('Product Quality', 'Customer Service', 'Pricing', 'Delivery', 'Other')\n",
    "-- AND ai_cat_urgency_level IN ('Critical', 'High', 'Medium', 'Low')\n",
    "-- AND ai_cat_response_priority IN ('Immediate Action', 'High Priority', 'Medium Priority', 'Low Priority')\n",
    "-- AND ai_sys_importance IN ('Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical')\n",
    "-- AND ai_sys_urgency IN ('Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical')\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "**NOTE**: Use `get_json_object(json_column, '$.field_name')` to extract fields from ai_gen/ai_query JSON output. Use `ai_cat_` prefix for categorical columns, `ai_txt_` prefix for narrative columns, and `ai_sys_` prefix for system columns. The LAST 7 columns must ALWAYS be: ai_txt_business_outcome (calculated measurable impact with Daily/Weekly/Monthly/Yearly breakdown and DISCLAIMER), ai_txt_executive_summary (references business outcome), ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data.\n",
    "\n",
    "**Example 2: ai_classify + ai_query (Advanced Pattern - Customer Segmentation)**\n",
    "```sql\n",
    "-- CREATE VIEW inspire_ai.default.customer_segmentation_retention_strategy AS\n",
    "-- Use Case: Customer Segmentation with Personalized Retention Strategy\n",
    "\n",
    "-- Step 1: Classify customers into value-based segments\n",
    "-- \uD83D\uDEA8 EVERY column must be COALESCE'd or have IS NOT NULL check!\n",
    "WITH customer_metrics AS (\n",
    "  SELECT DISTINCT\n",
    "    customer_id,                                              -- CRITICAL: filtered with IS NOT NULL\n",
    "    customer_name,                                            -- CRITICAL: filtered with IS NOT NULL\n",
    "    COALESCE(total_revenue, 0.0) AS total_revenue,            -- ✅ COALESCE'd\n",
    "    COALESCE(purchase_frequency, 0) AS purchase_frequency,    -- ✅ COALESCE'd\n",
    "    COALESCE(last_purchase_days_ago, 9999) AS last_purchase_days_ago,  -- ✅ COALESCE'd (high default = churned)\n",
    "    COALESCE(TRIM(product_category_preference), 'Unknown') AS product_category_preference,  -- ✅ COALESCE'd\n",
    "    COALESCE(support_tickets_count, 0) AS support_tickets_count  -- ✅ COALESCE'd\n",
    "  FROM `main`.`crm`.`customer_analytics` AS c\n",
    "  WHERE customer_id IS NOT NULL\n",
    "    AND customer_name IS NOT NULL\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at END of first CTE only\n",
    "),\n",
    "segmented AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_classify(\n",
    "      'Customer with $' || CAST(total_revenue AS STRING) || ' revenue, ' || \n",
    "      CAST(purchase_frequency AS STRING) || ' purchases/year, last purchase ' || \n",
    "      CAST(last_purchase_days_ago AS STRING) || ' days ago',\n",
    "      ARRAY('High Value VIP', 'High Value At-Risk', 'Medium Value Active', \n",
    "            'Medium Value Declining', 'Low Value', 'Churned')\n",
    "    ) AS ai_cat_value_tier\n",
    "  FROM customer_metrics\n",
    "),\n",
    "-- Step 2: Generate structured retention strategies with ai_cat_ + ai_txt_ + ai_sys_ columns\n",
    "enriched AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_query('{sql_model_serving}',\n",
    "      CONCAT('Customer Value Tier: ', ai_cat_value_tier, \n",
    "             '. Revenue: $', total_revenue,  -- CONCAT auto-converts\n",
    "             ', Frequency: ', purchase_frequency, '/year, ',\n",
    "             'Last Purchase: ', last_purchase_days_ago, ' days ago, ',\n",
    "             'Preferred Category: ', product_category_preference, ', ',\n",
    "             'Support Tickets: ', support_tickets_count, '. ',\n",
    "             'Output ONLY a JSON object with NO markdown fences, NO extra text, JUST the JSON. ',\n",
    "             'Format: {{\"ai_cat_churn_risk_level\": \"value\", \"ai_cat_retention_priority\": \"value\", \"ai_cat_engagement_readiness\": \"value\", \"ai_txt_segmentation_rationale\": \"text\", \"ai_txt_retention_strategy\": \"text\", \"ai_txt_outreach_plan\": \"text\", \"ai_txt_offer_recommendations\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "             'Required keys: ',\n",
    "             'ai_cat_churn_risk_level (MUST be exactly one of: Critical/High/Medium/Low/Minimal), ',\n",
    "             'ai_cat_retention_priority (MUST be exactly one of: Immediate Action/High Priority/Medium Priority/Low Priority/Monitor), ',\n",
    "             'ai_cat_engagement_readiness (MUST be exactly one of: Highly Receptive/Moderately Receptive/Low Receptivity/Difficult), ',\n",
    "             'ai_txt_segmentation_rationale (free text: why customer is in this tier, key metrics driving classification, 1-2 sentences), ',\n",
    "             'ai_txt_retention_strategy (free text: personalized retention strategy specific to this tier and customer behavior), ',\n",
    "             'ai_txt_outreach_plan (free text: specific outreach approach with recommended timing and channel), ',\n",
    "             'ai_txt_offer_recommendations (free text: specific recommended offers/incentives with expected impact). ',\n",
    "             'MANDATORY LAST 7 FIELDS (in this exact order): ',\n",
    "             '1) ai_txt_business_outcome - CALCULATED MEASURABLE BUSINESS IMPACT. Example: \"Upsell opportunity for Account [ID] worth $X ARR. Win probability: Y%. Expected value: $Z. Breakdown: Daily revenue potential: $X | Weekly: $X | Monthly: $X | Yearly: $X. DISCLAIMER: All numbers are AI estimates based on available data and must be validated by domain experts before business decisions.\" MUST include breakdown and disclaimer. ',\n",
    "             '2) ai_txt_executive_summary - executive brief for account manager in 2-3 sentences that REFERENCES the business outcome numbers, ',\n",
    "             '3) ai_sys_importance - business importance level (MUST be exactly one of: Very Low, Low, Medium, High, Very High, Critical), ',\n",
    "             '4) ai_sys_urgency - action urgency level (MUST be exactly one of: Very Low, Low, Medium, High, Very High, Critical), ',\n",
    "             '5) ai_sys_confidence (0.0-1.0) - your confidence score, ',\n",
    "             '6) ai_sys_feedback - start with \"I assessed my confidence at [X]% because...\" then explain reasoning, ',\n",
    "             '7) ai_sys_missing_data - MUST follow format: \"I can get higher confidence than [X]% if I can get access to [narrative about missing customer data]. {{\\\"missing_data\\\": [\\\"specific_dataset1\\\", \\\"specific_dataset2\\\"]}}\" - always end with JSON listing needed datasets. ',\n",
    "             'Output ONLY the JSON object, nothing else.')\n",
    "    ) AS insights\n",
    "  FROM segmented\n",
    "),\n",
    "-- Final output: Mix of ai_cat_ (filterable), ai_txt_ (narrative), and ai_sys_ (system) columns\n",
    "final_output AS (\n",
    "  SELECT \n",
    "    customer_id,\n",
    "    customer_name,\n",
    "    total_revenue,\n",
    "    ai_cat_value_tier,  -- CATEGORICAL from ai_classify\n",
    "    get_json_object(insights, '$.ai_cat_churn_risk_level') AS ai_cat_churn_risk_level,  -- CATEGORICAL: Critical/High/Medium/Low/Minimal\n",
    "    get_json_object(insights, '$.ai_cat_retention_priority') AS ai_cat_retention_priority,  -- CATEGORICAL: Immediate Action/High Priority/Medium Priority/Low Priority/Monitor\n",
    "    get_json_object(insights, '$.ai_cat_engagement_readiness') AS ai_cat_engagement_readiness,  -- CATEGORICAL: Highly Receptive/Moderately Receptive/Low Receptivity/Difficult\n",
    "    get_json_object(insights, '$.ai_txt_segmentation_rationale') AS ai_txt_segmentation_rationale,  -- NARRATIVE: Free text\n",
    "    get_json_object(insights, '$.ai_txt_retention_strategy') AS ai_txt_retention_strategy,  -- NARRATIVE: Free text\n",
    "    get_json_object(insights, '$.ai_txt_outreach_plan') AS ai_txt_outreach_plan,  -- NARRATIVE: Free text\n",
    "    get_json_object(insights, '$.ai_txt_offer_recommendations') AS ai_txt_offer_recommendations,  -- NARRATIVE: Free text\n",
    "    -- MANDATORY LAST 7 COLUMNS (in this exact order):\n",
    "    COALESCE(get_json_object(insights, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,  -- BUSINESS OUTCOME: Calculated Impact\n",
    "    COALESCE(get_json_object(insights, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,  -- NARRATIVE: Executive Summary (references business outcome)\n",
    "    COALESCE(get_json_object(insights, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,  -- SYSTEM: Importance Level\n",
    "    COALESCE(get_json_object(insights, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,  -- SYSTEM: Urgency Level\n",
    "    COALESCE(TRY_CAST(get_json_object(insights, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,  -- SYSTEM: Confidence Score\n",
    "    COALESCE(get_json_object(insights, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,  -- SYSTEM: Feedback\n",
    "    COALESCE(get_json_object(insights, '$.ai_sys_missing_data'), 'No missing data identified') AS ai_sys_missing_data  -- SYSTEM: Missing Data\n",
    "  FROM enriched\n",
    ")\n",
    "SELECT * FROM final_output\n",
    "-- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "-- WHERE ai_cat_value_tier IN ('High Value VIP', 'High Value At-Risk', 'Medium Value Active', 'Medium Value Declining', 'Low Value', 'Churned')\n",
    "-- AND ai_cat_churn_risk_level IN ('Critical', 'High', 'Medium', 'Low', 'Minimal')\n",
    "-- AND ai_cat_retention_priority IN ('Immediate Action', 'High Priority', 'Medium Priority', 'Low Priority', 'Monitor')\n",
    "-- AND ai_cat_engagement_readiness IN ('Highly Receptive', 'Moderately Receptive', 'Low Receptivity', 'Difficult')\n",
    "-- AND ai_sys_importance IN ('Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical')\n",
    "-- AND ai_sys_urgency IN ('Very Low', 'Low', 'Medium', 'High', 'Very High', 'Critical')\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "**NOTE**: Use `get_json_object(json_column, '$.field_name')` to extract fields from ai_gen/ai_query JSON output. Use `ai_cat_` prefix for categorical columns, `ai_txt_` prefix for narrative columns, and `ai_sys_` prefix for system columns. The LAST 7 columns must ALWAYS be: ai_txt_business_outcome (calculated measurable impact with Daily/Weekly/Monthly/Yearly breakdown and DISCLAIMER), ai_txt_executive_summary (references business outcome), ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data.\n",
    "\n",
    "**Example 3: ai_extract (Simple)**\n",
    "```sql\n",
    "-- CREATE VIEW inspire_ai.default.order_details_extracted AS\n",
    "-- Use Case: Extract Order Details from Notes\n",
    "-- Extracts structured data from unstructured order notes\n",
    "\n",
    "SELECT \n",
    "  order_id,\n",
    "  notes,\n",
    "  ai_extract(notes, \n",
    "    ARRAY('delivery_date', 'special_instructions', 'discount_code', 'priority_level')\n",
    "  ) AS extracted_data\n",
    "FROM `sales`.`orders`.`order_notes` AS o\n",
    "WHERE order_id IS NOT NULL\n",
    "  AND notes IS NOT NULL\n",
    "LIMIT 10;  -- ✅ LIMIT 10 for sampling\n",
    "```\n",
    "\n",
    "**Example 4: ai_query for Business Analysis**\n",
    "```sql\n",
    "-- CREATE VIEW inspire_ai.default.customer_churn_risk_assessment AS\n",
    "-- Use Case: Predict Customer Churn Risk\n",
    "-- Uses general-purpose LLM to analyze churn risk\n",
    "\n",
    "-- Step 1: Prepare data with COALESCE\n",
    "WITH customer_data AS (\n",
    "  SELECT \n",
    "    customer_id,\n",
    "    COALESCE(TRIM(customer_name), 'Unknown') AS customer_name,\n",
    "    COALESCE(account_age_days, 0) AS account_age_days,\n",
    "    COALESCE(recent_activity_score, 0.0) AS recent_activity_score\n",
    "  FROM `main`.`customer`.`profiles` AS c\n",
    "  WHERE customer_id IS NOT NULL AND customer_name IS NOT NULL\n",
    "  LIMIT 10\n",
    "),\n",
    "-- Step 2: Generate ai_sys_prompt\n",
    "prompt_generation AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    CONCAT('You are a Customer Success Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "           'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "           'Business priorities are: {enriched_business_priorities}. ',\n",
    "           'With 15 years of experience in retention strategy and customer success, ',\n",
    "           'your expertise aligns with the strategic initiative: Customer retention. ',\n",
    "           'Analyze churn risk for customer: ', customer_name, \n",
    "           ', Account age: ', account_age_days, ' days',\n",
    "           ', Activity score: ', recent_activity_score, '. ',\n",
    "           'Output ONLY a JSON object with NO markdown, NO extra text. ',\n",
    "           'Format: {{\"ai_cat_risk_level\": \"High/Medium/Low\", \"ai_txt_risk_factors\": \"text\", \"ai_txt_retention_plan\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "           'Output ONLY the JSON object, nothing else.') AS ai_sys_prompt\n",
    "  FROM customer_data\n",
    "),\n",
    "-- Step 3: Call ai_query\n",
    "analysis AS (\n",
    "  SELECT *, ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS churn_risk_assessment\n",
    "  FROM prompt_generation\n",
    ")\n",
    "SELECT \n",
    "  customer_id, customer_name, account_age_days, recent_activity_score,\n",
    "  get_json_object(churn_risk_assessment, '$.ai_cat_risk_level') AS ai_cat_risk_level,\n",
    "  get_json_object(churn_risk_assessment, '$.ai_txt_risk_factors') AS ai_txt_risk_factors,\n",
    "  get_json_object(churn_risk_assessment, '$.ai_txt_retention_plan') AS ai_txt_retention_plan,\n",
    "  COALESCE(get_json_object(churn_risk_assessment, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "  COALESCE(get_json_object(churn_risk_assessment, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "  COALESCE(get_json_object(churn_risk_assessment, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "  COALESCE(get_json_object(churn_risk_assessment, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "  COALESCE(TRY_CAST(get_json_object(churn_risk_assessment, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "  COALESCE(get_json_object(churn_risk_assessment, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "  COALESCE(get_json_object(churn_risk_assessment, '$.ai_sys_missing_data'), 'No missing data') AS ai_sys_missing_data,\n",
    "  ai_sys_prompt  -- ✅ LAST COLUMN\n",
    "FROM analysis;\n",
    "```\n",
    "\n",
    "**Example 5: ai_gen for Creative Content**\n",
    "```sql\n",
    "-- CREATE VIEW inspire_ai.default.personalized_marketing_messages AS\n",
    "-- Use Case: Generate Personalized Marketing Messages\n",
    "-- Creates targeted marketing content based on customer data\n",
    "\n",
    "-- Step 1: Prepare data with COALESCE\n",
    "WITH customer_segments AS (\n",
    "  SELECT \n",
    "    customer_id,\n",
    "    COALESCE(TRIM(customer_name), 'Unknown') AS customer_name,\n",
    "    COALESCE(TRIM(preferred_product_category), 'General') AS preferred_product_category,\n",
    "    COALESCE(lifetime_value, 0.0) AS lifetime_value\n",
    "  FROM `main`.`marketing`.`customer_segments` AS m\n",
    "  WHERE customer_id IS NOT NULL AND customer_name IS NOT NULL\n",
    "  LIMIT 10\n",
    "),\n",
    "-- Step 2: Generate ai_sys_prompt\n",
    "prompt_generation AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    CONCAT('You are a Marketing Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "           'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "           'Business priorities are: {enriched_business_priorities}. ',\n",
    "           'With 12 years of experience in personalized campaigns and customer engagement, ',\n",
    "           'your expertise aligns with the strategic initiative: Customer engagement. ',\n",
    "           'Create a personalized marketing email for ', customer_name,\n",
    "           ' who prefers ', preferred_product_category,\n",
    "           ' and has a lifetime value of $', lifetime_value, '. ',\n",
    "           'Include a special offer relevant to their interests. ',\n",
    "           'Output ONLY a JSON object with NO markdown, NO extra text. ',\n",
    "           'Format: {{\"ai_cat_campaign_priority\": \"High/Medium/Low\", \"ai_txt_subject_line\": \"text\", \"ai_txt_email_body\": \"text\", \"ai_txt_offer_details\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "           'Output ONLY the JSON object, nothing else.') AS ai_sys_prompt\n",
    "  FROM customer_segments\n",
    "),\n",
    "-- Step 3: Call ai_query\n",
    "analysis AS (\n",
    "  SELECT *, ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.5)) AS personalized_message\n",
    "  FROM prompt_generation\n",
    ")\n",
    "SELECT \n",
    "  customer_id, customer_name, preferred_product_category, lifetime_value,\n",
    "  get_json_object(personalized_message, '$.ai_cat_campaign_priority') AS ai_cat_campaign_priority,\n",
    "  get_json_object(personalized_message, '$.ai_txt_subject_line') AS ai_txt_subject_line,\n",
    "  get_json_object(personalized_message, '$.ai_txt_email_body') AS ai_txt_email_body,\n",
    "  get_json_object(personalized_message, '$.ai_txt_offer_details') AS ai_txt_offer_details,\n",
    "  COALESCE(get_json_object(personalized_message, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "  COALESCE(get_json_object(personalized_message, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "  COALESCE(get_json_object(personalized_message, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "  COALESCE(get_json_object(personalized_message, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "  COALESCE(TRY_CAST(get_json_object(personalized_message, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "  COALESCE(get_json_object(personalized_message, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "  COALESCE(get_json_object(personalized_message, '$.ai_sys_missing_data'), 'No missing data') AS ai_sys_missing_data,\n",
    "  ai_sys_prompt  -- ✅ LAST COLUMN\n",
    "FROM analysis;\n",
    "```\n",
    "\n",
    "**Example 5: ai_forecast (Time Series Forecasting)**\n",
    "```sql\n",
    "-- Basic pattern: historical CTE with 10:1 ratio → AI_FORECAST → results\n",
    "-- For 3-month horizon, use 30 months of history (10:1 ratio)\n",
    "WITH past AS (\n",
    "  SELECT DATE_TRUNC('month', order_date) AS ds, SUM(total_amount) AS revenue\n",
    "  FROM `sales`.`transactions`.`orders` AS o\n",
    "  WHERE order_date >= date_add(MONTH, -30, CURRENT_DATE())  -- 30 months history for 3-month forecast\n",
    "    AND order_date IS NOT NULL\n",
    "  GROUP BY DATE_TRUNC('month', order_date)\n",
    "  ORDER BY ds\n",
    ")\n",
    "SELECT ds, revenue_forecast, revenue_upper, revenue_lower\n",
    "FROM AI_FORECAST(TABLE(past), \n",
    "     time_col => 'ds', value_col => 'revenue',\n",
    "     horizon => (SELECT date_add(MONTH, 3, MAX(ds)) FROM past))  -- 3 months ahead\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "\n",
    "**Example 5 (Advanced): ai_forecast Variations**\n",
    "\n",
    "**Pattern A: Multi-Metric Forecasting with Groups**\n",
    "```sql\n",
    "-- Forecast multiple metrics (revenue + orders) by category with 10:1 ratio\n",
    "-- For 8-week horizon, use 80 weeks of history\n",
    "WITH past AS (\n",
    "  SELECT DATE_TRUNC('week', order_date) AS ds, product_category,\n",
    "         SUM(total_amount) AS revenue, COUNT(DISTINCT order_id) AS order_count\n",
    "  FROM `sales`.`transactions`.`orders` AS o\n",
    "  WHERE order_date >= date_add(WEEK, -80, CURRENT_DATE())  -- 80 weeks history for 8-week forecast\n",
    "    AND order_date IS NOT NULL\n",
    "    AND product_category IS NOT NULL\n",
    "  GROUP BY DATE_TRUNC('week', order_date), product_category\n",
    "  ORDER BY ds, product_category\n",
    ")\n",
    "SELECT ds, product_category, revenue_forecast, order_count_forecast\n",
    "FROM AI_FORECAST(TABLE(past), time_col => 'ds',\n",
    "     value_col => ARRAY('revenue', 'order_count'), group_col => 'product_category',\n",
    "     horizon => (SELECT date_add(WEEK, 8, MAX(ds)) FROM past),  -- 8 weeks ahead\n",
    "     parameters => '{{\"global_floor\": 0}}');  -- ✅ NO LIMIT\n",
    "```\n",
    "\n",
    "**Pattern B: Advanced Seasonality Control** (Adapt table/column names to YOUR schema)\n",
    "```sql\n",
    "-- Control weekly/daily seasonality with fourier orders and 10:1 ratio\n",
    "-- For 60-day horizon, use 600 days of history\n",
    "-- [ADAPT: Change catalog.schema.table and column names to match YOUR schema]\n",
    "WITH past AS (\n",
    "  SELECT DATE_TRUNC('day', activity_date) AS ds, entity_id, SUM(metric_value) AS metric\n",
    "  FROM `catalog`.`schema`.`your_table` AS t \n",
    "  WHERE activity_date >= date_add(DAY, -600, CURRENT_DATE())  -- 600 days history for 60-day forecast\n",
    "    AND activity_date IS NOT NULL\n",
    "    AND entity_id IS NOT NULL\n",
    "  GROUP BY DATE_TRUNC('day', activity_date), entity_id\n",
    "  ORDER BY ds, entity_id\n",
    ")\n",
    "SELECT ds, entity_id, metric_forecast, metric_upper, metric_lower\n",
    "FROM AI_FORECAST(TABLE(past), time_col => 'ds', value_col => 'metric',\n",
    "     group_col => 'entity_id',\n",
    "     horizon => (SELECT date_add(DAY, 60, MAX(ds)) FROM past),  -- 60 days ahead\n",
    "     parameters => '{{\"weekly_order\": 10, \"daily_order\": 5, \"global_floor\": 0}}');  -- ✅ NO LIMIT\n",
    "```\n",
    "\n",
    "**Pattern C: Forecast + Classification** (Adapt table/column names to YOUR schema)\n",
    "```sql\n",
    "-- Forecast metric AND classify into action buckets with 10:1 ratio\n",
    "-- For 14-day horizon, use 140 days of history\n",
    "-- [ADAPT: Change catalog.schema.table and column names to match YOUR schema]\n",
    "WITH past AS (\n",
    "  SELECT DATE_TRUNC('day', activity_date) AS ds, entity_id, SUM(quantity_value) AS metric\n",
    "  FROM `catalog`.`schema`.`your_table` AS t\n",
    "  WHERE activity_date >= date_add(DAY, -140, CURRENT_DATE())  -- 140 days history for 14-day forecast\n",
    "    AND activity_date IS NOT NULL\n",
    "    AND entity_id IS NOT NULL\n",
    "  GROUP BY DATE_TRUNC('day', activity_date), entity_id\n",
    "  ORDER BY ds, entity_id\n",
    "),\n",
    "forecasted_raw AS (\n",
    "  SELECT ds, entity_id, metric_forecast, metric_upper, metric_lower\n",
    "  FROM AI_FORECAST(TABLE(past), time_col => 'ds', value_col => 'metric',\n",
    "       group_col => 'entity_id',\n",
    "       horizon => (SELECT date_add(DAY, 14, MAX(ds)) FROM past))  -- ✅ NO LIMIT\n",
    "),\n",
    "-- \uD83D\uDEA8 Apply COALESCE HERE before using in CONCAT - NOT inside CONCAT!\n",
    "forecasted AS (\n",
    "  SELECT \n",
    "    ds,\n",
    "    entity_id,\n",
    "    COALESCE(ROUND(metric_forecast, 2), 0.0) AS metric_forecast,  -- ✅ COALESCE'd HERE\n",
    "    COALESCE(ROUND(metric_upper, 2), 0.0) AS metric_upper,\n",
    "    COALESCE(ROUND(metric_lower, 2), 0.0) AS metric_lower\n",
    "  FROM forecasted_raw\n",
    ")\n",
    "SELECT ds, entity_id, metric_forecast,  -- ✅ Already NULL-safe\n",
    "       ai_classify(CONCAT('Forecast: ', metric_forecast, ' units'),  -- ✅ NO COALESCE in CONCAT - already NULL-safe!\n",
    "         ARRAY('Critical - Action Required', 'Moderate - Monitor', 'Low - Reduce', 'Uncertain')) AS action\n",
    "FROM forecasted;  -- ✅ NO LIMIT\n",
    "```\n",
    "\n",
    "**Pattern D: Forecast + Recommendations (MANDATORY)** (Adapt table/column names to YOUR schema)\n",
    "```sql\n",
    "-- Forecast + generate actionable recommendations with 10:1 ratio\n",
    "-- For 12-week horizon, use 120 weeks of history\n",
    "-- [ADAPT: Change catalog.schema.table and column names to match YOUR schema]\n",
    "WITH past AS (\n",
    "  SELECT DATE_TRUNC('week', activity_date) AS ds, SUM(amount_value) AS metric\n",
    "  FROM `catalog`.`schema`.`your_table` AS t\n",
    "  WHERE activity_date >= date_add(WEEK, -120, CURRENT_DATE())  -- 120 weeks history for 12-week forecast\n",
    "    AND activity_date IS NOT NULL\n",
    "  GROUP BY DATE_TRUNC('week', activity_date)\n",
    "  ORDER BY ds\n",
    "),\n",
    "forecast_results AS (\n",
    "  SELECT \n",
    "    COALESCE(CAST(ds AS STRING), 'Unknown') AS ds_display,\n",
    "    COALESCE(ROUND(metric_forecast, 2), 0.0) AS metric_forecast,\n",
    "    COALESCE(ROUND(metric_upper, 2), 0.0) AS metric_upper,\n",
    "    COALESCE(ROUND(metric_lower, 2), 0.0) AS metric_lower\n",
    "  FROM AI_FORECAST(TABLE(past), time_col => 'ds', value_col => 'metric',\n",
    "       horizon => (SELECT date_add(WEEK, 12, MAX(ds)) FROM past))  -- 12 weeks ahead\n",
    ")\n",
    "SELECT ds_display, metric_forecast,\n",
    "       ai_query('{sql_model_serving}', CONCAT('Week ', ds_display, ': Forecast $',\n",
    "              metric_forecast,  -- CONCAT auto-converts DOUBLE\n",
    "              '. Provide 3 actionable recommendations')) AS recommendations\n",
    "FROM forecast_results;  -- ✅ NO LIMIT\n",
    "```\n",
    "\n",
    "**Example 6: Multi-Function Pipeline (SOPHISTICATED - Document File Processing)**\n",
    "```sql\n",
    "-- Use Case: Intelligent Document Processing Pipeline\n",
    "-- Parses invoice document files, extracts data, and classifies by urgency\n",
    "-- CRITICAL: ai_parse_document is used ONLY with unstructured document files from Unity Catalog volumes\n",
    "\n",
    "WITH document_files AS (\n",
    "  SELECT \n",
    "    path,\n",
    "    content,\n",
    "    ai_parse_document(\n",
    "      content,\n",
    "      map('version', '2.0')\n",
    "    ) AS parsed_doc\n",
    "  FROM READ_FILES('/Volumes/finance/documents/invoices/*.pdf', format => 'binaryFile')\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at END of first CTE only\n",
    "),\n",
    "extracted_text AS (\n",
    "  SELECT \n",
    "    path,\n",
    "    concat_ws(\n",
    "      '\\n\\n',\n",
    "      transform(\n",
    "        try_cast(parsed_doc:document:elements AS ARRAY<VARIANT>),\n",
    "        element -> try_cast(element:content AS STRING)\n",
    "      )\n",
    "    ) AS full_text\n",
    "  FROM document_files\n",
    "  WHERE try_cast(parsed_doc:error_status AS STRING) IS NULL\n",
    "  -- ✅ NO LIMIT in CTEs\n",
    "),\n",
    "extracted_data AS (\n",
    "  SELECT \n",
    "    path,\n",
    "    full_text,\n",
    "    ai_extract(full_text, \n",
    "      ARRAY('vendor_name', 'invoice_number', 'total_amount', 'due_date', 'payment_terms')\n",
    "    ) AS invoice_data\n",
    "  FROM extracted_text\n",
    "  -- ✅ NO LIMIT in intermediate CTEs\n",
    ")\n",
    "SELECT \n",
    "  path,\n",
    "  invoice_data,\n",
    "  ai_classify(\n",
    "    CONCAT('Invoice from: ', invoice_data['vendor_name'], \n",
    "           ', Amount: $', invoice_data['total_amount'],\n",
    "           ', Due: ', invoice_data['due_date']),\n",
    "    ARRAY('Urgent - Past Due', 'High Priority', 'Normal', 'Low Priority')\n",
    "  ) AS urgency_classification\n",
    "FROM extracted_data;  -- ✅ NO LIMIT in final SELECT\n",
    "```\n",
    "\n",
    "**CRITICAL NOTE FOR ai_parse_document:**\n",
    "- ai_parse_document MUST ONLY be used with unstructured document files (PDFs, images, Word docs, PowerPoints)\n",
    "- Use READ_FILES('/Volumes/path/to/files/*.{{pdf,jpg,png,doc,docx,ppt,pptx}}', format => 'binaryFile') to load binary content\n",
    "- NEVER use ai_parse_document with table columns or structured data already in Delta tables\n",
    "- The output is VARIANT type with schema version 2.0 containing document elements (text, tables, figures)\n",
    "- Extract text content from parsed_doc:document:elements array before using ai_extract\n",
    "- Filter out errors using WHERE try_cast(parsed_doc:error_status AS STRING) IS NULL\n",
    "\n",
    "**Example 7: JOIN with ai_query (SOPHISTICATED)**\n",
    "```sql\n",
    "-- Use Case: Generate Customer Insights with Purchase Context\n",
    "-- Joins customer and order data for rich context in LLM prompts\n",
    "\n",
    "-- \uD83D\uDEA8 EVERY column must be COALESCE'd or have IS NOT NULL check!\n",
    "WITH customer_orders AS (\n",
    "  SELECT \n",
    "    c.customer_id,                                              -- CRITICAL: filtered with IS NOT NULL\n",
    "    c.customer_name,                                            -- CRITICAL: filtered with IS NOT NULL\n",
    "    COALESCE(TRIM(c.customer_segment), 'Unknown Segment') AS customer_segment,  -- ✅ COALESCE'd\n",
    "    COALESCE(c.lifetime_value, 0.0) AS lifetime_value,          -- ✅ COALESCE'd\n",
    "    COALESCE(COUNT(o.order_id), 0) AS total_orders,             -- ✅ COALESCE'd (LEFT JOIN can produce NULL)\n",
    "    COALESCE(SUM(o.order_amount), 0.0) AS total_spent           -- ✅ COALESCE'd (LEFT JOIN can produce NULL)\n",
    "  FROM `main`.`customers`.`profiles` AS c\n",
    "  LEFT JOIN `main`.`sales`.`orders` AS o ON c.customer_id = o.customer_id\n",
    "  WHERE c.customer_id IS NOT NULL\n",
    "    AND c.customer_name IS NOT NULL\n",
    "  GROUP BY c.customer_id, c.customer_name, c.customer_segment, c.lifetime_value\n",
    "  LIMIT 10  -- ✅ LIMIT 10 in first CTE (GROUP BY provides uniqueness)\n",
    ")\n",
    "SELECT \n",
    "  customer_id,\n",
    "  customer_name,\n",
    "  ai_query('{sql_model_serving}',\n",
    "    CONCAT('Analyze this customer profile: Name: ', customer_name,\n",
    "           ', Segment: ', customer_segment,\n",
    "           ', Lifetime Value: $', lifetime_value,  -- CONCAT auto-converts\n",
    "           ', Total Orders: ', total_orders,\n",
    "           ', Total Spent: $', total_spent,\n",
    "           '. Provide actionable insights and retention strategies.')\n",
    "  ) AS customer_insights\n",
    "FROM customer_orders\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "\n",
    "**Example 8: ai_mask with Structured Justification and Strategy (DATA PRIVACY COMPLIANCE)**\n",
    "```sql\n",
    "-- Use Case: Mask PII Data with Compliance Documentation\n",
    "-- Step 1: Mask sensitive PII fields\n",
    "-- Step 2: Generate structured JSON with data risk classification and compliance strategy\n",
    "\n",
    "-- Step 1: Mask PII fields in customer records\n",
    "-- \uD83D\uDEA8 EVERY column must be COALESCE'd or have IS NOT NULL check!\n",
    "WITH masked_data AS (\n",
    "  SELECT DISTINCT\n",
    "    customer_id,                                              -- CRITICAL: filtered with IS NOT NULL\n",
    "    COALESCE(TRIM(customer_name), 'Unknown') AS customer_name,  -- ✅ COALESCE'd\n",
    "    COALESCE(TRIM(email), 'no-email@unknown.com') AS email,     -- ✅ COALESCE'd\n",
    "    COALESCE(TRIM(phone), '000-000-0000') AS phone,             -- ✅ COALESCE'd\n",
    "    COALESCE(TRIM(ssn), '000-00-0000') AS ssn,                  -- ✅ COALESCE'd\n",
    "    COALESCE(TRIM(credit_card), '0000-0000-0000-0000') AS credit_card,  -- ✅ COALESCE'd\n",
    "    ai_mask(COALESCE(TRIM(customer_name), 'Unknown'), ARRAY('PERSON')) AS customer_name_masked,\n",
    "    ai_mask(COALESCE(TRIM(email), 'no-email@unknown.com'), ARRAY('EMAIL')) AS email_masked,\n",
    "    ai_mask(COALESCE(TRIM(phone), '000-000-0000'), ARRAY('PHONE')) AS phone_masked,\n",
    "    ai_mask(COALESCE(TRIM(ssn), '000-00-0000'), ARRAY('SSN')) AS ssn_masked,\n",
    "    ai_mask(COALESCE(TRIM(credit_card), '0000-0000-0000-0000'), ARRAY('CREDIT_CARD')) AS credit_card_masked\n",
    "  FROM `main`.`customers`.`personal_data` AS c\n",
    "  WHERE customer_id IS NOT NULL\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at END of first CTE only\n",
    "),\n",
    "-- Step 2: Generate structured compliance documentation with categorical risk classifications\n",
    "enriched AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_query('{sql_model_serving}',\n",
    "      CONCAT('Analyze masked customer data. Fields masked: name (PERSON), email (EMAIL), phone (PHONE), SSN (SSN), credit card (CREDIT_CARD). ',\n",
    "             'Output ONLY a JSON object with NO markdown fences, NO extra text, JUST the JSON. ',\n",
    "             'Format: {{\"data_risk_type\": \"value\", \"risk_severity\": \"value\", \"compliance_status\": \"value\", \"primary_regulation\": \"value\", \"masking_rationale\": \"text\", \"data_protection_strategy\": \"text\", \"compliance_checklist\": \"text\", \"breach_risk_reduction\": \"text\", \"data_governance_summary\": \"text\"}}. ',\n",
    "             'Required keys: ',\n",
    "             'data_risk_type (MUST be exactly one of: PII/PHI/Financial Data/PII+Financial/PHI+PII/PHI+Financial/PII+PHI+Financial), ',\n",
    "             'risk_severity (MUST be exactly one of: Critical/High/Medium/Low), ',\n",
    "             'compliance_status (MUST be exactly one of: Fully Compliant/Compliant with Monitoring/At Risk/Non-Compliant), ',\n",
    "             'primary_regulation (MUST be exactly one of: GDPR/CCPA/HIPAA/PCI-DSS/GDPR+CCPA/Multiple), ',\n",
    "             'masking_rationale (free text: why these specific fields required masking for compliance and data protection), ',\n",
    "             'data_protection_strategy (free text: comprehensive strategy for secure data sharing, storage, and access control), ',\n",
    "             'compliance_checklist (free text: specific compliance steps completed and verification procedures), ',\n",
    "             'breach_risk_reduction (free text: how masking reduces breach impact, quantify risk reduction percentage if possible), ',\n",
    "             'data_governance_summary (free text: executive brief on data protection value and compliance posture in 2-3 sentences). ',\n",
    "             'Output ONLY the JSON object, nothing else.')\n",
    "    ) AS compliance_info\n",
    "  FROM masked_data\n",
    "),\n",
    "-- Final output: Masked data with mix of categorical and narrative compliance columns\n",
    "final_output AS (\n",
    "  SELECT \n",
    "    customer_id,\n",
    "    customer_name_masked,\n",
    "    email_masked,\n",
    "    phone_masked,\n",
    "    ssn_masked,\n",
    "    credit_card_masked,\n",
    "    get_json_object(compliance_info, '$.data_risk_type') AS data_risk_type,  -- CATEGORICAL: PII/PHI/Financial Data/combinations\n",
    "    get_json_object(compliance_info, '$.risk_severity') AS risk_severity,  -- CATEGORICAL: Critical/High/Medium/Low\n",
    "    get_json_object(compliance_info, '$.compliance_status') AS compliance_status,  -- CATEGORICAL: Fully Compliant/Compliant with Monitoring/At Risk/Non-Compliant\n",
    "    get_json_object(compliance_info, '$.primary_regulation') AS primary_regulation,  -- CATEGORICAL: GDPR/CCPA/HIPAA/PCI-DSS/combinations\n",
    "    get_json_object(compliance_info, '$.masking_rationale') AS masking_rationale,  -- NARRATIVE: Free text\n",
    "    get_json_object(compliance_info, '$.data_protection_strategy') AS data_protection_strategy,  -- NARRATIVE: Free text\n",
    "    get_json_object(compliance_info, '$.compliance_checklist') AS compliance_checklist,  -- NARRATIVE: Free text\n",
    "    get_json_object(compliance_info, '$.breach_risk_reduction') AS breach_risk_reduction,  -- NARRATIVE: Free text\n",
    "    get_json_object(compliance_info, '$.data_governance_summary') AS data_governance_summary  -- NARRATIVE: Free text\n",
    "  FROM enriched\n",
    ")\n",
    "SELECT * FROM final_output\n",
    "-- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "-- WHERE data_risk_type IN ('PII', 'PHI', 'Financial Data', 'PII+Financial', 'PHI+PII', 'PHI+Financial', 'PII+PHI+Financial')\n",
    "-- AND risk_severity IN ('Critical', 'High', 'Medium', 'Low')\n",
    "-- AND compliance_status IN ('Fully Compliant', 'Compliant with Monitoring', 'At Risk', 'Non-Compliant')\n",
    "-- AND primary_regulation IN ('GDPR', 'CCPA', 'HIPAA', 'PCI-DSS', 'GDPR+CCPA', 'Multiple')\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "**NOTE**: The data_risk_classification column specifies whether data is PII, PHI, Financial, or combination, helping businesses understand the specific regulatory requirements.\n",
    "\n",
    "**Example 9: ai_analyze_sentiment with Structured Insights (CUSTOMER FEEDBACK ANALYSIS)**\n",
    "```sql\n",
    "-- Use Case: Customer Feedback Analysis with Actionable Response Strategy\n",
    "-- Step 1: Analyze customer emotion in feedback\n",
    "-- Step 2: Generate structured JSON with emotion analysis and response recommendations\n",
    "\n",
    "-- Step 1: Analyze customer emotion in reviews\n",
    "-- \uD83D\uDEA8 EVERY column must be COALESCE'd or have IS NOT NULL check!\n",
    "WITH emotion_analysis AS (\n",
    "  SELECT DISTINCT\n",
    "    review_id,                                              -- CRITICAL: filtered with IS NOT NULL\n",
    "    customer_id,                                            -- CRITICAL: filtered with IS NOT NULL\n",
    "    COALESCE(TRIM(product_id), 'Unknown Product') AS product_id,  -- ✅ COALESCE'd\n",
    "    review_text,                                            -- CRITICAL: filtered with IS NOT NULL (used in AI)\n",
    "    COALESCE(rating, 0) AS rating,                          -- ✅ COALESCE'd\n",
    "    COALESCE(purchase_amount, 0.0) AS purchase_amount,      -- ✅ COALESCE'd\n",
    "    ai_analyze_sentiment(review_text) AS customer_emotion\n",
    "  FROM `main`.`reviews`.`customer_feedback` AS r\n",
    "  WHERE review_id IS NOT NULL\n",
    "    AND review_text IS NOT NULL\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at END of first CTE only\n",
    "),\n",
    "-- Step 2: Generate structured insights with categorical metrics and narrative strategies\n",
    "enriched AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_query('{sql_model_serving}',\n",
    "      CONCAT('Analyze customer emotion in this review showing ', customer_emotion, ' emotion ',\n",
    "             '(rating: ', rating, '/5, purchase: $', purchase_amount, '): \"',  -- CONCAT auto-converts\n",
    "             review_text, '\". ',\n",
    "             'Output ONLY a JSON object with NO markdown fences, NO extra text, JUST the JSON. ',\n",
    "             'Format: {{\"account_risk_level\": \"value\", \"response_urgency\": \"value\", \"primary_pain_point\": \"value\", \"recovery_difficulty\": \"value\", \"emotion_rationale\": \"text\", \"response_playbook\": \"text\", \"recovery_actions\": \"text\", \"product_service_enhancements\": \"text\", \"customer_journey_insights\": \"text\"}}. ',\n",
    "             'Required keys: ',\n",
    "             'account_risk_level (MUST be exactly one of: Critical/High/Medium/Low/Minimal), ',\n",
    "             'response_urgency (MUST be exactly one of: Immediate/Within 24hrs/Within Week/Routine/Monitor), ',\n",
    "             'primary_pain_point (MUST be exactly one of: Product Quality/Customer Service/Pricing/Delivery/Usability/Expectation Mismatch/Other), ',\n",
    "             'recovery_difficulty (MUST be exactly one of: Easy/Moderate/Difficult/Very Difficult/Lost), ',\n",
    "             'emotion_rationale (free text: why customer feels this way, specific phrases showing emotion, 1-2 sentences), ',\n",
    "             'response_playbook (free text: specific playbook steps to respond to this customer with timing and approach), ',\n",
    "             'recovery_actions (free text: immediate recovery actions with specific offers or gestures if negative emotion), ',\n",
    "             'product_service_enhancements (free text: specific product/service improvements needed based on feedback), ',\n",
    "             'customer_journey_insights (free text: executive brief on customer journey implications in 2-3 sentences). ',\n",
    "             'Output ONLY the JSON object, nothing else.')\n",
    "    ) AS insights\n",
    "  FROM emotion_analysis\n",
    "),\n",
    "-- Final output: Mix of categorical (filterable) and narrative (actionable) columns\n",
    "final_output AS (\n",
    "  SELECT \n",
    "    review_id,\n",
    "    customer_id,\n",
    "    product_id,\n",
    "    review_text,\n",
    "    customer_emotion,  -- CATEGORICAL from ai_analyze_sentiment\n",
    "    rating,\n",
    "    get_json_object(insights, '$.account_risk_level') AS account_risk_level,  -- CATEGORICAL: Critical/High/Medium/Low/Minimal\n",
    "    get_json_object(insights, '$.response_urgency') AS response_urgency,  -- CATEGORICAL: Immediate/Within 24hrs/Within Week/Routine/Monitor\n",
    "    get_json_object(insights, '$.primary_pain_point') AS primary_pain_point,  -- CATEGORICAL: Product Quality/Service/Pricing/Delivery/etc\n",
    "    get_json_object(insights, '$.recovery_difficulty') AS recovery_difficulty,  -- CATEGORICAL: Easy/Moderate/Difficult/Very Difficult/Lost\n",
    "    get_json_object(insights, '$.emotion_rationale') AS emotion_rationale,  -- NARRATIVE: Free text\n",
    "    get_json_object(insights, '$.response_playbook') AS response_playbook,  -- NARRATIVE: Free text\n",
    "    get_json_object(insights, '$.recovery_actions') AS recovery_actions,  -- NARRATIVE: Free text\n",
    "    get_json_object(insights, '$.product_service_enhancements') AS service_enhancements,  -- NARRATIVE: Free text\n",
    "    get_json_object(insights, '$.customer_journey_insights') AS customer_journey_insights  -- NARRATIVE: Free text\n",
    "  FROM enriched\n",
    ")\n",
    "SELECT * FROM final_output\n",
    "-- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "-- WHERE account_risk_level IN ('Critical', 'High', 'Medium', 'Low', 'Minimal')\n",
    "-- AND response_urgency IN ('Immediate', 'Within 24hrs', 'Within Week', 'Routine', 'Monitor')\n",
    "-- AND primary_pain_point IN ('Product Quality', 'Customer Service', 'Pricing', 'Delivery', 'Usability', 'Expectation Mismatch', 'Other')\n",
    "-- AND recovery_difficulty IN ('Easy', 'Moderate', 'Difficult', 'Very Difficult', 'Lost')\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "**NOTE**: Use business-relevant names (e.g., customer_emotion, feedback_tone, satisfaction_level) instead of generic \"sentiment\". Tailor column names to your business context.\n",
    "\n",
    "**Example 10: ai_similarity with Structured Deduplication Strategy**\n",
    "```sql\n",
    "-- Use Case: Customer Record Matching for Deduplication with Action Plan\n",
    "-- Step 1: Calculate match confidence between customer records\n",
    "-- Step 2: Generate structured JSON with match analysis and consolidation strategy\n",
    "\n",
    "-- Step 1: Find potential duplicate customers using semantic matching\n",
    "WITH match_analysis AS (\n",
    "  SELECT DISTINCT\n",
    "    c1.customer_id AS customer_id_1,\n",
    "    c1.customer_name AS name_1,\n",
    "    c1.email AS email_1,\n",
    "    c1.address AS address_1,\n",
    "    c2.customer_id AS customer_id_2,\n",
    "    c2.customer_name AS name_2,\n",
    "    c2.email AS email_2,\n",
    "    c2.address AS address_2,\n",
    "    ai_similarity(\n",
    "      CONCAT(c1.customer_name, ' ', c1.email, ' ', c1.address),\n",
    "      CONCAT(c2.customer_name, ' ', c2.email, ' ', c2.address)\n",
    "    ) AS match_confidence_score\n",
    "  FROM `main`.`customers`.`profiles` AS c1\n",
    "  CROSS JOIN `main`.`customers`.`profiles` AS c2\n",
    "  WHERE c1.customer_id IS NOT NULL\n",
    "    AND c2.customer_id IS NOT NULL\n",
    "    AND c1.customer_id < c2.customer_id  -- Self-join deduplication (exception: comparison needed for CROSS JOIN)\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at END of first CTE only\n",
    "),\n",
    "-- Step 2: Generate structured consolidation strategy with categorical classifications\n",
    "enriched AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_query('{sql_model_serving}',\n",
    "      CONCAT('Analyze potential duplicate customers with match confidence ', \n",
    "             CAST(match_confidence_score AS STRING), ' (0=different, 1=identical). ',\n",
    "             'Record A: ', name_1, ' (', email_1, '), ',\n",
    "             'Record B: ', name_2, ' (', email_2, '). ',\n",
    "             'Output ONLY a JSON object with NO markdown fences, NO extra text, JUST the JSON. ',\n",
    "             'Format: {{\"record_relationship\": \"value\", \"merge_priority\": \"value\", \"data_conflict_level\": \"value\", \"recommended_master\": \"value\", \"match_rationale\": \"text\", \"merge_execution_plan\": \"text\", \"data_reconciliation_steps\": \"text\", \"master_data_quality_impact\": \"text\", \"business_intelligence_value\": \"text\"}}. ',\n",
    "             'Required keys: ',\n",
    "             'record_relationship (MUST be exactly one of: Definite Duplicate/Probable Duplicate/Possible Match/Different Entity), ',\n",
    "             'merge_priority (MUST be exactly one of: Immediate/High/Medium/Low/Do Not Merge), ',\n",
    "             'data_conflict_level (MUST be exactly one of: No Conflicts/Minor Conflicts/Moderate Conflicts/Major Conflicts), ',\n",
    "             'recommended_master (MUST be exactly one of: Record A/Record B/Create New/Manual Review), ',\n",
    "             'match_rationale (free text: why these are likely same entity, which fields match, confidence reasoning, 1-2 sentences), ',\n",
    "             'merge_execution_plan (free text: step-by-step plan to merge records safely with data preservation strategy), ',\n",
    "             'data_reconciliation_steps (free text: how to reconcile conflicting data between records with decision rules), ',\n",
    "             'master_data_quality_impact (free text: how consolidation improves data quality, analytics accuracy, and business intelligence), ',\n",
    "             'business_intelligence_value (free text: executive brief on clean customer data value in 2 sentences). ',\n",
    "             'Output ONLY the JSON object, nothing else.')\n",
    "    ) AS insights\n",
    "  FROM match_analysis\n",
    "  -- NOTE: Filter high-confidence matches in application layer or use threshold in ai_query prompt\n",
    "  -- WHERE match_confidence_score > 0.7 would violate the no-value-comparison rule\n",
    "),\n",
    "-- Final output: Mix of categorical (filterable) and narrative (execution) columns\n",
    "final_output AS (\n",
    "  SELECT \n",
    "    customer_id_1,\n",
    "    name_1,\n",
    "    email_1,\n",
    "    customer_id_2,\n",
    "    name_2,\n",
    "    email_2,\n",
    "    match_confidence_score,\n",
    "    get_json_object(insights, '$.record_relationship') AS record_relationship,  -- CATEGORICAL: Definite Duplicate/Probable Duplicate/Possible Match/Different Entity\n",
    "    get_json_object(insights, '$.merge_priority') AS merge_priority,  -- CATEGORICAL: Immediate/High/Medium/Low/Do Not Merge\n",
    "    get_json_object(insights, '$.data_conflict_level') AS data_conflict_level,  -- CATEGORICAL: No Conflicts/Minor/Moderate/Major\n",
    "    get_json_object(insights, '$.recommended_master') AS recommended_master,  -- CATEGORICAL: Record A/Record B/Create New/Manual Review\n",
    "    get_json_object(insights, '$.match_rationale') AS match_rationale,  -- NARRATIVE: Free text\n",
    "    get_json_object(insights, '$.merge_execution_plan') AS merge_execution_plan,  -- NARRATIVE: Free text\n",
    "    get_json_object(insights, '$.data_reconciliation_steps') AS data_reconciliation_steps,  -- NARRATIVE: Free text\n",
    "    get_json_object(insights, '$.master_data_quality_impact') AS data_quality_impact,  -- NARRATIVE: Free text\n",
    "    get_json_object(insights, '$.business_intelligence_value') AS business_intelligence_value  -- NARRATIVE: Free text\n",
    "  FROM enriched\n",
    ")\n",
    "SELECT * FROM final_output\n",
    "-- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "-- WHERE record_relationship IN ('Definite Duplicate', 'Probable Duplicate', 'Possible Match', 'Different Entity')\n",
    "-- AND merge_priority IN ('Immediate', 'High', 'Medium', 'Low', 'Do Not Merge')\n",
    "-- AND data_conflict_level IN ('No Conflicts', 'Minor Conflicts', 'Moderate Conflicts', 'Major Conflicts')\n",
    "-- AND recommended_master IN ('Record A', 'Record B', 'Create New', 'Manual Review')\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "**NOTE**: Use business-relevant names (e.g., match_confidence_score, record_relationship) instead of generic \"similarity_score\". Tailor to your domain (e.g., product_match_score for products, vendor_similarity for vendors).\n",
    "\n",
    "**Example 11: ai_forecast with Enhanced Structured Recommendations (MANDATORY PATTERN)**\n",
    "```sql\n",
    "-- Use Case: Revenue Forecasting with Structured Business Strategy\n",
    "-- Step 1: Prepare historical time series data\n",
    "-- Step 2: Generate forecasts with prediction intervals\n",
    "-- Step 3: Generate structured JSON with justification and strategic recommendations\n",
    "\n",
    "-- Step 1: Prepare historical revenue data for forecasting\n",
    "WITH past AS (\n",
    "  SELECT \n",
    "    DATE_TRUNC('week', order_date) AS ds,\n",
    "    SUM(total_amount) AS revenue\n",
    "  FROM `sales`.`transactions`.`orders` AS o\n",
    "  WHERE order_date >= date_add(WEEK, -120, CURRENT_DATE())  -- 120 weeks history for 12-week forecast (10:1 ratio)\n",
    "    AND order_date IS NOT NULL\n",
    "  GROUP BY DATE_TRUNC('week', order_date)\n",
    "  ORDER BY ds\n",
    "),\n",
    "-- Step 2: Generate revenue forecasts with prediction intervals - COALESCE results\n",
    "forecast_results AS (\n",
    "  SELECT \n",
    "    COALESCE(CAST(ds AS STRING), 'Unknown') AS ds,\n",
    "    COALESCE(ROUND(revenue_forecast, 2), 0.0) AS revenue_forecast,\n",
    "    COALESCE(ROUND(revenue_upper, 2), 0.0) AS revenue_upper,\n",
    "    COALESCE(ROUND(revenue_lower, 2), 0.0) AS revenue_lower\n",
    "  FROM AI_FORECAST(\n",
    "    TABLE(past), \n",
    "    time_col => 'ds', \n",
    "    value_col => 'revenue',\n",
    "    horizon => (SELECT date_add(WEEK, 12, MAX(ds)) FROM past)  -- 12 weeks ahead\n",
    "  )\n",
    "  -- ✅ NO LIMIT in CTEs\n",
    "),\n",
    "-- Step 3: Generate ai_sys_prompt for forecast analysis\n",
    "forecast_prompt_generation AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    CONCAT('You are a Revenue Forecasting Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "           'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "           'Business priorities are: {enriched_business_priorities}. ',\n",
    "           'With 18 years of experience in revenue forecasting and financial planning, ',\n",
    "           'your expertise in predictive analytics and strategic resource allocation aligns with the strategic initiative: Revenue optimization. ',\n",
    "           'Analyze revenue forecast for week ', ds, ': ',  -- CONCAT auto-converts\n",
    "           'Predicted: $', revenue_forecast, ', ',\n",
    "           'Upper bound: $', revenue_upper, ', ',\n",
    "           'Lower bound: $', revenue_lower, '. ',\n",
    "           'Output ONLY a JSON object with NO markdown fences, NO extra text, JUST the JSON. ',\n",
    "           'Format: {{\"ai_cat_trend_direction\": \"value\", \"ai_cat_forecast_confidence\": \"value\", \"ai_cat_action_priority\": \"value\", \"ai_cat_risk_level\": \"value\", \"ai_cat_resource_allocation_need\": \"value\", \"ai_txt_forecast_justification\": \"text\", \"ai_txt_tactical_recommendations\": \"text\", \"ai_txt_strategic_initiatives\": \"text\", \"ai_txt_risk_mitigation_plan\": \"text\", \"ai_txt_upside_opportunities\": \"text\", \"ai_txt_resource_recommendations\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "           'CATEGORICAL FIELDS (ai_cat_ prefix): ',\n",
    "           'ai_cat_trend_direction (MUST be exactly one of: Strong Growth/Moderate Growth/Stable/Moderate Decline/Sharp Decline), ',\n",
    "           'ai_cat_forecast_confidence (MUST be exactly one of: Very High/High/Medium/Low/Very Low), ',\n",
    "           'ai_cat_action_priority (MUST be exactly one of: Immediate Action/High Priority/Medium Priority/Low Priority/Monitor), ',\n",
    "           'ai_cat_risk_level (MUST be exactly one of: Critical/High/Medium/Low/Minimal), ',\n",
    "           'ai_cat_resource_allocation_need (MUST be exactly one of: Significant Increase/Moderate Increase/Maintain/Moderate Decrease/Significant Decrease). ',\n",
    "           'NARRATIVE FIELDS (ai_txt_ prefix): ',\n",
    "           'ai_txt_forecast_justification (free text: why this forecast level, key drivers, confidence factors, 2 sentences), ',\n",
    "           'ai_txt_tactical_recommendations (free text: 3 specific tactical actions for this period with expected impact), ',\n",
    "           'ai_txt_strategic_initiatives (free text: long-term strategic initiatives based on forecast trends), ',\n",
    "           'ai_txt_risk_mitigation_plan (free text: specific risks if forecast not met and detailed mitigation plans), ',\n",
    "           'ai_txt_upside_opportunities (free text: how to exceed forecast with specific tactics and expected ROI), ',\n",
    "           'ai_txt_resource_recommendations (free text: specific staffing/inventory/capacity recommendations with quantities). ',\n",
    "           'MANDATORY LAST 7 FIELDS: ai_txt_business_outcome (CALCULATED MEASURABLE IMPACT with Daily/Weekly/Monthly/Yearly breakdown and DISCLAIMER), ai_txt_executive_summary (references business outcome numbers), ai_sys_importance, ai_sys_urgency, ai_sys_confidence, ai_sys_feedback, ai_sys_missing_data. ',\n",
    "           'ai_sys_missing_data format: \"I can get higher confidence than [X]% if I can get access to [narrative]. {{\\\"missing_data\\\": [\\\"dataset1\\\", \\\"dataset2\\\"]}}\" ',\n",
    "           'Output ONLY the JSON object, nothing else.') AS ai_sys_prompt\n",
    "  FROM forecast_results\n",
    "  -- ✅ NO LIMIT in CTEs\n",
    "),\n",
    "-- Step 4: Generate structured strategic recommendations with categorical and narrative columns\n",
    "enriched AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS insights\n",
    "  FROM forecast_prompt_generation\n",
    "  -- ✅ NO LIMIT in CTEs\n",
    "),\n",
    "-- Final output: Mix of categorical (ai_cat_), narrative (ai_txt_), and system (ai_sys_) columns\n",
    "-- ai_sys_prompt MUST be the LAST column\n",
    "final_output AS (\n",
    "  SELECT \n",
    "    ds AS forecast_week,\n",
    "    revenue_forecast,\n",
    "    revenue_upper AS upper_bound,\n",
    "    revenue_lower AS lower_bound,\n",
    "    get_json_object(insights, '$.ai_cat_trend_direction') AS ai_cat_trend_direction,  -- CATEGORICAL: Strong Growth/Moderate Growth/Stable/Moderate Decline/Sharp Decline\n",
    "    get_json_object(insights, '$.ai_cat_forecast_confidence') AS ai_cat_forecast_confidence,  -- CATEGORICAL: Very High/High/Medium/Low/Very Low\n",
    "    get_json_object(insights, '$.ai_cat_action_priority') AS ai_cat_action_priority,  -- CATEGORICAL: Immediate Action/High Priority/Medium Priority/Low Priority/Monitor\n",
    "    get_json_object(insights, '$.ai_cat_risk_level') AS ai_cat_risk_level,  -- CATEGORICAL: Critical/High/Medium/Low/Minimal\n",
    "    get_json_object(insights, '$.ai_cat_resource_allocation_need') AS ai_cat_resource_allocation_need,  -- CATEGORICAL: Significant Increase/Moderate Increase/Maintain/Moderate Decrease/Significant Decrease\n",
    "    get_json_object(insights, '$.ai_txt_forecast_justification') AS ai_txt_forecast_justification,  -- NARRATIVE: Free text\n",
    "    get_json_object(insights, '$.ai_txt_tactical_recommendations') AS ai_txt_tactical_recommendations,  -- NARRATIVE: Free text\n",
    "    get_json_object(insights, '$.ai_txt_strategic_initiatives') AS ai_txt_strategic_initiatives,  -- NARRATIVE: Free text\n",
    "    get_json_object(insights, '$.ai_txt_risk_mitigation_plan') AS ai_txt_risk_mitigation_plan,  -- NARRATIVE: Free text\n",
    "    get_json_object(insights, '$.ai_txt_upside_opportunities') AS ai_txt_upside_opportunities,  -- NARRATIVE: Free text\n",
    "    get_json_object(insights, '$.ai_txt_resource_recommendations') AS ai_txt_resource_recommendations,  -- NARRATIVE: Free text\n",
    "    -- MANDATORY LAST 7 COLUMNS (including ai_txt_business_outcome + ai_sys_prompt):\n",
    "    COALESCE(get_json_object(insights, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "    COALESCE(get_json_object(insights, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "    COALESCE(get_json_object(insights, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "    COALESCE(get_json_object(insights, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "    COALESCE(TRY_CAST(get_json_object(insights, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "    COALESCE(get_json_object(insights, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "    COALESCE(get_json_object(insights, '$.ai_sys_missing_data'), 'No missing data identified') AS ai_sys_missing_data,\n",
    "    ai_sys_prompt  -- ✅ LAST COLUMN: The exact prompt used for auditability\n",
    "  FROM enriched\n",
    ")\n",
    "SELECT * FROM final_output\n",
    "-- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "-- WHERE ai_cat_trend_direction IN ('Strong Growth', 'Moderate Growth', 'Stable', 'Moderate Decline', 'Sharp Decline')\n",
    "-- AND ai_cat_forecast_confidence IN ('Very High', 'High', 'Medium', 'Low', 'Very Low')\n",
    "-- AND ai_cat_action_priority IN ('Immediate Action', 'High Priority', 'Medium Priority', 'Low Priority', 'Monitor')\n",
    "-- AND ai_cat_risk_level IN ('Critical', 'High', 'Medium', 'Low', 'Minimal')\n",
    "-- AND ai_cat_resource_allocation_need IN ('Significant Increase', 'Moderate Increase', 'Maintain', 'Moderate Decrease', 'Significant Decrease')\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "**NOTE**: This pattern generates forecasts with comprehensive structured output including justification, tactical recommendations, strategic initiatives, risk mitigation, and executive narratives that tell the business story.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. SQL IMPLEMENTATION GUIDELINES FOR AI FUNCTIONS\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL ARRAY RESTRICTIONS:**\n",
    "\n",
    "Both `ai_classify` and `ai_extract` accept an array of labels/entities as their second parameter. You MUST follow these restrictions:\n",
    "\n",
    "1. **Maximum 20 items** - The array can contain at most 20 elements\n",
    "   - ✅ VALID: `ARRAY('label1', 'label2', ..., 'label20')` (20 items)\n",
    "   - ❌ INVALID: Arrays with 21 or more items will FAIL\n",
    "\n",
    "2. **Maximum 50 characters per item** - Each array element MUST be less than 50 characters\n",
    "   - ✅ VALID: `'High Priority'` (13 chars), `'Customer Service'` (16 chars)\n",
    "   - ❌ INVALID: `'High Priority Customer Service Escalation Required Immediately'` (63 chars)\n",
    "\n",
    "**BEST PRACTICES:**\n",
    "- Use concise, clear labels - avoid verbose descriptions\n",
    "- Abbreviate when necessary: `'Cust Svc'` instead of `'Customer Service Department'`\n",
    "- For ai_extract: Use short entity names like `'invoice_num'` instead of `'invoice_number_from_document'`\n",
    "- For ai_classify: Keep category names brief and meaningful\n",
    "- **CRITICAL**: Always follow classification with ai_gen/ai_query for recommendations\n",
    "\n",
    "**Examples:**\n",
    "```sql\n",
    "-- GOOD ✅ - Classification with recommendations (includes persona, ai_sys_ columns, and ai_sys_prompt)\n",
    "WITH classified AS (\n",
    "  SELECT DISTINCT \n",
    "    ticket_id,\n",
    "    COALESCE(TRIM(text), 'No description') AS text,\n",
    "    ai_classify(text, ARRAY('Urgent', 'High', 'Medium', 'Low')) AS ai_cat_priority\n",
    "  FROM tickets AS t\n",
    "  WHERE ticket_id IS NOT NULL\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at END of first CTE only\n",
    "),\n",
    "-- Step 2: Generate ai_sys_prompt\n",
    "prompt_generation AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    CONCAT('You are a Support Operations Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "           'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "           'Business priorities are: {enriched_business_priorities}. ',\n",
    "           'With 12 years of experience in support operations and customer success, ',\n",
    "           'your expertise aligns with the strategic initiative: Customer satisfaction. ',\n",
    "           'Generate action plan for ', ai_cat_priority, ' priority ticket: ', text, '. ',\n",
    "           'Output ONLY JSON with format: {{\"ai_txt_action_plan\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", ',\n",
    "           '\"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "           'Output ONLY the JSON object, nothing else.') AS ai_sys_prompt\n",
    "  FROM classified\n",
    "),\n",
    "-- Step 3: Call ai_query\n",
    "enriched AS (\n",
    "  SELECT *, ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS action_plan\n",
    "  FROM prompt_generation\n",
    "),\n",
    "-- Final output with ai_sys_prompt as LAST column\n",
    "final_output AS (\n",
    "  SELECT \n",
    "    ticket_id,\n",
    "    text,\n",
    "    ai_cat_priority,\n",
    "    get_json_object(action_plan, '$.ai_txt_action_plan') AS ai_txt_action_plan,\n",
    "    COALESCE(get_json_object(action_plan, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "    COALESCE(get_json_object(action_plan, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "    COALESCE(get_json_object(action_plan, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "    COALESCE(get_json_object(action_plan, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "    COALESCE(TRY_CAST(get_json_object(action_plan, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "    COALESCE(get_json_object(action_plan, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "    COALESCE(get_json_object(action_plan, '$.ai_sys_missing_data'), 'No missing data') AS ai_sys_missing_data,\n",
    "    ai_sys_prompt  -- ✅ LAST COLUMN\n",
    "  FROM enriched\n",
    ")\n",
    "SELECT * FROM final_output\n",
    "-- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "-- WHERE ai_cat_priority IN ('Urgent', 'High', 'Medium', 'Low')\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "\n",
    "-- INCOMPLETE ❌ - Classification without recommendations (LOW VALUE)\n",
    "SELECT ai_classify(text, ARRAY('Urgent', 'High', 'Medium', 'Low')) AS priority FROM tickets;\n",
    "\n",
    "-- BAD ❌ - Labels too long\n",
    "ai_classify(text, ARRAY('Extremely Urgent Customer Service Escalation Required'))  -- Too long!\n",
    "ai_extract(content, ARRAY('customer_full_legal_name_as_appears_on_document'))  -- Too long!\n",
    "```\n",
    "\n",
    "#### AI_FORECAST SQL Implementation Details\n",
    "\n",
    "**\uD83D\uDD25 CRITICAL: HISTORY-TO-HORIZON RATIO (ADAPTIVE BY TIME GRANULARITY) \uD83D\uDD25**\n",
    "\n",
    "**MANDATORY REQUIREMENT**: For ai_forecast input CTEs, you MUST use dynamic date filtering with appropriate history-to-horizon ratios based on time granularity instead of fixed LIMIT 1000.\n",
    "\n",
    "**WHY ADAPTIVE RATIOS?**\n",
    "- Different time granularities require different amounts of history\n",
    "- Must capture at least 2 complete seasonal cycles for pattern recognition\n",
    "- Balance between sufficient training data and data relevance\n",
    "- High-frequency data (hourly/minute) needs more observations but shorter calendar time\n",
    "- Low-frequency data (yearly) needs longer calendar time but fewer observations\n",
    "\n",
    "**ADAPTIVE RATIO TABLE BY TIME GRANULARITY:**\n",
    "\n",
    "| Time Granularity | Minimum History | Recommended Ratio | Reasoning |\n",
    "|------------------|----------------|-------------------|-----------|\n",
    "| **MINUTE** | 7 days | 7 days per hour forecast | Capture daily + weekly patterns (10,080 observations for 1 hour ahead) |\n",
    "| **HOUR** | 4-6 weeks | 4 weeks per day forecast | Capture weekly + monthly patterns (672 observations for 1 day ahead) |\n",
    "| **DAY** | 60-90 days | 10:1 ratio | Capture 2+ seasonal cycles (60-90 observations for 7-day forecast) |\n",
    "| **WEEK** | 2-3 years | 10:1 ratio | Capture 2+ yearly cycles (104-156 observations for 12-week forecast) |\n",
    "| **MONTH** | 3-5 years | 10:1 ratio | Capture 2-3+ yearly cycles (36-60 observations for 12-month forecast) |\n",
    "| **QUARTER** | 5-8 years | 8:1 ratio | Capture 2+ yearly cycles (20-32 observations for 4-quarter forecast) |\n",
    "| **YEAR** | 10-15 years | 3:1 ratio minimum | Industry standard for annual forecasting (10-15 observations minimum) |\n",
    "\n",
    "**IMPLEMENTATION PATTERNS BY GRANULARITY:**\n",
    "\n",
    "**1. HIGH-FREQUENCY (MINUTE, HOUR) - Need Many Observations, Short Calendar Time:**\n",
    "```sql\n",
    "-- MINUTE-LEVEL: For 1-hour forecast, use 7 days of history\n",
    "WHERE timestamp_col >= date_add(DAY, -7, CURRENT_TIMESTAMP())  -- 7 days = 10,080 minutes\n",
    "\n",
    "-- HOUR-LEVEL: For 24-hour (1 day) forecast, use 4 weeks of history\n",
    "WHERE timestamp_col >= date_add(WEEK, -4, CURRENT_TIMESTAMP())  -- 4 weeks = 672 hours\n",
    "```\n",
    "\n",
    "**2. MID-FREQUENCY (DAY, WEEK, MONTH) - Standard 10:1 Ratio:**\n",
    "```sql\n",
    "-- DAILY: For 7-day forecast, use 70 days (10 weeks) of history\n",
    "WHERE date_col >= date_add(DAY, -70, CURRENT_DATE())  -- 10:1 ratio\n",
    "\n",
    "-- WEEKLY: For 12-week forecast, use 120 weeks (~2.3 years) of history\n",
    "WHERE date_col >= date_add(WEEK, -120, CURRENT_DATE())  -- 10:1 ratio\n",
    "\n",
    "-- MONTHLY: For 12-month forecast, use 120 months (10 years) IF AVAILABLE, otherwise minimum 36 months\n",
    "WHERE date_col >= add_months(CURRENT_DATE(), -120)  -- 10:1 ratio (ideal)\n",
    "WHERE date_col >= add_months(CURRENT_DATE(), -36)   -- 3:1 ratio (minimum acceptable)\n",
    "```\n",
    "\n",
    "**3. LOW-FREQUENCY (QUARTER, YEAR) - Reduced Ratio Due to Data Availability:**\n",
    "```sql\n",
    "-- QUARTERLY: For 4-quarter forecast, use 32 quarters (~8 years) of history\n",
    "WHERE date_col >= date_add(QUARTER, -32, CURRENT_DATE())  -- 8:1 ratio\n",
    "\n",
    "-- YEARLY: For 3-year forecast, use 10-15 years IF AVAILABLE, otherwise minimum 6 years\n",
    "WHERE date_col >= date_add(YEAR, -15, CURRENT_DATE())  -- 5:1 ratio (ideal)\n",
    "WHERE date_col >= date_add(YEAR, -6, CURRENT_DATE())   -- 2:1 ratio (minimum for 2 cycles)\n",
    "```\n",
    "\n",
    "**COMPREHENSIVE EXAMPLES BY GRANULARITY:**\n",
    "\n",
    "| Forecast Horizon | History Period | WHERE Clause | Observations |\n",
    "|------------------|---------------|--------------|--------------|\n",
    "| 1 hour ahead (minute data) | 7 days | `WHERE ts >= date_add(DAY, -7, CURRENT_TIMESTAMP())` | ~10,080 minutes |\n",
    "| 24 hours ahead (hourly data) | 4 weeks | `WHERE ts >= date_add(WEEK, -4, CURRENT_TIMESTAMP())` | ~672 hours |\n",
    "| 7 days ahead (daily data) | 70 days | `WHERE date >= date_add(DAY, -70, CURRENT_DATE())` | 70 days |\n",
    "| 4 weeks ahead (weekly data) | 40 weeks | `WHERE date >= date_add(WEEK, -40, CURRENT_DATE())` | 40 weeks |\n",
    "| 3 months ahead (monthly data) | 30-36 months | `WHERE date >= add_months(CURRENT_DATE(), -36)` | 36 months |\n",
    "| 4 quarters ahead (quarterly data) | 32 quarters | `WHERE date >= date_add(QUARTER, -32, CURRENT_DATE())` | 32 quarters (~8 years) |\n",
    "| 3 years ahead (yearly data) | 10-15 years | `WHERE date >= date_add(YEAR, -12, CURRENT_DATE())` | 12 years |\n",
    "\n",
    "**COMPLETE EXAMPLES BY GRANULARITY:**\n",
    "\n",
    "**Example 1: Monthly Forecast (Standard 10:1)**\n",
    "```sql\n",
    "-- For 3-month forecast, use 30 months of history (10:1 ratio)\n",
    "WITH past AS (\n",
    "  SELECT \n",
    "    product_id,\n",
    "    DATE_TRUNC('month', order_date) AS ds,\n",
    "    SUM(revenue) AS revenue\n",
    "  FROM `catalog`.`schema`.`orders` AS o\n",
    "  WHERE order_date >= add_months(CURRENT_DATE(), -30)  -- 10:1 ratio: 30 months for 3-month forecast\n",
    "    AND order_date IS NOT NULL\n",
    "    AND product_id IS NOT NULL\n",
    "  GROUP BY product_id, DATE_TRUNC('month', order_date)\n",
    "  ORDER BY ds\n",
    ")\n",
    "SELECT * FROM AI_FORECAST(\n",
    "  TABLE(past),\n",
    "  time_col => 'ds',\n",
    "  value_col => 'revenue',\n",
    "  group_col => 'product_id',\n",
    "  horizon => (SELECT date_add(MONTH, 3, MAX(ds)) FROM past)  -- 3 months ahead\n",
    ");  -- ✅ NO LIMIT\n",
    "```\n",
    "\n",
    "**Example 2: Hourly Forecast (High-Frequency)**\n",
    "```sql\n",
    "-- For 24-hour forecast, use 4 weeks of history (captures daily + weekly patterns)\n",
    "WITH past AS (\n",
    "  SELECT \n",
    "    server_id,\n",
    "    DATE_TRUNC('hour', request_timestamp) AS ds,\n",
    "    COUNT(*) AS request_count,\n",
    "    AVG(response_time_ms) AS avg_response_time\n",
    "  FROM `catalog`.`schema`.`server_logs` AS s\n",
    "  WHERE request_timestamp >= date_add(WEEK, -4, CURRENT_TIMESTAMP())  -- 4 weeks for hourly data\n",
    "    AND request_timestamp IS NOT NULL\n",
    "    AND server_id IS NOT NULL\n",
    "  GROUP BY server_id, DATE_TRUNC('hour', request_timestamp)\n",
    "  ORDER BY ds\n",
    ")\n",
    "SELECT * FROM AI_FORECAST(\n",
    "  TABLE(past),\n",
    "  time_col => 'ds',\n",
    "  value_col => ARRAY('request_count', 'avg_response_time'),\n",
    "  group_col => 'server_id',\n",
    "  horizon => (SELECT date_add(HOUR, 24, MAX(ds)) FROM past)  -- 24 hours ahead\n",
    ");  -- ✅ NO LIMIT\n",
    "```\n",
    "\n",
    "**Example 3: Yearly Forecast (Low-Frequency)**\n",
    "```sql\n",
    "-- For 3-year forecast, use 12 years of history (4:1 ratio - practical for annual data)\n",
    "WITH past AS (\n",
    "  SELECT \n",
    "    region_id,\n",
    "    DATE_TRUNC('year', fiscal_year_end) AS ds,\n",
    "    SUM(annual_revenue) AS revenue\n",
    "  FROM `catalog`.`schema`.`financial_results` AS f\n",
    "  WHERE fiscal_year_end >= date_add(YEAR, -12, CURRENT_DATE())  -- 12 years for 3-year forecast\n",
    "    AND fiscal_year_end IS NOT NULL\n",
    "    AND region_id IS NOT NULL\n",
    "  GROUP BY region_id, DATE_TRUNC('year', fiscal_year_end)\n",
    "  ORDER BY ds\n",
    ")\n",
    "SELECT * FROM AI_FORECAST(\n",
    "  TABLE(past),\n",
    "  time_col => 'ds',\n",
    "  value_col => 'revenue',\n",
    "  group_col => 'region_id',\n",
    "  horizon => (SELECT date_add(YEAR, 3, MAX(ds)) FROM past)  -- 3 years ahead\n",
    ");  -- ✅ NO LIMIT\n",
    "```\n",
    "\n",
    "**CRITICAL RULES (ADAPTIVE BY GRANULARITY):**\n",
    "1. ✅ ALWAYS use appropriate ratio based on time granularity (see table above)\n",
    "2. ✅ ALWAYS use same UNIT for both history and horizon (HOUR, DAY, WEEK, MONTH, QUARTER, YEAR)\n",
    "3. ✅ ALWAYS include timestamp/date IS NOT NULL check\n",
    "4. ✅ HIGH-FREQUENCY (minute/hour): Use calendar time (days/weeks) not multiplier\n",
    "5. ✅ MID-FREQUENCY (day/week/month): Use 10:1 ratio as standard\n",
    "6. ✅ LOW-FREQUENCY (quarter/year): Use reduced ratio (8:1 for quarter, 3-5:1 for year)\n",
    "7. ✅ For MONTHLY with 12-month horizon: Use minimum 36 months (3 years) if 120 months not available\n",
    "8. ✅ For YEARLY: Use minimum 2× horizon (2 cycles) but prefer 10-15 years if available\n",
    "9. ❌ NEVER use fixed LIMIT 1000\n",
    "10. ❌ NEVER use arbitrary date values like '2023-01-01'\n",
    "11. ❌ NEVER use same multiplier (10:1) for all time granularities\n",
    "\n",
    "**\uD83D\uDD25 MANDATORY RECOMMENDATION REQUIREMENT FOR AI_FORECAST:**\n",
    "\n",
    "**EVERY ai_forecast query MUST include row-level recommendations using ai_query**\n",
    "\n",
    "**RECOMMENDATION IMPLEMENTATION PATTERNS (MANDATORY):**\n",
    "\n",
    "**Pattern 1 - ai_forecast + ai_query**: Generate natural language recommendations\n",
    "```sql\n",
    "-- Step 1: Historical data for forecasting with 10:1 ratio (300 days for 30-day forecast)\n",
    "WITH past AS (\n",
    "  SELECT \n",
    "    product_id,  -- \uD83D\uDD25 MUST include entity ID for group_col\n",
    "    order_date AS ds, \n",
    "    SUM(revenue) AS revenue\n",
    "  FROM `catalog`.`schema`.`orders` AS o\n",
    "  WHERE order_date >= date_add(DAY, -300, CURRENT_DATE())  -- 300 days history for 30-day forecast (10:1 ratio)\n",
    "    AND order_date IS NOT NULL\n",
    "    AND product_id IS NOT NULL\n",
    "  GROUP BY product_id, order_date\n",
    "  ORDER BY ds\n",
    "),\n",
    "-- Step 2: Generate revenue forecast with prediction intervals (with group_col)\n",
    "forecast_results AS (\n",
    "  SELECT * FROM AI_FORECAST(\n",
    "    TABLE(past), \n",
    "    time_col => 'ds', \n",
    "    value_col => 'revenue',\n",
    "    group_col => 'product_id',  -- \uD83D\uDD25 MANDATORY: needed for joining back\n",
    "    horizon => (SELECT date_add(DAY, 30, MAX(ds)) FROM past)  -- 30 days ahead\n",
    "  )\n",
    "  -- ✅ NO LIMIT in CTEs\n",
    "),\n",
    "-- Step 3: JOIN back to original table to get product details\n",
    "forecast_with_context AS (\n",
    "  SELECT \n",
    "    f.*,\n",
    "    p.product_name,\n",
    "    p.category,\n",
    "    p.cost_per_unit\n",
    "  FROM forecast_results AS f\n",
    "  LEFT JOIN `catalog`.`schema`.`products` AS p\n",
    "    ON f.product_id = p.product_id  -- JOIN using group_col\n",
    "  -- ✅ NO LIMIT in CTEs\n",
    "),\n",
    "-- Step 4: Add row-level actionable recommendations for each forecast\n",
    "enriched AS (\n",
    "  SELECT *,\n",
    "    ai_query('{sql_model_serving}', CONCAT('Product: ', product_name, \n",
    "                  ', Category: ', category,\n",
    "                  ', Forecast revenue: $', revenue_forecast,  -- CONCAT auto-converts\n",
    "                  ' for period ', ds, \n",
    "                  '. Provide 3 specific actionable recommendations. ',\n",
    "                  'Output ONLY JSON: {{\"ai_cat_forecast_action\": \"value\", \"ai_txt_recommendations\": \"text\"}}')) AS recommendations\n",
    "  FROM forecast_with_context\n",
    "  -- ✅ NO LIMIT in CTEs\n",
    "),\n",
    "-- Final output: Forecast with actionable business recommendations\n",
    "final_output AS (\n",
    "  SELECT * FROM enriched\n",
    ")\n",
    "SELECT * FROM final_output\n",
    "-- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "-- WHERE category IN ('CategoryA', 'CategoryB', 'CategoryC')\n",
    "-- AND get_json_object(recommendations, '$.ai_cat_forecast_action') IN ('Increase Inventory', 'Maintain', 'Reduce Inventory', 'Promote', 'Monitor')\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "\n",
    "**BEST PRACTICES FOR AI_FORECAST:**\n",
    "- **\uD83D\uDD25 ALWAYS specify group_col \uD83D\uDD25** - Use entity ID columns (customer_id, product_id, route_id, store_id, etc.) as the group_col so you can join forecast results back to original table\n",
    "- Always use WHERE clause with date filtering using ADAPTIVE ratios based on time granularity (see History-to-Horizon Ratio table)\n",
    "- Use dynamic horizon: `horizon => (SELECT date_add(UNIT, X, MAX(ds)) FROM past)` where UNIT is HOUR, DAY, WEEK, MONTH, QUARTER, or YEAR (no quotes)\n",
    "- Ensure input data has unique rows (no duplicates for time + group combinations)\n",
    "- **ALWAYS add a JOIN CTE after AI_FORECAST** to retrieve additional columns from original table using group_col as JOIN key\n",
    "- Use `parameters => '{{\"global_floor\": 0}}'` for non-negative metrics (revenue, quantity, etc.)\n",
    "- **\uD83D\uDEA8 CRITICAL: Parameters MUST use SINGLE QUOTES on the outside \uD83D\uDEA8**\n",
    "  - ✅ CORRECT: `parameters => '{{\"global_floor\": 0}}'` (single quotes wrapping JSON)\n",
    "  - ❌ WRONG: `parameters => \"{{\"global_floor\": 0}}\"` (double quotes - WILL FAIL)\n",
    "  - The JSON content inside uses double quotes for keys/strings, but the SQL string literal MUST use single quotes\n",
    "- **NOTE**: In prompt templates, double braces are used to escape and produce single braces in output\n",
    "- For sparse data, explicitly specify `frequency` parameter\n",
    "- **LIMIT 10 only in FIRST CTE** - at the END of the first CTE, nowhere else\n",
    "\n",
    "**\uD83D\uDD25 MANDATORY RECOMMENDATION REQUIREMENT FOR AI_CLASSIFY:**\n",
    "\n",
    "**EVERY ai_classify query MUST include row-level recommendations using ai_query**\n",
    "\n",
    "**RECOMMENDATION IMPLEMENTATION PATTERNS (MANDATORY):**\n",
    "\n",
    "**Pattern 1 - ai_classify + ai_query**: Generate actionable recommendations based on classification\n",
    "```sql\n",
    "-- Step 1: Customer records with business metrics\n",
    "-- \uD83D\uDEA8 EVERY column must be COALESCE'd or have IS NOT NULL check!\n",
    "WITH customer_data AS (\n",
    "  SELECT DISTINCT\n",
    "    customer_id,                                              -- CRITICAL: filtered with IS NOT NULL\n",
    "    COALESCE(TRIM(customer_name), 'Unknown Customer') AS customer_name,  -- ✅ COALESCE'd\n",
    "    COALESCE(TRIM(feedback_text), 'No feedback provided') AS feedback_text,  -- ✅ COALESCE'd\n",
    "    COALESCE(total_purchases, 0) AS total_purchases,          -- ✅ COALESCE'd\n",
    "    COALESCE(lifetime_value, 0.0) AS lifetime_value,          -- ✅ COALESCE'd\n",
    "    COALESCE(days_since_last_purchase, 9999) AS days_since_last_purchase,  -- ✅ COALESCE'd (high default = churned)\n",
    "    COALESCE(support_tickets_count, 0) AS support_tickets_count  -- ✅ COALESCE'd\n",
    "  FROM `catalog`.`schema`.`customers` AS c\n",
    "  WHERE customer_id IS NOT NULL\n",
    "  LIMIT 10  -- ✅ LIMIT 10 at END of first CTE only\n",
    "),\n",
    "-- Step 2: Classify customers into segments\n",
    "classified AS (\n",
    "  SELECT \n",
    "    *,\n",
    "    ai_classify(\n",
    "      'Customer with $' || CAST(lifetime_value AS STRING) || ' LTV, ' ||\n",
    "      CAST(total_purchases AS STRING) || ' purchases, last active ' ||\n",
    "      CAST(days_since_last_purchase AS STRING) || ' days ago',\n",
    "      ARRAY('High Value VIP', 'High Value At-Risk', 'Medium Value Active', \n",
    "            'Medium Value Declining', 'Low Value', 'Churned')\n",
    "    ) AS customer_segment\n",
    "  FROM customer_data\n",
    ")\n",
    "-- Final: Generate personalized strategies based on segment + context\n",
    "SELECT \n",
    "  customer_id,\n",
    "  customer_name,\n",
    "  customer_segment,\n",
    "  ai_query('{sql_model_serving}',\n",
    "    'Customer Segment: ' || customer_segment || \n",
    "    '. Lifetime Value: $' || CAST(lifetime_value AS STRING) ||\n",
    "    ', Total Purchases: ' || CAST(total_purchases AS STRING) ||\n",
    "    ', Days Since Last Purchase: ' || CAST(days_since_last_purchase AS STRING) ||\n",
    "    ', Support Tickets: ' || CAST(support_tickets_count AS STRING) ||\n",
    "    '. Generate a personalized retention strategy with: 1) Specific outreach approach, ' ||\n",
    "    '2) Recommended offers or incentives, 3) Engagement tactics, 4) Risk mitigation steps if applicable.'\n",
    "  ) AS retention_strategy\n",
    "FROM classified\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "\n",
    "**Pattern 2 - ai_classify + ai_query with rich context**: Use general-purpose AI with comprehensive business context\n",
    "```sql\n",
    "-- Step 1: Support tickets with context\n",
    "WITH ticket_data AS (\n",
    "  SELECT DISTINCT\n",
    "    ticket_id,\n",
    "    COALESCE(TRIM(customer_id), 'Unknown Customer') AS customer_id,\n",
    "    COALESCE(TRIM(ticket_description), 'No description') AS ticket_description,\n",
    "    COALESCE(TRIM(product_id), 'Unknown Product') AS product_id,\n",
    "    COALESCE(TRIM(issue_severity), 'Unknown') AS issue_severity,\n",
    "    COALESCE(response_time_hours, 0.0) AS response_time_hours\n",
    "  FROM `catalog`.`schema`.`support_tickets` AS t\n",
    "  WHERE ticket_id IS NOT NULL AND ticket_description IS NOT NULL\n",
    "  LIMIT 10\n",
    "),\n",
    "-- Step 2: Classify tickets by urgency/complexity\n",
    "classified AS (\n",
    "  SELECT *,\n",
    "    ai_classify(ticket_description, ARRAY('Critical Urgent', 'High Priority', 'Medium Priority', 'Low Priority', 'Info Request')) AS ticket_priority\n",
    "  FROM ticket_data\n",
    "),\n",
    "-- Step 3: Generate ai_sys_prompt\n",
    "prompt_generation AS (\n",
    "  SELECT *,\n",
    "    CONCAT(\n",
    "      'You are a Support Operations Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "      'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "      'Business priorities are: {enriched_business_priorities}. ',\n",
    "      'With 15 years of experience in support operations, your expertise aligns with the strategic initiative: Customer satisfaction. ',\n",
    "      'Priority Level: ', ticket_priority, '. Issue: ', ticket_description, '. Product: ', product_id, '. ',\n",
    "      'Generate a detailed resolution plan with: 1) Immediate actions, 2) Escalation path if needed, 3) Estimated resolution time, 4) Customer communication template. ',\n",
    "      'Output ONLY a JSON object with NO markdown, NO extra text. ',\n",
    "      'Format: {{\"ai_txt_immediate_actions\": \"text\", \"ai_txt_escalation_path\": \"text\", \"ai_txt_resolution_time\": \"text\", \"ai_txt_communication_template\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "      'Output ONLY the JSON object, nothing else.'\n",
    "    ) AS ai_sys_prompt\n",
    "  FROM classified\n",
    "),\n",
    "-- Step 4: Call ai_query\n",
    "enriched AS (\n",
    "  SELECT *, ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS resolution_plan\n",
    "  FROM prompt_generation\n",
    ")\n",
    "-- Final output with ai_sys_prompt as LAST column\n",
    "SELECT \n",
    "  ticket_id, customer_id, ticket_description, product_id, ticket_priority,\n",
    "  get_json_object(resolution_plan, '$.ai_txt_immediate_actions') AS ai_txt_immediate_actions,\n",
    "  get_json_object(resolution_plan, '$.ai_txt_escalation_path') AS ai_txt_escalation_path,\n",
    "  get_json_object(resolution_plan, '$.ai_txt_resolution_time') AS ai_txt_resolution_time,\n",
    "  get_json_object(resolution_plan, '$.ai_txt_communication_template') AS ai_txt_communication_template,\n",
    "  COALESCE(get_json_object(resolution_plan, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "  COALESCE(get_json_object(resolution_plan, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "  COALESCE(get_json_object(resolution_plan, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "  COALESCE(get_json_object(resolution_plan, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "  COALESCE(TRY_CAST(get_json_object(resolution_plan, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "  COALESCE(get_json_object(resolution_plan, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "  COALESCE(get_json_object(resolution_plan, '$.ai_sys_missing_data'), 'No missing data') AS ai_sys_missing_data,\n",
    "  ai_sys_prompt  -- ✅ LAST COLUMN\n",
    "FROM enriched;\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "\n",
    "**\uD83D\uDD25 STATISTICAL FUNCTIONS - SQL IMPLEMENTATION PATTERNS:**\n",
    "\n",
    "**Pattern 1: Correlation Analysis → AI Interpretation**\n",
    "```sql\n",
    "-- \uD83D\uDEA8 Statistical functions can return NULL - always COALESCE results!\n",
    "WITH correlation_analysis AS (\n",
    "  SELECT\n",
    "    COALESCE(CORR(marketing_spend, revenue), 0.0) AS marketing_revenue_correlation,\n",
    "    COALESCE(CORR(customer_satisfaction, churn_rate), 0.0) AS satisfaction_churn_correlation,\n",
    "    COALESCE(REGR_SLOPE(revenue, marketing_spend), 0.0) AS revenue_per_marketing_dollar,\n",
    "    COALESCE(REGR_R2(revenue, marketing_spend), 0.0) AS predictive_power\n",
    "  FROM `catalog`.`schema`.`table` AS t\n",
    "  WHERE marketing_spend IS NOT NULL AND revenue IS NOT NULL\n",
    "    AND customer_satisfaction IS NOT NULL AND churn_rate IS NOT NULL\n",
    "  LIMIT 10\n",
    "),\n",
    "-- Step 2: Generate ai_sys_prompt\n",
    "prompt_generation AS (\n",
    "  SELECT *,\n",
    "    CONCAT(\n",
    "      'You are a Marketing Analytics Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "      'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "      'Business priorities are: {enriched_business_priorities}. ',\n",
    "      'With 15 years of experience in ROI optimization and marketing analytics, ',\n",
    "      'your expertise aligns with the strategic initiative: Marketing effectiveness. ',\n",
    "      'Marketing-Revenue Correlation: ', marketing_revenue_correlation,\n",
    "      '. Satisfaction-Churn Correlation: ', satisfaction_churn_correlation,\n",
    "      '. Revenue per marketing dollar: ', revenue_per_marketing_dollar,\n",
    "      '. Predictive power (R²): ', predictive_power, '. ',\n",
    "      'Output ONLY a JSON object with NO markdown, NO extra text. ',\n",
    "      'Format: {{\"ai_txt_key_drivers\": \"text\", \"ai_txt_investment_recommendations\": \"text\", \"ai_txt_risk_mitigation\": \"text\", \"ai_txt_optimization_opportunities\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "      'Output ONLY the JSON object, nothing else.'\n",
    "    ) AS ai_sys_prompt\n",
    "  FROM correlation_analysis\n",
    "),\n",
    "-- Step 3: Call ai_query\n",
    "enriched AS (\n",
    "  SELECT *, ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS strategic_recommendations\n",
    "  FROM prompt_generation\n",
    "),\n",
    "final_output AS (\n",
    "  SELECT \n",
    "    marketing_revenue_correlation, satisfaction_churn_correlation, revenue_per_marketing_dollar, predictive_power,\n",
    "    get_json_object(strategic_recommendations, '$.ai_txt_key_drivers') AS ai_txt_key_drivers,\n",
    "    get_json_object(strategic_recommendations, '$.ai_txt_investment_recommendations') AS ai_txt_investment_recommendations,\n",
    "    COALESCE(get_json_object(strategic_recommendations, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "    COALESCE(get_json_object(strategic_recommendations, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "    COALESCE(get_json_object(strategic_recommendations, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "    COALESCE(get_json_object(strategic_recommendations, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "    COALESCE(TRY_CAST(get_json_object(strategic_recommendations, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "    COALESCE(get_json_object(strategic_recommendations, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "    COALESCE(get_json_object(strategic_recommendations, '$.ai_sys_missing_data'), 'No missing data') AS ai_sys_missing_data,\n",
    "    ai_sys_prompt  -- ✅ LAST COLUMN\n",
    "  FROM enriched\n",
    ")\n",
    "SELECT * FROM final_output;\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "\n",
    "**Pattern 2: Trend Detection → AI Strategy**\n",
    "```sql\n",
    "WITH trend_metrics AS (\n",
    "  SELECT\n",
    "    COALESCE(TRIM(product_category), 'Unknown') AS product_category,\n",
    "    COALESCE(REGR_SLOPE(sales, month_number), 0.0) AS sales_growth_rate,\n",
    "    COALESCE(REGR_INTERCEPT(sales, month_number), 0.0) AS baseline_sales,\n",
    "    COALESCE(REGR_R2(sales, month_number), 0.0) AS trend_reliability,\n",
    "    COALESCE(STDDEV_POP(sales), 0.0) AS sales_volatility,\n",
    "    COALESCE(SKEWNESS(sales), 0.0) AS distribution_skew\n",
    "  FROM `catalog`.`schema`.`sales_data` AS s\n",
    "  GROUP BY product_category\n",
    "  LIMIT 10\n",
    "),\n",
    "prompt_generation AS (\n",
    "  SELECT *,\n",
    "    CONCAT(\n",
    "      'You are a Product Strategy Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "      'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "      'Business priorities are: {enriched_business_priorities}. ',\n",
    "      'With 18 years of experience in portfolio management, your expertise aligns with the strategic initiative: Product growth. ',\n",
    "      'Product Category: ', product_category,\n",
    "      '. Growth Rate: ', sales_growth_rate,\n",
    "      '. Volatility: ', sales_volatility,\n",
    "      '. Trend Reliability: ', trend_reliability,\n",
    "      '. Distribution Skew: ', distribution_skew, '. ',\n",
    "      'Output ONLY a JSON object with NO markdown, NO extra text. ',\n",
    "      'Format: {{\"ai_cat_trend_classification\": \"accelerating/stable/declining\", \"ai_txt_growth_strategy\": \"text\", \"ai_txt_risk_assessment\": \"text\", \"ai_txt_investment_priorities\": \"text\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "      'Output ONLY the JSON object, nothing else.'\n",
    "    ) AS ai_sys_prompt\n",
    "  FROM trend_metrics\n",
    "),\n",
    "enriched AS (\n",
    "  SELECT *, ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS category_strategy\n",
    "  FROM prompt_generation\n",
    "),\n",
    "final_output AS (\n",
    "  SELECT \n",
    "    product_category, sales_growth_rate, sales_volatility, trend_reliability, distribution_skew,\n",
    "    get_json_object(category_strategy, '$.ai_cat_trend_classification') AS ai_cat_trend_classification,\n",
    "    get_json_object(category_strategy, '$.ai_txt_growth_strategy') AS ai_txt_growth_strategy,\n",
    "    COALESCE(get_json_object(category_strategy, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "    COALESCE(get_json_object(category_strategy, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "    COALESCE(get_json_object(category_strategy, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "    COALESCE(get_json_object(category_strategy, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "    COALESCE(TRY_CAST(get_json_object(category_strategy, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "    COALESCE(get_json_object(category_strategy, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "    COALESCE(get_json_object(category_strategy, '$.ai_sys_missing_data'), 'No missing data') AS ai_sys_missing_data,\n",
    "    ai_sys_prompt  -- ✅ LAST COLUMN\n",
    "  FROM enriched\n",
    ")\n",
    "SELECT * FROM final_output;\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "\n",
    "**Pattern 3: Deviation Detection → AI Classification & Response**\n",
    "```sql\n",
    "WITH deviation_analysis AS (\n",
    "  SELECT DISTINCT\n",
    "    customer_id,\n",
    "    COALESCE(purchase_amount, 0.0) AS purchase_amount,\n",
    "    COALESCE(ROUND(AVG(purchase_amount) OVER (), 2), 0.0) AS avg_purchase,\n",
    "    COALESCE(ROUND(STDDEV_POP(purchase_amount) OVER (), 2), 0.0) AS stddev_purchase,\n",
    "    COALESCE(ROUND((purchase_amount - AVG(purchase_amount) OVER ()) / NULLIF(STDDEV_POP(purchase_amount) OVER (), 0), 2), 0.0) AS z_score,\n",
    "    COALESCE(ROUND(PERCENTILE_APPROX(purchase_amount, 0.95) OVER (), 2), 0.0) AS p95_threshold,\n",
    "    COALESCE(NTILE(10) OVER (ORDER BY purchase_amount), 5) AS decile\n",
    "  FROM `catalog`.`schema`.`purchases` AS p\n",
    "  WHERE customer_id IS NOT NULL\n",
    "  LIMIT 10\n",
    "),\n",
    "classified AS (\n",
    "  SELECT *,\n",
    "    ai_classify(CONCAT('Z-score: ', z_score, ', Decile: ', decile), ARRAY('High Value VIP', 'Premium Customer', 'Standard Customer', 'At-Risk Low Spender', 'Outlier Anomaly')) AS ai_cat_customer_segment\n",
    "  FROM deviation_analysis\n",
    "),\n",
    "prompt_generation AS (\n",
    "  SELECT *,\n",
    "    CONCAT(\n",
    "      'You are a Customer Analytics Director for {business_name} which is focused on {enriched_business_context}. ',\n",
    "      'The organization''s strategic goals include: {enriched_strategic_goals}. ',\n",
    "      'Business priorities are: {enriched_business_priorities}. ',\n",
    "      'With 14 years of experience in segmentation, your expertise aligns with the strategic initiative: Customer value optimization. ',\n",
    "      'Customer deviation analysis - Z-score: ', z_score,\n",
    "      ', Decile: ', decile,\n",
    "      ', Purchase: $', purchase_amount,\n",
    "      ', Average: $', avg_purchase, '. ',\n",
    "      'Output ONLY a JSON object with NO markdown, NO extra text. ',\n",
    "      'Format: {{\"ai_txt_segment_rationale\": \"text\", \"ai_txt_engagement_strategy\": \"text\", \"ai_txt_personalized_offers\": \"text\", \"ai_cat_retention_risk\": \"Critical/High/Medium/Low/Minimal\", \"ai_txt_business_outcome\": \"text\", \"ai_txt_executive_summary\": \"text\", \"ai_sys_importance\": \"High\", \"ai_sys_urgency\": \"High\", \"ai_sys_confidence\": 0.85, \"ai_sys_feedback\": \"...\", \"ai_sys_missing_data\": \"...\"}}. ',\n",
    "      'Output ONLY the JSON object, nothing else.'\n",
    "    ) AS ai_sys_prompt\n",
    "  FROM classified\n",
    "),\n",
    "enriched AS (\n",
    "  SELECT *, ai_query('{sql_model_serving}', ai_sys_prompt, modelParameters => named_struct('temperature', 0.4)) AS engagement_strategy\n",
    "  FROM prompt_generation\n",
    "),\n",
    "final_output AS (\n",
    "  SELECT \n",
    "    customer_id, purchase_amount, avg_purchase, z_score, decile, ai_cat_customer_segment,\n",
    "    get_json_object(engagement_strategy, '$.ai_txt_segment_rationale') AS ai_txt_segment_rationale,\n",
    "    get_json_object(engagement_strategy, '$.ai_txt_engagement_strategy') AS ai_txt_engagement_strategy,\n",
    "    get_json_object(engagement_strategy, '$.ai_cat_retention_risk') AS ai_cat_retention_risk,\n",
    "    COALESCE(get_json_object(engagement_strategy, '$.ai_txt_business_outcome'), 'No business outcome calculated') AS ai_txt_business_outcome,\n",
    "    COALESCE(get_json_object(engagement_strategy, '$.ai_txt_executive_summary'), 'No summary') AS ai_txt_executive_summary,\n",
    "    COALESCE(get_json_object(engagement_strategy, '$.ai_sys_importance'), 'Medium') AS ai_sys_importance,\n",
    "    COALESCE(get_json_object(engagement_strategy, '$.ai_sys_urgency'), 'Medium') AS ai_sys_urgency,\n",
    "    COALESCE(TRY_CAST(get_json_object(engagement_strategy, '$.ai_sys_confidence') AS DECIMAL(3,2)), 0.0) AS ai_sys_confidence,\n",
    "    COALESCE(get_json_object(engagement_strategy, '$.ai_sys_feedback'), 'No feedback') AS ai_sys_feedback,\n",
    "    COALESCE(get_json_object(engagement_strategy, '$.ai_sys_missing_data'), 'No missing data') AS ai_sys_missing_data,\n",
    "    ai_sys_prompt  -- ✅ LAST COLUMN\n",
    "  FROM enriched\n",
    ")\n",
    "SELECT * FROM final_output\n",
    ";\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 9. OUTPUT FORMAT\n",
    "\n",
    "Return ONLY the raw SQL query. Do NOT wrap in JSON.\n",
    "- Start with `-- Use Case: [ID] - [Name]` comment\n",
    "- Then the complete SQL query with all CTEs\n",
    "- **LIMIT 10 only in FIRST CTE** - at the END of the first CTE statement, nowhere else\n",
    "- No markdown fences, no JSON wrapper, no conversational text, no preamble.\n",
    "- Use only columns from \"Columns From Use Case\" and \"AVAILABLE TABLES AND COLUMNS\".\n",
    "\n",
    "**\uD83D\uDD25 SQL LENGTH - NO ARTIFICIAL LIMITS \uD83D\uDD25**\n",
    "- Generate as MANY CTEs as needed to fully implement the use case (3-10 CTEs is normal)\n",
    "- Generate as MANY lines of code as required - there is NO line limit\n",
    "- Do NOT artificially shorten or simplify the SQL\n",
    "- Complex use cases should have complex, comprehensive SQL\n",
    "- Include ALL statistical functions, ALL AI functions, ALL transformations needed\n",
    "- A typical sophisticated query should have **200 to 600 lines** of code\n",
    "- NEVER sacrifice completeness for brevity\n",
    "\n",
    "**\uD83D\uDEA8 FIRST CTE MUST USE DISTINCT \uD83D\uDEA8**\n",
    "- ALWAYS use `SELECT DISTINCT` in the first CTE to eliminate duplicate records\n",
    "- Duplicates will cascade errors through all downstream analysis\n",
    "\n",
    "---\n",
    "\n",
    "### 10. FINAL CHECKLIST\n",
    "\n",
    "Before generating, verify:\n",
    "✅ All string literals use SINGLE QUOTES\n",
    "✅ Arrays have MAX 20 elements\n",
    "✅ For ai_classify/ai_extract: Each array item MUST be < 50 characters\n",
    "✅ LIMIT 10 at END of FIRST CTE only - NO LIMIT in other CTEs or final SELECT\n",
    "✅ No WHERE clauses except IS NULL / IS NOT NULL\n",
    "✅ All AI functions from the use case are used\n",
    "✅ Tables are fully qualified (catalog.schema.table)\n",
    "✅ CONCAT syntax is correct (quotes on literals, not on columns)\n",
    "✅ ai_query uses the configured model endpoint: {sql_model_serving}\n",
    "✅ ai_parse_document ONLY used with READ_FILES for document files, NOT table columns\n",
    "✅ SQL is COMPREHENSIVE - no artificial length limits, include ALL needed CTEs and functions\n",
    "✅ **EVERY CTE SELECT has a FROM clause** - SELECT *, ... FROM previous_cte (NO SELECT without FROM!)\n",
    "\n",
    "---\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL: SCHEMA VALIDATION BEFORE GENERATING \uD83D\uDEA8**\n",
    "\n",
    "Before you generate any SQL, check if the \"AVAILABLE TABLES AND COLUMNS\" section above contains actual table and column definitions.\n",
    "DO NOT output any checking messages, status indicators, or confirmation text.\n",
    "\n",
    "**IF THE SCHEMA IS EMPTY**: Output ONLY this (no markdown fences):\n",
    "-- Use Case: {use_case_id} - Schema Missing\n",
    "-- Required tables: {tables_involved}\n",
    "SELECT 'Schema missing - manual SQL required' AS error_message;\n",
    "\n",
    "**IF THE SCHEMA IS PROVIDED**: Output ONLY the raw SQL query starting with:\n",
    "-- Use Case: N01-AI01 - Customer Sentiment Analysis\n",
    "-- [description]\n",
    "WITH ...\n",
    "\n",
    "\uD83D\uDEAB DO NOT OUTPUT ANY OF THESE:\n",
    "- \"# SCHEMA VALIDATION CHECK\"\n",
    "- \"Checking...\"\n",
    "- \"✅ SCHEMA PROVIDED\"\n",
    "- \"Proceeding with SQL generation...\"\n",
    "- Any markdown code fences\n",
    "- Any explanatory text\n",
    "\n",
    "---\n",
    "\n",
    "### \uD83D\uDD25\uD83D\uDD25\uD83D\uDD25 FINAL COMPREHENSIVE VALIDATION CHECKLIST - MANDATORY BEFORE SUBMITTING SQL \uD83D\uDD25\uD83D\uDD25\uD83D\uDD25\n",
    "\n",
    "**BEFORE YOU SUBMIT YOUR SQL, YOU MUST VERIFY EVERY SINGLE ITEM BELOW. MISSING EVEN ONE ITEM WILL CAUSE QUERY FAILURE.**\n",
    "\n",
    "#### **SECTION 1: NULL HANDLING - ZERO TOLERANCE**\n",
    "\n",
    "☐ **⛔ FIRST CTE: EVERY SINGLE COLUMN must have NULL protection - NO EXCEPTIONS! ⛔**\n",
    "   - Scan EACH column one by one - if ANY column lacks COALESCE or IS NOT NULL check, FIX IT!\n",
    "   - Common mistake: Forgetting workspaceName, accountName, etc. even when other columns are protected\n",
    "☐ **CRITICAL**: Every column used in ANY CONCAT for AI functions has COALESCE applied in a PREVIOUS CTE\n",
    "☐ **\uD83D\uDEA8 CRITICAL \uD83D\uDEA8**: ALL COALESCE, CAST, ROUND, TRIM operations done in PREVIOUS CTE (NEVER inside CONCAT!)\n",
    "☐ **CRITICAL**: Numeric columns: `COALESCE(ROUND(col, 2), 0.0)` - keep as DOUBLE, CONCAT auto-converts\n",
    "☐ **CRITICAL**: String columns: `COALESCE(TRIM(col), 'Default')` - with business-friendly defaults\n",
    "☐ **CRITICAL**: Critical columns (IDs, required fields) filtered with WHERE...IS NOT NULL (NOT COALESCEd)\n",
    "☐ **CRITICAL**: No column in any CONCAT can possibly be NULL after transformations\n",
    "☐ **CRITICAL**: The prompt building CTE has NO COALESCE, NO CAST, NO ROUND, NO TRIM - only clean CONCAT\n",
    "☐ **\uD83D\uDEA8 CRITICAL \uD83D\uDEA8**: ALL COALESCE default STRING values have SINGLE QUOTES (unquoted = SYNTAX ERROR!):\n",
    "   - ✅ CORRECT: `COALESCE(TRIM(name), 'Unknown Customer')` -- 'Unknown Customer' has quotes\n",
    "   - ✅ CORRECT: `COALESCE(TRIM(category), 'Not Specified')` -- 'Not Specified' has quotes\n",
    "   - ✅ CORRECT: `COALESCE(TRIM(status), 'Pending Review')` -- 'Pending Review' has quotes\n",
    "   - ✅ CORRECT: `COALESCE(TRIM(region), 'Unassigned Region')` -- 'Unassigned Region' has quotes\n",
    "\n",
    "#### **SECTION 2: AI_FORECAST REQUIREMENTS - MANDATORY**\n",
    "\n",
    "☐ **MANDATORY**: Input CTE uses GROUP BY on time column to ensure unique values per group\n",
    "☐ **MANDATORY**: GROUP BY includes: all group_col columns + time column\n",
    "☐ **MANDATORY**: Value columns use aggregate functions (SUM, AVG, COUNT, MAX, MIN)\n",
    "☐ **MANDATORY**: Input CTE uses WHERE clause with date filtering using adaptive ratios (high-freq: fixed periods, mid-freq: 10:1, low-freq: reduced ratios)\n",
    "☐ **MANDATORY**: Horizon uses date_add(UNIT, X, MAX(time_col)) with dynamic calculation\n",
    "☐ **MANDATORY**: UNIT is DAY, WEEK, MONTH, or QUARTER (no quotes)\n",
    "☐ **MANDATORY**: group_col is specified (enables joining back to original table)\n",
    "☐ **MANDATORY**: Filter NULL forecasted values: WHERE {{value_col}}_forecast IS NOT NULL\n",
    "☐ **MANDATORY**: If multiple value_col, filter ALL: WHERE col1_forecast IS NOT NULL AND col2_forecast IS NOT NULL\n",
    "\n",
    "#### **SECTION 3: SCHEMA ADHERENCE - ZERO HALLUCINATION**\n",
    "\n",
    "☐ Every table name exists in \"AVAILABLE TABLES AND COLUMNS\" section\n",
    "☐ Every column name exists in its table in \"AVAILABLE TABLES AND COLUMNS\" section\n",
    "☐ All tables fully qualified: `` `catalog`.`schema`.`table` ``\n",
    "☐ Every table has an alias immediately after table name\n",
    "☐ No invented/assumed column names (id, name, date, status, etc.)\n",
    "☐ JOIN keys exist in BOTH tables being joined\n",
    "☐ All columns in final SELECT exist in the last CTE\n",
    "\n",
    "#### **SECTION 4: QUOTE USAGE - THE #1 MOST COMMON ERROR - DO NOT SKIP!**\n",
    "\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 MANDATORY COALESCE STRING QUOTE VALIDATION \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "☐ **⛔ STOP AND CHECK: Scan EVERY COALESCE in your SQL for missing quotes! ⛔**\n",
    "☐ **WRONG**: `COALESCE(TRIM(name), Unknown)` - NO QUOTES = SYNTAX ERROR!\n",
    "☐ **CORRECT**: `COALESCE(TRIM(name), 'Unknown')` - WITH QUOTES = WORKS!\n",
    "☐ **WRONG**: `COALESCE(TRIM(status), Pending Review)` - NO QUOTES = SYNTAX ERROR!\n",
    "☐ **CORRECT**: `COALESCE(TRIM(status), 'Pending Review')` - WITH QUOTES = WORKS!\n",
    "☐ **Rule**: ANY text after the comma in COALESCE MUST have 'single quotes'\n",
    "☐ **Exception**: Numbers (0.0, 0, 123) and booleans (TRUE, FALSE) do NOT need quotes\n",
    "\n",
    "**General Quote Rules:**\n",
    "☐ String literals: ALWAYS single quotes `'text'`\n",
    "☐ Column names in CONCAT: NO quotes (e.g., `column_name`)\n",
    "☐ ARRAY items: Single quotes `ARRAY('item1', 'item2')`\n",
    "☐ ARRAY limitations: Max 20 items, each <50 chars\n",
    "☐ AI_FORECAST parameters: Single quotes wrap JSON `'{{\"key\": \"value\"}}'`\n",
    "☐ No double quotes used for string literals anywhere\n",
    "☐ AI_FORECAST column names: `time_col => 'ds'` (column name AS string literal!)\n",
    "\n",
    "#### **SECTION 5: CTE STRUCTURE AND NAMING**\n",
    "\n",
    "☐ Business-friendly CTE names (NOT: cte1, temp, data, results, final)\n",
    "☐ Single WITH statement with all CTEs comma-separated\n",
    "☐ Every CTE documented with \"-- Step X:\" comment\n",
    "☐ Final SELECT has comment: \"-- Final output: {{description}}\"\n",
    "☐ Use SELECT * in intermediate CTEs to preserve columns\n",
    "☐ No columns dropped in intermediate CTEs that are needed in final SELECT\n",
    "\n",
    "#### **SECTION 6: AI FUNCTION SPECIFIC REQUIREMENTS**\n",
    "\n",
    "☐ **ai_query**: Prompt starts with persona (role + years + expertise)\n",
    "☐ **ai_query**: Prompt includes \"Output ONLY JSON with NO markdown fences, NO extra text, JUST the JSON\"\n",
    "☐ **ai_query**: JSON format shown: `{{\"key\": \"value\"}}`\n",
    "☐ **ai_query**: Prompt ends with \"Output ONLY the JSON object, nothing else.\"\n",
    "☐ **ai_query**: Extract JSON with get_json_object(), NOT dot notation\n",
    "☐ **ai_query**: 3-5 categorical columns + 2-4 narrative columns in JSON\n",
    "☐ **ai_classify**: ARRAY has ≤20 items, each <50 chars\n",
    "☐ **ai_extract**: ARRAY has ≤20 items, each <50 chars\n",
    "☐ **ai_parse_document**: ONLY used with READ_FILES for unstructured docs (NOT table columns)\n",
    "\n",
    "#### **SECTION 7: BUSINESS REQUIREMENTS**\n",
    "\n",
    "☐ Column names are business-friendly (NOT: classification, sentiment, similarity)\n",
    "☐ Categorical columns have max 20 distinct values for filtering\n",
    "☐ **\uD83D\uDEA8 Narrative columns MUST identify the principal with key attributes \uD83D\uDEA8**:\n",
    "   - ❌ WRONG: \"The data shows high fuel consumption\"\n",
    "   - ✅ CORRECT: \"Flight EK005 DXB-LHR (A380) shows fuel consumption of 4800kg/hr\"\n",
    "   - Include entity ID/name + identifiers (route, type) + then analysis\n",
    "☐ Combine multiple functions creatively when the use case benefits from it (AI + statistical)\n",
    "☐ LIMIT 10 at END of FIRST CTE only - NO LIMIT in other CTEs or final SELECT\n",
    "☐ **FIRST CTE uses SELECT DISTINCT** to eliminate duplicate records\n",
    "☐ SQL is COMPREHENSIVE - 3-10 CTEs, 200-600 lines as needed (no artificial length limit)\n",
    "\n",
    "#### **SECTION 8: SYNTAX AND DIALECT**\n",
    "\n",
    "☐ Data types: STRING (not VARCHAR), DOUBLE (not DECIMAL), BIGINT, TIMESTAMP\n",
    "☐ Date functions: DATE_TRUNC, CURRENT_DATE(), date_add\n",
    "☐ No WHERE clause value comparisons (only IS NULL / IS NOT NULL)\n",
    "☐ No HAVING clauses with specific values\n",
    "☐ No hardcoded WHERE filters like WHERE status = 'active'\n",
    "\n",
    "#### **SECTION 9: AI_FORECAST SPECIFIC POST-GENERATION FILTERS**\n",
    "\n",
    "☐ **CRITICAL**: After AI_FORECAST, added a CTE to filter WHERE {{value_col}}_forecast IS NOT NULL\n",
    "☐ **CRITICAL**: If joining forecast back to original table, join happens AFTER filtering NULL forecasts\n",
    "☐ **CRITICAL**: All forecast result columns checked for NULL ({{value}}_forecast, {{value}}_upper, {{value}}_lower)\n",
    "\n",
    "#### **SECTION 10: FINAL OUTPUT CTE WITH COMMENTED FILTERS (MANDATORY)**\n",
    "\n",
    "☐ **MANDATORY**: Wrap final SELECT in a `final_output` CTE\n",
    "☐ **MANDATORY**: Add `SELECT * FROM final_output` as the final statement\n",
    "☐ **MANDATORY**: Add commented WHERE clause listing ALL categorical column values\n",
    "☐ **MANDATORY**: End SQL with `--END OF GENERATED SQL` marker (CRITICAL for truncation detection)\n",
    "☐ Format: `-- TO DO: Use WHERE filtering below for further narrowing down the selected results`\n",
    "☐ Each ai_cat_ column listed with its possible values in commented WHERE clause\n",
    "☐ Example pattern:\n",
    "   ```sql\n",
    "   final_output AS (\n",
    "     SELECT ... FROM previous_cte\n",
    "   )\n",
    "   SELECT * FROM final_output\n",
    "   -- TO DO: Use WHERE filtering below for further narrowing down the selected results\n",
    "   -- WHERE ai_cat_column1 IN ('Value1', 'Value2', 'Value3')\n",
    "   -- AND ai_cat_column2 IN ('A', 'B', 'C')\n",
    "   ;\n",
    "\n",
    "   --END OF GENERATED SQL\n",
    "   ```\n",
    "\n",
    "#### **SECTION 11: FINAL PRE-SUBMISSION CHECK**\n",
    "\n",
    "☐ SQL starts with `-- Use Case: [ID] - [Name]` comment\n",
    "☐ **MANDATORY**: SQL ends with `--END OF GENERATED SQL` marker\n",
    "☐ No JSON wrapper around the SQL (raw SQL only)\n",
    "☐ No text before the SQL\n",
    "☐ No text after last SQL statement\n",
    "☐ No markdown code fences (```sql or ```)\n",
    "☐ No explanatory text (\"Here is...\", \"I've generated...\")\n",
    "☐ All CONCAT operations are NULL-safe (verified in Section 1)\n",
    "☐ AI_FORECAST input has unique time values per group (verified in Section 2)\n",
    "☐ AI_FORECAST output filtered for NULL forecasts (verified in Section 9)\n",
    "☐ Final output wrapped in final_output CTE with commented WHERE filters (verified in Section 10)\n",
    "\n",
    "---\n",
    "\n",
    "**\uD83D\uDD25 REMEMBER: If you miss even ONE item in this checklist, the query WILL FAIL. Take your time to verify EVERY item. \uD83D\uDD25**\n",
    "\n",
    "---\n",
    "\n",
    "**Generate the production-ready Databricks SQL query now. Be SOPHISTICATED, INNOVATIVE, and SYNTACTICALLY PERFECT.**\n",
    "\n",
    "\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 ABSOLUTE RULE - OUTPUT FORMAT - ZERO TOLERANCE \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\n",
    "\n",
    "❌ ABSOLUTELY FORBIDDEN - DO NOT OUTPUT ANY OF THESE:\n",
    "- \"# SCHEMA VALIDATION CHECK\" or any schema checking text\n",
    "- \"Checking...\" or \"Proceeding...\" or \"✅\" or any status indicators\n",
    "- \"SCHEMA PROVIDED\" or any schema confirmation messages\n",
    "- Any markdown headings (# or ## or ###)\n",
    "- Any markdown code fences (```sql or ``` or ```anything```)\n",
    "- Any explanatory text (\"Here is...\", \"I've...\", \"The...\", \"Let me...\")\n",
    "- Any thoughts, reasoning, or analysis descriptions\n",
    "- Any text BEFORE the SQL query starts\n",
    "- Any text AFTER the SQL query ends\n",
    "\n",
    "✅ YOUR RESPONSE MUST BE EXACTLY THIS FORMAT (NO markdown fences, just the raw SQL):\n",
    "\n",
    "-- Use Case: [ID] - [Name]\n",
    "-- [Brief description of what the query does]\n",
    "\n",
    "WITH cte_name AS (\n",
    "  ...\n",
    ")\n",
    "SELECT * FROM final_cte;\n",
    "\n",
    "--END OF GENERATED SQL\n",
    "\n",
    "\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 CRITICAL: END MARKER REQUIREMENT \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8\n",
    "\uD83D\uDEA8 YOU MUST END YOUR SQL WITH THE EXACT MARKER: --END OF GENERATED SQL\n",
    "\uD83D\uDEA8 This marker is MANDATORY and used to detect truncation\n",
    "\uD83D\uDEA8 If this marker is missing, the SQL will be considered INCOMPLETE and will be regenerated\n",
    "\n",
    "\uD83D\uDEA8 THE VERY FIRST CHARACTER OF YOUR RESPONSE MUST BE: --\n",
    "\uD83D\uDEA8 THERE MUST BE NO TEXT BEFORE THE FIRST SQL COMMENT\n",
    "\uD83D\uDEA8 YOUR RESPONSE STARTS WITH `-- Use Case:` AND NOTHING ELSE BEFORE IT\n",
    "\uD83D\uDEA8 YOUR RESPONSE MUST END WITH `--END OF GENERATED SQL` AND NOTHING ELSE AFTER IT\n",
    "\"\"\"\n",
    "\n",
    "log_print(\"PROMPT_TEMPLATES dictionary defined successfully with all required prompts.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# --- Global Logger ---\n",
    "# This will be configured by the DatabricksInspire class\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Custom Exceptions ---\n",
    "class InputTooLongError(RuntimeError):\n",
    "    \"\"\"Raised when input exceeds the model's context limit.\"\"\"\n",
    "    pass\n",
    "\n",
    "class TruncatedResponseError(RuntimeError):\n",
    "    \"\"\"Raised when LLM response is truncated (missing END marker).\"\"\"\n",
    "    pass\n",
    "\n",
    "# ==============================================================================\n",
    "# CENTRALIZED DATA STRUCTURES (Maximizing Reuse & Reducing LOC)\n",
    "# ==============================================================================\n",
    "\n",
    "# ==============================================================================\n",
    "# UNIFIED USE CASE GENERATION PILLARS\n",
    "# Three pillars with consistent format: function, business_value, example_use_cases\n",
    "# ==============================================================================\n",
    "\n",
    "# ==============================================================================\n",
    "# DOCUMENTATION GENERATORS (Creating formatted docs from data structures)\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_ai_functions_doc(format_type=\"detailed\"):\n",
    "    \"\"\"\n",
    "    Generates AI functions documentation from centralized data structure.\n",
    "    \n",
    "    Args:\n",
    "        format_type: \"summary\" for simple list, \"detailed\" for full documentation, \"unified\" for new format\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string ready to be inserted into prompts\n",
    "    \"\"\"\n",
    "    if format_type == \"summary\":\n",
    "        return \", \".join([f\"`{data['function']}`\" for data in AI_FUNCTIONS.values()])\n",
    "    \n",
    "    elif format_type == \"detailed\" or format_type == \"unified\":\n",
    "        docs = []\n",
    "        for idx, (func_name, data) in enumerate(AI_FUNCTIONS.items(), 1):\n",
    "            doc = f\"**{idx}. {func_name}**\\n\\n\"\n",
    "            doc += f\"  * **Function:** `{data['function']}`\\n\"\n",
    "            doc += f\"  * **Business Value:** {data['business_value']}\\n\"\n",
    "            doc += f\"  * **Example Use Cases:** {data['example_use_cases']}\"\n",
    "            docs.append(doc)\n",
    "        \n",
    "        return \"\\n\\n\".join(docs)\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def get_ai_function_list():\n",
    "    \"\"\"Returns comma-separated list of AI function names for documentation.\"\"\"\n",
    "    return \", \".join([f\"`{data['function']}`\" for data in AI_FUNCTIONS.values()])\n",
    "\n",
    "def generate_statistical_functions_doc(format_type=\"detailed\"):\n",
    "    \"\"\"\n",
    "    Generates Statistical Functions documentation from centralized data structure.\n",
    "    \n",
    "    Args:\n",
    "        format_type: \"summary\" for simple list, \"detailed\" for full documentation, \"table\" for markdown table\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string ready to be inserted into prompts\n",
    "    \"\"\"\n",
    "    if format_type == \"summary\":\n",
    "        return \", \".join([f\"`{data['function']}`\" for data in STATISTICAL_FUNCTIONS.values()])\n",
    "    \n",
    "    elif format_type == \"detailed\":\n",
    "        docs = []\n",
    "        for idx, (func_name, data) in enumerate(STATISTICAL_FUNCTIONS.items(), 1):\n",
    "            doc = f\"**{idx}. {data['function']}**\\n\\n\"\n",
    "            doc += f\"  * **Function:** `{data['function']}`\\n\"\n",
    "            doc += f\"  * **Business Value:** {data['business_value']}\\n\"\n",
    "            doc += f\"  * **Use Cases:** {data['use_cases']}\\n\"\n",
    "            doc += f\"  * **Category:** {data['category']}\"\n",
    "            docs.append(doc)\n",
    "        \n",
    "        return \"\\n\\n\".join(docs)\n",
    "    \n",
    "    elif format_type == \"table\":\n",
    "        rows = []\n",
    "        rows.append(\"| Function | Business Value & Use Cases |\")\n",
    "        rows.append(\"|----------|---------------------------|\")\n",
    "        for func_name, data in STATISTICAL_FUNCTIONS.items():\n",
    "            value_and_cases = f\"**{data['business_value']}**<br>• {data['use_cases'].replace(' • ', '<br>• ')}\"\n",
    "            rows.append(f\"| **{data['function']}** | {value_and_cases} |\")\n",
    "        \n",
    "        return \"\\n\".join(rows)\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def get_statistical_function_list():\n",
    "    \"\"\"Returns comma-separated list of statistical function names for documentation.\"\"\"\n",
    "    return \", \".join([f\"`{data['function']}`\" for data in STATISTICAL_FUNCTIONS.values()])\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. REQUIRED HELPER FUNCTIONS\n",
    "# (Dependencies for AIAgent and DatabricksInspire)\n",
    "# ==============================================================================\n",
    "\n",
    "### --- Logging ---\n",
    "\n",
    "class ConsoleErrorFormatter(logging.Formatter):\n",
    "    \"\"\"A custom formatter that logs error messages but not stack traces to the console.\"\"\"\n",
    "    def format(self, record):\n",
    "        original_exc_info = record.exc_info\n",
    "        original_exc_text = record.exc_text\n",
    "        if record.levelno >= logging.ERROR:\n",
    "            record.exc_info = None\n",
    "            record.exc_text = None\n",
    "        formatted_message = super().format(record)\n",
    "        record.exc_info = original_exc_info\n",
    "        record.exc_text = original_exc_text\n",
    "        return formatted_message\n",
    "\n",
    "def setup_logging(output_dir):\n",
    "    \"\"\"Configures dual logging: detailed logs to a file and high-level logs to the console.\"\"\"\n",
    "    log_file_path = os.path.join(output_dir, \"log.txt\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    root_logger = logging.getLogger() # Get root logger\n",
    "    root_logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    if root_logger.hasHandlers():\n",
    "        root_logger.handlers.clear()\n",
    "\n",
    "    # --- File Handler (Detailed) ---\n",
    "    file_handler = logging.FileHandler(log_file_path, mode='w')\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    file_formatter = logging.Formatter(\n",
    "        '%(asctime)s - %(name)s - %(levelname)s - %(message)s [in %(pathname)s:%(lineno)d]', \n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "    root_logger.addHandler(file_handler)\n",
    "\n",
    "    # --- Console Handler (High-Level, Clean) ---\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "    console_formatter = ConsoleErrorFormatter(\n",
    "        '%(asctime)s - %(levelname)s - %(message)s', \n",
    "        datefmt='%H:%M:%S'\n",
    "    )\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "    root_logger.addHandler(console_handler)\n",
    "    \n",
    "    logger.info(f\"Logging configured. High-level logs to console, detailed logs to {log_file_path}\")\n",
    "\n",
    "def print_ascii_banner():\n",
    "    \"\"\"Prints the Databricks Inspire AI ASCII art banner.\"\"\"\n",
    "    print(DATABRICKS_INSPIRE_BANNER)\n",
    "\n",
    "def extract_honesty_score(response: str, logger: logging.Logger = None) -> tuple:\n",
    "    \"\"\"\n",
    "    Extracts the honesty score and justification from an LLM response.\n",
    "    Supports multiple formats: JSON wrapper, CSV columns, SQL comments.\n",
    "    \n",
    "    Args:\n",
    "        response: The raw LLM response text\n",
    "        logger: Optional logger for debug output\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (score: int or None, justification: str or None, cleaned_response: str)\n",
    "               - score: The honesty score (0-100) or None if not found\n",
    "               - justification: The justification text (max 250 chars) or None if not found\n",
    "               - cleaned_response: The response with honesty data removed for downstream processing\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import json as json_module\n",
    "    \n",
    "    if not response:\n",
    "        return None, None, response\n",
    "    \n",
    "    score = None\n",
    "    justification = None\n",
    "    cleaned_response = response\n",
    "    \n",
    "    try:\n",
    "        response_stripped = response.strip()\n",
    "        \n",
    "        if response_stripped.startswith('{') and '\"honesty_score\"' in response_stripped:\n",
    "            try:\n",
    "                parsed = json_module.loads(response_stripped)\n",
    "                if isinstance(parsed, dict) and 'honesty_score' in parsed:\n",
    "                    score = int(parsed.get('honesty_score', 0))\n",
    "                    justification = str(parsed.get('honesty_justification', ''))[:250]\n",
    "                    if 'data' in parsed:\n",
    "                        cleaned_response = json_module.dumps(parsed['data'], ensure_ascii=False)\n",
    "                    else:\n",
    "                        cleaned_parsed = {k: v for k, v in parsed.items() \n",
    "                                         if k not in ('honesty_score', 'honesty_justification')}\n",
    "                        cleaned_response = json_module.dumps(cleaned_parsed, ensure_ascii=False)\n",
    "            except json_module.JSONDecodeError:\n",
    "                pass\n",
    "        \n",
    "        if score is None and response_stripped.startswith('--'):\n",
    "            sql_score_pattern = r'^--\\s*HONESTY_SCORE:\\s*(\\d+)'\n",
    "            sql_just_pattern = r'^--\\s*HONESTY_JUSTIFICATION:\\s*(.+?)$'\n",
    "            \n",
    "            lines = response_stripped.split('\\n')\n",
    "            cleaned_lines = []\n",
    "            for line in lines:\n",
    "                score_match = re.match(sql_score_pattern, line.strip())\n",
    "                if score_match:\n",
    "                    score = int(score_match.group(1))\n",
    "                    continue\n",
    "                just_match = re.match(sql_just_pattern, line.strip())\n",
    "                if just_match:\n",
    "                    justification = just_match.group(1).strip()[:250]\n",
    "                    continue\n",
    "                cleaned_lines.append(line)\n",
    "            cleaned_response = '\\n'.join(cleaned_lines)\n",
    "        \n",
    "        if score is None and ('honesty_score' in response_stripped.lower() or 'honesty_justification' in response_stripped.lower()):\n",
    "            lines = response_stripped.split('\\n')\n",
    "            if len(lines) > 0:\n",
    "                header_line = lines[0]\n",
    "                if 'honesty_score' in header_line.lower():\n",
    "                    import csv\n",
    "                    from io import StringIO\n",
    "                    try:\n",
    "                        reader = csv.reader(StringIO(response_stripped))\n",
    "                        rows = list(reader)\n",
    "                        if len(rows) > 1:\n",
    "                            header = [h.lower().strip().strip('\"') for h in rows[0]]\n",
    "                            score_idx = None\n",
    "                            just_idx = None\n",
    "                            for i, h in enumerate(header):\n",
    "                                if 'honesty_score' in h:\n",
    "                                    score_idx = i\n",
    "                                elif 'honesty_justification' in h:\n",
    "                                    just_idx = i\n",
    "                            \n",
    "                            if score_idx is not None and len(rows) > 1:\n",
    "                                try:\n",
    "                                    score = int(rows[1][score_idx])\n",
    "                                except (ValueError, IndexError):\n",
    "                                    pass\n",
    "                            if just_idx is not None and len(rows) > 1:\n",
    "                                try:\n",
    "                                    justification = str(rows[1][just_idx])[:250]\n",
    "                                except IndexError:\n",
    "                                    pass\n",
    "                            \n",
    "                            if score_idx is not None or just_idx is not None:\n",
    "                                new_header = [h for i, h in enumerate(rows[0]) \n",
    "                                             if i != score_idx and i != just_idx]\n",
    "                                new_rows = [new_header]\n",
    "                                for row in rows[1:]:\n",
    "                                    new_row = [v for i, v in enumerate(row) \n",
    "                                              if i != score_idx and i != just_idx]\n",
    "                                    new_rows.append(new_row)\n",
    "                                \n",
    "                                output = StringIO()\n",
    "                                writer = csv.writer(output)\n",
    "                                writer.writerows(new_rows)\n",
    "                                cleaned_response = output.getvalue().strip()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        \n",
    "        if score is None and '|' in response_stripped and 'honesty_score' in response_stripped.lower():\n",
    "            lines = response_stripped.split('\\n')\n",
    "            header_line = None\n",
    "            header_idx = -1\n",
    "            for idx, line in enumerate(lines):\n",
    "                if '|' in line and 'honesty_score' in line.lower():\n",
    "                    header_line = line\n",
    "                    header_idx = idx\n",
    "                    break\n",
    "            \n",
    "            if header_line:\n",
    "                cells = [c.strip().strip('\"').lower() for c in header_line.split('|')]\n",
    "                score_idx = None\n",
    "                just_idx = None\n",
    "                for i, cell in enumerate(cells):\n",
    "                    if 'honesty_score' in cell:\n",
    "                        score_idx = i\n",
    "                    elif 'honesty_justification' in cell:\n",
    "                        just_idx = i\n",
    "                \n",
    "                if score_idx is not None and header_idx + 2 < len(lines):\n",
    "                    data_line = lines[header_idx + 2] if lines[header_idx + 1].replace('|', '').replace('-', '').strip() == '' else lines[header_idx + 1]\n",
    "                    data_cells = [c.strip().strip('\"') for c in data_line.split('|')]\n",
    "                    \n",
    "                    if score_idx < len(data_cells):\n",
    "                        try:\n",
    "                            score = int(data_cells[score_idx])\n",
    "                        except ValueError:\n",
    "                            pass\n",
    "                    if just_idx is not None and just_idx < len(data_cells):\n",
    "                        justification = data_cells[just_idx][:250]\n",
    "                    \n",
    "                    cleaned_lines = []\n",
    "                    for line in lines:\n",
    "                        if '|' in line:\n",
    "                            parts = line.split('|')\n",
    "                            new_parts = [p for i, p in enumerate(parts) if i != score_idx and i != just_idx]\n",
    "                            cleaned_lines.append('|'.join(new_parts))\n",
    "                        else:\n",
    "                            cleaned_lines.append(line)\n",
    "                    cleaned_response = '\\n'.join(cleaned_lines)\n",
    "        \n",
    "        if score is not None:\n",
    "            if score < 0:\n",
    "                score = 0\n",
    "            elif score > 100:\n",
    "                score = 100\n",
    "        \n",
    "    except Exception as e:\n",
    "        if logger:\n",
    "            logger.debug(f\"Failed to extract honesty score: {e}\")\n",
    "        cleaned_response = response\n",
    "    \n",
    "    return score, justification, cleaned_response\n",
    "\n",
    "### --- AIAgent Dependencies ---\n",
    "# (Assumed to be available for the AIAgent class)\n",
    "\n",
    "def replace_single_quote(text: str) -> str:\n",
    "    \"\"\"Escapes single quotes and backslashes for Spark SQL strings.\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    return text.replace(r\"\\\\\", r\"\\\\\\\\\").replace(\"'\", \"''\")\n",
    "\n",
    "def execute_sql(spark: SparkSession, query: str, logger: logging.Logger):\n",
    "    \"\"\"\n",
    "    Executes a Spark SQL query and returns the collected rows.\n",
    "    \n",
    "    Args:\n",
    "        spark: SparkSession instance\n",
    "        query: SQL query to execute\n",
    "        logger: Logger instance\n",
    "    \n",
    "    Returns:\n",
    "        Collected rows from the query\n",
    "        \n",
    "    Raises:\n",
    "        Exception: For SQL execution errors\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.debug(f\"Executing Spark SQL: {query[:200]}...\")\n",
    "        result = spark.sql(query).collect()\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Spark SQL query failed: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_and_format_prompt(prompt_key: str, prompt_vars: dict, logger: logging.Logger) -> str:\n",
    "    try:\n",
    "        # Check if the prompt_key (variable name) exists in the global scope\n",
    "        if prompt_key not in globals():\n",
    "            raise NameError(f\"Global prompt variable '{prompt_key}' not found. Please make sure the cell defining it has been run.\")\n",
    "            \n",
    "        # Get the template string from the global variable\n",
    "        template = globals()[prompt_key]\n",
    "        \n",
    "        if not template or not isinstance(template, str):\n",
    "             raise ValueError(f\"Global prompt variable '{prompt_key}' is empty or not a string.\")\n",
    "\n",
    "        # Format the template using the provided dictionary\n",
    "        return template.format(**prompt_vars)\n",
    "    except KeyError as e:\n",
    "        logger.error(f\"Missing key in prompt vars for '{prompt_key}': {get_clean_error_message(e)}\")\n",
    "        # Re-raise with more context\n",
    "        raise KeyError(f\"Missing formatting key {e} for prompt '{prompt_key}'\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load or format prompt for '{prompt_key}': {get_clean_error_message(e)}\")\n",
    "        raise\n",
    "\n",
    "def clean_csv_response(raw_string: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes markdown code fences from a CSV response WITHOUT extracting JSON.\n",
    "    This is specifically for CSV responses where we don't want to treat [ or { as JSON markers.\n",
    "    \"\"\"\n",
    "    if not raw_string: return \"\"\n",
    "    \n",
    "    cleaned = raw_string.strip()\n",
    "    \n",
    "    # Remove markdown code fences - handle multiple patterns\n",
    "    # Pattern 1: ```csv\\n{...}\\n``` or ```\\n{...}\\n```\n",
    "    cleaned = re.sub(r'^```(?:csv|json)?\\s*\\n?', '', cleaned, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    cleaned = re.sub(r'\\n?```\\s*$', '', cleaned, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    \n",
    "    # Pattern 2: Handle cases where ``` appears in the middle (trailing after content)\n",
    "    cleaned = re.sub(r'```(?:csv|json)?\\s*$', '', cleaned, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    \n",
    "    return cleaned.strip()\n",
    "\n",
    "\n",
    "def clean_json_response(raw_string: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes markdown code fences and other noise from a raw LLM response.\n",
    "    Also extracts JSON object/array if extra text is present before or after.\n",
    "    Renamed from clean_llm_response to match AIAgent dependency.\n",
    "    \"\"\"\n",
    "    if not raw_string: return \"\"\n",
    "    \n",
    "    cleaned = raw_string.strip()\n",
    "    \n",
    "    # Remove markdown code fences - handle multiple patterns\n",
    "    # Pattern 1: ```json\\n{...}\\n``` or ```\\n{...}\\n```\n",
    "    cleaned = re.sub(r'^```(?:json|csv)?\\s*', '', cleaned, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    cleaned = re.sub(r'\\s*```\\s*$', '', cleaned, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    \n",
    "    # Pattern 2: Handle cases where ``` appears in the middle (trailing after JSON)\n",
    "    cleaned = re.sub(r'```(?:json|csv)?\\s*$', '', cleaned, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    \n",
    "    cleaned = cleaned.strip()\n",
    "    \n",
    "    # Try to extract JSON object or array from the response\n",
    "    # Look for the first { or [ and the last matching } or ]\n",
    "    start_obj = cleaned.find('{')\n",
    "    start_arr = cleaned.find('[')\n",
    "    \n",
    "    # Determine which comes first\n",
    "    if start_obj == -1 and start_arr == -1:\n",
    "        return cleaned  # No JSON found, return as-is\n",
    "    elif start_obj == -1:\n",
    "        start = start_arr\n",
    "        end_char = ']'\n",
    "    elif start_arr == -1:\n",
    "        start = start_obj\n",
    "        end_char = '}'\n",
    "    else:\n",
    "        start = min(start_obj, start_arr)\n",
    "        end_char = '}' if start == start_obj else ']'\n",
    "    \n",
    "    # Find the last occurrence of the closing character\n",
    "    # Use a more robust method to find the matching closing brace/bracket\n",
    "    end = -1\n",
    "    depth = 0\n",
    "    open_char = '{' if end_char == '}' else '['\n",
    "    \n",
    "    # Find the matching closing character by tracking depth\n",
    "    for i in range(start, len(cleaned)):\n",
    "        if cleaned[i] == open_char:\n",
    "            depth += 1\n",
    "        elif cleaned[i] == end_char:\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                end = i\n",
    "                break\n",
    "    \n",
    "    # Fallback to rfind if depth tracking doesn't work\n",
    "    if end == -1:\n",
    "        end = cleaned.rfind(end_char)\n",
    "    \n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        # Extract the JSON portion\n",
    "        json_portion = cleaned[start:end+1]\n",
    "        return json_portion\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def retry_with_logging(func, max_attempts=1, logger=None, fallback=None, context=\"\"):\n",
    "    \"\"\"\n",
    "    Generic retry wrapper for functions that may fail transiently.\n",
    "    \n",
    "    Args:\n",
    "        func: Callable to execute (should take no arguments; use lambda if needed)\n",
    "        max_attempts: Maximum number of retry attempts (default: 3)\n",
    "        logger: Logger instance for logging retries (optional)\n",
    "        fallback: Fallback value or callable to return on failure (optional)\n",
    "        context: Context string for logging (e.g., \"Domain consolidation for English\")\n",
    "    \n",
    "    Returns:\n",
    "        Result of func() on success, fallback on failure (if provided), otherwise raises\n",
    "    \n",
    "    Raises:\n",
    "        Last exception if all attempts fail and no fallback provided\n",
    "    \"\"\"\n",
    "    last_exception = None\n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        try:\n",
    "            if attempt > 1 and logger:\n",
    "                logger.info(f\"Retry attempt {attempt}/{max_attempts}{f' for {context}' if context else ''}...\")\n",
    "            return func()\n",
    "        except Exception as e:\n",
    "            last_exception = e\n",
    "            if logger:\n",
    "                error_msg = get_clean_error_message(e)\n",
    "                if attempt == max_attempts:\n",
    "                    logger.error(f\"All {max_attempts} attempts failed{f' for {context}' if context else ''}: {error_msg}\")\n",
    "                else:\n",
    "                    logger.warning(f\"Attempt {attempt}/{max_attempts} failed{f' for {context}' if context else ''}: {error_msg}\")\n",
    "            if attempt == max_attempts:\n",
    "                if fallback is not None:\n",
    "                    if callable(fallback):\n",
    "                        return fallback()\n",
    "                    return fallback\n",
    "                raise last_exception\n",
    "\n",
    "# ==============================================================================\n",
    "# CENTRALIZED UTILITY CLASSES (Code Reuse & LOC Reduction)\n",
    "# ==============================================================================\n",
    "\n",
    "class RetryHandler:\n",
    "    \"\"\"\n",
    "    Centralized retry handler with exponential backoff and flexible error handling.\n",
    "    Replaces all scattered retry logic throughout the codebase.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def execute_with_retry(\n",
    "        func,\n",
    "        max_attempts=1,\n",
    "        logger=None,\n",
    "        context=\"\",\n",
    "        fallback=None,\n",
    "        exponential_backoff=True,\n",
    "        base_delay=1.0,\n",
    "        max_delay=60.0,\n",
    "        retryable_errors=None,\n",
    "        non_retryable_errors=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Execute a function with retry logic and exponential backoff.\n",
    "        \n",
    "        Args:\n",
    "            func: Callable to execute\n",
    "            max_attempts: Maximum retry attempts (default: 1)\n",
    "            logger: Logger instance for tracking\n",
    "            context: Context string for logging\n",
    "            fallback: Fallback value on failure\n",
    "            exponential_backoff: Use exponential backoff (default: True)\n",
    "            base_delay: Base delay in seconds (default: 1.0)\n",
    "            max_delay: Maximum delay between retries (default: 60.0)\n",
    "            retryable_errors: List of error types/keywords that should be retried\n",
    "            non_retryable_errors: List of error types/keywords that should NOT be retried\n",
    "            \n",
    "        Returns:\n",
    "            Result of func() on success, fallback on failure\n",
    "        \"\"\"\n",
    "        import time\n",
    "        last_exception = None\n",
    "        \n",
    "        for attempt in range(1, max_attempts + 1):\n",
    "            try:\n",
    "                if attempt > 1 and logger:\n",
    "                    logger.info(f\"\uD83D\uDD04 Retry attempt {attempt}/{max_attempts}{f' for {context}' if context else ''}...\")\n",
    "                return func()\n",
    "            except Exception as e:\n",
    "                last_exception = e\n",
    "                error_str = str(e).lower()\n",
    "                \n",
    "                if non_retryable_errors:\n",
    "                    is_non_retryable = any(\n",
    "                        (isinstance(err, type) and isinstance(e, err)) or \n",
    "                        (isinstance(err, str) and err.lower() in error_str)\n",
    "                        for err in non_retryable_errors\n",
    "                    )\n",
    "                    if is_non_retryable:\n",
    "                        if logger:\n",
    "                            logger.error(f\"❌ Non-retryable error{f' for {context}' if context else ''}: {get_clean_error_message(e)}\")\n",
    "                        if fallback is not None:\n",
    "                            return fallback() if callable(fallback) else fallback\n",
    "                        raise\n",
    "                \n",
    "                is_retryable = True\n",
    "                if retryable_errors:\n",
    "                    is_retryable = any(\n",
    "                        (isinstance(err, type) and isinstance(e, err)) or \n",
    "                        (isinstance(err, str) and err.lower() in error_str)\n",
    "                        for err in retryable_errors\n",
    "                    )\n",
    "                \n",
    "                if not is_retryable or attempt == max_attempts:\n",
    "                    if logger:\n",
    "                        logger.error(f\"❌ Failed after {max_attempts} attempts{f' for {context}' if context else ''}: {get_clean_error_message(e)}\")\n",
    "                    if fallback is not None:\n",
    "                        return fallback() if callable(fallback) else fallback\n",
    "                    raise\n",
    "                \n",
    "                if exponential_backoff:\n",
    "                    wait_time = min(base_delay * (2 ** (attempt - 1)), max_delay)\n",
    "                    jitter = random.uniform(0, wait_time * 0.1)\n",
    "                    wait_time += jitter\n",
    "                else:\n",
    "                    wait_time = base_delay\n",
    "                \n",
    "                if logger:\n",
    "                    logger.warning(f\"⚠️  Attempt {attempt} failed{f' for {context}' if context else ''}: {get_clean_error_message(e)}\")\n",
    "                    logger.info(f\"   Waiting {wait_time:.1f}s before retry...\")\n",
    "                \n",
    "                time.sleep(wait_time)\n",
    "        \n",
    "        if fallback is not None:\n",
    "            return fallback() if callable(fallback) else fallback\n",
    "        raise last_exception\n",
    "\n",
    "\n",
    "class ParallelExecutor:\n",
    "    \"\"\"\n",
    "    Centralized parallel execution manager using ThreadPoolExecutor.\n",
    "    Replaces all scattered ThreadPoolExecutor usage throughout the codebase.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def execute_parallel(\n",
    "        tasks,\n",
    "        max_workers,\n",
    "        task_name=\"Task\",\n",
    "        logger=None,\n",
    "        timeout_per_task=None,\n",
    "        total_timeout=None,\n",
    "        thread_name_prefix=\"Worker\",\n",
    "        return_exceptions=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Execute multiple tasks in parallel using ThreadPoolExecutor.\n",
    "        \n",
    "        Args:\n",
    "            tasks: List of callables or (callable, args) tuples\n",
    "            max_workers: Maximum number of parallel workers\n",
    "            task_name: Name for logging purposes\n",
    "            logger: Logger instance\n",
    "            timeout_per_task: Timeout per individual task in seconds\n",
    "            total_timeout: Total timeout for all tasks in seconds\n",
    "            thread_name_prefix: Prefix for thread names\n",
    "            return_exceptions: If True, return exceptions instead of raising them\n",
    "            \n",
    "        Returns:\n",
    "            List of results (or exceptions if return_exceptions=True)\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        exceptions = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix=thread_name_prefix) as executor:\n",
    "            future_to_task = {}\n",
    "            for i, task in enumerate(tasks):\n",
    "                if isinstance(task, tuple):\n",
    "                    func, args = task\n",
    "                    future = executor.submit(func, *args)\n",
    "                else:\n",
    "                    future = executor.submit(task)\n",
    "                future_to_task[future] = i\n",
    "            \n",
    "            try:\n",
    "                for future in concurrent.futures.as_completed(future_to_task, timeout=total_timeout):\n",
    "                    task_idx = future_to_task[future]\n",
    "                    try:\n",
    "                        result = future.result(timeout=timeout_per_task)\n",
    "                        results.append((task_idx, result))\n",
    "                    except concurrent.futures.TimeoutError:\n",
    "                        error_msg = f\"{task_name} #{task_idx} timed out\"\n",
    "                        if logger:\n",
    "                            logger.warning(f\"⏱️  {error_msg}\")\n",
    "                        if return_exceptions:\n",
    "                            results.append((task_idx, TimeoutError(error_msg)))\n",
    "                        else:\n",
    "                            exceptions.append((task_idx, TimeoutError(error_msg)))\n",
    "                    except Exception as e:\n",
    "                        if logger:\n",
    "                            logger.warning(f\"❌ {task_name} #{task_idx} failed: {get_clean_error_message(e)}\")\n",
    "                        if return_exceptions:\n",
    "                            results.append((task_idx, e))\n",
    "                        else:\n",
    "                            exceptions.append((task_idx, e))\n",
    "            except concurrent.futures.TimeoutError:\n",
    "                if logger:\n",
    "                    logger.error(f\"⏱️  Total timeout ({total_timeout}s) exceeded for {task_name}\")\n",
    "                if not return_exceptions:\n",
    "                    raise\n",
    "        \n",
    "        results.sort(key=lambda x: x[0])\n",
    "        \n",
    "        if not return_exceptions and exceptions:\n",
    "            if logger:\n",
    "                logger.error(f\"❌ {len(exceptions)} {task_name}(s) failed\")\n",
    "            raise exceptions[0][1]\n",
    "        \n",
    "        return [r[1] for r in results]\n",
    "\n",
    "\n",
    "class CSVParser:\n",
    "    \"\"\"\n",
    "    Centralized CSV parsing utility with consistent error handling.\n",
    "    Replaces all scattered csv.DictReader usage throughout the codebase.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_csv_string(\n",
    "        csv_data,\n",
    "        logger=None,\n",
    "        context=\"\",\n",
    "        quoting=csv.QUOTE_ALL,\n",
    "        delimiter=',',\n",
    "        skipinitialspace=True,\n",
    "        expected_fields=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parse CSV string into list of dictionaries.\n",
    "        \n",
    "        Args:\n",
    "            csv_data: CSV string data\n",
    "            logger: Logger instance\n",
    "            context: Context string for logging\n",
    "            quoting: CSV quoting mode (default: QUOTE_ALL)\n",
    "            delimiter: Field delimiter (default: ',')\n",
    "            skipinitialspace: Skip initial spaces (default: True)\n",
    "            expected_fields: Optional list of expected field names for validation\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries (one per row)\n",
    "        \"\"\"\n",
    "        if not csv_data or not csv_data.strip():\n",
    "            if logger:\n",
    "                logger.warning(f\"⚠️  Empty CSV data{f' for {context}' if context else ''}\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            reader = csv.DictReader(\n",
    "                io.StringIO(csv_data),\n",
    "                quoting=quoting,\n",
    "                delimiter=delimiter,\n",
    "                skipinitialspace=skipinitialspace\n",
    "            )\n",
    "            rows = list(reader)\n",
    "            \n",
    "            if expected_fields and rows:\n",
    "                actual_fields = set(rows[0].keys())\n",
    "                expected_set = set(expected_fields)\n",
    "                missing_fields = expected_set - actual_fields\n",
    "                if missing_fields and logger:\n",
    "                    logger.warning(f\"⚠️  Missing expected CSV fields{f' for {context}' if context else ''}: {missing_fields}\")\n",
    "            \n",
    "            if logger:\n",
    "                logger.debug(f\"✅ Parsed {len(rows)} CSV rows{f' for {context}' if context else ''}\")\n",
    "            \n",
    "            return rows\n",
    "        except Exception as e:\n",
    "            if logger:\n",
    "                logger.error(f\"❌ CSV parsing failed{f' for {context}' if context else ''}: {get_clean_error_message(e)}\")\n",
    "            return []\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_csv_list(\n",
    "        csv_data,\n",
    "        logger=None,\n",
    "        context=\"\",\n",
    "        quoting=csv.QUOTE_ALL,\n",
    "        delimiter=',',\n",
    "        quotechar='\"',\n",
    "        skipinitialspace=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parse CSV string into list of lists (for non-dictionary CSV).\n",
    "        \n",
    "        Args:\n",
    "            csv_data: CSV string data\n",
    "            logger: Logger instance\n",
    "            context: Context string for logging\n",
    "            quoting: CSV quoting mode (default: QUOTE_ALL)\n",
    "            delimiter: Field delimiter (default: ',')\n",
    "            quotechar: Quote character (default: '\"')\n",
    "            skipinitialspace: Skip initial spaces (default: True)\n",
    "            \n",
    "        Returns:\n",
    "            List of lists (one per row)\n",
    "        \"\"\"\n",
    "        if not csv_data or not csv_data.strip():\n",
    "            if logger:\n",
    "                logger.warning(f\"⚠️  Empty CSV data{f' for {context}' if context else ''}\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            reader = csv.reader(\n",
    "                io.StringIO(csv_data),\n",
    "                delimiter=delimiter,\n",
    "                quotechar=quotechar,\n",
    "                quoting=quoting,\n",
    "                skipinitialspace=skipinitialspace\n",
    "            )\n",
    "            rows = list(reader)\n",
    "            \n",
    "            if logger:\n",
    "                logger.debug(f\"✅ Parsed {len(rows)} CSV rows{f' for {context}' if context else ''}\")\n",
    "            \n",
    "            return rows\n",
    "        except Exception as e:\n",
    "            if logger:\n",
    "                logger.error(f\"❌ CSV parsing failed{f' for {context}' if context else ''}: {get_clean_error_message(e)}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "class JSONParser:\n",
    "    \"\"\"\n",
    "    Centralized JSON parsing utility with consistent error handling.\n",
    "    Replaces scattered json.loads/dumps usage throughout the codebase.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_loads(json_string, logger=None, context=\"\", fallback=None):\n",
    "        \"\"\"\n",
    "        Safely parse JSON string with error handling.\n",
    "        \n",
    "        Args:\n",
    "            json_string: JSON string to parse\n",
    "            logger: Logger instance\n",
    "            context: Context string for logging\n",
    "            fallback: Fallback value on parsing failure\n",
    "            \n",
    "        Returns:\n",
    "            Parsed JSON object or fallback value\n",
    "        \"\"\"\n",
    "        if not json_string:\n",
    "            return fallback\n",
    "        \n",
    "        try:\n",
    "            return json.loads(json_string)\n",
    "        except json.JSONDecodeError as e:\n",
    "            if logger:\n",
    "                logger.warning(f\"⚠️  JSON parsing failed{f' for {context}' if context else ''}: {str(e)[:100]}\")\n",
    "            return fallback\n",
    "        except Exception as e:\n",
    "            if logger:\n",
    "                logger.error(f\"❌ Unexpected error parsing JSON{f' for {context}' if context else ''}: {get_clean_error_message(e)}\")\n",
    "            return fallback\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_dumps(obj, logger=None, context=\"\", fallback=\"{}\", indent=None, separators=None):\n",
    "        \"\"\"\n",
    "        Safely serialize object to JSON string with error handling.\n",
    "        \n",
    "        Args:\n",
    "            obj: Object to serialize\n",
    "            logger: Logger instance\n",
    "            context: Context string for logging\n",
    "            fallback: Fallback string on serialization failure\n",
    "            indent: Indentation level (default: None)\n",
    "            separators: Custom separators (default: None)\n",
    "            \n",
    "        Returns:\n",
    "            JSON string or fallback value\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if separators:\n",
    "                return json.dumps(obj, indent=indent, separators=separators)\n",
    "            return json.dumps(obj, indent=indent)\n",
    "        except TypeError as e:\n",
    "            if logger:\n",
    "                logger.warning(f\"⚠️  JSON serialization failed{f' for {context}' if context else ''}: {str(e)[:100]}\")\n",
    "            return fallback\n",
    "        except Exception as e:\n",
    "            if logger:\n",
    "                logger.error(f\"❌ Unexpected error serializing JSON{f' for {context}' if context else ''}: {get_clean_error_message(e)}\")\n",
    "            return fallback\n",
    "\n",
    "\n",
    "class TimeoutHandler:\n",
    "    \"\"\"\n",
    "    Centralized timeout handling utility.\n",
    "    Provides consistent timeout behavior across the codebase.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def execute_with_timeout(func, timeout_seconds, logger=None, context=\"\", fallback=None):\n",
    "        \"\"\"\n",
    "        Execute a function with a timeout.\n",
    "        \n",
    "        Args:\n",
    "            func: Callable to execute\n",
    "            timeout_seconds: Timeout in seconds\n",
    "            logger: Logger instance\n",
    "            context: Context string for logging\n",
    "            fallback: Fallback value on timeout\n",
    "            \n",
    "        Returns:\n",
    "            Result of func() or fallback on timeout\n",
    "        \"\"\"\n",
    "        from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=1, thread_name_prefix=\"Timeout\") as executor:\n",
    "            future = executor.submit(func)\n",
    "            try:\n",
    "                result = future.result(timeout=timeout_seconds)\n",
    "                return result\n",
    "            except FuturesTimeoutError:\n",
    "                if logger:\n",
    "                    logger.warning(f\"⏱️  Timeout ({timeout_seconds}s) exceeded{f' for {context}' if context else ''}\")\n",
    "                if fallback is not None:\n",
    "                    return fallback() if callable(fallback) else fallback\n",
    "                raise TimeoutError(f\"Operation timed out after {timeout_seconds}s{f' for {context}' if context else ''}\")\n",
    "            except Exception as e:\n",
    "                if logger:\n",
    "                    logger.error(f\"❌ Error during execution{f' for {context}' if context else ''}: {get_clean_error_message(e)}\")\n",
    "                raise\n",
    "\n",
    "\n",
    "\n",
    "# DBTITLE 1,Table Size Analyzer & Dynamic Batch Optimizer\n",
    "\n",
    "class TableSizeInfo:\n",
    "    \"\"\"Metadata about a table's size and structure.\"\"\"\n",
    "    def __init__(self, catalog, schema, table, num_columns=0, estimated_row_count=0, size_category=\"unknown\"):\n",
    "        self.catalog = catalog\n",
    "        self.schema = schema\n",
    "        self.table = table\n",
    "        self.num_columns = num_columns\n",
    "        self.estimated_row_count = estimated_row_count\n",
    "        self.size_category = size_category  # \"small\", \"medium\", \"wide\", \"very_wide\"\n",
    "        self.memory_weight = self._calculate_memory_weight()\n",
    "    \n",
    "    def _calculate_memory_weight(self):\n",
    "        \"\"\"Calculate memory weight for batching decisions.\"\"\"\n",
    "        if self.num_columns > 1000:\n",
    "            return self.num_columns * 10  # Very heavy\n",
    "        elif self.num_columns > 250:\n",
    "            return self.num_columns * 5   # Heavy\n",
    "        elif self.num_columns > 100:\n",
    "            return self.num_columns * 2   # Medium\n",
    "        else:\n",
    "            return self.num_columns       # Light\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.catalog}.{self.schema}.{self.table} ({self.num_columns} cols, {self.size_category})\"\n",
    "\n",
    "\n",
    "class TableSizeAnalyzer:\n",
    "    \"\"\"\n",
    "    Two-pass analyzer: First pass collects table sizes, second pass uses this info for intelligent batching.\n",
    "    \"\"\"\n",
    "    def __init__(self, spark, logger):\n",
    "        self.spark = spark\n",
    "        self.logger = logger\n",
    "        self.size_cache = {}  # {(catalog, schema, table): TableSizeInfo}\n",
    "    \n",
    "    def analyze_table_sizes_batch(self, table_tuples, use_info_schema_map, max_parallelism=20):\n",
    "        \"\"\"\n",
    "        Analyze sizes for a batch of tables efficiently using parallel queries.\n",
    "        \n",
    "        Args:\n",
    "            table_tuples: List of (catalog, schema, table) tuples\n",
    "            use_info_schema_map: Dict mapping catalog -> bool for information_schema support\n",
    "            max_parallelism: Maximum number of parallel queries (default: 20)\n",
    "            \n",
    "        Returns:\n",
    "            List of TableSizeInfo objects\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Group by catalog.schema for efficient querying\n",
    "        schema_groups = defaultdict(list)\n",
    "        for cat, schema, table in table_tuples:\n",
    "            schema_groups[(cat, schema)].append(table)\n",
    "        \n",
    "        # Process schemas in parallel for speed\n",
    "        def analyze_schema_group(schema_key_and_tables):\n",
    "            (catalog, schema), tables = schema_key_and_tables\n",
    "            use_info_schema = use_info_schema_map.get(catalog, False)\n",
    "            schema_results = []\n",
    "            \n",
    "            self.logger.debug(f\"   Analyzing {len(tables)} tables in {catalog}.{schema}...\")\n",
    "            \n",
    "            try:\n",
    "                if use_info_schema:\n",
    "                    # Batch query using information_schema (much faster)\n",
    "                    table_list = \"','\".join(tables)\n",
    "                    query = f\"\"\"\n",
    "                        SELECT table_name, COUNT(*) as num_columns\n",
    "                        FROM `{catalog}`.`information_schema`.`columns`\n",
    "                        WHERE table_schema = '{schema}'\n",
    "                        AND table_name IN ('{table_list}')\n",
    "                        GROUP BY table_name\n",
    "                    \"\"\"\n",
    "                    df = self.spark.sql(query)\n",
    "                    column_counts = {row.table_name: row.num_columns for row in df.collect()}\n",
    "                    \n",
    "                    for table in tables:\n",
    "                        num_cols = column_counts.get(table, 0)\n",
    "                        if num_cols == 0:\n",
    "                            self.logger.debug(f\"No columns found for {catalog}.{schema}.{table}, skipping size analysis\")\n",
    "                            continue\n",
    "                        \n",
    "                        size_info = TableSizeInfo(\n",
    "                            catalog, schema, table,\n",
    "                            num_columns=num_cols,\n",
    "                            size_category=self._categorize_table(num_cols)\n",
    "                        )\n",
    "                        schema_results.append(size_info)\n",
    "                        self.size_cache[(catalog, schema, table)] = size_info\n",
    "                else:\n",
    "                    # Fallback: DESCRIBE each table individually (slower)\n",
    "                    for table in tables:\n",
    "                        num_cols = self._get_column_count_fallback(catalog, schema, table)\n",
    "                        if num_cols == 0:\n",
    "                            continue\n",
    "                        \n",
    "                        size_info = TableSizeInfo(\n",
    "                            catalog, schema, table,\n",
    "                            num_columns=num_cols,\n",
    "                            size_category=self._categorize_table(num_cols)\n",
    "                        )\n",
    "                        schema_results.append(size_info)\n",
    "                        self.size_cache[(catalog, schema, table)] = size_info\n",
    "                        \n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error analyzing tables in {catalog}.{schema}: {get_clean_error_message(e)}\")\n",
    "                # Fallback: assume medium size\n",
    "                for table in tables:\n",
    "                    size_info = TableSizeInfo(catalog, schema, table, num_columns=50, size_category=\"medium\")\n",
    "                    schema_results.append(size_info)\n",
    "                    self.size_cache[(catalog, schema, table)] = size_info\n",
    "            \n",
    "            return schema_results\n",
    "        \n",
    "        # Execute schema analysis - parallel or sequential based on max_parallelism\n",
    "        if max_parallelism == 1:\n",
    "            # Sequential execution when nested in another thread pool\n",
    "            for item in schema_groups.items():\n",
    "                try:\n",
    "                    schema_results = analyze_schema_group(item)\n",
    "                    results.extend(schema_results)\n",
    "                except Exception as e:\n",
    "                    schema_key = item[0]\n",
    "                    self.logger.error(f\"Failed to analyze schema {schema_key}: {get_clean_error_message(e)}\")\n",
    "        else:\n",
    "            # Parallel execution for top-level calls\n",
    "            with ThreadPoolExecutor(max_workers=max_parallelism, thread_name_prefix=\"SchemaAnalyzer\") as executor:\n",
    "                futures = {executor.submit(analyze_schema_group, item): item[0] \n",
    "                          for item in schema_groups.items()}\n",
    "                \n",
    "                # Add timeout to prevent indefinite hangs (5 minutes per schema group)\n",
    "                total_timeout = len(futures) * 300\n",
    "                for future in concurrent.futures.as_completed(futures, timeout=total_timeout):\n",
    "                    schema_key = futures[future]\n",
    "                    try:\n",
    "                        schema_results = future.result(timeout=300)\n",
    "                        results.extend(schema_results)\n",
    "                    except concurrent.futures.TimeoutError:\n",
    "                        self.logger.error(f\"Schema analysis timed out for {schema_key} after 5 minutes\")\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Failed to analyze schema {schema_key}: {get_clean_error_message(e)}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _get_column_count_fallback(self, catalog, schema, table):\n",
    "        \"\"\"Fallback method to get column count using DESCRIBE.\"\"\"\n",
    "        try:\n",
    "            fq_table = f\"`{catalog}`.`{schema}`.`{table}`\"\n",
    "            df = self.spark.sql(f\"DESCRIBE TABLE {fq_table}\")\n",
    "            # Filter out partition info and metadata rows\n",
    "            count = df.filter(~col(\"col_name\").startswith(\"#\")).count()\n",
    "            return count\n",
    "        except Exception as e:\n",
    "            self.logger.debug(f\"Error getting column count for {catalog}.{schema}.{table}: {e}\")\n",
    "            return 0\n",
    "    \n",
    "    def _categorize_table(self, num_columns):\n",
    "        \"\"\"Categorize table based on column count.\"\"\"\n",
    "        if num_columns > 1000:\n",
    "            return \"very_wide\"\n",
    "        elif num_columns > 250:\n",
    "            return \"wide\"\n",
    "        elif num_columns > 100:\n",
    "            return \"medium\"\n",
    "        else:\n",
    "            return \"small\"\n",
    "    \n",
    "    def get_cached_size(self, catalog, schema, table):\n",
    "        \"\"\"Get cached size info for a table.\"\"\"\n",
    "        return self.size_cache.get((catalog, schema, table))\n",
    "\n",
    "\n",
    "class DynamicBatchOptimizer:\n",
    "    \"\"\"\n",
    "    Intelligently groups tables into batches based on their size and memory requirements.\n",
    "    \n",
    "    Strategy:\n",
    "    - Very wide tables (>1000 cols): 1-2 per batch\n",
    "    - Wide tables (250-1000 cols): 5-10 per batch\n",
    "    - Medium tables (100-250 cols): 20-50 per batch\n",
    "    - Small tables (<100 cols): 100-500 per batch\n",
    "    \"\"\"\n",
    "    \n",
    "    # Memory weights for batching (in arbitrary units)\n",
    "    MAX_BATCH_WEIGHT = 10000  # Adjust based on cluster memory\n",
    "    MIN_BATCH_SIZE = 1\n",
    "    MAX_BATCH_SIZE = 500\n",
    "    \n",
    "    def __init__(self, logger, max_batch_weight=None):\n",
    "        self.logger = logger\n",
    "        self.max_batch_weight = max_batch_weight or self.MAX_BATCH_WEIGHT\n",
    "    \n",
    "    def create_optimized_batches(self, table_size_infos):\n",
    "        \"\"\"\n",
    "        Create optimized batches from table size information.\n",
    "        \n",
    "        Args:\n",
    "            table_size_infos: List of TableSizeInfo objects\n",
    "            \n",
    "        Returns:\n",
    "            List of lists, where each inner list is a batch of (catalog, schema, table) tuples\n",
    "        \"\"\"\n",
    "        if not table_size_infos:\n",
    "            return []\n",
    "        \n",
    "        # Sort tables by size (largest first for better packing)\n",
    "        sorted_tables = sorted(table_size_infos, key=lambda t: t.memory_weight, reverse=True)\n",
    "        \n",
    "        batches = []\n",
    "        current_batch = []\n",
    "        current_weight = 0\n",
    "        \n",
    "        for table_info in sorted_tables:\n",
    "            table_tuple = (table_info.catalog, table_info.schema, table_info.table)\n",
    "            table_weight = table_info.memory_weight\n",
    "            \n",
    "            # Check if adding this table would exceed batch weight\n",
    "            if current_batch and (current_weight + table_weight > self.max_batch_weight or \n",
    "                                 len(current_batch) >= self.MAX_BATCH_SIZE):\n",
    "                # Start new batch\n",
    "                batches.append(current_batch)\n",
    "                self.logger.debug(f\"Created batch with {len(current_batch)} tables, weight={current_weight}\")\n",
    "                current_batch = []\n",
    "                current_weight = 0\n",
    "            \n",
    "            # Add table to current batch\n",
    "            current_batch.append(table_tuple)\n",
    "            current_weight += table_weight\n",
    "        \n",
    "        # Add final batch\n",
    "        if current_batch:\n",
    "            batches.append(current_batch)\n",
    "            self.logger.debug(f\"Created final batch with {len(current_batch)} tables, weight={current_weight}\")\n",
    "        \n",
    "        # Log batch statistics\n",
    "        self._log_batch_stats(batches, table_size_infos)\n",
    "        \n",
    "        return batches\n",
    "    \n",
    "    def _log_batch_stats(self, batches, table_size_infos):\n",
    "        \"\"\"Log statistics about the created batches.\"\"\"\n",
    "        total_tables = len(table_size_infos)\n",
    "        num_batches = len(batches)\n",
    "        \n",
    "        size_categories = defaultdict(int)\n",
    "        for table_info in table_size_infos:\n",
    "            size_categories[table_info.size_category] += 1\n",
    "        \n",
    "        avg_batch_size = total_tables / num_batches if num_batches > 0 else 0\n",
    "        \n",
    "        self.logger.info(f\"\uD83D\uDCCA Dynamic Batch Optimization Complete:\")\n",
    "        self.logger.info(f\"   • Total tables: {total_tables}\")\n",
    "        self.logger.info(f\"   • Created batches: {num_batches}\")\n",
    "        self.logger.info(f\"   • Average batch size: {avg_batch_size:.1f} tables\")\n",
    "        self.logger.info(f\"   • Table size distribution:\")\n",
    "        self.logger.info(f\"      - Small (<100 cols): {size_categories['small']}\")\n",
    "        self.logger.info(f\"      - Medium (100-250 cols): {size_categories['medium']}\")\n",
    "        self.logger.info(f\"      - Wide (250-1000 cols): {size_categories['wide']}\")\n",
    "        self.logger.info(f\"      - Very Wide (>1000 cols): {size_categories['very_wide']}\")\n",
    "\n",
    "\n",
    "class ColumnSampler:\n",
    "    \"\"\"\n",
    "    Samples columns from very wide tables to reduce memory footprint.\n",
    "    \n",
    "    For tables with >250 columns, intelligently selects representative columns:\n",
    "    - All primary keys, foreign keys\n",
    "    - Columns with business-meaningful names\n",
    "    - Sample of remaining columns\n",
    "    \"\"\"\n",
    "    \n",
    "    WIDE_TABLE_THRESHOLD = 250\n",
    "    TARGET_SAMPLE_SIZE = 200\n",
    "    \n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "    \n",
    "    def should_sample(self, num_columns):\n",
    "        \"\"\"Determine if column sampling is needed.\"\"\"\n",
    "        return num_columns > self.WIDE_TABLE_THRESHOLD\n",
    "    \n",
    "    def sample_columns(self, column_details, table_info):\n",
    "        \"\"\"\n",
    "        Sample columns from a wide table.\n",
    "        \n",
    "        Args:\n",
    "            column_details: List of (catalog, schema, table, col_name, data_type, comment) tuples\n",
    "            table_info: TableSizeInfo object\n",
    "            \n",
    "        Returns:\n",
    "            Sampled list of column details + metadata about sampling\n",
    "        \"\"\"\n",
    "        if not self.should_sample(len(column_details)):\n",
    "            return column_details, False  # No sampling needed\n",
    "        \n",
    "        self.logger.info(f\"\uD83C\uDFAF Sampling columns for wide table {table_info}: {len(column_details)} -> ~{self.TARGET_SAMPLE_SIZE} cols\")\n",
    "        \n",
    "        # Categorize columns\n",
    "        key_columns = []\n",
    "        business_columns = []\n",
    "        other_columns = []\n",
    "        \n",
    "        # Business keywords to identify important columns\n",
    "        business_keywords = [\n",
    "            'id', 'key', 'name', 'date', 'time', 'amount', 'total', 'count', 'quantity',\n",
    "            'price', 'cost', 'revenue', 'customer', 'order', 'product', 'status',\n",
    "            'type', 'category', 'description', 'address', 'email', 'phone'\n",
    "        ]\n",
    "        \n",
    "        for col_detail in column_details:\n",
    "            col_name = col_detail[3].lower()\n",
    "            \n",
    "            # Identify key columns (id, primary key patterns)\n",
    "            if 'id' in col_name or 'key' in col_name or col_name.endswith('_pk') or col_name.endswith('_fk'):\n",
    "                key_columns.append(col_detail)\n",
    "            # Identify business-relevant columns\n",
    "            elif any(keyword in col_name for keyword in business_keywords):\n",
    "                business_columns.append(col_detail)\n",
    "            else:\n",
    "                other_columns.append(col_detail)\n",
    "        \n",
    "        # Build sampled list\n",
    "        sampled = []\n",
    "        \n",
    "        # Always include all key columns\n",
    "        sampled.extend(key_columns)\n",
    "        \n",
    "        # Include as many business columns as possible\n",
    "        remaining_slots = self.TARGET_SAMPLE_SIZE - len(sampled)\n",
    "        if remaining_slots > 0:\n",
    "            sampled.extend(business_columns[:remaining_slots])\n",
    "        \n",
    "        # Fill remaining with evenly spaced sample from other columns\n",
    "        remaining_slots = self.TARGET_SAMPLE_SIZE - len(sampled)\n",
    "        if remaining_slots > 0 and other_columns:\n",
    "            step = max(1, len(other_columns) // remaining_slots)\n",
    "            sampled.extend(other_columns[::step][:remaining_slots])\n",
    "        \n",
    "        self.logger.info(f\"   ✓ Sampled: {len(key_columns)} key cols + {len(business_columns[:remaining_slots])} business cols + \"\n",
    "                        f\"{len(sampled) - len(key_columns) - len(business_columns[:remaining_slots])} other cols = {len(sampled)} total\")\n",
    "        \n",
    "        return sampled, True  # Return sampled columns + flag indicating sampling occurred\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,DataLoader\n",
    "\n",
    "class DataLoader:\n",
    "    # === MODIFIED: Added tables parameter + memory optimization features ===\n",
    "    def __init__(self, catalogs: str, schemas: str, tables: str, logger: logging.Logger, \n",
    "                 enable_two_pass=True, enable_column_sampling=True, streaming_batch_size=1000,\n",
    "                 max_parallelism=10, schema_timeout_seconds=900):\n",
    "        self.spark = SparkSession.builder.getOrCreate()\n",
    "        self.max_parallelism = max_parallelism  # For parallel schema discovery and column loading\n",
    "        self.schema_timeout_seconds = schema_timeout_seconds  # Timeout per schema query (15 minutes)\n",
    "        self.logger = logger\n",
    "        self.foreign_key_graph = defaultdict(list)\n",
    "        \n",
    "        # === NEW: Memory optimization features ===\n",
    "        self.enable_two_pass = enable_two_pass  # Enable intelligent batching based on table sizes\n",
    "        self.enable_column_sampling = enable_column_sampling  # Sample columns from very wide tables\n",
    "        self.streaming_batch_size = streaming_batch_size  # Number of tables to process in each streaming chunk\n",
    "        \n",
    "        # === NEW: Initialize optimization components ===\n",
    "        self.size_analyzer = TableSizeAnalyzer(self.spark, self.logger) if enable_two_pass else None\n",
    "        self.batch_optimizer = DynamicBatchOptimizer(self.logger) if enable_two_pass else None\n",
    "        self.column_sampler = ColumnSampler(self.logger) if enable_column_sampling else None\n",
    "\n",
    "        # === MODIFIED: Process schemas, catalogs, and individual tables ===\n",
    "        # Use utility functions to normalize identifiers (strip backticks from user input)\n",
    "        self.schemas_to_process = [s.strip() for s in schemas.split(',') if s.strip()]\n",
    "        self.catalogs_to_process = [normalize_identifier(c) for c in catalogs.split(',') if c.strip()]\n",
    "        self.tables_to_process = [t.strip() for t in tables.split(',') if t.strip()]\n",
    "        \n",
    "        # Get all unique catalogs mentioned to check capabilities\n",
    "        self.catalog_capabilities = {}\n",
    "        unique_catalogs = set(self.catalogs_to_process)\n",
    "        for s in self.schemas_to_process:\n",
    "            cat, _ = parse_two_level_name(s)\n",
    "            if cat:\n",
    "                unique_catalogs.add(cat)\n",
    "        for t in self.tables_to_process:\n",
    "            cat, _, _ = parse_three_level_name(t)\n",
    "            if cat:\n",
    "                unique_catalogs.add(cat)\n",
    "        \n",
    "        self.logger.info(f\"Initializing capabilities for catalogs: {unique_catalogs}\")\n",
    "        for catalog in unique_catalogs:\n",
    "            self.catalog_capabilities[catalog] = self._check_catalog_capability(catalog)\n",
    "        self.logger.info(f\"Capabilities found: {self.catalog_capabilities}\")\n",
    "\n",
    "        # === MODIFIED: Build the database queue and individual tables list ===\n",
    "        self.database_queue = []\n",
    "        db_set = set()\n",
    "        \n",
    "        # Track individual tables separately\n",
    "        self.individual_tables = []  # List of (catalog, schema, table) tuples\n",
    "        \n",
    "        # Track explicitly provided schemas (to know which schemas should expand ALL tables)\n",
    "        self.explicit_schemas_set = set()  # Set of (catalog, schema) tuples\n",
    "\n",
    "        # 1. From explicit schemas (use parse_two_level_name for consistent normalization)\n",
    "        for s in self.schemas_to_process:\n",
    "            cat, db = parse_two_level_name(s)\n",
    "            if cat and db:\n",
    "                db_set.add((cat, db))\n",
    "                self.explicit_schemas_set.add((cat, db))  # Mark as explicitly provided\n",
    "            else:\n",
    "                self.logger.warning(f\"Skipping malformed schema name: {s}\")\n",
    "        \n",
    "        # 2. From individual tables (use parse_three_level_name for consistent normalization)\n",
    "        for t in self.tables_to_process:\n",
    "            cat, db, table = parse_three_level_name(t)\n",
    "            if cat and db and table:\n",
    "                self.individual_tables.append((cat, db, table))\n",
    "                # Also add the schema to db_set so we can process it\n",
    "                db_set.add((cat, db))\n",
    "            else:\n",
    "                self.logger.warning(f\"Skipping malformed table name: {t} (expected format: catalog.schema.table)\")\n",
    "        \n",
    "        # 3. From catalogs (schemas from catalogs should also expand ALL tables)\n",
    "        for cat in self.catalogs_to_process:\n",
    "            if not self.catalog_capabilities.get(cat, False):\n",
    "                 self.logger.warning(f\"Skipping schema discovery for catalog `{cat}`: Lacks information_schema support and fallback failed.\")\n",
    "                 continue\n",
    "            self.logger.info(f\"Fetching schemas for catalog: {cat}\")\n",
    "            schemas_in_cat = self._fetch_schemas_for_catalog(cat)\n",
    "            for db in schemas_in_cat:\n",
    "                db_set.add((cat, db))\n",
    "                self.explicit_schemas_set.add((cat, db))  # Mark as explicit - expand ALL tables\n",
    "        \n",
    "        self.database_queue = sorted(list(db_set)) # Sort for deterministic order\n",
    "        self.logger.info(f\"Found {len(self.database_queue)} unique databases to process.\")\n",
    "        \n",
    "        # Log explicit schemas (will expand ALL tables)\n",
    "        if self.explicit_schemas_set:\n",
    "            explicit_names = [f\"{cat}.{db}\" for cat, db in sorted(self.explicit_schemas_set)]\n",
    "            self.logger.info(f\"\uD83D\uDCCB Explicit schemas (will expand ALL tables): {', '.join(explicit_names)}\")\n",
    "        \n",
    "        # Log individual tables (specific tables only)\n",
    "        if self.individual_tables:\n",
    "            db_stats = {}\n",
    "            for cat, db, tbl in self.individual_tables:\n",
    "                key = f\"{cat}.{db}\"\n",
    "                db_stats[key] = db_stats.get(key, 0) + 1\n",
    "            self.logger.info(f\"Found {len(self.individual_tables)} individual tables to process across {len(db_stats)} databases:\")\n",
    "            for db_key, count in sorted(db_stats.items()):\n",
    "                self.logger.info(f\"  database {db_key}: {count} tables loaded\")\n",
    "        \n",
    "        # === NEW: State variables for table-level batching ===\n",
    "        self.all_table_tuples = []  # List of (catalog, schema, table) tuples\n",
    "        self.optimized_batches = []  # List of optimized batches (two-pass mode)\n",
    "        self.current_batch_idx = 0   # Current batch index (two-pass mode)\n",
    "        self.current_table_idx = 0   # Current position in all_table_tuples (single-pass mode)\n",
    "        self._tables_initialized = False\n",
    "        self._size_analysis_complete = False\n",
    "\n",
    "    def _check_catalog_capability(self, catalog_name: str) -> bool:\n",
    "        try:\n",
    "            self.spark.sql(f\"SELECT 1 FROM `{catalog_name}`.`information_schema`.`schemata` LIMIT 1\").collect()\n",
    "            return True\n",
    "        except Exception:\n",
    "            self.logger.info(f\"Catalog `{catalog_name}` does not support information_schema. Will attempt fallback 'SHOW' commands.\")\n",
    "            return False # Rely on fallbacks\n",
    "\n",
    "    def _fetch_schemas_for_catalog(self, catalog_name: str):\n",
    "        use_info_schema = self.catalog_capabilities.get(catalog_name, False)\n",
    "\n",
    "        try:\n",
    "            if not use_info_schema:\n",
    "                raise Exception(\"Catalog does not support information_schema, using fallback.\")\n",
    "            \n",
    "            query = f\"\"\"\n",
    "                SELECT `schema_name` FROM `{catalog_name}`.`information_schema`.`schemata` \n",
    "                WHERE `schema_name` != 'information_schema' \n",
    "                ORDER BY `schema_name`\n",
    "            \"\"\"\n",
    "            df = self.spark.sql(query)\n",
    "            return [row.schema_name for row in df.collect()]\n",
    "        except Exception as e_info:\n",
    "            try:\n",
    "                query = f\"SHOW SCHEMAS IN `{catalog_name}`\"\n",
    "                df = self.spark.sql(query)\n",
    "                col_name = \"databaseName\" if \"databaseName\" in df.columns else \"namespace\"\n",
    "                all_schemas = [row[col_name] for row in df.orderBy(col_name).collect()]\n",
    "                return [s for s in all_schemas if s != 'information_schema']\n",
    "            except Exception as e_show_schemas:\n",
    "                self.logger.error(f\"Error listing schemas for catalog `{catalog_name}`. All fallbacks failed: {e_show_schemas}\")\n",
    "                return []\n",
    "\n",
    "    # === MODIFIED: Added streaming/pagination support for memory efficiency ===\n",
    "    def _fetch_tables_for_schema(self, catalog_name: str, schema_name: str, limit=None, offset=0):\n",
    "        \"\"\"\n",
    "        Fetch tables for a schema with optional pagination.\n",
    "        \n",
    "        Args:\n",
    "            catalog_name: Catalog name\n",
    "            schema_name: Schema name\n",
    "            limit: Optional limit for pagination (for very large schemas)\n",
    "            offset: Offset for pagination\n",
    "            \n",
    "        Returns:\n",
    "            List of fully qualified table names\n",
    "        \"\"\"\n",
    "        cat_normalized = normalize_identifier(catalog_name)\n",
    "        schema_normalized = normalize_identifier(schema_name)\n",
    "        fq_schema = build_fqn(cat_normalized, schema_normalized)\n",
    "        tables = []\n",
    "        use_info_schema = self.catalog_capabilities.get(cat_normalized, False)\n",
    "\n",
    "        try:\n",
    "            if not use_info_schema:\n",
    "                raise Exception(\"Catalog does not support information_schema, using fallback.\")\n",
    "            \n",
    "            # Use pagination for large schemas\n",
    "            limit_clause = f\"LIMIT {limit} OFFSET {offset}\" if limit else \"\"\n",
    "            cat_quoted = quote_identifier(cat_normalized)\n",
    "            query = f\"\"\"\n",
    "                SELECT `table_name` FROM {cat_quoted}.`information_schema`.`tables`\n",
    "                WHERE `table_schema` = '{schema_normalized}'\n",
    "                ORDER BY `table_name`\n",
    "                {limit_clause}\n",
    "            \"\"\"\n",
    "            df = self.spark.sql(query)\n",
    "            \n",
    "            # Use iterative collection for large result sets\n",
    "            if limit and limit > 1000:\n",
    "                # Stream in chunks to avoid memory issues\n",
    "                chunk_size = 1000\n",
    "                collected = []\n",
    "                temp_df = df\n",
    "                while True:\n",
    "                    chunk = temp_df.limit(chunk_size).collect()\n",
    "                    if not chunk:\n",
    "                        break\n",
    "                    collected.extend(chunk)\n",
    "                    if len(chunk) < chunk_size:\n",
    "                        break\n",
    "                tables = [f\"{fq_schema}.`{row.table_name}`\" for row in collected]\n",
    "            else:\n",
    "                tables = [f\"{fq_schema}.`{row.table_name}`\" for row in df.collect()]\n",
    "                \n",
    "        except Exception:\n",
    "            try:\n",
    "                query = f\"SHOW TABLES IN {fq_schema}\"\n",
    "                df = self.spark.sql(query)\n",
    "                if 'isTemporary' in df.columns:\n",
    "                    df = df.filter(df.isTemporary == False)\n",
    "                    \n",
    "                # Apply pagination if specified\n",
    "                if limit:\n",
    "                    df = df.orderBy(\"tableName\").limit(limit).offset(offset)\n",
    "                else:\n",
    "                    df = df.orderBy(\"tableName\")\n",
    "                    \n",
    "                tables = [f\"{fq_schema}.`{row.tableName}`\" for row in df.collect()]\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error listing tables for {fq_schema}: {get_clean_error_message(e)}\")\n",
    "                tables = []\n",
    "        \n",
    "        return tables\n",
    "    \n",
    "    def _fetch_tables_for_schema_streaming(self, catalog_name: str, schema_name: str, chunk_size=1000):\n",
    "        \"\"\"\n",
    "        Generator that yields tables in chunks for memory-efficient streaming.\n",
    "        \n",
    "        Args:\n",
    "            catalog_name: Catalog name  \n",
    "            schema_name: Schema name\n",
    "            chunk_size: Number of tables to yield per chunk\n",
    "            \n",
    "        Yields:\n",
    "            Lists of fully qualified table names\n",
    "        \"\"\"\n",
    "        offset = 0\n",
    "        while True:\n",
    "            tables = self._fetch_tables_for_schema(catalog_name, schema_name, limit=chunk_size, offset=offset)\n",
    "            if not tables:\n",
    "                break\n",
    "            yield tables\n",
    "            if len(tables) < chunk_size:\n",
    "                break  # Last chunk\n",
    "            offset += chunk_size\n",
    "\n",
    "    def _get_table_details(self, catalog: str, schema: str, table: str, apply_sampling=True):\n",
    "        \"\"\"\n",
    "        Get table column details with optional column sampling for very wide tables.\n",
    "        \n",
    "        Args:\n",
    "            catalog: Catalog name\n",
    "            schema: Schema name\n",
    "            table: Table name\n",
    "            apply_sampling: Whether to apply column sampling for wide tables\n",
    "            \n",
    "        Returns:\n",
    "            List of (catalog, schema, table, column_name, data_type, comment) tuples\n",
    "        \"\"\"\n",
    "        details = []\n",
    "        cat_normalized = normalize_identifier(catalog)\n",
    "        schema_normalized = normalize_identifier(schema)\n",
    "        table_normalized = normalize_identifier(table)\n",
    "        fq_table_name = build_fqn(cat_normalized, schema_normalized, table_normalized)\n",
    "        use_info_schema = self.catalog_capabilities.get(cat_normalized, False)\n",
    "        \n",
    "        try:\n",
    "            if not use_info_schema:\n",
    "                raise Exception(\"Catalog does not support information_schema, using fallback.\")\n",
    "\n",
    "            cat_quoted = quote_identifier(cat_normalized)\n",
    "            query = f\"\"\"\n",
    "                SELECT `table_catalog`, `table_schema`, `table_name`, `column_name`, `data_type`, `comment`\n",
    "                FROM {cat_quoted}.`information_schema`.`columns`\n",
    "                WHERE `table_schema` = '{schema_normalized}' AND `table_name` = '{table_normalized}'\n",
    "                ORDER BY `ordinal_position`\n",
    "            \"\"\"\n",
    "            df = self.spark.sql(query)\n",
    "            for row in df.toLocalIterator():\n",
    "                details.append((\n",
    "                    row.table_catalog, \n",
    "                    row.table_schema, \n",
    "                    row.table_name, \n",
    "                    row.column_name, \n",
    "                    row.data_type, \n",
    "                    row.comment\n",
    "                ))\n",
    "            try:\n",
    "                fk_rows = self._get_foreign_keys(catalog, schema, table)\n",
    "                if fk_rows:\n",
    "                    self.foreign_key_graph[(catalog, schema, table)] = fk_rows\n",
    "            except Exception:\n",
    "                pass\n",
    "            if not details:\n",
    "                # Don't raise exception, just try fallback\n",
    "                pass\n",
    "            else:\n",
    "                # Apply column sampling if enabled and needed\n",
    "                if apply_sampling and self.column_sampler and len(details) > ColumnSampler.WIDE_TABLE_THRESHOLD:\n",
    "                    table_info = self.size_analyzer.get_cached_size(catalog, schema, table) if self.size_analyzer else None\n",
    "                    if not table_info:\n",
    "                        table_info = TableSizeInfo(catalog, schema, table, num_columns=len(details))\n",
    "                    details, was_sampled = self.column_sampler.sample_columns(details, table_info)\n",
    "                return details\n",
    "                \n",
    "        except Exception:\n",
    "            pass # Fallthrough to DESCRIBE\n",
    "            \n",
    "        try:\n",
    "            query = f\"DESCRIBE TABLE {fq_table_name}\"\n",
    "            df = self.spark.sql(query)\n",
    "            for row in df.toLocalIterator():\n",
    "                if row.col_name and not row.col_name.startswith('#'):\n",
    "                    details.append((\n",
    "                        catalog,\n",
    "                        schema,\n",
    "                        table,\n",
    "                        row.col_name,\n",
    "                        row.data_type,\n",
    "                        row.comment\n",
    "                    ))\n",
    "            \n",
    "            try:\n",
    "                fk_rows = self._get_foreign_keys(catalog, schema, table)\n",
    "                if fk_rows:\n",
    "                    self.foreign_key_graph[(catalog, schema, table)] = fk_rows\n",
    "            except Exception:\n",
    "                pass\n",
    "            # Apply column sampling if enabled and needed\n",
    "            if apply_sampling and self.column_sampler and len(details) > ColumnSampler.WIDE_TABLE_THRESHOLD:\n",
    "                table_info = self.size_analyzer.get_cached_size(catalog, schema, table) if self.size_analyzer else None\n",
    "                if not table_info:\n",
    "                    table_info = TableSizeInfo(catalog, schema, table, num_columns=len(details))\n",
    "                details, was_sampled = self.column_sampler.sample_columns(details, table_info)\n",
    "                \n",
    "            return details\n",
    "        except Exception as e:\n",
    "            # Suppress permission denied errors - this tool works at METADATA level only\n",
    "            error_msg = str(e).lower()\n",
    "            if \"permission\" in error_msg or \"unauthorized\" in error_msg or \"access\" in error_msg:\n",
    "                # Silently skip tables without SELECT permission - this is expected behavior\n",
    "                pass  # No logging for expected access permission issues\n",
    "            else:\n",
    "                # Log other errors at debug level only\n",
    "                pass  # Suppress low-level metadata errors\n",
    "            return []\n",
    "\n",
    "    def _get_foreign_keys(self, catalog: str, schema: str, table: str):\n",
    "        key = (catalog, schema, table)\n",
    "        if key in self.foreign_key_graph:\n",
    "            return self.foreign_key_graph[key]\n",
    "        \n",
    "        cat_normalized = normalize_identifier(catalog)\n",
    "        schema_normalized = normalize_identifier(schema)\n",
    "        table_normalized = normalize_identifier(table)\n",
    "        cat_quoted = quote_identifier(cat_normalized)\n",
    "        \n",
    "        try:\n",
    "            query = f\"\"\"\n",
    "                SELECT table_catalog,\n",
    "                       table_schema,\n",
    "                       table_name,\n",
    "                       column_name,\n",
    "                       referenced_table_catalog,\n",
    "                       referenced_table_schema,\n",
    "                       referenced_table_name,\n",
    "                       referenced_column_name\n",
    "                FROM {cat_quoted}.`information_schema`.`key_column_usage`\n",
    "                WHERE table_schema = '{schema_normalized}'\n",
    "                  AND table_name = '{table_normalized}'\n",
    "                  AND referenced_table_name IS NOT NULL\n",
    "            \"\"\"\n",
    "            df = self.spark.sql(query)\n",
    "            rels = [\n",
    "                (\n",
    "                    row.table_catalog,\n",
    "                    row.table_schema,\n",
    "                    row.table_name,\n",
    "                    row.column_name,\n",
    "                    row.referenced_table_catalog,\n",
    "                    row.referenced_table_schema,\n",
    "                    row.referenced_table_name,\n",
    "                    row.referenced_column_name\n",
    "                )\n",
    "                for row in df.collect()\n",
    "            ]\n",
    "            self.foreign_key_graph[key] = rels\n",
    "            return rels\n",
    "        except Exception as e:\n",
    "            error_str = str(e).lower()\n",
    "            if 'table or view not found' in error_str or 'key_column_usage' in error_str:\n",
    "                self.logger.debug(f\"key_column_usage table not available in catalog {cat_normalized} (older Databricks version)\")\n",
    "            elif 'cannot resolve' in error_str or 'referenced_table' in error_str:\n",
    "                self.logger.debug(f\"referenced_table columns not available in key_column_usage (older Databricks version)\")\n",
    "            self.foreign_key_graph[key] = []\n",
    "            return []\n",
    "\n",
    "    def get_foreign_key_relations(self, table_tuples: set):\n",
    "        relations = []\n",
    "        for tbl in table_tuples:\n",
    "            relations.extend(self.foreign_key_graph.get(tbl, []))\n",
    "        return relations\n",
    "\n",
    "    # === REMOVED: _fill_from_direct_tables, _fill_from_schemas, _fill_from_catalogs ===\n",
    "\n",
    "    # === MODIFIED: Support both single-pass and two-pass optimized modes ===\n",
    "    def getNextTables(self, batch_size=None):\n",
    "        \"\"\"\n",
    "        Returns a batch of tables with their column details.\n",
    "        Maintains state across all catalogs and databases, continuing seamlessly\n",
    "        across database boundaries.\n",
    "        \n",
    "        Supports two modes:\n",
    "        1. Single-pass mode (default): Uses batch_size parameter for simple batching\n",
    "        2. Two-pass mode (enable_two_pass=True): Uses pre-optimized batches, ignores batch_size\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int): Maximum number of tables to return in this batch (single-pass mode only)\n",
    "            \n",
    "        Returns:\n",
    "            list of tuples: Each tuple is (catalog, schema, table, column_name, data_type, comment)\n",
    "            None: if all tables have been processed\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize table list on first call\n",
    "        if not self._tables_initialized:\n",
    "            self.logger.info(\"Initializing all tables from all databases...\")\n",
    "            self._initialize_all_tables()\n",
    "            self._tables_initialized = True\n",
    "        \n",
    "        # === TWO-PASS OPTIMIZED MODE ===\n",
    "        if self.enable_two_pass and self._size_analysis_complete:\n",
    "            # Check if we've exhausted all batches\n",
    "            if self.current_batch_idx >= len(self.optimized_batches):\n",
    "                return None  # Signal no more batches\n",
    "            \n",
    "            # Get the next optimized batch\n",
    "            batch_table_tuples = self.optimized_batches[self.current_batch_idx]\n",
    "            batch_num = self.current_batch_idx + 1\n",
    "            \n",
    "            self.logger.info(f\"\uD83D\uDCE6 Fetching optimized batch {batch_num}/{len(self.optimized_batches)}: \"\n",
    "                           f\"{len(batch_table_tuples)} tables\")\n",
    "            \n",
    "            # Fetch column details for this optimized batch in parallel\n",
    "            # ADAPTIVE PARALLELISM: Fixed for metadata (DB connection limits)\n",
    "            column_parallelism, reason = calculate_adaptive_parallelism(\n",
    "                \"column_fetch\", self.max_parallelism, \n",
    "                num_items=len(batch_table_tuples),\n",
    "                is_llm_operation=False, logger=self.logger\n",
    "            )\n",
    "            \n",
    "            all_column_details = []\n",
    "            \n",
    "            def fetch_details_with_sampling(args):\n",
    "                # Apply sampling based on enable_column_sampling flag\n",
    "                return self._get_table_details(*args, apply_sampling=self.enable_column_sampling)\n",
    "            \n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=column_parallelism) as executor:\n",
    "                results = executor.map(fetch_details_with_sampling, batch_table_tuples)\n",
    "                \n",
    "                for column_list in results:\n",
    "                    if column_list:\n",
    "                        all_column_details.extend(column_list)\n",
    "            \n",
    "            self.logger.info(f\"   ✓ Batch {batch_num} loaded: {len(all_column_details)} columns from {len(batch_table_tuples)} tables\")\n",
    "            \n",
    "            # Update position for next call\n",
    "            self.current_batch_idx += 1\n",
    "            \n",
    "            # Return the column details for this optimized batch\n",
    "            return all_column_details\n",
    "        \n",
    "        # === SINGLE-PASS MODE (backward compatible) ===\n",
    "        else:\n",
    "            if batch_size is None:\n",
    "                self.logger.warning(\"batch_size not provided in single-pass mode, using default of 25\")\n",
    "                batch_size = 25\n",
    "                \n",
    "            # Check if we've exhausted all tables\n",
    "            if self.current_table_idx >= len(self.all_table_tuples):\n",
    "                return None  # Signal no more tables\n",
    "                \n",
    "            # Get the next batch of tables\n",
    "            end_idx = min(self.current_table_idx + batch_size, len(self.all_table_tuples))\n",
    "            batch_table_tuples = self.all_table_tuples[self.current_table_idx:end_idx]\n",
    "            \n",
    "            self.logger.info(f\"Fetching batch of {len(batch_table_tuples)} tables (indices {self.current_table_idx} to {end_idx-1} of {len(self.all_table_tuples)} total)\")\n",
    "            \n",
    "            # Fetch column details for this batch of tables in parallel\n",
    "            # ADAPTIVE PARALLELISM: Fixed for metadata (DB connection limits)\n",
    "            column_parallelism, reason = calculate_adaptive_parallelism(\n",
    "                \"column_fetch\", self.max_parallelism,\n",
    "                num_items=len(batch_table_tuples),\n",
    "                is_llm_operation=False, logger=self.logger\n",
    "            )\n",
    "            \n",
    "            all_column_details = []\n",
    "            \n",
    "            def fetch_details_with_sampling(args):\n",
    "                return self._get_table_details(*args, apply_sampling=self.enable_column_sampling)\n",
    "            \n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=column_parallelism) as executor:\n",
    "                results = executor.map(fetch_details_with_sampling, batch_table_tuples)\n",
    "                \n",
    "                for column_list in results:\n",
    "                    if column_list:\n",
    "                        all_column_details.extend(column_list)\n",
    "            \n",
    "            # Batch complete - high-level logging only\n",
    "            \n",
    "            # Update position for next call\n",
    "            self.current_table_idx = end_idx\n",
    "            \n",
    "            # Return the column details for this batch\n",
    "            return all_column_details\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the data loader to start from the beginning.\"\"\"\n",
    "        self.current_table_idx = 0\n",
    "        self.current_batch_idx = 0\n",
    "        self.logger.info(\"DataLoader reset to beginning\")\n",
    "    \n",
    "    def _initialize_all_tables(self):\n",
    "        \"\"\"\n",
    "        Fetches all table names from all databases in the queue and stores them\n",
    "        as (catalog, schema, table) tuples for later batch processing.\n",
    "        \n",
    "        Supports both streaming (memory-efficient) and two-pass (size-optimized) modes.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Discovering all tables from {len(self.database_queue)} databases...\")\n",
    "        \n",
    "        # Check if we have specific individual tables to filter by\n",
    "        individual_tables_set = set(self.individual_tables) if self.individual_tables else None\n",
    "        \n",
    "        # Get set of explicitly provided schemas (these should expand ALL tables, not filter)\n",
    "        explicit_schemas_set = getattr(self, 'explicit_schemas_set', set())\n",
    "        \n",
    "        # Log discovery strategy\n",
    "        if explicit_schemas_set and individual_tables_set:\n",
    "            explicit_schema_names = [f\"{cat}.{db}\" for cat, db in explicit_schemas_set]\n",
    "            self.logger.info(f\"\uD83D\uDCCB Discovery strategy: {len(explicit_schemas_set)} explicit schemas will expand ALL tables: {', '.join(explicit_schema_names)}\")\n",
    "            self.logger.info(f\"\uD83D\uDCCB {len(individual_tables_set)} individual tables will be included from other schemas\")\n",
    "        \n",
    "        # === TWO-PASS MODE: First analyze table sizes, then create optimized batches ===\n",
    "        if self.enable_two_pass:\n",
    "            self.logger.info(\"\uD83D\uDD04 TWO-PASS MODE ENABLED: Analyzing table sizes for intelligent batching...\")\n",
    "            self.logger.info(f\"⚡ Using parallel schema discovery with {self.max_parallelism} workers for speed\")\n",
    "            \n",
    "            # PASS 1: Discover all tables and analyze their sizes IN PARALLEL\n",
    "            all_table_size_infos = []\n",
    "            \n",
    "            # Function to process a single schema in parallel\n",
    "            def discover_and_analyze_schema(schema_tuple):\n",
    "                catalog_name, schema_name = schema_tuple\n",
    "                cat_normalized = normalize_identifier(catalog_name)\n",
    "                schema_normalized = normalize_identifier(schema_name)\n",
    "                db_fqn = build_fqn(cat_normalized, schema_normalized)\n",
    "                self.logger.info(f\"\uD83D\uDD0D Starting discovery for {db_fqn}...\")\n",
    "                \n",
    "                schema_table_tuples = []\n",
    "                schema_key = (cat_normalized, schema_normalized)\n",
    "                is_explicit_schema = schema_key in explicit_schemas_set\n",
    "                \n",
    "                # Check if schema has a huge number of tables (>10K) - use streaming\n",
    "                try:\n",
    "                    self.logger.debug(f\"   Counting tables in {db_fqn}...\")\n",
    "                    cat_quoted = quote_identifier(cat_normalized)\n",
    "                    count_query = f\"\"\"\n",
    "                        SELECT COUNT(*) as cnt FROM {cat_quoted}.`information_schema`.`tables`\n",
    "                        WHERE `table_schema` = '{schema_normalized}'\n",
    "                    \"\"\"\n",
    "                    table_count = self.spark.sql(count_query).collect()[0].cnt\n",
    "                    self.logger.debug(f\"   {db_fqn} has {table_count} tables\")\n",
    "                    use_streaming = table_count > 10000\n",
    "                except Exception as e:\n",
    "                    # Extract clean error message without stack trace for console\n",
    "                    error_msg = get_clean_error_message(e)\n",
    "                    self.logger.warning(f\"   Could not count tables in {db_fqn}: {error_msg}. Using non-streaming mode.\")\n",
    "                    self.logger.debug(f\"   Full error details: {e}\")  # Full trace to log file only\n",
    "                    use_streaming = False  # Fallback to non-streaming\n",
    "                \n",
    "                if use_streaming:\n",
    "                    self.logger.info(f\"   Large schema detected in {db_fqn} ({table_count:,} tables). Using streaming mode...\")\n",
    "                    # Process in chunks\n",
    "                    for table_chunk in self._fetch_tables_for_schema_streaming(cat_normalized, schema_normalized, chunk_size=self.streaming_batch_size):\n",
    "                        for fq_table_name in table_chunk:\n",
    "                            cat, db, tbl = parse_three_level_name(fq_table_name)\n",
    "                            if not (cat and db and tbl):\n",
    "                                self.logger.warning(f\"Skipping malformed table name: {fq_table_name}\")\n",
    "                                continue\n",
    "                            table_tuple = (cat, db, tbl)\n",
    "                            \n",
    "                            # Only filter by individual_tables if this schema was NOT explicitly provided\n",
    "                            # Explicit schemas should expand ALL tables\n",
    "                            if not is_explicit_schema and individual_tables_set and table_tuple not in individual_tables_set:\n",
    "                                continue\n",
    "                            \n",
    "                            schema_table_tuples.append(table_tuple)\n",
    "                else:\n",
    "                    # Non-streaming mode for smaller schemas\n",
    "                    table_names_fq = self._fetch_tables_for_schema(cat_normalized, schema_normalized)\n",
    "                    \n",
    "                    if not table_names_fq:\n",
    "                        self.logger.debug(f\"No tables found in {db_fqn}. Skipping.\")\n",
    "                        return []  # Return empty list for this schema\n",
    "                    \n",
    "                    for fq_table_name in table_names_fq:\n",
    "                        cat, db, tbl = parse_three_level_name(fq_table_name)\n",
    "                        if not (cat and db and tbl):\n",
    "                            self.logger.warning(f\"Skipping malformed table name: {fq_table_name}\")\n",
    "                            continue\n",
    "                        table_tuple = (cat, db, tbl)\n",
    "                        \n",
    "                        # Only filter by individual_tables if this schema was NOT explicitly provided\n",
    "                        # Explicit schemas should expand ALL tables\n",
    "                        if not is_explicit_schema and individual_tables_set and table_tuple not in individual_tables_set:\n",
    "                            continue\n",
    "                        \n",
    "                        schema_table_tuples.append(table_tuple)\n",
    "                \n",
    "                # Analyze sizes for this schema's tables\n",
    "                schema_size_infos = []\n",
    "                if schema_table_tuples:\n",
    "                    self.logger.debug(f\"   Analyzing column counts for {len(schema_table_tuples)} tables in {db_fqn}...\")\n",
    "                    \n",
    "                    # Process in chunks to avoid memory issues\n",
    "                    # IMPORTANT: Use max_parallelism=1 to avoid nested parallelism deadlock\n",
    "                    # (outer loop already runs schemas in parallel with self.max_parallelism workers)\n",
    "                    chunk_size = 100\n",
    "                    for i in range(0, len(schema_table_tuples), chunk_size):\n",
    "                        chunk = schema_table_tuples[i:i+chunk_size]\n",
    "                        size_infos = self.size_analyzer.analyze_table_sizes_batch(chunk, self.catalog_capabilities, max_parallelism=1)\n",
    "                        schema_size_infos.extend(size_infos)\n",
    "                    \n",
    "                    self.logger.info(f\"   ✓ {db_fqn}: Found {len(schema_table_tuples)} tables\")\n",
    "                \n",
    "                return schema_size_infos\n",
    "            \n",
    "            # ADAPTIVE PARALLELISM: Fixed for metadata (DB connection limits)\n",
    "            discovery_parallelism, reason = calculate_adaptive_parallelism(\n",
    "                \"schema_discovery\", self.max_parallelism,\n",
    "                num_items=len(self.database_queue),\n",
    "                is_llm_operation=False, logger=self.logger\n",
    "            )\n",
    "            log_adaptive_parallelism_decision(\"schema_discovery\", discovery_parallelism, self.max_parallelism, reason)\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=discovery_parallelism, thread_name_prefix=\"SchemaDiscovery\") as executor:\n",
    "                futures = {executor.submit(discover_and_analyze_schema, schema_tuple): schema_tuple \n",
    "                          for schema_tuple in self.database_queue}\n",
    "                \n",
    "                completed = 0\n",
    "                total = len(self.database_queue)\n",
    "                self.logger.info(f\"      Submitted {total} schemas for parallel discovery...\")\n",
    "                \n",
    "                # Add timeout to prevent infinite hangs\n",
    "                timeout_per_schema = self.schema_timeout_seconds\n",
    "                \n",
    "                for future in concurrent.futures.as_completed(futures, timeout=timeout_per_schema * total):\n",
    "                    schema_tuple = futures[future]\n",
    "                    completed += 1\n",
    "                    try:\n",
    "                        # Add per-future timeout as well\n",
    "                        schema_size_infos = future.result(timeout=timeout_per_schema)\n",
    "                        all_table_size_infos.extend(schema_size_infos)\n",
    "                        self.logger.info(f\"      Progress: {completed}/{total} schemas analyzed, {len(all_table_size_infos)} tables discovered so far\")\n",
    "                    except concurrent.futures.TimeoutError:\n",
    "                        timeout_minutes = timeout_per_schema // 60\n",
    "                        self.logger.error(f\"⏱️  Timeout analyzing schema {schema_tuple} (>{timeout_minutes} min). Skipping this schema.\")\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Failed to analyze schema {schema_tuple}: {e}\")\n",
    "                \n",
    "                # Log final completion\n",
    "                if completed < total:\n",
    "                    self.logger.warning(f\"⚠️  Only {completed}/{total} schemas completed. {total - completed} schemas timed out or failed.\")\n",
    "            \n",
    "            self.logger.info(f\"\uD83D\uDCCA Pass 1 complete: Analyzed {len(all_table_size_infos)} tables\")\n",
    "            \n",
    "            # Deduplicate tables (in case same table was included via both schema and individual table)\n",
    "            seen_tables = set()\n",
    "            unique_table_size_infos = []\n",
    "            for info in all_table_size_infos:\n",
    "                table_key = (info.catalog, info.schema, info.table)\n",
    "                if table_key not in seen_tables:\n",
    "                    seen_tables.add(table_key)\n",
    "                    unique_table_size_infos.append(info)\n",
    "            \n",
    "            if len(unique_table_size_infos) < len(all_table_size_infos):\n",
    "                duplicates_removed = len(all_table_size_infos) - len(unique_table_size_infos)\n",
    "                self.logger.info(f\"\uD83D\uDD04 Deduplicated: Removed {duplicates_removed} duplicate tables, {len(unique_table_size_infos)} unique tables remaining\")\n",
    "                all_table_size_infos = unique_table_size_infos\n",
    "            \n",
    "            # PASS 2: Create optimized batches based on size analysis\n",
    "            self.logger.info(\"\uD83C\uDFAF Pass 2: Creating optimized batches based on table sizes...\")\n",
    "            self.optimized_batches = self.batch_optimizer.create_optimized_batches(all_table_size_infos)\n",
    "            self._size_analysis_complete = True\n",
    "            \n",
    "            # Store all table tuples for reference\n",
    "            self.all_table_tuples = [\n",
    "                (info.catalog, info.schema, info.table) for info in all_table_size_infos\n",
    "            ]\n",
    "            \n",
    "            self.logger.info(f\"✅ Two-pass initialization complete: {len(self.all_table_tuples)} tables in {len(self.optimized_batches)} optimized batches\")\n",
    "        \n",
    "        # === SINGLE-PASS MODE: Standard behavior (for backward compatibility) ===\n",
    "        else:\n",
    "            self.logger.info(\"\uD83D\uDCCB SINGLE-PASS MODE: Standard table discovery with parallel queries...\")\n",
    "            self.logger.info(f\"⚡ Using {self.max_parallelism} parallel workers for schema queries\")\n",
    "            \n",
    "            # Function to fetch tables for a single schema\n",
    "            def fetch_schema_tables(schema_tuple):\n",
    "                catalog_name, schema_name = schema_tuple\n",
    "                cat_normalized = normalize_identifier(catalog_name)\n",
    "                schema_normalized = normalize_identifier(schema_name)\n",
    "                db_fqn = build_fqn(cat_normalized, schema_normalized)\n",
    "                self.logger.info(f\"\uD83D\uDD0D Starting table fetch for {db_fqn}...\")\n",
    "                \n",
    "                schema_key = (cat_normalized, schema_normalized)\n",
    "                is_explicit_schema = schema_key in explicit_schemas_set\n",
    "                \n",
    "                # Fetch all table names for this database\n",
    "                table_names_fq = self._fetch_tables_for_schema(cat_normalized, schema_normalized)\n",
    "                \n",
    "                schema_tuples = []\n",
    "                if not table_names_fq:\n",
    "                    self.logger.debug(f\"No tables found in {db_fqn}. Skipping.\")\n",
    "                    return schema_tuples\n",
    "                    \n",
    "                # Parse into (cat, schema, table) tuples\n",
    "                for fq_table_name in table_names_fq:\n",
    "                    cat, db, tbl = parse_three_level_name(fq_table_name)\n",
    "                    if not (cat and db and tbl):\n",
    "                        self.logger.warning(f\"Skipping malformed table name: {fq_table_name}\")\n",
    "                        continue\n",
    "                    table_tuple = (cat, db, tbl)\n",
    "                    \n",
    "                    # Explicit schemas expand ALL tables; otherwise filter by individual_tables\n",
    "                    if is_explicit_schema:\n",
    "                        # Schema was explicitly provided - include ALL tables\n",
    "                        schema_tuples.append(table_tuple)\n",
    "                    elif individual_tables_set:\n",
    "                        # Schema came from individual tables - only include those specific tables\n",
    "                        if table_tuple in individual_tables_set:\n",
    "                            schema_tuples.append(table_tuple)\n",
    "                    else:\n",
    "                        # No individual tables specified - include all\n",
    "                        schema_tuples.append(table_tuple)\n",
    "                \n",
    "                self.logger.info(f\"   ✓ {db_fqn}: Found {len(schema_tuples)} tables\")\n",
    "                return schema_tuples\n",
    "            \n",
    "            # ADAPTIVE PARALLELISM: Fixed for metadata (DB connection limits)\n",
    "            table_discovery_parallelism, reason = calculate_adaptive_parallelism(\n",
    "                \"table_discovery\", self.max_parallelism,\n",
    "                num_items=len(self.database_queue),\n",
    "                is_llm_operation=False, logger=self.logger\n",
    "            )\n",
    "            log_adaptive_parallelism_decision(\"table_discovery\", table_discovery_parallelism, self.max_parallelism, reason)\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=table_discovery_parallelism, thread_name_prefix=\"TableDiscovery\") as executor:\n",
    "                futures = {executor.submit(fetch_schema_tables, schema_tuple): schema_tuple \n",
    "                          for schema_tuple in self.database_queue}\n",
    "                \n",
    "                completed = 0\n",
    "                total = len(self.database_queue)\n",
    "                self.logger.info(f\"      Submitted {total} schemas for parallel discovery...\")\n",
    "                \n",
    "                # Add timeout to prevent infinite hangs\n",
    "                timeout_per_schema = self.schema_timeout_seconds\n",
    "                \n",
    "                for future in concurrent.futures.as_completed(futures, timeout=timeout_per_schema * total):\n",
    "                    schema_tuple = futures[future]\n",
    "                    completed += 1\n",
    "                    try:\n",
    "                        # Add per-future timeout as well\n",
    "                        schema_tuples = future.result(timeout=timeout_per_schema)\n",
    "                        self.all_table_tuples.extend(schema_tuples)\n",
    "                        self.logger.info(f\"      Progress: {completed}/{total} schemas processed, {len(self.all_table_tuples)} tables discovered so far\")\n",
    "                    except concurrent.futures.TimeoutError:\n",
    "                        timeout_minutes = timeout_per_schema // 60\n",
    "                        self.logger.error(f\"⏱️  Timeout fetching tables for schema {schema_tuple} (>{timeout_minutes} min). Skipping this schema.\")\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Failed to fetch tables for schema {schema_tuple}: {e}\")\n",
    "                \n",
    "                # Log final completion\n",
    "                if completed < total:\n",
    "                    self.logger.warning(f\"⚠️  Only {completed}/{total} schemas completed. {total - completed} schemas timed out or failed.\")\n",
    "            \n",
    "            # Deduplicate tables (in case same table was included via both schema and individual table)\n",
    "            original_count = len(self.all_table_tuples)\n",
    "            self.all_table_tuples = list(dict.fromkeys(self.all_table_tuples))  # Preserves order, removes duplicates\n",
    "            if len(self.all_table_tuples) < original_count:\n",
    "                duplicates_removed = original_count - len(self.all_table_tuples)\n",
    "                self.logger.info(f\"\uD83D\uDD04 Deduplicated: Removed {duplicates_removed} duplicate tables, {len(self.all_table_tuples)} unique tables remaining\")\n",
    "            \n",
    "            self.logger.info(f\"Table discovery complete. Found {len(self.all_table_tuples)} total tables across all databases.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,LakeViewDashboard\n",
    "# Chart Type Reference Guide:\n",
    "# The generator maps SQL columns to chart fields in two ways:\n",
    "# 1. By Prefix: Looks for columns starting with a specific prefix (e.g., `category_`, `value_`).\n",
    "# 2. By Position: If no prefixes are found, it uses the column order from the SELECT statement.\n",
    "#\n",
    "# Supported formats (optional columns in brackets `[]`):\n",
    "# --------------------------------------------------------------------------------\n",
    "# Bar Chart:      category_col, value_col, [group_col]\n",
    "# Pie Chart:      category_col, value_col\n",
    "# Line Chart:     x_axis_col, y_axis_col, [group_col]\n",
    "# Area Chart:     x_axis_col, y_axis_col, [group_col]\n",
    "# Scatter Plot:   x_axis_col, y_axis_col, [group_col]\n",
    "# Heatmap:        x_axis_col, y_axis_col, color_col\n",
    "# Combo Chart:    x_axis_col, bar_value_col, line_value_col\n",
    "#\n",
    "# Counter:        value_col\n",
    "# Histogram:      value_col\n",
    "# Funnel Chart:   stage_col, value_col\n",
    "#\n",
    "# Box Plot:       category_col, min_col, q1_col, median_col, q3_col, max_col\n",
    "# Sankey Chart:   source_col, destination_col, value_col\n",
    "# Pivot Table:    row_col, column_col, cell_value_col\n",
    "# Choropleth Map: location_col, value_col\n",
    "# Symbol Map:     lat_col, lon_col, [size_col], [group_col]\n",
    "#\n",
    "# Table:          col_1, col_2, ... (all columns are displayed)\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# (Chart Type Reference Guide... unchanged)\n",
    "\n",
    "class WidgetFailedToCreate(Exception):\n",
    "    \"\"\"Custom exception raised when widget creation fails due to unmet requirements.\"\"\"\n",
    "    pass\n",
    "\n",
    "class LakeViewDashboard:\n",
    "    \"\"\"A fluent API for generating Databricks Lakeview dashboards.\"\"\"\n",
    "    def __init__(self, name:str, logger: logging.Logger):\n",
    "        self.name = name\n",
    "        self.logger = logger  # <-- 1. Logger is now an instance variable\n",
    "        self.WIDGET_WIDTH = 3\n",
    "        self.WIDGET_HEIGHT = 6\n",
    "        self.COLUMN_COUNT = 2\n",
    "        self.dashboard = {\"datasets\": [], \"pages\": [], \"uiSettings\": {\"theme\": {\"widgetHeaderAlignment\": \"ALIGNMENT_UNSPECIFIED\"}}}\n",
    "        self.current_page = None\n",
    "        self.dataset_map = {}\n",
    "        self.page_map = {}\n",
    "\n",
    "    def _find_col(self, columns, prefixes, index):\n",
    "        for prefix in prefixes:\n",
    "            col = next((c for c in columns if c.startswith(prefix)), None)\n",
    "            if col: return col\n",
    "        if index < len(columns): return columns[index]\n",
    "        # Return None if not found, to be caught by spec builders\n",
    "        return None\n",
    "\n",
    "    def _get_or_create_dataset(self, query, title):\n",
    "        query_key = query.strip().lower()\n",
    "        if query_key in self.dataset_map: return self.dataset_map[query_key]\n",
    "        dataset_name = uuid.uuid4().hex[:8]\n",
    "        dataset = {\"name\": dataset_name, \"displayName\": title, \"queryLines\": [query.strip()]}\n",
    "        self.dashboard[\"datasets\"].append(dataset)\n",
    "        self.dataset_map[query_key] = dataset_name\n",
    "        return dataset_name\n",
    "\n",
    "    def _parse_sql_columns(self, sql_query):\n",
    "        query = re.sub(r'--.*', '', sql_query)\n",
    "        query = re.sub(r'/\\*.*?\\*/', '', query, flags=re.DOTALL)\n",
    "        query = ' '.join(query.split())\n",
    "        query_upper = query.upper()\n",
    "\n",
    "        select_pos = query_upper.find(\"SELECT \")\n",
    "        if select_pos == -1: return []\n",
    "\n",
    "        paren_depth = 0\n",
    "        main_from_pos = -1\n",
    "        cursor = select_pos + 6\n",
    "        while cursor < len(query):\n",
    "            char = query[cursor]\n",
    "            if char == '(': paren_depth += 1\n",
    "            elif char == ')': paren_depth -= 1\n",
    "            if paren_depth == 0 and query_upper[cursor:cursor+6] == ' FROM ':\n",
    "                main_from_pos = cursor\n",
    "                break\n",
    "            cursor += 1\n",
    "\n",
    "        if main_from_pos == -1: return []\n",
    "        select_part = query[select_pos + 6 : main_from_pos].strip()\n",
    "        if not select_part or select_part == '*': return []\n",
    "            \n",
    "        columns_exprs = []\n",
    "        current_col_start = 0\n",
    "        paren_depth = 0\n",
    "        for i, char in enumerate(select_part):\n",
    "            if char == '(': paren_depth += 1\n",
    "            elif char == ')': paren_depth -= 1\n",
    "            elif char == ',' and paren_depth == 0:\n",
    "                columns_exprs.append(select_part[current_col_start:i].strip())\n",
    "                current_col_start = i + 1\n",
    "        columns_exprs.append(select_part[current_col_start:].strip())\n",
    "        \n",
    "        aliases = []\n",
    "        for col_expr in columns_exprs:\n",
    "            parts = re.split(r'\\s+as\\s+', col_expr, flags=re.IGNORECASE)\n",
    "            alias = parts[-1] if len(parts) > 1 else col_expr.split('.')[-1].split()[-1]\n",
    "            aliases.append(alias.strip('`\"\\''))\n",
    "            \n",
    "        return aliases\n",
    "    \n",
    "    def page(self, title):\n",
    "        if title not in self.page_map:\n",
    "            page_name = uuid.uuid4().hex[:8]\n",
    "            new_page = {\"name\": page_name, \"displayName\": title, \"layout\": [], \"pageType\": \"PAGE_TYPE_CANVAS\", \"_column_counts\": [0] * self.COLUMN_COUNT}\n",
    "            self.dashboard[\"pages\"].append(new_page)\n",
    "            self.page_map[title] = new_page\n",
    "        self.current_page = self.page_map[title]\n",
    "        return self\n",
    "\n",
    "    def save_to_file(self, file_path):\n",
    "        # This is now handled by the external write_to_dbfs function\n",
    "        # but we'll leave the helper logic.\n",
    "        for p in self.dashboard.get(\"pages\", []): p.pop(\"_column_counts\", None)\n",
    "        dir_path = os.path.dirname(file_path)\n",
    "        if dir_path: os.makedirs(dir_path, exist_ok=True)\n",
    "        with open(file_path, \"w\") as f: json.dump(self.dashboard, f, indent=2)\n",
    "        self.logger.info(f\"Dashboard successfully saved to {file_path}\")\n",
    "        return self\n",
    "\n",
    "    # --- 2. NEW: validate_viz method ---\n",
    "    def validate_viz(self, viz_type: str, viz_query: str, viz_title: str):\n",
    "        \"\"\"\n",
    "        Runs a dry-run of the widget creation process to validate it.\n",
    "        Raises WidgetFailedToCreate or other errors if validation fails.\n",
    "        \"\"\"\n",
    "        viz_function_name = viz_type.lower().replace(\"-\", \"_\")\n",
    "        \n",
    "        # 1. Check if type is supported\n",
    "        try:\n",
    "            spec_builder = getattr(self, f\"_build_{viz_function_name}_spec\")\n",
    "        except AttributeError:\n",
    "            raise WidgetFailedToCreate(f\"Visualization type '{viz_type}' is not supported.\")\n",
    "        \n",
    "        # 2. Check if columns can be parsed\n",
    "        columns = self._parse_sql_columns(viz_query)\n",
    "        if not columns:\n",
    "            raise WidgetFailedToCreate(f\"Could not parse any columns from query, likely due to 'SELECT *'. Query: {viz_query}\")\n",
    "        \n",
    "        # 3. Check if spec builder runs without error (e.g., IndexError)\n",
    "        try:\n",
    "            spec_builder(columns, viz_title)\n",
    "        except Exception as e:\n",
    "            raise WidgetFailedToCreate(f\"Failed to build widget spec (e.g., missing required columns): {e}\")\n",
    "        \n",
    "        # If all checks pass, return the parsed data to avoid re-doing work\n",
    "        return columns, spec_builder, viz_function_name\n",
    "\n",
    "    def _add_chart(self, query, title, chart_type_fn, columns, spec):\n",
    "        \"\"\"Internal method to add a chart, assuming validation has passed.\"\"\"\n",
    "        if not self.current_page: self.page(\"Main Page\")\n",
    "        dataset_name = self._get_or_create_dataset(query, title)\n",
    "        \n",
    "        is_disaggregated = chart_type_fn not in (\"histogram\", \"box\", \"table\")\n",
    "        fields = [{\"name\": c, \"expression\": f\"`{c}`\"} for c in columns]\n",
    "        if chart_type_fn == \"histogram\":\n",
    "            value_col = self._find_col(columns, ('value_',), 0) or columns[0]\n",
    "            fields = [{\"name\": f\"bin({value_col}, binWidth=10)\", \"expression\": f\"BIN_FLOOR(`{value_col}`, 10)\"}, {\"name\": \"count(*)\", \"expression\": \"COUNT(`*`)\"}]\n",
    "        \n",
    "        widget_query = {\"datasetName\": dataset_name, \"fields\": fields, \"disaggregated\": is_disaggregated}\n",
    "        widget = {\"name\": uuid.uuid4().hex[:8], \"queries\": [{\"name\": \"main_query\", \"query\": widget_query}], \"spec\": spec}\n",
    "        \n",
    "        if chart_type_fn == \"choropleth_map\":\n",
    "            width, height = self.COLUMN_COUNT * self.WIDGET_WIDTH, self.WIDGET_HEIGHT\n",
    "            max_row = max(self.current_page[\"_column_counts\"]) if self.current_page[\"_column_counts\"] else 0\n",
    "            position = {\"x\": 0, \"y\": max_row, \"width\": width, \"height\": height}\n",
    "            self.current_page[\"layout\"].append({\"widget\": widget, \"position\": position})\n",
    "            new_row_count = max_row + height\n",
    "            for i in range(self.COLUMN_COUNT): self.current_page[\"_column_counts\"][i] = new_row_count\n",
    "        else:\n",
    "            target_column = self.current_page[\"_column_counts\"].index(min(self.current_page[\"_column_counts\"]))\n",
    "            position = {\"x\": target_column * self.WIDGET_WIDTH, \"y\": self.current_page[\"_column_counts\"][target_column], \"width\": self.WIDGET_WIDTH, \"height\": self.WIDGET_HEIGHT}\n",
    "            self.current_page[\"layout\"].append({\"widget\": widget, \"position\": position})\n",
    "            self.current_page[\"_column_counts\"][target_column] += self.WIDGET_HEIGHT\n",
    "        \n",
    "        # --- 4. Success Logging (as requested) ---\n",
    "        self.logger.info(f\"{self.name}.{self.current_page['displayName']}: Added widget '{title}' ({chart_type_fn})\")\n",
    "        \n",
    "    # --- (All _build_..._spec methods remain unchanged) ---\n",
    "    def _get_display_name(self, col_name): \n",
    "        if col_name is None: return \"N/A\"\n",
    "        return col_name.split(\"_\", 1)[-1].replace(\"_\", \" \").title()\n",
    "    def _build_counter_spec(self, c, t): return {\"version\": 2, \"widgetType\": \"counter\", \"frame\": {\"title\": t, \"showTitle\": True}, \"encodings\": {\"value\": {\"fieldName\": self._find_col(c, ('value_',), 0), \"displayName\": self._get_display_name(self._find_col(c, ('value_',), 0))}}}\n",
    "    def _build_table_spec(self, c, t): return {\"version\": 2, \"widgetType\": \"table\", \"encodings\": {\"columns\": [{\"fieldName\": col, \"displayName\": self._get_display_name(col)} for col in c]}, \"frame\": {\"title\": t, \"showTitle\": True}}\n",
    "    def _build_bar_spec(self, c, t): encodings = {\"x\": {\"fieldName\": self._find_col(c, ('category_', 'x_'), 0), \"scale\": {\"type\": \"categorical\", \"sort\": {\"by\": \"y-reversed\"}}}, \"y\": {\"fieldName\": self._find_col(c, ('value_', 'y_'), 1), \"scale\": {\"type\": \"quantitative\"}}}; color = self._find_col(c, ('group_', 'color_'), 2);_ = encodings.update({\"color\": {\"fieldName\": color, \"scale\": {\"type\": \"categorical\"}}}) if color else \"\"; return {\"version\": 3, \"widgetType\": \"bar\", \"encodings\": encodings, \"frame\": {\"title\": t, \"showTitle\": True}}\n",
    "    def _build_pie_spec(self, c, t): return {\"version\": 3, \"widgetType\": \"pie\", \"frame\": {\"title\": t, \"showTitle\": True}, \"encodings\": {\"angle\": {\"fieldName\": self._find_col(c, ('value_', 'angle_', 'y_'), 1), \"scale\": {\"type\": \"quantitative\"}}, \"color\": {\"fieldName\": self._find_col(c, ('category_', 'x_'), 0), \"scale\": {\"type\": \"categorical\"}}, \"label\": {\"show\": True}}}\n",
    "    def _build_line_spec(self, c, t): encodings = {\"x\": {\"fieldName\": self._find_col(c, ('x_',), 0), \"scale\": {\"type\": \"temporal\"}}, \"y\": {\"fieldName\": self._find_col(c, ('y_',), 1), \"scale\": {\"type\": \"quantitative\"}}}; color = self._find_col(c, ('group_', 'color_'), 2);_ = encodings.update({\"color\": {\"fieldName\": color, \"scale\": {\"type\": \"categorical\"}}}) if color else \"\"; return {\"version\": 3, \"widgetType\": \"line\", \"encodings\": encodings, \"frame\": {\"title\": t, \"showTitle\": True}}\n",
    "    def _build_area_spec(self, c, t): encodings = {\"x\": {\"fieldName\": self._find_col(c, ('x_',), 0), \"scale\": {\"type\": \"temporal\"}}, \"y\": {\"fieldName\": self._find_col(c, ('y_', 'value_'), 1), \"scale\": {\"type\": \"quantitative\"}}}; color = self._find_col(c, ('group_', 'color_'), 2);_ = encodings.update({\"color\": {\"fieldName\": color, \"scale\": {\"type\": \"categorical\"}}}) if color else \"\"; return {\"version\": 3, \"widgetType\": \"area\", \"encodings\": encodings, \"frame\": {\"title\": t, \"showTitle\": True}}\n",
    "    def _build_scatter_spec(self, c, t): encodings = {\"x\": {\"fieldName\": self._find_col(c, ('x_',), 0), \"scale\": {\"type\": \"quantitative\"}}, \"y\": {\"fieldName\": self._find_col(c, ('y_',), 1), \"scale\": {\"type\": \"quantitative\"}}}; color = self._find_col(c, ('group_', 'color_'), 2);_ = encodings.update({\"color\": {\"fieldName\": color, \"scale\": {\"type\": \"categorical\"}}}) if color else \"\"; return {\"version\": 3, \"widgetType\": \"scatter\", \"encodings\": encodings, \"frame\": {\"title\": t, \"showTitle\": True}}\n",
    "    def _build_heatmap_spec(self, c, t): return {\"version\": 3, \"widgetType\": \"heatmap\", \"encodings\": {\"x\": {\"fieldName\": self._find_col(c, ('x_',), 0), \"scale\": {\"type\": \"categorical\"}}, \"y\": {\"fieldName\": self._find_col(c, ('y_',), 1), \"scale\": {\"type\": \"categorical\"}}, \"color\": {\"fieldName\": self._find_col(c, ('color_',), 2), \"scale\": {\"type\": \"quantitative\"}}}, \"frame\": {\"title\": t, \"showTitle\": True}}\n",
    "    def _build_histogram_spec(self, c, t): v = self._find_col(c, ('value_',), 0) or c[0]; return {\"version\": 3, \"widgetType\": \"histogram\", \"encodings\": {\"x\": {\"fieldName\": f\"bin({v}, binWidth=10)\", \"scale\": {\"type\": \"quantitative\"}}, \"y\": {\"fieldName\": \"count(*)\", \"scale\": {\"type\": \"quantitative\"}}}, \"frame\": {\"title\": t, \"showTitle\": True}}\n",
    "    def _build_box_spec(self, c, t): y = {\"whiskerStart\": {\"fieldName\": self._find_col(c, ('minimum_', 'min_'), 1)}, \"boxStart\": {\"fieldName\": self._find_col(c, ('q1_',), 2)}, \"boxMid\": {\"fieldName\": self._find_col(c, ('median_',), 3)}, \"boxEnd\": {\"fieldName\": self._find_col(c, ('q3_',), 4)}, \"whiskerEnd\": {\"fieldName\": self._find_col(c, ('maximum_', 'max_'), 5)}, \"scale\": {\"type\": \"quantitative\"}}; return {\"version\": 3, \"widgetType\": \"box\", \"frame\": {\"title\": t, \"showTitle\": True}, \"encodings\": {\"x\": {\"fieldName\": self._find_col(c, ('category_',), 0), \"scale\": {\"type\": \"categorical\"}}, \"y\": y}}\n",
    "    def _build_combo_spec(self, c, t): y = {\"primary\": {\"fields\": [{\"fieldName\": self._find_col(c, ('bar_',), 1), \"seriesType\": \"bar\"}]}, \"secondary\": {\"fields\": [{\"fieldName\": self._find_col(c, ('line_',), 2), \"seriesType\": \"line\"}]}, \"scale\": {\"type\": \"quantitative\"}, \"dualAxis\": True}; return {\"version\": 1, \"widgetType\": \"combo\", \"frame\": {\"title\": t, \"showTitle\": True}, \"encodings\": {\"x\": {\"fieldName\": self._find_col(c, ('x_',), 0), \"scale\": {\"type\": \"temporal\"}}, \"y\": y}}\n",
    "    def _build_sankey_spec(self, c, t): return {\"version\": 1, \"widgetType\": \"sankey\", \"encodings\": {\"value\": {\"fieldName\": self._find_col(c, ('value_',), 2)}, \"stages\": [{\"fieldName\": self._find_col(c, ('source_',), 0)}, {\"fieldName\": self._find_col(c, ('destination_',), 1)}]}, \"frame\": {\"title\": t, \"showTitle\": True}}\n",
    "    def _build_pivot_spec(self, c, t): return {\"version\": 3, \"widgetType\": \"pivot\", \"encodings\": {\"rows\": [{\"fieldName\": self._find_col(c, ('row_',), 0)}], \"columns\": [{\"fieldName\": self._find_col(c, ('column_',), 1)}], \"cell\": {\"type\": \"multi-cell\", \"fields\": [{\"fieldName\": self._find_col(c, ('cell_',), 2), \"cellType\": \"text\"}]}}, \"frame\": {\"title\": t, \"showTitle\": True}}\n",
    "    def _build_funnel_spec(self, c, t): return {\"version\": 3, \"widgetType\": \"funnel\", \"encodings\": {\"x\": {\"fieldName\": self._find_col(c, ('value_',), 1), \"scale\": {\"type\": \"quantitative\"}}, \"y\": {\"fieldName\": self._find_col(c, ('stage_',), 0), \"scale\": {\"type\": \"categorical\"}}, \"label\": {\"show\": True}}, \"frame\": {\"title\": t, \"showTitle\": True}}\n",
    "    def _build_choropleth_map_spec(self, c, t): return {\"version\": 1, \"widgetType\": \"choropleth-map\", \"frame\": {\"title\": t, \"showTitle\": True}, \"encodings\": {\"color\": {\"fieldName\": self._find_col(c, ('value_',), 1), \"scale\": {\"type\": \"quantitative\", \"colorRamp\": {\"mode\": \"scheme\", \"scheme\": \"blues\"}}}, \"region\": {\"regionType\": \"mapbox-v4-admin\", \"admin0\": {\"fieldName\": self._find_col(c, ('location_',), 0), \"type\": \"field\", \"geographicRole\": \"admin0-iso-3166-1-alpha-3\"}}}}\n",
    "    def _build_symbol_map_spec(self, c, t): encodings = {\"coordinates\": {\"latitude\": {\"fieldName\": self._find_col(c, ('lat_', 'latitude_'), 0)}, \"longitude\": {\"fieldName\": self._find_col(c, ('lon_', 'longitude_'), 1)}}}; size = self._find_col(c, ('size_', 'value_'), 2); color = self._find_col(c, ('color_', 'group_'), 3);_ = encodings.update({\"size\": {\"fieldName\": size, \"scale\": {\"type\": \"quantitative\"}}}) if size else \"\";_ = encodings.update({\"color\": {\"fieldName\": color, \"scale\": {\"type\": \"categorical\"}}}) if color else \"\"; return {\"version\": 2, \"widgetType\": \"symbol-map\", \"frame\": {\"title\": t, \"showTitle\": True}, \"encodings\": encodings}\n",
    "\n",
    "    # --- 3. Corrected add_viz ---\n",
    "    def add_viz(self, viz_type: str, viz_title: str, viz_query: str):\n",
    "        \"\"\"\n",
    "        Validates and adds a visualization to the current page.\n",
    "        Returns True on success, False on \"expected\" failure.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # First, validate the visualization. This can raise errors.\n",
    "            columns, spec_builder, viz_function_name = self.validate_viz(viz_type, viz_query, viz_title)\n",
    "            \n",
    "            # Re-run spec builder to get the final spec\n",
    "            spec = spec_builder(columns, viz_title)\n",
    "\n",
    "            # If validation passes, call the internal _add_chart method\n",
    "            self._add_chart(viz_query, viz_title, viz_function_name, columns, spec)\n",
    "            \n",
    "            return True # Return True on success\n",
    "        \n",
    "        except (WidgetFailedToCreate, AttributeError, IndexError, TypeError) as e:\n",
    "            # Catch \"expected\" failures (bad type, bad SQL, bad spec, NoneType errors)\n",
    "            self.logger.warning(f\"Skipping widget '{viz_title}' during final assembly: {e}\")\n",
    "            return False # Return False on \"expected\" failure\n",
    "        except Exception as e:\n",
    "            # Catch any other unexpected errors\n",
    "            self.logger.error(f\"Unexpected error adding widget '{viz_title}': {e}\")\n",
    "            return False # Return False on \"unexpected\" failure\n",
    "        \n",
    "PROMPT_TEMPLATES[\"DASHBOARDS_GEN_PROMPT\"] = \"\"\"\n",
    "### 0. PERSONA ACTIVATION\n",
    "\n",
    "You are a **Principal Business Intelligence Engineer** and an industry specialist with deep expertise in the `{industry}` industry, `{description}`. You are a master of designing and implementing comprehensive analytical dashboards on the Databricks platform. Your work focuses on creating actionable, multi-page dashboards that translate complex data into critical business insights through effective data visualization.\n",
    "\n",
    "-----\n",
    "\n",
    "### 1. DEFINITIONS\n",
    "\n",
    "  * **Dashboard Page:** A dedicated view within the dashboard.\n",
    "  * **Query:** A multi-join or single-table Databricks SQL select statement that retrieves data for a visualization.\n",
    "  * **Widget:** A data visualization component (e.g., bar chart, line chart).\n",
    "\n",
    "-----\n",
    "\n",
    "### 2. CONTEXT\n",
    "\n",
    "  * **Industry:** `{industry}`\n",
    "  * **Business Domain:** `{domain}` (This is given as `catalog.schema`)\n",
    "  * **Target Page Name:** `{page_name}`\n",
    "  * **Boolean Values Format:** `{boolean_format}`\n",
    "  * **Date Format:** `{date_format}`\n",
    "  * **Timestamp Format:** `{datetime_format}`\n",
    "\n",
    "-----\n",
    "\n",
    "### 3. CORE TASK\n",
    "\n",
    "Your core task is to create a comprehensive set of widgets for a **single dashboard page named `{page_name}`**. You will generate a curated set of SQL queries that power various widgets. **Crucially, every query you generate MUST have a clear and significant business value**, representing critical reports, KPIs, and performance measures from the business domain.\n",
    "\n",
    "-----\n",
    "\n",
    "### 4. WORKFLOW & RULES\n",
    "\n",
    "1.  **Analyze Data Model & Plan Joins:**\n",
    "\n",
    "      * Thoroughly analyze the **database schema markdown** in **Section 9**.\n",
    "      * Before writing any SQL, mentally identify potential join keys between tables. Look for columns with similar names and purposes. This planning is crucial for generating valid, insightful queries.\n",
    "\n",
    "2.  **Design Business-Critical Widgets:**\n",
    "\n",
    "      * All generated queries will be for a single page named `{page_name}`.\n",
    "      * The total number of generated queries **must be between `{min_dashboard_visualizations}` and `{max_dashboard_visualizations}`**.\n",
    "      * For every widget idea, first define the business question it answers (e.g., 'Which marketing channels are driving the most sales?'). Then, design a query to answer it.\n",
    "\n",
    "3.  **Generate Insightful SQL Queries:**\n",
    "\n",
    "    **MOST CRITICAL RULE: STRICT SCHEMA ADHERENCE**\n",
    "\n",
    "      * **ZERO TOLERANCE FOR HALLUCINATION:** You **MUST NOT** invent, guess, assume, or infer any table or column names. Every identifier you use must exist in the schema.\n",
    "      * **EXACT NAMES ONLY:** Every single table and column identifier in your SQL **MUST BE an exact, case-sensitive copy-paste** from the table and column names in the **schema markdown** (Section 9).\n",
    "          * For tables, you **MUST** use the fully qualified, three-level name provided (e.g., `main.customer_data.accounts`).\n",
    "          * For columns, you **MUST** use the exact column name provided (e.g., `creation_date_erdat`).\n",
    "      * **VALID JOINS ONLY:** Every `JOIN` condition **MUST** use columns that are explicitly present in the respective tables as defined in the schema.\n",
    "\n",
    "    **SQL Syntax & Best Practices**\n",
    "\n",
    "      * All queries **MUST** use the Databricks SQL dialect. Use table aliases for clarity.\n",
    "      * **Prioritize multi-join queries** that combine data from different tables to uncover complex insights. **Single-table queries are acceptable ONLY IF they provide a direct, high-value business metric** (e.g., a critical KPI like 'Total Active Customers' or 'Overall Defect Rate'). Simple single-table data dumps without clear business value are forbidden.\n",
    "      * **DATE UNIT EXTRACTION:** Use `EXTRACT(<UNIT> FROM <DATE/TIMESTAMP COLUMN>)`, not `DATE_TRUNC`.\n",
    "      * **WHERE CLAUSE FOR DASHBOARDS:** For dashboard queries showing business metrics, you MAY use WHERE clauses with known business values (e.g., `WHERE status = 'active'` for KPI counters). Note: This differs from general SQL generation where value filtering is forbidden because data values are unknown.\n",
    "      * **USE SINGLE QUOTES FOR ALL SQL STRING LITERALS.** Example: `WHERE LOWER(status) = 'active'`.\n",
    "\n",
    "4.  **Configure Widget Visualizations:**\n",
    "\n",
    "      * Select the most appropriate widget type for each query from the list in **Section 8**.\n",
    "      * Diversify widget types. Aim to use at least 80% of the available widget types.\n",
    "      * Limit the total number of `counter` and `table` widgets to **AT MOST** two of each type.\n",
    "\n",
    "-----\n",
    "\n",
    "### 5. NAMING & FORMATTING CONVENTIONS\n",
    "\n",
    "  * **SQL Column Aliases (CRITICAL):** For every widget, the SQL column aliases in the `SELECT` statement **MUST EXACTLY MATCH** the required field prefixes specified in **Section 8**.\n",
    "  * **Box Plot Pre-aggregation:** The SQL query for a `box` plot **MUST** return pre-calculated values for `MIN()`, `APPROX_PERCENTILE(..., 0.25)`, `MEDIAN()`, `APPROX_PERCENTILE(..., 0.75)`, and `MAX()`, aliased with the correct prefixes (`min_`, `q1_`, etc.).\n",
    "\n",
    "-----\n",
    "\n",
    "### 6. OUTPUT REQUIREMENTS\n",
    "\n",
    "  * **CSV OUTPUT:** Your entire response **MUST** be a single, valid CSV file.\n",
    "\n",
    "  * **CSV STRUCTURE:**\n",
    "\n",
    "      * The first line **MUST** be the header row: `title,widget,query`.\n",
    "      * Each subsequent line represents a single widget.\n",
    "      * Fields must be properly CSV-escaped.\n",
    "\n",
    "  * **CSV Columns:**\n",
    "\n",
    "      * `title`: A descriptive business title for the widget.\n",
    "      * `widget`: The type of the visualization widget (selected from Section 8).\n",
    "      * `query`: The full Databricks SQL query for the widget.\n",
    "\n",
    "#### Example Output\n",
    "\n",
    "```csv\n",
    "title,widget,query\n",
    "'Subscriber Count by Loyalty Tier','bar','SELECT s.loyalty_tier AS category_tier, COUNT(s.profile_id) AS value_count FROM main.subscriber.profile AS s JOIN web_logs.logs.visits AS v ON s.profile_id = v.user_id GROUP BY s.loyalty_tier;'\n",
    "'Total Active Customers','counter','SELECT COUNT(DISTINCT profile_id) AS value_count FROM main.subscriber.profile WHERE LOWER(status) = 'active';'\n",
    "````\n",
    "\n",
    "-----\n",
    "\n",
    "### 7. FINAL CHECKS (META-INSTRUCTIONS)\n",
    "\n",
    "  * **SELF-CORRECTION MANDATORY:** Before finalizing your response, perform a rigorous validation pass. For each generated query, you must:\n",
    "    1.  **Check Business Value:** Ask yourself, 'What critical business question does this answer?' If the answer is not clear, discard or improve the query.\n",
    "    2.  **Verify Schema Adherence:** Meticulously check every table and column name against the schema in Section 9. Ensure all identifiers are an exact copy-paste.\n",
    "    3.  **Validate Syntax & Aliases:** Ensure the query is valid Databricks SQL and that the column aliases match the prefixes in Section 8.\n",
    "  * **DISCARD INVALID QUERIES:** If a query fails any of these checks, you **MUST** either correct it or discard it completely. **Do not output a query that you know is invalid or lacks business value.**\n",
    "\n",
    "-----\n",
    "\n",
    "### 8. REFERENCE: WIDGET SPECIFICATIONS & REQUIRED FIELDS\n",
    "\n",
    "`[ { 'chart': 'counter', 'used_for': 'Displaying a single, key performance indicator (KPI) prominently. It can optionally compare the primary value against a secondary target value.', 'fields': [ {'name': 'value', 'prefix': 'value_', 'used_for': 'A numeric column for the primary KPI to be displayed.', 'type': 'numeric', 'status': 'mandatory'}, {'name': 'target', 'prefix': 'target_', 'used_for': 'An optional numeric column representing a target value for comparison.', 'type': 'numeric', 'status': 'optional'} ] }, { 'chart': 'bar', 'used_for': 'Comparing numerical values across categories or showing metric changes over time. The layout can be configured to stack or group bars.', 'fields': [ {'name': 'x', 'prefix': 'category_', 'used_for': 'A categorical or temporal column for the axis labels.', 'type': 'categorical or temporal', 'status': 'mandatory'}, {'name': 'y', 'prefix': 'value_', 'used_for': 'A numeric column determining the length of the bars.', 'type': 'numeric', 'status': 'mandatory'}, {'name': 'color', 'prefix': 'group_', 'used_for': 'A categorical column to group or stack the bars.', 'type': 'categorical', 'status': 'optional'} ] }, { 'chart': 'line', 'used_for': 'Showing a trend or progression of a numerical value over a continuous interval, most commonly time.', 'fields': [ {'name': 'x', 'prefix': 'x_', 'used_for': 'A continuous column, typically a date or timestamp, for the horizontal axis.', 'type': 'temporal', 'status': 'mandatory'}, {'name': 'y', 'prefix': 'y_', 'used_for': 'A numeric column representing the value that changes over time.', 'type': 'numeric', 'status': 'mandatory'}, {'name': 'color', 'prefix': 'group_', 'used_for': 'An optional categorical column to plot multiple lines on the same chart.', 'type': 'categorical', 'status': 'optional'} ] }, { 'chart': 'area', 'used_for': 'Showing how a group's numeric values change over a second variable (like time), combining line and bar charts. The layout can be stacked.', 'fields': [ {'name': 'x', 'prefix': 'x_', 'used_for': 'A continuous column, typically a date or timestamp.', 'type': 'temporal', 'status': 'mandatory'}, {'name': 'y', 'prefix': 'y_', 'used_for': 'A numeric column representing the magnitude over time.', 'type': 'numeric', 'status': 'mandatory'}, {'name': 'color', 'prefix': 'group_', 'used_for': 'A categorical column to create stacked areas.', 'type': 'categorical', 'status': 'optional'} ] }, { 'chart': 'pie', 'used_for': 'Illustrating the proportional distribution or percentage share of different categories that make up a whole. Best for a small number of categories.', 'fields': [ {'name': 'color', 'prefix': 'category_', 'used_for': 'A categorical column representing the slices of the pie.', 'type': 'categorical', 'status': 'mandatory'}, {'name': 'angle', 'prefix': 'value_', 'used_for': 'A numeric column determining the size (angle) of each slice.', 'type': 'numeric', 'status': 'mandatory'} ] }, { 'chart': 'scatter', 'used_for': 'Visualizing the relationship between two numerical variables. A third and fourth dimension can be added using color and marker size (creating a Bubble Chart).', 'fields': [ {'name': 'x', 'prefix': 'x_', 'used_for': 'The first numeric column for the horizontal axis.', 'type': 'numeric', 'status': 'mandatory'}, {'name': 'y', 'prefix': 'y_', 'used_for': 'The second numeric column for the vertical axis.', 'type': 'numeric', 'status': 'mandatory'}, {'name': 'color', 'prefix': 'group_', 'used_for': 'An optional categorical column to color-code the points.', 'type': 'categorical', 'status': 'optional'}, {'name': 'size', 'prefix': 'size_', 'used_for': 'An optional numeric column to control the marker size, turning the scatter into a bubble chart.', 'type': 'numeric', 'status': 'optional'} ] }, { 'chart': 'histogram', 'used_for': 'Representing the frequency distribution of a single numerical variable by grouping values into ranges (bins). The number of bins is configurable.', 'fields': [ {'name': 'value', 'prefix': 'value_', 'used_for': 'A single numeric column whose distribution is to be plotted.', 'type': 'numeric', 'status': 'mandatory'} ] }, { 'chart': 'combo', 'used_for': 'Combining line and bar charts to compare measures with different scales or units over the same categories. Supports a dual Y-axis.', 'fields': [ {'name': 'x', 'prefix': 'x_', 'used_for': 'A categorical or time-based column for the shared horizontal axis.', 'type': 'categorical or temporal', 'status': 'mandatory'}, {'name': 'y_primary', 'prefix': 'bar_', 'used_for': 'The first numeric column, typically displayed as bars on the left Y-axis.', 'type': 'numeric', 'status': 'mandatory'}, {'name': 'y_secondary', 'prefix': 'line_', 'used_for': 'The second numeric column, typically displayed as a line, optionally on a separate right Y-axis.', 'type': 'numeric', 'status': 'mandatory'} ] }, { 'chart': 'choropleth-map', 'used_for': 'Visualizing geographical data by shading regions (countries, states) based on a metric. Requires a 'Geographic role' to be configured for the location field.', 'fields': [ {'name': 'region', 'prefix': 'location_', 'used_for': 'A column containing geographic identifiers (e.g., country codes, state names).', 'type': 'string (geographic)', 'status': 'mandatory'}, {'name': 'color', 'prefix': 'value_', 'used_for': 'A numeric column that determines the color intensity of each region.', 'type': 'numeric', 'status': 'mandatory'} ] }, { 'chart': 'symbol-map', 'used_for': 'Displaying quantitative data as symbols placed at specific geographic coordinates on a map.', 'fields': [ {'name': 'latitude', 'prefix': 'lat_', 'used_for': 'A numeric column containing the latitude coordinates.', 'type': 'numeric (latitude)', 'status': 'mandatory'}, {'name': 'longitude', 'prefix': 'lon_', 'used_for': 'A numeric column containing the longitude coordinates.', 'type': 'numeric (longitude)', 'status': 'mandatory'}, {'name': 'color', 'prefix': 'group_', 'used_for': 'An optional categorical or quantitative column to color-code the points.', 'type': 'categorical or numeric', 'status': 'optional'}, {'name': 'size', 'prefix': 'size_', 'used_for': 'An optional numeric column to control the marker size.', 'type': 'numeric', 'status': 'optional'} ] }, { 'chart': 'heatmap', 'used_for': 'Visualizing the magnitude of a metric across the intersection of two categorical variables, using color intensity in a grid.', 'fields': [ {'name': 'x', 'prefix': 'x_', 'used_for': 'A categorical column for the horizontal axis.', 'type': 'categorical', 'status': 'mandatory'}, {'name': 'y', 'prefix': 'y_', 'used_for': 'A categorical column for the vertical axis.', 'type': 'categorical', 'status': 'mandatory'}, {'name': 'color', 'prefix': 'color_', 'used_for': 'A numeric column that determines the color of the cell at the x/y intersection.', 'type': 'numeric', 'status': 'mandatory'} ] }, { 'chart': 'sankey', 'used_for': 'Illustrating the flow and magnitude of data between different stages or categories. The width of the connections is proportional to the flow quantity.', 'fields': [ {'name': 'stage', 'prefix': 'source_', 'used_for': 'A categorical column representing the source node.', 'type': 'categorical', 'status': 'mandatory'}, {'name': 'stage', 'prefix': 'destination_', 'used_for': 'A categorical column representing the destination node.', 'type': 'categorical', 'status': 'mandatory'}, {'name': 'value', 'prefix': 'value_', 'used_for': 'A numeric column representing the magnitude of the flow between nodes.', 'type': 'numeric', 'status': 'mandatory'} ] }, { 'chart': 'funnel', 'used_for': 'Visualizing the progressive reduction of data as it passes through sequential stages in a process (e.g., a sales pipeline or user conversion).', 'fields': [ {'name': 'stage', 'prefix': 'stage_', 'used_for': 'A categorical column that defines the ordered stages of the funnel.', 'type': 'categorical', 'status': 'mandatory'}, {'name': 'value', 'prefix': 'value_', 'used_for': 'A numeric column representing the count or amount at each stage.', 'type': 'numeric', 'status': 'mandatory'} ] }, { 'chart': 'box', 'used_for': 'Displaying the distribution summary of numerical data through quartiles, optionally grouped by category. Shows median, range, and potential outliers.', 'fields': [ {'name': 'category', 'prefix': 'category_', 'used_for': 'A categorical column to create multiple box plots for comparison.', 'type': 'categorical', 'status': 'mandatory'}, {'name': 'min', 'prefix': 'min_', 'used_for': 'A numeric column for the minimum value (lower whisker).', 'type': 'numeric', 'status': 'mandatory'}, {'name': 'q1', 'prefix': 'q1_', 'used_for': 'A numeric column for the first quartile (25th percentile).', 'type': 'numeric', 'status': 'mandatory'}, {'name': 'median', 'prefix': 'median_', 'used_for': 'A numeric column for the median (50th percentile).', 'type': 'numeric', 'status': 'mandatory'}, {'name': 'q3', 'prefix': 'q3_', 'used_for': 'A numeric column for the third quartile (75th percentile).', 'type': 'numeric', 'status': 'mandatory'}, {'name': 'max', 'prefix': 'max_', 'used_for': 'A numeric column for the maximum value (upper whisker).', 'type': 'numeric', 'status': 'mandatory'} ] }, { 'chart': 'table', 'used_for': 'Displaying detailed data in a grid. Offers advanced customization for columns, including reordering, conditional formatting, and special data type rendering (HTML, links, images, JSON).', 'fields': [ {'name': 'any', 'prefix': 'any', 'used_for': 'Any number of columns to be displayed.', 'type': 'any', 'status': 'mandatory'} ] }, { 'chart': 'pivot', 'used_for': 'Aggregating and reorganizing records into a cross-tabulated grid, similar to a PIVOT statement in SQL. Groups data by rows and columns to show aggregated values.', 'fields': [ {'name': 'row', 'prefix': 'row_', 'used_for': 'A categorical column to group data into rows.', 'type': 'categorical', 'status': 'mandatory'}, {'name': 'column', 'prefix': 'column_', 'used_for': 'A categorical column to group data into columns.', 'type': 'categorical', 'status': 'mandatory'}, {'name': 'cell', 'prefix': 'cell_', 'used_for': 'A numeric column with an aggregation (e.g., SUM, COUNT) for the intersecting cells.', 'type': 'numeric', 'status': 'mandatory'} ] } ]`\n",
    "\n",
    "-----\n",
    "\n",
    "### 9. DATABASE SCHEMA DEFINITION\n",
    "\n",
    "{schema_markdown}\n",
    "\n",
    "-----\n",
    "\n",
    "### 10. FINAL INSTRUCTION\n",
    "\n",
    "Begin generation of the CSV output now.\n",
    "\n",
    "\uD83D\uDEA8 ABSOLUTE RULE - OUTPUT FORMAT \uD83D\uDEA8:\n",
    "\n",
    "❌ DO NOT INCLUDE:\n",
    "- Any conversational or explanatory text (\"Here is...\", \"I've...\", \"Based on...\")\n",
    "- Any thoughts or analysis descriptions\n",
    "\n",
    "✅ OUTPUT REQUIREMENTS:\n",
    "- Your response must START with: title,widget,query,honesty_score,honesty_justification\n",
    "- Include honesty columns in header and all rows\n",
    "\"\"\" + HONESTY_CHECK_CSV\n",
    "\n",
    "# --- 1i. Business vs Technical Table Filtering Prompt (NEW - CRITICAL FOR QUALITY) ---\n",
    "PROMPT_TEMPLATES[\"FILTER_BUSINESS_TABLES_PROMPT\"] = \"\"\"You are a **Senior Data Architect** and **Business Domain Expert** specializing in identifying business-relevant data assets.\n",
    "\n",
    "**CRITICAL TASK**: Analyze the provided list of database tables and classify each one as either:\n",
    "1. **BUSINESS DATA TABLE** - Contains ANY business data related to operations, transactions, customers, products, services, or business processes\n",
    "2. **TECHNICAL TABLE** - Contains PURELY IT INFRASTRUCTURE data with NO business relevance (backend system logs, database monitoring, application debugging, IT governance)\n",
    "\n",
    "**BUSINESS CONTEXT**:\n",
    "- **Business Name**: {business_name}\n",
    "- **Industry**: {industry}\n",
    "- **Business Description**: {business_context}\n",
    "- **Exclusion Strategy**: {exclusion_strategy}\n",
    "\n",
    "{additional_context_section}\n",
    "\n",
    "**\uD83D\uDEA8 CONTEXT-AWARE CLASSIFICATION - CRITICAL \uD83D\uDEA8**:\n",
    "**The business context is CRUCIAL for classification.** Terms that are technical for most businesses may be BUSINESS DATA for companies where those terms are core to their business model.\n",
    "\n",
    "**EXAMPLES OF CONTEXT-AWARE CLASSIFICATION**:\n",
    "- **Example 1 - Databricks/Data Platform Companies**:\n",
    "  - ✅ BUSINESS: `clusters`, `jobs`, `pipelines`, `workflows`, `compute`, `warehouses`, `models` (core product/service tables)\n",
    "  - ❌ TECHNICAL: `cluster_logs`, `job_run_logs`, `pipeline_execution_logs`, `system_events`, `error_traces`, `debug_snapshots`\n",
    "  - **Reasoning**: For Databricks, \"cluster\" is a billable product feature (business), but \"cluster_logs\" are still technical debugging data\n",
    "  \n",
    "- **Example 2 - Healthcare/Medical Device Companies**:\n",
    "  - ✅ BUSINESS: `devices`, `sensors`, `telemetry`, `device_events`, `device_configurations` (core product data)\n",
    "  - ❌ TECHNICAL: `device_firmware_logs`, `system_diagnostics`, `internal_health_checks`, `deployment_history`\n",
    "  \n",
    "- **Example 3 - Logistics/Transportation Companies**:\n",
    "  - ✅ BUSINESS: `vehicles`, `routes`, `gps_tracking`, `driver_activity`, `vehicle_telemetry` (core operations)\n",
    "  - ❌ TECHNICAL: `vehicle_diagnostic_logs`, `system_error_logs`, `app_crash_reports`, `backend_performance`\n",
    "\n",
    "**UNIVERSAL TECHNICAL PATTERNS** (Always technical, regardless of business context):\n",
    "- **Logs & Auditing**: `*_logs`, `*_audit_trail`, `*_history`, `*_changelog`, `audit_*`, `log_*` (system/debug logs)\n",
    "- **Snapshots & Backups**: `*_snapshot`, `*_backup`, `snapshot_*`, `backup_*` (database backups)\n",
    "- **System Metadata**: `*_metadata`, `*_schema`, `information_schema.*`, `sys.*`, `system.*`\n",
    "- **Monitoring & Health**: `*_metrics`, `*_health`, `*_status`, `*_monitoring`, `performance_*`, `monitoring_*`\n",
    "- **ETL/Pipeline Internals**: `*_job_run`, `*_pipeline_execution`, `*_load_status`, `etl_*`, `pipeline_*` (orchestration)\n",
    "- **Error/Debug**: `*_error`, `*_exception`, `*_debug`, `error_*`, `exception_*`, `debug_*`\n",
    "- **Configuration/Settings**: `*_config`, `*_settings`, `*_parameters`, `config_*`, `settings_*` (system settings)\n",
    "- **Testing/Staging**: `*_test`, `*_staging`, `*_temp`, `test_*`, `staging_*`, `temp_*`\n",
    "- **ML/PLATFORM INFRA METADATA**: Model registry, pipeline/runtime/config, training artifacts, or other platform/ops metadata tables are TECHNICAL unless the business sells the platform itself. When in doubt, treat infrastructure/meta tables as technical and prefer tables with direct business transactions/entities.\n",
    "\n",
    "**EXCEPTION**: If the business explicitly provides these as products (e.g., observability/monitoring company selling logs as a product), then they are business tables.\n",
    "\n",
    "**\uD83D\uDEA8 EXCLUSION STRATEGY-SPECIFIC RULES \uD83D\uDEA8**:\n",
    "\n",
    "**STRATEGY: {exclusion_strategy}**\n",
    "\n",
    "{strategy_rules}\n",
    "\n",
    "**\uD83D\uDEA8 DATA CATEGORY CLASSIFICATION RULES - SEMANTIC DEFINITIONS \uD83D\uDEA8**:\n",
    "\n",
    "**You MUST classify each BUSINESS table into exactly ONE of these three categories based on SEMANTIC analysis, NOT based on score:**\n",
    "\n",
    "---\n",
    "\n",
    "**\uD83D\uDCCA TRANSACTIONAL DATA** (Records of business events - the \"VERBS\" of the business):\n",
    "| Characteristic | Description |\n",
    "|----------------|-------------|\n",
    "| **Definition** | Records of business events, activities, and transactions that happen over time |\n",
    "| **Stability** | Immutable once created (append-only), rarely updated after creation |\n",
    "| **Volume** | High volume, grows continuously over time |\n",
    "| **Time-sensitive** | Has a PRIMARY BUSINESS timestamp column indicating WHEN THE EVENT OCCURRED |\n",
    "| **Lifecycle** | Created, typically never updated (may be archived or soft-deleted) |\n",
    "| **Key Question** | \"Does each row represent a discrete BUSINESS EVENT that happened at a specific time?\" → If YES → TRANSACTIONAL |\n",
    "| **Examples** | Orders, invoices, payments, shipments, bookings, transactions, log entries, clicks, events, transfers, claims, incidents, service_requests, production_runs, quality_measurements, sensor_readings (time-series) |\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL: TIMESTAMP COLUMN ANALYSIS \uD83D\uDEA8**\n",
    "\n",
    "**NOT ALL TIMESTAMP COLUMNS INDICATE TRANSACTIONAL DATA!** You must distinguish between:\n",
    "\n",
    "**✅ TRUE TRANSACTIONAL TIMESTAMPS** (Indicate WHEN the business event OCCURRED):\n",
    "| Column Pattern | Purpose | Example Tables |\n",
    "|----------------|---------|----------------|\n",
    "| `transaction_date`, `transaction_time` | When the transaction happened | payments, transfers |\n",
    "| `order_date`, `order_time` | When the order was placed | orders, purchases |\n",
    "| `event_date`, `event_time`, `event_timestamp` | When the event occurred | events, incidents |\n",
    "| `created_at`, `created_date` (in event tables) | When the event record was created | logs, activities |\n",
    "| `shipment_date`, `delivery_date` | When shipment/delivery occurred | shipments, deliveries |\n",
    "| `measurement_time`, `reading_time` | When measurement was taken | sensor_data, quality_checks |\n",
    "| `start_time`, `end_time` | Duration of an activity/event | production_runs, shifts |\n",
    "| `booking_date`, `reservation_date` | When booking was made | bookings, reservations |\n",
    "| `payment_date`, `invoice_date` | When financial event occurred | payments, invoices |\n",
    "| `effective_date`, `posted_date` | When transaction became effective | journal_entries, postings |\n",
    "\n",
    "**❌ HOUSEKEEPING/AUDIT TIMESTAMPS** (Do NOT indicate transactional data - just record maintenance):\n",
    "| Column Pattern | Purpose | Found In |\n",
    "|----------------|---------|----------|\n",
    "| `last_updated`, `last_update`, `updated_at` | When row was last modified | ALL table types |\n",
    "| `modified_date`, `modified_at`, `modify_date` | When row was modified | ALL table types |\n",
    "| `last_modified`, `last_modified_date` | Audit trail for changes | ALL table types |\n",
    "| `updated_by`, `modified_by` | Who made the change | ALL table types |\n",
    "| `created_at`, `created_date` (in entity tables) | When entity was first created | MASTER tables |\n",
    "| `record_created`, `record_updated` | ETL/DWH housekeeping | ALL table types |\n",
    "| `etl_timestamp`, `load_date`, `insert_date` | Data pipeline metadata | ALL table types |\n",
    "| `sync_date`, `refresh_date` | Data synchronization | ALL table types |\n",
    "| `valid_from`, `valid_to` (SCD Type 2) | Slowly changing dimension tracking | MASTER tables |\n",
    "| `is_active`, `is_deleted`, `deleted_at` | Soft delete flags | ALL table types |\n",
    "\n",
    "**\uD83D\uDD11 KEY DISTINCTION RULES**:\n",
    "\n",
    "1. **MASTER tables CAN have timestamps** - `customer.created_at` and `customer.last_updated` do NOT make it transactional\n",
    "2. **REFERENCE tables CAN have timestamps** - `country_codes.last_updated` is just housekeeping\n",
    "3. **Look at the PRIMARY PURPOSE of the table, not just the presence of timestamps**\n",
    "4. **Ask: \"Is this timestamp the REASON the row exists, or just metadata ABOUT the row?\"**\n",
    "   - If timestamp IS the reason (event happened) → TRANSACTIONAL\n",
    "   - If timestamp is metadata about the row → NOT transactional (check if MASTER or REFERENCE)\n",
    "\n",
    "**Identification Rules for TRANSACTIONAL**:\n",
    "1. **Primary business timestamp exists** - A timestamp that represents WHEN THE BUSINESS EVENT OCCURRED (not just housekeeping)\n",
    "2. **Each row = one discrete event** - The table records WHAT HAPPENED, not WHAT EXISTS\n",
    "3. **High insert frequency, low/no update frequency** - Events are typically immutable once recorded\n",
    "4. **References MASTER data** - Has foreign keys to entities (customer_id, product_id, employee_id)\n",
    "5. **Time-series nature** - Data grows continuously over time, rarely deleted\n",
    "6. **Business meaning is temporal** - \"Order #123 was placed on 2024-01-15\" vs \"Customer John exists\"\n",
    "\n",
    "**⚠️ COMMON MISTAKES TO AVOID**:\n",
    "- ❌ `customer` table with `created_at`, `last_updated` → Still MASTER (timestamps are housekeeping)\n",
    "- ❌ `product` table with `modified_date` → Still MASTER (timestamp is audit trail)\n",
    "- ❌ `country_codes` with `last_updated` → Still REFERENCE (timestamp is maintenance)\n",
    "- ❌ `employee` with `hire_date`, `termination_date` → Still MASTER (dates describe entity lifecycle, not events)\n",
    "- ✅ `customer_orders` with `order_date` → TRANSACTIONAL (timestamp IS the event)\n",
    "- ✅ `production_runs` with `run_start_time`, `run_end_time` → TRANSACTIONAL (timestamps define the event)\n",
    "\n",
    "---\n",
    "\n",
    "**\uD83D\uDC64 MASTER DATA** (Core business entities - the \"NOUNS\" of the business):\n",
    "| Characteristic | Description |\n",
    "|----------------|-------------|\n",
    "| **Definition** | Core business entities that define WHO, WHAT, WHERE of the business |\n",
    "| **Stability** | Changes infrequently but CAN be updated (low volatility) |\n",
    "| **Uniqueness** | Each row represents a unique business entity with a lifecycle |\n",
    "| **Shared** | Used/referenced across multiple systems and processes |\n",
    "| **Lifecycle** | Created once, updated occasionally, rarely deleted (soft-delete common) |\n",
    "| **Key Question** | \"Does this table represent a core business ENTITY that EXISTS (not an event that HAPPENED)?\" → If YES → MASTER |\n",
    "| **Examples** | Customers, employees, products, vendors, suppliers, accounts, contracts, assets, locations, equipment, vehicles, patients, projects, policies, members, partners, anodes, pots, machines |\n",
    "\n",
    "**\uD83D\uDEA8 MASTER DATA CAN HAVE TIMESTAMPS - THIS DOES NOT MAKE THEM TRANSACTIONAL \uD83D\uDEA8**\n",
    "\n",
    "| Common MASTER Table Columns | Purpose | Still MASTER? |\n",
    "|-----------------------------|---------|---------------|\n",
    "| `created_at`, `created_date` | When entity was first created | ✅ YES |\n",
    "| `last_updated`, `updated_at`, `modified_date` | Housekeeping/audit | ✅ YES |\n",
    "| `hire_date`, `termination_date` (employee) | Entity lifecycle dates | ✅ YES |\n",
    "| `start_date`, `end_date` (contract) | Contract validity period | ✅ YES |\n",
    "| `registration_date` (customer) | When customer registered | ✅ YES |\n",
    "| `manufacture_date` (product/asset) | When asset was made | ✅ YES |\n",
    "| `installation_date` (equipment) | When equipment installed | ✅ YES |\n",
    "| `birth_date`, `join_date` | Entity attributes | ✅ YES |\n",
    "| `valid_from`, `valid_to` | SCD Type 2 versioning | ✅ YES |\n",
    "\n",
    "**KEY INSIGHT**: MASTER tables describe ENTITIES THAT EXIST. The timestamps in MASTER tables describe:\n",
    "- WHEN the entity was created/modified (housekeeping)\n",
    "- ATTRIBUTES of the entity (hire_date is an attribute of employee)\n",
    "- NOT \"an event that occurred\" - the entity IS the subject, not a record of an action\n",
    "\n",
    "**Identification Rules for MASTER**:\n",
    "1. **Represents a unique business entity** - Has a natural business identifier (customer_id, employee_number, product_code)\n",
    "2. **Entity-centric, not event-centric** - Describes WHAT/WHO exists, not WHAT happened\n",
    "3. **Has lifecycle states** - active, inactive, suspended, terminated, pending\n",
    "4. **Referenced by TRANSACTIONAL tables** - Foreign keys point TO this table (e.g., orders.customer_id → customers.id)\n",
    "5. **Updates modify the entity** - Address changes, status updates, attribute corrections\n",
    "6. **Would be managed by a business steward** - HR owns employees, Sales owns customers\n",
    "7. **Relatively low volume** - Hundreds to millions of entities, not billions of events\n",
    "8. **Timestamps are METADATA, not the primary business data** - `last_updated` is housekeeping, not business content\n",
    "\n",
    "**Entity Lifecycle Dates vs Event Dates**:\n",
    "- `employee.hire_date` = ATTRIBUTE of the employee entity → MASTER\n",
    "- `employee_timesheet.work_date` = WHEN the work event occurred → TRANSACTIONAL\n",
    "- `contract.start_date` = ATTRIBUTE defining contract period → MASTER  \n",
    "- `contract_payment.payment_date` = WHEN payment event occurred → TRANSACTIONAL\n",
    "- `equipment.installation_date` = ATTRIBUTE of equipment → MASTER\n",
    "- `equipment_maintenance.maintenance_date` = WHEN maintenance event occurred → TRANSACTIONAL\n",
    "\n",
    "---\n",
    "\n",
    "**\uD83D\uDCCB REFERENCE DATA** (Lookup values and classifications - the \"ADJECTIVES\" of the business):\n",
    "| Characteristic | Description |\n",
    "|----------------|-------------|\n",
    "| **Definition** | Lookup values, codes, and classifications used to categorize/describe other data |\n",
    "| **Stability** | Very stable, changes RARELY (often governed by standards or regulations) |\n",
    "| **Purpose** | Standardizes and categorizes MASTER and TRANSACTIONAL data |\n",
    "| **Scope** | Often industry-wide standards, regulatory codes, or company-controlled lists |\n",
    "| **Size** | Typically SMALL, finite sets of values (dozens to hundreds, rarely thousands) |\n",
    "| **Key Question** | \"Is this a FINITE, CONTROLLED list of codes/types/categories used to CLASSIFY other data?\" → If YES → REFERENCE |\n",
    "| **Examples** | Country codes, currency codes, status codes, product_categories, gender, units_of_measure, payment_types, order_status, priority_levels, industry_codes, language_codes, timezones, alloy_grades, shift_types, material_types, quality_grades |\n",
    "\n",
    "**\uD83D\uDEA8 REFERENCE DATA CAN HAVE TIMESTAMPS - THIS DOES NOT MAKE THEM TRANSACTIONAL \uD83D\uDEA8**\n",
    "\n",
    "| Common REFERENCE Table Columns | Purpose | Still REFERENCE? |\n",
    "|--------------------------------|---------|------------------|\n",
    "| `last_updated`, `modified_date` | Housekeeping when code was edited | ✅ YES |\n",
    "| `created_at` | When the code was first added | ✅ YES |\n",
    "| `effective_date` | When code became valid | ✅ YES |\n",
    "| `expiry_date`, `deprecated_date` | When code is no longer valid | ✅ YES |\n",
    "| `valid_from`, `valid_to` | Code validity period | ✅ YES |\n",
    "\n",
    "**Identification Rules for REFERENCE**:\n",
    "1. **Small, finite, controlled set** - Typically < 1000 rows, often < 100\n",
    "2. **Used for classification/categorization** - Provides dropdown values, categorizes other data\n",
    "3. **Code + Description pattern** - Often has `code`, `name`, `description` columns\n",
    "4. **Referenced BY other tables** - MASTER and TRANSACTIONAL tables have foreign keys TO this table\n",
    "5. **Rarely changes** - Adding a new status code is a governance event, not daily operation\n",
    "6. **Industry or regulatory standards** - ISO codes, regulatory classifications, standard enumerations\n",
    "7. **No transactional history** - You don't track \"payment type usage over time\" in this table\n",
    "8. **Business rules/validation** - Used to validate and constrain data entry\n",
    "\n",
    "**REFERENCE vs MASTER Distinction**:\n",
    "| Aspect | REFERENCE | MASTER |\n",
    "|--------|-----------|--------|\n",
    "| Row count | Typically < 1000 | Can be millions |\n",
    "| Changes | Rarely (governance) | Regularly (business operations) |\n",
    "| Purpose | Classify/categorize | Represent entities |\n",
    "| Ownership | Usually IT/Data Governance | Business departments |\n",
    "| Examples | status_types, country_codes | customers, products |\n",
    "\n",
    "**Common REFERENCE Table Patterns**:\n",
    "- `*_type`, `*_types` (payment_type, order_type, material_type)\n",
    "- `*_status`, `*_statuses` (order_status, customer_status)\n",
    "- `*_code`, `*_codes` (country_code, currency_code, reason_code)\n",
    "- `*_category`, `*_categories` (product_category, expense_category)\n",
    "- `*_grade`, `*_grades` (quality_grade, alloy_grade, credit_grade)\n",
    "- `*_level`, `*_levels` (priority_level, severity_level)\n",
    "- `*_class`, `*_classification` (risk_class, material_classification)\n",
    "- Singular lookup names (gender, currency, country, language, timezone)\n",
    "\n",
    "---\n",
    "\n",
    "**\uD83D\uDD00 QUICK DECISION FRAMEWORK** (Ask in this order):\n",
    "\n",
    "**Step 1: Analyze the TABLE PURPOSE (not just columns)**\n",
    "- What is the PRIMARY purpose of this table?\n",
    "- Does it record EVENTS (things that happen) or ENTITIES (things that exist)?\n",
    "\n",
    "**Step 2: Analyze TIMESTAMP columns carefully**\n",
    "- Is the timestamp the REASON the row exists (event timestamp)?\n",
    "- Or is it just HOUSEKEEPING (last_updated, modified_at)?\n",
    "\n",
    "**Step 3: Apply this decision tree:**\n",
    "\n",
    "```\n",
    "START → Is this table a FINITE SET of codes/categories (< 1000 rows)?\n",
    "        │\n",
    "        ├─ YES → Does it CLASSIFY/CATEGORIZE other data? → YES → **REFERENCE**\n",
    "        │        └─ NO → Might be small MASTER table\n",
    "        │\n",
    "        └─ NO → Does each row represent a discrete BUSINESS EVENT that HAPPENED?\n",
    "                │\n",
    "                ├─ YES → Does it have a PRIMARY BUSINESS TIMESTAMP (not just last_updated)?\n",
    "                │        │\n",
    "                │        ├─ YES → Is the table INSERT-heavy with rare/no updates? → **TRANSACTIONAL**\n",
    "                │        └─ NO → Check if it's actually MASTER with event-like naming\n",
    "                │\n",
    "                └─ NO → Does it represent a core business ENTITY with lifecycle?\n",
    "                        │\n",
    "                        ├─ YES → **MASTER** (even if it has created_at, last_updated)\n",
    "                        └─ NO → Re-evaluate: likely MASTER or needs more context\n",
    "```\n",
    "\n",
    "**Step 4: Validate with these questions:**\n",
    "\n",
    "| Question | TRANSACTIONAL | MASTER | REFERENCE |\n",
    "|----------|---------------|--------|-----------|\n",
    "| Does each row = one event? | ✅ Yes | ❌ No | ❌ No |\n",
    "| Is it INSERT-heavy, UPDATE-rare? | ✅ Yes | ❌ No (updates common) | ❌ No |\n",
    "| Does it have a business event timestamp? | ✅ Yes | ⚠️ Maybe (but housekeeping) | ⚠️ Maybe (but housekeeping) |\n",
    "| Is it a finite controlled list? | ❌ No | ❌ No | ✅ Yes |\n",
    "| Is row count typically < 1000? | ❌ No (can be billions) | ⚠️ Varies | ✅ Usually |\n",
    "| Does it represent an entity with lifecycle? | ❌ No | ✅ Yes | ❌ No |\n",
    "| Is it referenced BY other tables via FK? | ❌ No (it references) | ✅ Yes | ✅ Yes |\n",
    "\n",
    "---\n",
    "\n",
    "**⚠️ AMBIGUOUS/TRICKY CASES - DETAILED ANALYSIS ⚠️**:\n",
    "\n",
    "These tables are commonly misclassified. Study these examples carefully:\n",
    "\n",
    "| Table Name | Has Timestamps? | Correct Category | Reasoning |\n",
    "|------------|-----------------|------------------|-----------|\n",
    "| `customer` | `created_at`, `last_updated` | **MASTER** | Entity that EXISTS, timestamps are housekeeping |\n",
    "| `customer_order` | `order_date`, `created_at` | **TRANSACTIONAL** | Event that HAPPENED, order_date is business timestamp |\n",
    "| `employee` | `hire_date`, `last_updated`, `termination_date` | **MASTER** | Entity with lifecycle, dates are ATTRIBUTES not events |\n",
    "| `employee_timesheet` | `work_date`, `clock_in`, `clock_out` | **TRANSACTIONAL** | Event recording work that HAPPENED |\n",
    "| `product` | `created_at`, `launch_date`, `last_updated` | **MASTER** | Entity, launch_date is an attribute |\n",
    "| `product_sale` | `sale_date`, `sale_time` | **TRANSACTIONAL** | Event of sale that HAPPENED |\n",
    "| `equipment` | `installation_date`, `last_maintenance_date` | **MASTER** | Entity, dates are attributes |\n",
    "| `equipment_maintenance` | `maintenance_date`, `start_time`, `end_time` | **TRANSACTIONAL** | Event of maintenance that HAPPENED |\n",
    "| `contract` | `start_date`, `end_date`, `signed_date` | **MASTER** | Entity, dates define contract period (attributes) |\n",
    "| `contract_payment` | `payment_date`, `due_date` | **TRANSACTIONAL** | Event of payment that HAPPENED |\n",
    "| `inventory` | `last_count_date`, `last_updated` | **MASTER** | Entity (current state), dates are housekeeping |\n",
    "| `inventory_movement` | `movement_date`, `transaction_time` | **TRANSACTIONAL** | Event of stock movement that HAPPENED |\n",
    "| `price` / `price_list` | `effective_date`, `expiry_date` | **MASTER** or **REFERENCE** | Depends on granularity - if per-product it's MASTER, if generic codes it's REFERENCE |\n",
    "| `status_type` | `created_at`, `last_updated` | **REFERENCE** | Finite codes, timestamps are housekeeping |\n",
    "| `audit_log` (business) | `audit_timestamp`, `event_time` | **TRANSACTIONAL** | Business audit events that HAPPENED |\n",
    "| `country` / `country_code` | `last_updated` | **REFERENCE** | Finite standard codes, timestamp is housekeeping |\n",
    "| `anode` | `created_date`, `last_updated` | **MASTER** | Physical entity tracked individually |\n",
    "| `anode_consumption` | `consumption_date`, `consumption_time` | **TRANSACTIONAL** | Event of anode being consumed |\n",
    "| `alloy_grade` | `last_updated` | **REFERENCE** | Finite set of alloy specifications |\n",
    "| `production_batch` | `batch_date`, `start_time`, `end_time` | **TRANSACTIONAL** | Event of production that HAPPENED |\n",
    "| `quality_test_result` | `test_date`, `test_time` | **TRANSACTIONAL** | Event of test that HAPPENED |\n",
    "| `shift` / `shift_type` | `last_updated` | **REFERENCE** | Finite codes (Day/Night/Swing) |\n",
    "| `shift_schedule` | `shift_date`, `start_time` | **TRANSACTIONAL** | Specific shift occurrence that HAPPENED |\n",
    "\n",
    "**\uD83D\uDD11 KEY PATTERNS TO REMEMBER**:\n",
    "\n",
    "1. **\"*_log\" tables**: Usually TRANSACTIONAL (events recorded over time)\n",
    "2. **\"*_history\" tables**: Usually TRANSACTIONAL (historical events)\n",
    "3. **\"*_type\" / \"*_status\" / \"*_code\" tables**: Usually REFERENCE (lookup codes)\n",
    "4. **\"*_movement\" / \"*_transfer\" / \"*_transaction\" tables**: Usually TRANSACTIONAL\n",
    "5. **Singular entity names** (customer, product, employee): Usually MASTER\n",
    "6. **Tables with lifecycle dates as ATTRIBUTES**: Usually MASTER (hire_date, start_date)\n",
    "7. **Tables where each row = a point-in-time event**: TRANSACTIONAL\n",
    "8. **Tables that could be a dropdown menu**: REFERENCE\n",
    "\n",
    "---\n",
    "\n",
    "**\uD83C\uDFED INDUSTRY-SPECIFIC EXAMPLES (Manufacturing/Aluminum Smelting)**:\n",
    "\n",
    "| Table | Category | Reasoning |\n",
    "|-------|----------|-----------|\n",
    "| `employee` | MASTER | Core entity - workforce members with lifecycle |\n",
    "| `customer` | MASTER | Core entity - business relationships |\n",
    "| `product` | MASTER | Core entity - what is manufactured/sold |\n",
    "| `equipment` | MASTER | Core entity - physical assets with lifecycle |\n",
    "| `anode` | MASTER | Core entity - physical items tracked individually |\n",
    "| `pot` | MASTER | Core entity - smelting equipment with lifecycle |\n",
    "| `alloy` | REFERENCE | Finite set of alloy grades/specifications |\n",
    "| `shift_type` | REFERENCE | Lookup - Day/Night/Swing shifts |\n",
    "| `product_category` | REFERENCE | Lookup - Product classification codes |\n",
    "| `country` | REFERENCE | Lookup - Standard country codes |\n",
    "| `production_run` | TRANSACTIONAL | Event - Records production activity |\n",
    "| `quality_measurement` | TRANSACTIONAL | Event - Records test results with timestamps |\n",
    "| `metal_transfer` | TRANSACTIONAL | Event - Records material movements |\n",
    "| `order` | TRANSACTIONAL | Event - Customer purchase events |\n",
    "| `shipment` | TRANSACTIONAL | Event - Delivery events |\n",
    "\n",
    "---\n",
    "\n",
    "**\uD83D\uDEA8 CLASSIFICATION PRIORITY RULES \uD83D\uDEA8**:\n",
    "\n",
    "- **RULE #1**: Use SEMANTIC analysis first, NOT score-based inference\n",
    "- **RULE #2**: If table has timestamp + records events → **TRANSACTIONAL** (regardless of score)\n",
    "- **RULE #3**: If table is finite lookup/codes → **REFERENCE** (regardless of score)\n",
    "- **RULE #4**: If table represents core entities with lifecycle → **MASTER**\n",
    "- **RULE #5**: When in doubt between MASTER and TRANSACTIONAL → check for timestamps\n",
    "- **RULE #6**: When in doubt between MASTER and REFERENCE → check if it's a finite controlled list\n",
    "- **RULE #7**: When in doubt overall, classify as **MASTER** (safer default for entities)\n",
    "\n",
    "**❌ TECHNICAL TABLES** (EXCLUDE ONLY IF PURELY IT INFRASTRUCTURE):\n",
    "**ONLY classify as TECHNICAL if the table is PURELY for IT TEAMS managing internal systems:**\n",
    "- **IT System Logs**: Application error logs, API debug logs, backend service logs, system exception tracking\n",
    "- **IT Monitoring**: Database performance metrics, server health checks, infrastructure monitoring, resource utilization (CPU, memory, disk)\n",
    "- **IT Configuration**: Backend application settings, system parameters, feature flags for developers, deployment configs\n",
    "- **Database Metadata**: Schema version control, database migration tracking, table/column metadata for database admins\n",
    "- **IT Governance**: Data lineage for IT purposes, ETL job status, pipeline orchestration, data load tracking for data engineers\n",
    "- **IT Security**: System access logs (not business user activity), IT audit trails, penetration testing results, vulnerability scans\n",
    "- **Developer Tools**: Version control metadata, CI/CD pipeline logs, build artifacts, test execution logs\n",
    "- **System Tables**: `information_schema`, `sys`, `system` schemas - database catalog tables\n",
    "- **RULE**: Majority of columns are technical (error stack traces, system IDs, internal status codes, JSON configs for IT)\n",
    "- **RULE**: Primary consumers are IT/DevOps/Data Engineering teams, NOT business users\n",
    "- **RULE**: Data has ZERO business value and is ONLY used to maintain IT infrastructure\n",
    "\n",
    "**CRITICAL EDGE CASES - DEFAULT TO BUSINESS**:\n",
    "- **User Activity Logs**: If tracking business user behavior, customer actions, or business events → **BUSINESS** (TRANSACTIONAL). Only if tracking IT system access/authentication → **TECHNICAL**\n",
    "- **Audit Tables**: If tracking ANY business transactions, data changes, or business events → **BUSINESS** (TRANSACTIONAL). Only if tracking IT system changes → **TECHNICAL**\n",
    "- **Configuration Tables**: If storing business rules, pricing configs, product settings → **BUSINESS** (REFERENCE or MASTER). Only if storing IT system settings → **TECHNICAL**\n",
    "- **Snapshot Tables**: If historical snapshots of business data for reporting/analytics → **BUSINESS** (TRANSACTIONAL). Only if database backups/snapshots → **TECHNICAL**\n",
    "- **Metadata Tables**: If describing business entities, data dictionaries for business users → **BUSINESS** (REFERENCE). Only if database schema metadata → **TECHNICAL**\n",
    "- **Mixed Tables**: If >10% of columns contain business data → **BUSINESS**. Otherwise → **TECHNICAL**\n",
    "\n",
    "**EXAMPLES WITH DATA CATEGORY**:\n",
    "- ✅ BUSINESS/TRANSACTIONAL: `customer_activity_log` (business event tracking with timestamps), `order_audit` (business transaction history), `device_telemetry` (IoT time-series data)\n",
    "- ✅ BUSINESS/TRANSACTIONAL: `api_usage_log` (if tracking customer API usage for billing/analytics with timestamps)\n",
    "- ✅ BUSINESS/MASTER: `customer` (core entity), `product` (core entity), `employee` (core entity), `equipment` (physical asset)\n",
    "- ✅ BUSINESS/REFERENCE: `country_code` (lookup), `product_category` (classification), `status_type` (finite codes), `alloy_grade` (specifications)\n",
    "- ❌ TECHNICAL: `application_error_log` (IT debugging), `database_query_performance` (IT monitoring), `etl_job_runs` (IT data engineering)\n",
    "- ❌ TECHNICAL: `system_health_checks` (IT infrastructure), `deployment_history` (IT DevOps), `schema_migrations` (IT database admin)\n",
    "\n",
    "**BUSINESS SCORE** (Indicates business criticality, NOT used for category inference):\n",
    "For BUSINESS tables, assign a score from 1-100 indicating how business-critical the table is:\n",
    "- **90-100**: High-frequency transactional tables (orders, sales, transactions, payments, production_runs)\n",
    "- **80-89**: Core master data entities (customers, products, employees, equipment, assets)\n",
    "- **60-79**: Important operational/transactional tables (inventory movements, appointments, schedules, quality checks)\n",
    "- **40-59**: Supporting master/reference data (business configuration, business lookup tables)\n",
    "- **20-39**: Static reference data (country codes, currencies, categories) - limited use case value\n",
    "- **1-19**: Marginally business-relevant tables (derived/aggregate tables, borderline cases)\n",
    "\n",
    "**⚠️ IMPORTANT**: Business Score indicates CRITICALITY, NOT data category. You MUST determine Data Category using SEMANTIC rules above, NOT by score.\n",
    "\n",
    "For TECHNICAL tables, always use score: 0\n",
    "\n",
    "**YOUR TASK**:\n",
    "Review the list of tables below and return a **CSV** with the following columns:\n",
    "- `Table Name`: Fully-qualified table name (catalog.schema.table)\n",
    "- `Classification`: Either \"BUSINESS\" or \"TECHNICAL\"\n",
    "- `Data Category`: For BUSINESS tables only, classify as \"MASTER\", \"TRANSACTIONAL\", or \"REFERENCE\". For TECHNICAL tables, use \"TECHNICAL\".\n",
    "- `Business Score`: Integer from 0-100 (0 for TECHNICAL, 1-100 for BUSINESS based on criticality)\n",
    "- `Reason`: Brief reason for classification (max 100 characters)\n",
    "\n",
    "**CRITICAL REQUIREMENTS**:\n",
    "1. You MUST classify EVERY table in the input list\n",
    "2. Each table must appear in exactly ONE row\n",
    "3. Use the FULL table name format: `catalog.schema.table`\n",
    "4. Classification must be either \"BUSINESS\" or \"TECHNICAL\" (no other values)\n",
    "5. Data Category must be one of: \"MASTER\", \"TRANSACTIONAL\", \"REFERENCE\", \"TECHNICAL\"\n",
    "6. **\uD83D\uDEA8 CRITICAL**: Data Category MUST be determined using SEMANTIC rules:\n",
    "   - **TRANSACTIONAL**: Records events with timestamps (orders, payments, logs, measurements)\n",
    "   - **MASTER**: Core business entities with lifecycle (customers, products, employees, assets)\n",
    "   - **REFERENCE**: Finite lookup codes/classifications (country_codes, status_types, categories)\n",
    "   - **TECHNICAL**: For TECHNICAL tables only\n",
    "7. Business Score must be an integer: 0 for TECHNICAL, 1-100 for BUSINESS (indicates criticality, NOT category)\n",
    "8. When in doubt, prefer \"BUSINESS\" classification (safer to include than exclude)\n",
    "9. When in doubt on category: Check for timestamps → TRANSACTIONAL; Check for finite codes → REFERENCE; Otherwise → MASTER\n",
    "\n",
    "**TABLES TO CLASSIFY**:\n",
    "{tables_markdown}\n",
    "\n",
    "**OUTPUT FORMAT** (CSV ONLY - NO OTHER TEXT):\n",
    "```csv\n",
    "\"Table Name\",\"Classification\",\"Data Category\",\"Business Score\",\"Reason\"\n",
    "\"catalog.schema.customers\",\"BUSINESS\",\"MASTER\",\"85\",\"Core entity - unique business entities with lifecycle\"\n",
    "\"catalog.schema.orders\",\"BUSINESS\",\"TRANSACTIONAL\",\"95\",\"Event records - timestamped business transactions\"\n",
    "\"catalog.schema.payments\",\"BUSINESS\",\"TRANSACTIONAL\",\"92\",\"Event records - financial transaction events\"\n",
    "\"catalog.schema.employees\",\"BUSINESS\",\"MASTER\",\"82\",\"Core entity - workforce members with lifecycle\"\n",
    "\"catalog.schema.equipment\",\"BUSINESS\",\"MASTER\",\"80\",\"Core entity - physical assets tracked individually\"\n",
    "\"catalog.schema.country_codes\",\"BUSINESS\",\"REFERENCE\",\"25\",\"Lookup codes - finite set of standard classifications\"\n",
    "\"catalog.schema.product_categories\",\"BUSINESS\",\"REFERENCE\",\"35\",\"Lookup codes - finite set used to classify products\"\n",
    "\"catalog.schema.status_types\",\"BUSINESS\",\"REFERENCE\",\"30\",\"Lookup codes - finite enumeration of status values\"\n",
    "\"catalog.schema.alloy_grades\",\"BUSINESS\",\"REFERENCE\",\"38\",\"Lookup codes - finite set of alloy specifications\"\n",
    "\"catalog.schema.api_logs\",\"TECHNICAL\",\"TECHNICAL\",\"0\",\"IT infrastructure - system debugging logs\"\n",
    "\"catalog.schema.etl_job_runs\",\"TECHNICAL\",\"TECHNICAL\",\"0\",\"IT infrastructure - pipeline orchestration metadata\"\n",
    "```\n",
    "\n",
    "**CRITICAL CSV FORMATTING RULES**:\n",
    "1. First line MUST be the header: \"Table Name\",\"Classification\",\"Data Category\",\"Business Score\",\"Reason\"\n",
    "2. ALL fields MUST be enclosed in double quotes (\")\n",
    "3. Each row must have exactly 5 fields\n",
    "4. Classification must be EXACTLY \"BUSINESS\" or \"TECHNICAL\" (case-sensitive)\n",
    "5. Data Category must be EXACTLY \"MASTER\", \"TRANSACTIONAL\", \"REFERENCE\", or \"TECHNICAL\"\n",
    "6. Business Score must be a valid integer (0-100)\n",
    "6. NO markdown code fences (```csv) - just the CSV content\n",
    "7. NO explanatory text before or after the CSV\n",
    "\n",
    "**VALIDATION CHECKLIST**:\n",
    "✓ Every input table appears in exactly one CSV row\n",
    "✓ Business tables are relevant to {industry} industry\n",
    "✓ Technical tables are clearly system/metadata focused\n",
    "✓ All TECHNICAL tables have Business Score = 0 and Data Category = \"TECHNICAL\"\n",
    "✓ All BUSINESS tables have Business Score between 1-100\n",
    "✓ Data Category is determined SEMANTICALLY (not by score):\n",
    "  ✓ TRANSACTIONAL = event tables with timestamps (orders, payments, logs)\n",
    "  ✓ MASTER = entity tables with lifecycle (customers, products, employees)\n",
    "  ✓ REFERENCE = lookup/code tables (country_codes, status_types, categories)\n",
    "✓ Output is valid CSV starting with header row\n",
    "\n",
    "\uD83D\uDEA8 ABSOLUTE RULE - OUTPUT FORMAT \uD83D\uDEA8:\n",
    "\n",
    "❌ DO NOT INCLUDE:\n",
    "- Any conversational or explanatory text (\"Here are...\", \"I've...\", \"Based on...\")\n",
    "- Any thoughts or analysis descriptions\n",
    "\n",
    "✅ OUTPUT REQUIREMENTS:\n",
    "- Your response must START with: \"Table Name\",\"Classification\",\"Business Score\",\"Reason\",\"honesty_score\",\"honesty_justification\"\n",
    "- Include honesty columns in header and all rows\n",
    "\"\"\" + HONESTY_CHECK_CSV\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,AIAgent\n",
    "# ==============================================================================\n",
    "# 2.5. AI AGENT CLASS (MODIFIED)\n",
    "# ==============================================================================\n",
    "\n",
    "class AIAgent:\n",
    "    _total_ai_calls = 0\n",
    "    _total_input_chars = 0\n",
    "    _total_output_chars = 0\n",
    "    _step_stats = defaultdict(lambda: {\"calls\": 0, \"input_chars\": 0, \"output_chars\": 0})\n",
    "\n",
    "    # === MODIFIED: Added prompt_templates parameter ===\n",
    "    def __init__(self, spark, logger, worker_llm_config, judge_llm_config, prompt_templates: dict,\n",
    "                 default_timeout_seconds: int = 300, max_retry_attempts: int = 1):\n",
    "        self.spark = spark\n",
    "        self.logger = logger\n",
    "        self.worker_llm = worker_llm_config\n",
    "        self.judge_llm = judge_llm_config\n",
    "        self.prompt_templates = prompt_templates  # <-- NEW: Store the dictionary\n",
    "        self.current_language = \"English\"  # Default language for context limit calculations\n",
    "        self.default_timeout_seconds = default_timeout_seconds\n",
    "        # Number of retries after the first attempt (global knob)\n",
    "        self.max_retry_attempts = max(0, max_retry_attempts)\n",
    "        \n",
    "        # === NEW: Check if prompt dictionary is empty ===\n",
    "        if not self.prompt_templates:\n",
    "            self.logger.warning(\"AIAgent initialized with an empty prompt_templates dictionary.\")\n",
    "    \n",
    "    def set_language(self, language: str):\n",
    "        \"\"\"\n",
    "        Set the current language for context limit calculations.\n",
    "        \n",
    "        Args:\n",
    "            language: Language name (e.g., \"English\", \"French\", \"Spanish\")\n",
    "        \"\"\"\n",
    "        self.current_language = language\n",
    "        # Language set - no debug logging needed\n",
    "\n",
    "    # === NEW: Internal helper function for loading prompts ===\n",
    "    def _load_and_format_prompt(self, prompt_key: str, prompt_vars: dict) -> str:\n",
    "        \"\"\"\n",
    "        Loads a prompt template from the internal dictionary and formats it.\n",
    "        \"\"\"\n",
    "        if prompt_key not in self.prompt_templates:\n",
    "            self.logger.error(f\"Prompt key '{prompt_key}' not found in prompt dictionary.\")\n",
    "            raise KeyError(f\"Prompt key '{prompt_key}' not found in AIAgent's prompt_templates.\")\n",
    "        \n",
    "        prompt_template = self.prompt_templates[prompt_key]\n",
    "        \n",
    "        try:\n",
    "            return prompt_template.format(**prompt_vars)\n",
    "        except KeyError as e:\n",
    "            self.logger.error(f\"Failed to format prompt '{prompt_key}'. Missing variable: {e}\")\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"An unexpected error occurred during prompt formatting for '{prompt_key}': {e}\")\n",
    "            raise\n",
    "\n",
    "    def _call_ai_query(self, prompt: str, prompt_name: str, response_schema=None, model_override=None, timeout_seconds=None, max_retries=None, display_name=None) -> str:\n",
    "        \"\"\"\n",
    "        Calls Databricks AI query function with the given prompt.\n",
    "        Now tracks statistics by prompt_name (actual template name) instead of step_name.\n",
    "        Includes resilience for \"Input is too long\" errors.\n",
    "        Performs pre-flight check to ensure prompt respects language-aware context limits.\n",
    "        Handles throttling and timeouts with automatic retry logic.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The prompt text\n",
    "            prompt_name: Name of the prompt template (for logging)\n",
    "            response_schema: Optional JSON schema for structured responses\n",
    "            model_override: Optional model name override\n",
    "            timeout_seconds: Timeout in seconds for LLM call (default: 420 seconds unless overridden)\n",
    "            max_retries: Maximum number of retry attempts for throttling/timeouts (default: 3)\n",
    "            display_name: Optional display name for heartbeat logs (defaults to prompt_name)\n",
    "        \n",
    "        Returns:\n",
    "            Raw response string from the LLM\n",
    "        \"\"\"\n",
    "        import time\n",
    "        from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError\n",
    "        \n",
    "        model = model_override if model_override else self.worker_llm\n",
    "        \n",
    "        heartbeat_name = display_name if display_name else prompt_name\n",
    "        \n",
    "        # Resolve timeout/retries from configurable defaults\n",
    "        attempts_allowed = (max_retries if max_retries is not None else self.max_retry_attempts) + 1\n",
    "        timeout_val = timeout_seconds if timeout_seconds is not None else self.default_timeout_seconds\n",
    "        \n",
    "        # SAFETY: Ensure timeout_val is positive and not None\n",
    "        if not timeout_val or timeout_val <= 0:\n",
    "            self.logger.warning(f\"Timeout value '{timeout_val}' is invalid. Enforcing safety fallback of 300s.\")\n",
    "            timeout_val = 300\n",
    "\n",
    "        # Try the LLM call with retries for throttling and timeouts\n",
    "        for attempt in range(1, attempts_allowed + 1):\n",
    "            try:\n",
    "            \n",
    "                if attempt > 1:\n",
    "                    self.logger.info(f\"\uD83D\uDD04 [{prompt_name}] Retry attempt {attempt}/{attempts_allowed} after error...\")\n",
    "                \n",
    "                # PRE-FLIGHT CHECK: Ensure prompt length is within language-aware context limit\n",
    "                # Uses model-specific token limits from TECHNICAL_CONTEXT\n",
    "                max_context_chars = get_max_context_chars(self.current_language, prompt_name)\n",
    "                prompt_len = len(prompt)\n",
    "                if prompt_len > max_context_chars:\n",
    "                    self.logger.error(\n",
    "                        f\"Prompt length ({prompt_len:,} chars) exceeds max context limit ({max_context_chars:,} chars) \"\n",
    "                        f\"for language '{self.current_language}' and prompt: {prompt_name}. This will likely fail.\"\n",
    "                    )\n",
    "                    # Raise the error immediately rather than sending to the model\n",
    "                    raise InputTooLongError(\n",
    "                        f\"Input length: {prompt_len:,} characters exceeds context limit of {max_context_chars:,} \"\n",
    "                        f\"for language '{self.current_language}'. \"\n",
    "                        f\"Prompt: {prompt_name}, Model: {model}. Please batch your input.\"\n",
    "                    )\n",
    "                \n",
    "                # Log a debug message if we're close to the limit (>90%)\n",
    "                if prompt_len > (max_context_chars * 0.9):\n",
    "                    # Prompt approaching context limit - suppress debug log\n",
    "                    pass\n",
    "                \n",
    "                # Prepare response_format if schema is provided\n",
    "                response_format_str = \"\"\n",
    "                \n",
    "                # CRITICAL FIX: Set max_tokens to prevent output truncation\n",
    "                # Claude Sonnet 4 defaults to only 1000 output tokens without explicit max_tokens!\n",
    "                # This caused queries exceeding ~350 lines to be truncated mid-response.\n",
    "                output_token_limit = get_model_output_token_limit(prompt_name)\n",
    "                # Use 90% of the model's max output tokens as a safe limit, with a minimum floor of 32000 tokens\n",
    "                max_output_tokens = max(32000, int(output_token_limit * 0.9))\n",
    "                \n",
    "                # Log the max_tokens being used for debugging truncation issues\n",
    "                self.logger.info(f\"   [{prompt_name}] Setting max_tokens={max_output_tokens:,} (model limit: {output_token_limit:,})\")\n",
    "                \n",
    "                if response_schema:\n",
    "                    response_format_str = json.dumps({\"type\": \"json_schema\", \"json_schema\": response_schema}, separators=(',', ':')).replace(\"'\", \"''\")\n",
    "                    ai_query_sql = f\"SELECT ai_query('{model}', '{replace_single_quote(prompt)}', responseFormat => '{response_format_str}', modelParameters => named_struct('max_tokens', {max_output_tokens})) AS ai_response\"\n",
    "                else:\n",
    "                    ai_query_sql = f\"SELECT ai_query('{model}', '{replace_single_quote(prompt)}', modelParameters => named_struct('max_tokens', {max_output_tokens})) AS ai_response\"\n",
    "                \n",
    "                # Execute with timeout using simple Thread with watchdog pattern\n",
    "                response_rows = None\n",
    "                error_holder = [None]\n",
    "                completed_flag = [False]\n",
    "                \n",
    "                def execute_query():\n",
    "                    try:\n",
    "                        error_holder[0] = None\n",
    "                        result = execute_sql(self.spark, ai_query_sql, self.logger)\n",
    "                        nonlocal response_rows\n",
    "                        response_rows = result\n",
    "                        completed_flag[0] = True\n",
    "                    except Exception as e:\n",
    "                        error_holder[0] = e\n",
    "                        completed_flag[0] = True\n",
    "                \n",
    "                query_thread = threading.Thread(target=execute_query, name=f\"LLM_Query_{prompt_name}\")\n",
    "                query_thread.daemon = True\n",
    "                start_time = time.time()\n",
    "                query_thread.start()\n",
    "                \n",
    "                # Use polling with small intervals instead of single long join\n",
    "                # This ensures we can detect hangs even if join() misbehaves\n",
    "                poll_interval = 5  # Check every 5 seconds\n",
    "                while True:\n",
    "                    query_thread.join(timeout=poll_interval)\n",
    "                    elapsed = time.time() - start_time\n",
    "                    \n",
    "                    if completed_flag[0] or not query_thread.is_alive():\n",
    "                        break\n",
    "                    \n",
    "                    if elapsed >= timeout_val:\n",
    "                        log_print(f\"⏱️  [{prompt_name}] LLM call TIMED OUT after {elapsed:.1f}s (attempt {attempt}/{attempts_allowed}) - Thread still alive\", level=\"ERROR\")\n",
    "                        self.logger.error(f\"⏱️  [{prompt_name}] LLM call timed out after {elapsed:.1f} seconds (attempt {attempt}/{attempts_allowed})\")\n",
    "                        break\n",
    "                    \n",
    "                    # Log heartbeat every 60 seconds to show progress\n",
    "                    if elapsed > 0 and int(elapsed) % 60 == 0 and int(elapsed) != int(elapsed - poll_interval):\n",
    "                        log_print(f\"[{heartbeat_name}] Still waiting... {elapsed:.0f}s elapsed (timeout: {timeout_val}s)\")\n",
    "                \n",
    "                if query_thread.is_alive():\n",
    "                    raise Exception(f\"LLM call timed out after {timeout_val} seconds\")\n",
    "                \n",
    "                if error_holder[0] is not None:\n",
    "                    raise error_holder[0]\n",
    "                \n",
    "                raw_response = response_rows[0].ai_response if response_rows and response_rows[0] else \"\"\n",
    "                \n",
    "                honesty_score, honesty_justification, cleaned_response = extract_honesty_score(raw_response, self.logger)\n",
    "                \n",
    "                if honesty_score is not None:\n",
    "                    if honesty_justification:\n",
    "                        self.logger.info(f\"\uD83D\uDD2E✨ HONESTY CHECK [{prompt_name}] Score: {honesty_score}% | {honesty_justification} ✨\uD83D\uDD2E\")\n",
    "                    else:\n",
    "                        self.logger.info(f\"\uD83D\uDD2E✨ HONESTY CHECK [{prompt_name}] Score: {honesty_score}% ✨\uD83D\uDD2E\")\n",
    "                \n",
    "                input_len = len(prompt)\n",
    "                output_len = len(cleaned_response)\n",
    "\n",
    "                AIAgent._total_ai_calls += 1\n",
    "                AIAgent._total_input_chars += input_len\n",
    "                AIAgent._total_output_chars += output_len\n",
    "                \n",
    "                # Track by prompt_name (actual template name) instead of step_name\n",
    "                AIAgent._step_stats[prompt_name][\"calls\"] += 1\n",
    "                AIAgent._step_stats[prompt_name][\"input_chars\"] += input_len\n",
    "                AIAgent._step_stats[prompt_name][\"output_chars\"] += output_len\n",
    "                \n",
    "                # TRUNCATION DETECTION: Check for mandatory END marker\n",
    "                # For SQL generation, we require the response to end with \"--END OF GENERATED SQL\"\n",
    "                # NOTE: SQL FIX prompt does NOT require this marker - it just returns fixed SQL\n",
    "                SQL_END_MARKER = \"--END OF GENERATED SQL\"\n",
    "                is_sql_generation = prompt_name == \"USE_CASE_SQL_GEN_PROMPT\"\n",
    "                \n",
    "                if is_sql_generation and cleaned_response and len(cleaned_response) > 100:\n",
    "                    # Check if the END marker is present\n",
    "                    has_end_marker = SQL_END_MARKER in cleaned_response\n",
    "                    \n",
    "                    if not has_end_marker:\n",
    "                        self.logger.warning(\n",
    "                            f\"⚠️  [{prompt_name}] SQL TRUNCATED - Missing '{SQL_END_MARKER}' marker! \"\n",
    "                            f\"Output: {output_len:,} chars, max_tokens: {max_output_tokens:,}. \"\n",
    "                            f\"Last 100 chars: ...{cleaned_response[-100:]}\"\n",
    "                        )\n",
    "                        # Raise a specific error so caller can handle truncation\n",
    "                        raise TruncatedResponseError(\n",
    "                            f\"SQL response truncated - missing '{SQL_END_MARKER}' marker. \"\n",
    "                            f\"Output length: {output_len:,} chars. Consider reducing input context.\"\n",
    "                        )\n",
    "                    else:\n",
    "                        self.logger.info(f\"   [{prompt_name}] SQL complete - END marker found\")\n",
    "                \n",
    "                # Success - return cleaned response (without honesty section)\n",
    "                return cleaned_response\n",
    "                \n",
    "            except InputTooLongError:\n",
    "                # Don't retry InputTooLongError - let caller handle with context reduction\n",
    "                raise\n",
    "            except TruncatedResponseError:\n",
    "                # Don't retry TruncatedResponseError - let caller handle with context reduction\n",
    "                raise\n",
    "            except Exception as e:\n",
    "                error_msg = str(e).lower()\n",
    "                \n",
    "                # FIRST: Check for \"Input is too long\" errors BEFORE checking for retryable errors\n",
    "                # This includes Databricks-specific error formats\n",
    "                is_input_too_long = any(keyword in error_msg for keyword in [\n",
    "                    'input is too long', 'too long for requested model', 'input length',\n",
    "                    'exceeds context limit', 'context window', 'token limit exceeded',\n",
    "                    'maximum context length'\n",
    "                ])\n",
    "                \n",
    "                # Also check for Databricks HTTP 400 errors with \"bad_request\" and input length messages\n",
    "                is_databricks_input_error = (\n",
    "                    ('400' in error_msg or 'bad_request' in error_msg or 'bad request' in error_msg) and\n",
    "                    ('input' in error_msg or 'length' in error_msg or 'model' in error_msg)\n",
    "                )\n",
    "                \n",
    "                if is_input_too_long or is_databricks_input_error:\n",
    "                    # This is an \"input too long\" error - raise immediately without retry\n",
    "                    self.logger.error(\n",
    "                        f\"❌ [{prompt_name}] Input too long error detected (will not retry): {str(e)[:300]}\"\n",
    "                    )\n",
    "                    raise InputTooLongError(\n",
    "                        f\"Input length: {len(prompt)} characters exceeds model's context limit. \"\n",
    "                        f\"Prompt: {prompt_name}, Model: {model}\"\n",
    "                    ) from e\n",
    "                \n",
    "                # Check for retryable errors (only if NOT input too long)\n",
    "                is_throttling = any(keyword in error_msg for keyword in [\n",
    "                    'throttl', 'rate limit', 'too many requests', 'quota', '429',\n",
    "                    'resource exhausted', 'capacity', 'overload'\n",
    "                ])\n",
    "                is_timeout = any(keyword in error_msg for keyword in [\n",
    "                    'timeout', 'timed out', 'deadline', 'time limit'\n",
    "                ])\n",
    "                is_server_error = any(keyword in error_msg for keyword in [\n",
    "                    '500', '502', '503', '504', 'internal server', 'service unavailable',\n",
    "                    'bad gateway', 'gateway timeout'\n",
    "                ])\n",
    "                \n",
    "                is_retryable = is_throttling or is_timeout or is_server_error\n",
    "                \n",
    "                if is_retryable and attempt < attempts_allowed:\n",
    "                    # Calculate exponential backoff wait time\n",
    "                    wait_time = min(2 ** attempt * 5, 120)  # 5s, 10s, 20s... max 120s\n",
    "                    \n",
    "                    error_type = \"Throttling\" if is_throttling else (\"Timeout\" if is_timeout else \"Server error\")\n",
    "                    self.logger.warning(\n",
    "                        f\"⚠️  [{prompt_name}] {error_type} detected (attempt {attempt}/{attempts_allowed}): {str(e)[:200]}\"\n",
    "                    )\n",
    "                    self.logger.info(f\"   Waiting {wait_time}s before retry...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue  # Retry\n",
    "                else:\n",
    "                    # Non-retryable error or max retries exceeded\n",
    "                    if is_retryable:\n",
    "                        self.logger.error(\n",
    "                            f\"❌ [{prompt_name}] Max retries ({attempts_allowed - 1}) exceeded for retryable error: {str(e)[:200]}\"\n",
    "                        )\n",
    "                    \n",
    "                    # If we get here, it's a non-retryable error\n",
    "                    self.logger.error(f\"AI Query function failed (Prompt: {prompt_name}). Error: {e}\\nModel: {model}\")\n",
    "                    raise\n",
    "        \n",
    "        # If we exit the loop without returning, all retries failed\n",
    "        raise Exception(f\"LLM call failed after {attempts_allowed} attempts for prompt: {prompt_name}\")\n",
    "\n",
    "    # === MODIFIED: Uses internal _load_and_format_prompt ===\n",
    "    def run_worker(self, step_name, worker_prompt_path, prompt_vars, response_schema, model_override=None, timeout_override=None, max_retries_override=None):\n",
    "        # Running AI worker - high-level logging only\n",
    "        try:\n",
    "            # === MODIFIED ===\n",
    "            worker_prompt = self._load_and_format_prompt(worker_prompt_path, prompt_vars)\n",
    "            \n",
    "            if not worker_prompt:\n",
    "                raise ValueError(f\"Failed to load prompt: {worker_prompt_path}\")\n",
    "            \n",
    "            # Use model from LLM_MODEL_CONFIG if no override provided\n",
    "            if model_override is None and worker_prompt_path in LLM_MODEL_CONFIG:\n",
    "                model_override = LLM_MODEL_CONFIG[worker_prompt_path]\n",
    "                # Model selected from config - no debug logging needed\n",
    "            \n",
    "            # Pass the prompt_path (template name) to _call_ai_query for tracking\n",
    "            raw_response = self._call_ai_query(\n",
    "                worker_prompt,\n",
    "                worker_prompt_path,\n",
    "                response_schema,\n",
    "                model_override,\n",
    "                timeout_seconds=timeout_override,\n",
    "                max_retries=max_retries_override,\n",
    "                display_name=step_name\n",
    "            )\n",
    "            \n",
    "            # Return based on schema presence\n",
    "            if response_schema:\n",
    "                return clean_json_response(raw_response)\n",
    "            else:\n",
    "                # This is the path for CSV\n",
    "                if not raw_response:\n",
    "                    self.logger.warning(f\"AI Worker for {step_name} (Raw) returned an empty response.\")\n",
    "                    return \"\" # Return empty string\n",
    "                return raw_response\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"AI Worker process failed for {step_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    # --- START OF MODIFICATIONS ---\n",
    "\n",
    "    def _deep_parse_json_values(self, data, task):\n",
    "        \"\"\"\n",
    "        (Helper) Parses known stringified keys ('attributes', 'domains') within a data object.\n",
    "        Operates on the dictionary, not the JSON string.\n",
    "        \"\"\"\n",
    "        if not isinstance(data, dict):\n",
    "            return data # Not a dict (e.g., dashboard list), can't fix\n",
    "\n",
    "        keys_to_check = []\n",
    "        if task == 'attributes':\n",
    "            keys_to_check = ['attributes']\n",
    "        elif task == 'domains':\n",
    "            keys_to_check = ['domains']\n",
    "        \n",
    "        for key in keys_to_check:\n",
    "            value = data.get(key)\n",
    "            if isinstance(value, str):\n",
    "                try:\n",
    "                    data[key] = json.loads(value) # Replace string with parsed object\n",
    "                except (json.JSONDecodeError, TypeError):\n",
    "                    self.logger.warning(f\"Failed to deep-parse stringified key '{key}' in task '{task}'.\")\n",
    "                    data[key] = [] # Set to empty list on failure\n",
    "        return data\n",
    "\n",
    "    # === MODIFIED: Uses internal _load_and_format_prompt ===\n",
    "    def run_worker_judge(self, step_name, worker_prompt_path, judge_prompt_path, base_prompt_vars, worker_response_schema, config, randomization_params={}, task_info_lambda=None, validation_lambda=None):\n",
    "        \n",
    "        task_type, log_context = task_info_lambda(base_prompt_vars) if task_info_lambda else (\"unknown task\", \"\")\n",
    "        # Worker/judge process starting - high-level logging only\n",
    "        \n",
    "        # --- Added Retry Loop ---\n",
    "        for attempt in range(config[\"MAX_RETRIES\"]):\n",
    "            try:\n",
    "                # --- Worker Generation ---\n",
    "                worker_outputs = []\n",
    "                for i in range(2):\n",
    "                    randomized_vars = base_prompt_vars.copy()\n",
    "                    for key, val in randomization_params.items():\n",
    "                        base_min = int(config[\"PROMPT_VARIABLES\"][f\"min_{key}\"])\n",
    "                        base_max = int(config[\"PROMPT_VARIABLES\"][f\"max_{key}\"])\n",
    "                        new_min = base_min + random.randint(0, val)\n",
    "                        randomized_vars[f\"min_{key}\"] = new_min\n",
    "                        randomized_vars[f\"max_{key}\"] = max(new_min + 1, base_max + random.randint(0, val))\n",
    "                    \n",
    "                    # === MODIFIED ===\n",
    "                    worker_prompt = self._load_and_format_prompt(worker_prompt_path, randomized_vars)\n",
    "                    \n",
    "                    # --- Worker call ---\n",
    "                    worker_step_name = f\"{step_name}_worker_{i+1}\"\n",
    "                    raw_response = self._call_ai_query(worker_prompt, worker_step_name, worker_response_schema) \n",
    "                    worker_outputs.append(clean_json_response(raw_response))\n",
    "\n",
    "                # --- Helper Functions (Modified to use _deep_parse) ---\n",
    "                def summarize_output(json_string, task):\n",
    "                    try:\n",
    "                        data = json.loads(json_string)\n",
    "                        data = self._deep_parse_json_values(data, task) # Fix stringified values\n",
    "                        if task == 'domains': return f\"domains: {', '.join([d.get('domain', 'N/A') for d in data.get('domains', [])])}\"\n",
    "                        if task == 'attributes': return f\"{len(data.get('attributes', []))} attributes\"\n",
    "                        if 'dashboard' in task: return f\"{sum(len(p.get('queries', [])) for p in data)} dashboard queries\"\n",
    "                        return \"summary not available\"\n",
    "                    except (json.JSONDecodeError, TypeError): return \"invalid JSON\"\n",
    "\n",
    "                def is_response_empty(json_string, task):\n",
    "                    try:\n",
    "                        data = json.loads(json_string)\n",
    "                        if not data: return True\n",
    "                        data = self._deep_parse_json_values(data, task) # Fix stringified values\n",
    "                        if task == 'domains': return not data.get('domains')\n",
    "                        if task == 'attributes': return not data.get('attributes')\n",
    "                        if 'dashboard' in task: return not isinstance(data, list) or not any(p.get('queries') for p in data)\n",
    "                        return True\n",
    "                    except (json.JSONDecodeError, TypeError):\n",
    "                        return True\n",
    "\n",
    "                def get_best_worker_output(outputs, task):\n",
    "                    best_output, max_count = \"\", -1\n",
    "                    for out_str in outputs:\n",
    "                        if is_response_empty(out_str, task): continue\n",
    "                        try:\n",
    "                            data, count = json.loads(out_str), 0\n",
    "                            data = self._deep_parse_json_values(data, task) # Fix stringified values\n",
    "                            if task == 'domains': count = len(data.get('domains', []))\n",
    "                            elif task == 'attributes': count = len(data.get('attributes', []))\n",
    "                            elif 'dashboard' in task: count = sum(len(p.get('queries', [])) for p in data)\n",
    "                            if count > max_count:\n",
    "                                max_count, best_output = count, out_str\n",
    "                        except (json.JSONDecodeError, TypeError): continue\n",
    "                    return best_output if best_output else max(outputs, key=len, default=\"\")\n",
    "                \n",
    "                # --- End Helper Functions ---\n",
    "\n",
    "                for i, output in enumerate(worker_outputs): self.logger.debug(f\"LLM Worker {i+1} suggested {log_context}: {summarize_output(output, task_type)}\")\n",
    "                \n",
    "                best_worker_fallback = get_best_worker_output(worker_outputs, task_type)\n",
    "                \n",
    "                # --- START: NEW JUDGE-SKIP LOGIC ---\n",
    "                skip_judge = False\n",
    "                final_cleaned_json = \"\"\n",
    "                \n",
    "                if task_type == 'domains' and judge_prompt_path:\n",
    "                    try:\n",
    "                        data1 = json.loads(worker_outputs[0])\n",
    "                        data1 = self._deep_parse_json_values(data1, task_type)\n",
    "                        domains1 = set(d.get('domain') for d in data1.get('domains', []))\n",
    "                        \n",
    "                        data2 = json.loads(worker_outputs[1])\n",
    "                        data2 = self._deep_parse_json_values(data2, task_type)\n",
    "                        domains2 = set(d.get('domain') for d in data2.get('domains', []))\n",
    "                        \n",
    "                        if domains1 and domains1 == domains2:\n",
    "                            self.logger.debug(\"Worker domain outputs match. Skipping judge and using worker 1 output.\")\n",
    "                            final_cleaned_json = worker_outputs[0]\n",
    "                            skip_judge = True\n",
    "                    except Exception as e:\n",
    "                        self.logger.warning(f\"Could not compare worker outputs for judge skip: {e}\")\n",
    "                # --- END: NEW JUDGE-SKIP LOGIC ---\n",
    "\n",
    "                if not judge_prompt_path or skip_judge:\n",
    "                    if not judge_prompt_path:\n",
    "                        self.logger.debug(\"No judge prompt provided. Skipping judge and using best worker output.\")\n",
    "                    \n",
    "                    if not final_cleaned_json:\n",
    "                        final_cleaned_json = best_worker_fallback\n",
    "                else:\n",
    "                    judge_prompt_vars = {**base_prompt_vars, **{f'llm{i+1}_output': out for i, out in enumerate(worker_outputs)}}\n",
    "                    \n",
    "                    # === MODIFIED ===\n",
    "                    judge_prompt = self._load_and_format_prompt(judge_prompt_path, judge_prompt_vars)\n",
    "                    \n",
    "                    if len(judge_prompt) > 120000:\n",
    "                        self.logger.debug(f\"Judge prompt too long ({len(judge_prompt)} chars). Using best worker response.\")\n",
    "                        final_cleaned_json = best_worker_fallback\n",
    "                    else:\n",
    "                        judge_step_name = f\"{step_name}_judge\"\n",
    "                        final_raw_response = self._call_ai_query(judge_prompt, judge_step_name, worker_response_schema) \n",
    "                        final_cleaned_json = clean_json_response(final_raw_response)\n",
    "                        if is_response_empty(final_cleaned_json, task_type):\n",
    "                            self.logger.warning(f\"Judge returned a malformed or empty result for {task_type}. Rejecting and using best worker output.\")\n",
    "                            final_cleaned_json = best_worker_fallback\n",
    "                \n",
    "                try:\n",
    "                    final_data = json.loads(final_cleaned_json)\n",
    "                    final_data = self._deep_parse_json_values(final_data, task_type)\n",
    "                    final_fixed_json_string = json.dumps(final_data)\n",
    "                except (json.JSONDecodeError, TypeError):\n",
    "                    self.logger.error(f\"Failed to parse or fix final JSON output for {task_type}. Using raw cleaned JSON for validation.\")\n",
    "                    final_fixed_json_string = final_cleaned_json\n",
    "\n",
    "                if validation_lambda:\n",
    "                    validation_lambda(final_fixed_json_string, task_type)\n",
    "                \n",
    "                self.logger.debug(f\"Judge adjudicated the final output {log_context}, resulting in: {summarize_output(final_fixed_json_string, task_type)}\")\n",
    "                return final_fixed_json_string\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Attempt {attempt + 1}/{config['MAX_RETRIES']} for {step_name} {log_context} failed: {e}\")\n",
    "                if attempt == config[\"MAX_RETRIES\"] - 1:\n",
    "                    self.logger.error(f\"FAILED to generate valid output for {step_name} {log_context} after all retries.\")\n",
    "                    raise e\n",
    "        \n",
    "        raise Exception(f\"AI Worker/Judge for {step_name} failed after all retries.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_summary_report():\n",
    "        \"\"\"\n",
    "        Generates a summary report of AI usage, grouped by prompt type.\n",
    "        More generic and aggregated across all instances.\n",
    "        \"\"\"\n",
    "        report = []\n",
    "        report.append(\"\\n\" + \"=\"*70)\n",
    "        report.append(\"--- \uD83D\uDCCA AI Usage Summary ---\")\n",
    "        report.append(\"=\"*70)\n",
    "        report.append(f\"Total AI Calls:     {AIAgent._total_ai_calls}\")\n",
    "        \n",
    "        # Calculate estimated tokens (chars / 4)\n",
    "        input_tokens = AIAgent._total_input_chars / 4\n",
    "        output_tokens = AIAgent._total_output_chars / 4\n",
    "        \n",
    "        report.append(f\"Total Input Tokens:  ~{input_tokens:,.2f}  ({AIAgent._total_input_chars:,} chars)\")\n",
    "        report.append(f\"Total Output Tokens: ~{output_tokens:,.2f}  ({AIAgent._total_output_chars:,} chars)\")\n",
    "        report.append(\"\\n--- Prompt Type Details ---\")\n",
    "        \n",
    "        if not AIAgent._step_stats:\n",
    "            report.append(\"No AI calls were tracked.\")\n",
    "        else:\n",
    "            # Group by prompt type (extract the general prompt name from step names)\n",
    "            prompt_aggregates = {}\n",
    "            for step_name, stats in AIAgent._step_stats.items():\n",
    "                # Extract prompt type from step name (e.g., \"Generate_UseCases_Batch_5\" -> \"Generate_UseCases\")\n",
    "                # Common patterns: \"PromptName_Language_Batch_X\", \"PromptName_Batch_X\", etc\n",
    "                parts = step_name.split('_')\n",
    "                \n",
    "                # Identify the core prompt name\n",
    "                prompt_type = step_name\n",
    "                if 'Batch' in step_name:\n",
    "                    # Extract everything before \"_Batch\"\n",
    "                    batch_idx = step_name.find('_Batch')\n",
    "                    if batch_idx > 0:\n",
    "                        prompt_type = step_name[:batch_idx]\n",
    "                elif any(lang in step_name for lang in ['English', 'Arabic', 'Chinese', 'French', 'Spanish']):\n",
    "                    # Remove language suffix\n",
    "                    for lang in ['English', 'Arabic', 'Chinese', 'French', 'Spanish', 'German', 'Portuguese', 'Italian', 'Japanese', 'Korean']:\n",
    "                        if f\"_{lang}\" in step_name:\n",
    "                            prompt_type = step_name.replace(f\"_{lang}\", \"\")\n",
    "                            break\n",
    "                \n",
    "                # Aggregate stats by prompt type\n",
    "                if prompt_type not in prompt_aggregates:\n",
    "                    prompt_aggregates[prompt_type] = {'calls': 0, 'input_chars': 0, 'output_chars': 0}\n",
    "                \n",
    "                prompt_aggregates[prompt_type]['calls'] += stats['calls']\n",
    "                prompt_aggregates[prompt_type]['input_chars'] += stats['input_chars']\n",
    "                prompt_aggregates[prompt_type]['output_chars'] += stats['output_chars']\n",
    "            \n",
    "            # Display aggregated results\n",
    "            prompt_col_width = 40\n",
    "            header = f\"{'Prompt Type':<{prompt_col_width}} | {'Calls':<7} | {'Input Tokens':<18} | {'Output Tokens':<18}\"\n",
    "            report.append(header)\n",
    "            report.append(\"-\" * len(header))\n",
    "            \n",
    "            # Sort by number of calls (descending)\n",
    "            for prompt_type, stats in sorted(prompt_aggregates.items(), key=lambda x: x[1]['calls'], reverse=True):\n",
    "                input_tok = stats['input_chars'] / 4\n",
    "                output_tok = stats['output_chars'] / 4\n",
    "                report.append(f\"{prompt_type:<{prompt_col_width}} | {stats['calls']:<7} | ~{input_tok:<16,.0f} | ~{output_tok:<16,.0f}\")\n",
    "        \n",
    "        report.append(\"=\"*70)\n",
    "        \n",
    "        final_report_str = \"\\n\".join(report)\n",
    "        log_print(final_report_str)\n",
    "        return final_report_str\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Translations\n",
    "# ==============================================================================\n",
    "# 1.5. TRANSLATION SERVICE (MODIFIED)\n",
    "# ==============================================================================\n",
    "\n",
    "class TranslationService:\n",
    "    \"\"\"\n",
    "    Handles all language translation tasks by calling an AI agent in parallel.\n",
    "    Relies on an AIAgent that has been initialized with the PROMPT_TEMPLATES dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    # === MODIFIED: Added pdf_disclaimer_title and updated titles ===\n",
    "    ENGLISH_TRANSLATIONS = {\n",
    "        \"main_title\": \"Databricks Agent Bricks Use Case Generator\",\n",
    "        \"intro\": \"This notebook contains AI-generated use cases based on your schemas. Below is a summary of the generated scenarios by business domain.\",\n",
    "        \"domain\": \"Business Domain\",\n",
    "        \"total\": \"Total Use Cases\",\n",
    "        \"summaries\": \"Use Case Summaries\",\n",
    "        \"sum_id\": \"ID\",\n",
    "        \"sum_name\": \"Name\",\n",
    "        \"sum_value\": \"Business Value\",\n",
    "        \"sum_outcome\": \"Expected Outcome\",\n",
    "        \"warning_header\": \"WARNING\",\n",
    "        \"warning_body\": \"Do not run this notebook. It is intended for demonstration and cataloging purposes only. The SQL queries are examples and may require review before execution.\",\n",
    "        \"disclaimer\": \"This content is AI-generated and for demonstration purposes only. All SQL queries are examples and must be validated for syntax and safety by a qualified engineer before being used in any production environment. Databricks is not liable for any issues arising from the use of this code.\",\n",
    "        \"detailed_scenarios\": \"Use Cases Details\",\n",
    "        \"aspect\": \"Aspect\",\n",
    "        \"description\": \"Description\",\n",
    "        \"aspect_domain\": \"Business Domain\",\n",
    "        \"type\": \"Type\",\n",
    "        \"analytics_technique\": \"Analytics Technique\",\n",
    "        \"primary_table\": \"Primary Table\",\n",
    "        \"priority\": \"Priority\",\n",
    "        # Value translations for Type field\n",
    "        \"value_type_problem\": \"Problem\",\n",
    "        \"value_type_risk\": \"Risk\",\n",
    "        \"value_type_opportunity\": \"Opportunity\",\n",
    "        \"value_type_improvement\": \"Improvement\",\n",
    "        # Value translations for Priority field\n",
    "        \"value_priority_ultra_high\": \"Ultra High\",\n",
    "        \"value_priority_very_high\": \"Very High\",\n",
    "        \"value_priority_high\": \"High\",\n",
    "        \"value_priority_medium\": \"Medium\",\n",
    "        \"value_priority_low\": \"Low\",\n",
    "        \"value_priority_very_low\": \"Very Low\",\n",
    "        \"value_priority_ultra_low\": \"Ultra Low\",\n",
    "        \"statement\": \"Statement\",\n",
    "        \"solution\": \"Solution\",\n",
    "        \"aspect_beneficiary\": \"Beneficiary\",\n",
    "        \"beneficiary\": \"Beneficiary\",\n",
    "        \"aspect_sponsor\": \"Sponsor\",\n",
    "        \"sponsor\": \"Sponsor\",\n",
    "        \"business_priority_alignment\": \"Business Priority Alignment\",\n",
    "        \"strategic_goals_alignment\": \"Strategic Goals Alignment\",\n",
    "        \"subdomain\": \"Subdomain\",\n",
    "        \"aspect_value\": \"Business Value\",\n",
    "        \"business_value\": \"Business Value\",\n",
    "        \"aspect_tables\": \"Tables Involved\",\n",
    "        \"aspect_ai_function\": \"AI Function\",\n",
    "        \"aspect_analytics_technique\": \"Analytics Technique\",\n",
    "        \"aspect_primary_table\": \"Primary Table\",\n",
    "        \"aspect_priority\": \"Priority\",\n",
    "        # Scoring field labels (for Excel)\n",
    "        \"strategic_alignment\": \"Strategic Alignment\",\n",
    "        \"return_on_investment\": \"Return on Investment\",\n",
    "        \"reusability\": \"Reusability\",\n",
    "        \"time_to_value\": \"Time to Value\",\n",
    "        \"data_availability\": \"Data Availability\",\n",
    "        \"data_accessibility\": \"Data Accessibility\",\n",
    "        \"architecture_fitness\": \"Architecture Fitness\",\n",
    "        \"team_skills\": \"Team Skills\",\n",
    "        \"domain_knowledge\": \"Domain Knowledge\",\n",
    "        \"people_allocation\": \"People Allocation\",\n",
    "        \"budget_allocation\": \"Budget Allocation\",\n",
    "        \"time_to_production\": \"Time to Production\",\n",
    "        \"value_score\": \"Value\",\n",
    "        \"feasibility_score\": \"Feasibility\",\n",
    "        \"priority_score\": \"Priority Score\",\n",
    "        \"pdf_title\": \"Databricks Agent Bricks Strategic AI Use Cases\", # MODIFIED\n",
    "        \"pdf_for\": \"For\",\n",
    "        \"pdf_exec_summary\": \"Executive Summary\",\n",
    "        \"pdf_toc_title\": \"Use Case Domains\",\n",
    "        \"pdf_detailed_view\": \"Detailed Use Case Catalog\",\n",
    "        \"pdf_disclaimer_title\": \"Disclaimer\", # NEW\n",
    "        \"pdf_fallback_summary_p1\": \"This document outlines {total_cases} high-value analytical use cases identified for {business_name}. These scenarios, powered by Databricks Agent Bricks, are designed to drive significant business outcomes by leveraging your existing data assets.\",\n",
    "        \"pdf_fallback_summary_p2\": \"The following pages provide a detailed breakdown of these opportunities, categorized by business domain, to help prioritize your AI initiatives.\",\n",
    "        \"pptx_main_title\": \"Databricks Agent Bricks Strategic AI Use Cases\", # MODIFIED\n",
    "        \"pptx_for\": \"For\",\n",
    "        \"pptx_disclaimer_title\": \"Disclaimer\",\n",
    "        \"pptx_domain_suffix\": \"Use Cases\",\n",
    "        # NEW: Query result example translations\n",
    "        \"example_results\": \"Example Results\",\n",
    "        \"error_no_results\": \"Could not generate results, Check Notebook: {notebook_name} and use case id {use_case_id}\",\n",
    "        \"input_data_original\": \"Input Data (Original Values)\",\n",
    "        \"ai_generated_output\": \"AI-Generated Results (Output)\",\n",
    "        \"column\": \"Column\",\n",
    "        \"value\": \"Value\",\n",
    "        \"executive_summary_not_available\": \"Executive summary not available.\",\n",
    "        \"domain_summary_not_available\": \"Domain summary not available.\",\n",
    "        \"summary_not_available\": \"Summary not available.\",\n",
    "        # Strategic Goal/Priority Alignment value translations\n",
    "        \"value_general_improvement\": \"General Improvement\",\n",
    "        \"value_reduce_cost\": \"Reduce Cost\",\n",
    "        \"value_increase_revenue\": \"Increase Revenue\",\n",
    "        \"value_boost_productivity\": \"Boost Productivity\",\n",
    "        \"value_mitigate_risk\": \"Mitigate Risk\",\n",
    "        \"value_protect_revenue\": \"Protect Revenue\",\n",
    "        \"value_align_to_regulations\": \"Align to Regulations\",\n",
    "        \"value_improve_customer_experience\": \"Improve Customer Experience\",\n",
    "        \"value_enable_data_driven_decisions\": \"Enable Data-Driven Decisions\",\n",
    "        \"value_optimize_operations\": \"Optimize Operations\",\n",
    "        \"value_empower_talent\": \"Empower Talent\",\n",
    "        \"value_enhance_experience\": \"Enhance Experience\",\n",
    "        \"value_drive_innovation\": \"Drive Innovation\",\n",
    "        \"value_achieve_esg\": \"Achieve ESG\",\n",
    "        \"value_execute_strategy\": \"Execute Strategy\",\n",
    "        # Analytics Technique value translations\n",
    "        \"value_forecasting\": \"Forecasting\",\n",
    "        \"value_classification\": \"Classification\",\n",
    "        \"value_anomaly_detection\": \"Anomaly Detection\",\n",
    "        \"value_cohort_analysis\": \"Cohort Analysis\",\n",
    "        \"value_segmentation\": \"Segmentation\",\n",
    "        \"value_sentiment_analysis\": \"Sentiment Analysis\",\n",
    "        \"value_trend_analysis\": \"Trend Analysis\",\n",
    "        \"value_prescriptive_analytics\": \"Prescriptive Analytics\",\n",
    "        \"value_root_cause_analysis\": \"Root Cause Analysis\",\n",
    "        \"value_optimization\": \"Optimization\",\n",
    "        \"value_recommendation\": \"Recommendation\",\n",
    "        \"value_time_series_analysis\": \"Time Series Analysis\",\n",
    "        \"value_predictive_analytics\": \"Predictive Analytics\",\n",
    "        \"value_descriptive_analytics\": \"Descriptive Analytics\"\n",
    "    }\n",
    "\n",
    "    def __init__(self, ai_agent, logger=None):\n",
    "        \"\"\"\n",
    "        Initializes the TranslationService with an AIAgent instance.\n",
    "        \"\"\"\n",
    "        self.ai_agent = ai_agent\n",
    "        self.logger = logger or logging.getLogger(self.__class__.__name__)\n",
    "        self.translation_cache = {} # Cache for UI elements\n",
    "\n",
    "    # Complete fallback translations for ALL keys - ensures translations are 100% reliable\n",
    "    TRANSLATION_FALLBACKS = {\n",
    "        \"Arabic\": {\n",
    "            \"main_title\": \"مولد حالات استخدام Databricks Agent Bricks\",\n",
    "            \"intro\": \"يحتوي هذا الدفتر على حالات استخدام تم إنشاؤها بواسطة الذكاء الاصطناعي استناداً إلى مخططاتك. فيما يلي ملخص للسيناريوهات المُنشأة حسب مجال الأعمال.\",\n",
    "            \"domain\": \"مجال الأعمال\",\n",
    "            \"total\": \"إجمالي حالات الاستخدام\",\n",
    "            \"summaries\": \"ملخصات حالات الاستخدام\",\n",
    "            \"sum_id\": \"المعرف\",\n",
    "            \"sum_name\": \"الاسم\",\n",
    "            \"sum_value\": \"القيمة التجارية\",\n",
    "            \"sum_outcome\": \"النتيجة المتوقعة\",\n",
    "            \"warning_header\": \"تحذير\",\n",
    "            \"warning_body\": \"لا تقم بتشغيل هذا الدفتر. وهو مخصص للعرض التوضيحي والفهرسة فقط. استعلامات SQL هي أمثلة وقد تتطلب المراجعة قبل التنفيذ.\",\n",
    "            \"disclaimer\": \"هذا المحتوى تم إنشاؤه بواسطة الذكاء الاصطناعي ولأغراض العرض التوضيحي فقط. جميع استعلامات SQL هي أمثلة ويجب التحقق من صحتها وأمانها من قبل مهندس مؤهل قبل استخدامها في أي بيئة إنتاج.\",\n",
    "            \"detailed_scenarios\": \"تفاصيل حالات الاستخدام\",\n",
    "            \"aspect\": \"الجانب\",\n",
    "            \"description\": \"الوصف\",\n",
    "            \"aspect_domain\": \"مجال الأعمال\",\n",
    "            \"type\": \"النوع\",\n",
    "            \"analytics_technique\": \"تقنية التحليل\",\n",
    "            \"primary_table\": \"الجدول الرئيسي\",\n",
    "            \"priority\": \"الأولوية\",\n",
    "            \"value_type_problem\": \"مشكلة\",\n",
    "            \"value_type_risk\": \"مخاطرة\",\n",
    "            \"value_type_opportunity\": \"فرصة\",\n",
    "            \"value_type_improvement\": \"تحسين\",\n",
    "            \"value_priority_ultra_high\": \"عالية للغاية\",\n",
    "            \"value_priority_very_high\": \"عالية جداً\",\n",
    "            \"value_priority_high\": \"عالية\",\n",
    "            \"value_priority_medium\": \"متوسطة\",\n",
    "            \"value_priority_low\": \"منخفضة\",\n",
    "            \"value_priority_very_low\": \"منخفضة جداً\",\n",
    "            \"value_priority_ultra_low\": \"منخفضة للغاية\",\n",
    "            \"statement\": \"البيان\",\n",
    "            \"solution\": \"الحل\",\n",
    "            \"aspect_beneficiary\": \"المستفيد\",\n",
    "            \"beneficiary\": \"المستفيد\",\n",
    "            \"aspect_sponsor\": \"الراعي\",\n",
    "            \"sponsor\": \"الراعي\",\n",
    "            \"business_priority_alignment\": \"توافق أولوية الأعمال\",\n",
    "            \"strategic_goals_alignment\": \"التوافق مع الأهداف الاستراتيجية\",\n",
    "            \"subdomain\": \"النطاق الفرعي\",\n",
    "            \"aspect_value\": \"القيمة التجارية\",\n",
    "            \"business_value\": \"القيمة التجارية\",\n",
    "            \"aspect_tables\": \"الجداول المستخدمة\",\n",
    "            \"aspect_ai_function\": \"وظيفة الذكاء الاصطناعي\",\n",
    "            \"aspect_analytics_technique\": \"تقنية التحليل\",\n",
    "            \"aspect_primary_table\": \"الجدول الرئيسي\",\n",
    "            \"aspect_priority\": \"الأولوية\",\n",
    "            \"strategic_alignment\": \"التوافق الاستراتيجي\",\n",
    "            \"return_on_investment\": \"العائد على الاستثمار\",\n",
    "            \"reusability\": \"قابلية إعادة الاستخدام\",\n",
    "            \"time_to_value\": \"الوقت للقيمة\",\n",
    "            \"data_availability\": \"توفر البيانات\",\n",
    "            \"data_accessibility\": \"إمكانية الوصول للبيانات\",\n",
    "            \"architecture_fitness\": \"ملاءمة البنية\",\n",
    "            \"team_skills\": \"مهارات الفريق\",\n",
    "            \"domain_knowledge\": \"المعرفة بالمجال\",\n",
    "            \"people_allocation\": \"تخصيص الموارد البشرية\",\n",
    "            \"budget_allocation\": \"تخصيص الميزانية\",\n",
    "            \"time_to_production\": \"الوقت للإنتاج\",\n",
    "            \"value_score\": \"درجة القيمة\",\n",
    "            \"feasibility_score\": \"درجة الجدوى\",\n",
    "            \"priority_score\": \"درجة الأولوية\",\n",
    "            \"pdf_title\": \"حالات استخدام الذكاء الاصطناعي الاستراتيجية من Databricks Agent Bricks\",\n",
    "            \"pdf_for\": \"لـ\",\n",
    "            \"pdf_exec_summary\": \"الملخص التنفيذي\",\n",
    "            \"pdf_toc_title\": \"مجالات حالات الاستخدام\",\n",
    "            \"pdf_detailed_view\": \"كتالوج حالات الاستخدام التفصيلية\",\n",
    "            \"pdf_disclaimer_title\": \"إخلاء المسؤولية\",\n",
    "            \"pdf_fallback_summary_p1\": \"يوضح هذا المستند {total_cases} حالة استخدام تحليلية عالية القيمة تم تحديدها لـ {business_name}. هذه السيناريوهات، المدعومة بـ Databricks Agent Bricks، مصممة لتحقيق نتائج أعمال مهمة من خلال الاستفادة من أصول البيانات الحالية.\",\n",
    "            \"pdf_fallback_summary_p2\": \"توفر الصفحات التالية تفصيلاً مفصلاً لهذه الفرص، مصنفة حسب مجال الأعمال، للمساعدة في تحديد أولويات مبادرات الذكاء الاصطناعي الخاصة بك.\",\n",
    "            \"pptx_main_title\": \"حالات استخدام الذكاء الاصطناعي الاستراتيجية من Databricks Agent Bricks\",\n",
    "            \"pptx_for\": \"لـ\",\n",
    "            \"pptx_disclaimer_title\": \"إخلاء المسؤولية\",\n",
    "            \"pptx_domain_suffix\": \"حالات الاستخدام\",\n",
    "            \"example_results\": \"نتائج المثال\",\n",
    "            \"error_no_results\": \"تعذر إنشاء النتائج، تحقق من الدفتر: {notebook_name} ومعرف حالة الاستخدام {use_case_id}\",\n",
    "            \"input_data_original\": \"بيانات الإدخال (القيم الأصلية)\",\n",
    "            \"ai_generated_output\": \"مخرجات الذكاء الاصطناعي\",\n",
    "            \"column\": \"العمود\",\n",
    "            \"value\": \"القيمة\",\n",
    "            \"executive_summary_not_available\": \"الملخص التنفيذي غير متوفر.\",\n",
    "            \"domain_summary_not_available\": \"ملخص المجال غير متوفر.\",\n",
    "            \"summary_not_available\": \"الملخص غير متوفر.\",\n",
    "            \"value_general_improvement\": \"تحسين عام\",\n",
    "            \"value_reduce_cost\": \"تقليل التكلفة\",\n",
    "            \"value_increase_revenue\": \"زيادة الإيرادات\",\n",
    "            \"value_boost_productivity\": \"تعزيز الإنتاجية\",\n",
    "            \"value_mitigate_risk\": \"تخفيف المخاطر\",\n",
    "            \"value_protect_revenue\": \"حماية الإيرادات\",\n",
    "            \"value_align_to_regulations\": \"الامتثال للوائح\",\n",
    "            \"value_improve_customer_experience\": \"تحسين تجربة العملاء\",\n",
    "            \"value_enable_data_driven_decisions\": \"تمكين القرارات المبنية على البيانات\",\n",
    "            \"value_optimize_operations\": \"تحسين العمليات\",\n",
    "            \"value_empower_talent\": \"تمكين المواهب\",\n",
    "            \"value_enhance_experience\": \"تعزيز التجربة\",\n",
    "            \"value_drive_innovation\": \"دفع الابتكار\",\n",
    "            \"value_achieve_esg\": \"تحقيق ESG\",\n",
    "            \"value_execute_strategy\": \"تنفيذ الاستراتيجية\",\n",
    "            \"value_forecasting\": \"التنبؤ\",\n",
    "            \"value_classification\": \"التصنيف\",\n",
    "            \"value_anomaly_detection\": \"كشف الشذوذ\",\n",
    "            \"value_cohort_analysis\": \"تحليل الأتراب\",\n",
    "            \"value_segmentation\": \"التجزئة\",\n",
    "            \"value_sentiment_analysis\": \"تحليل المشاعر\",\n",
    "            \"value_trend_analysis\": \"تحليل الاتجاهات\",\n",
    "            \"value_prescriptive_analytics\": \"التحليلات الوصفية\",\n",
    "            \"value_root_cause_analysis\": \"تحليل السبب الجذري\",\n",
    "            \"value_optimization\": \"التحسين\",\n",
    "            \"value_recommendation\": \"التوصية\",\n",
    "            \"value_time_series_analysis\": \"تحليل السلاسل الزمنية\",\n",
    "            \"value_predictive_analytics\": \"التحليلات التنبؤية\",\n",
    "            \"value_descriptive_analytics\": \"التحليلات الوصفية\",\n",
    "        },\n",
    "        \"Spanish\": {\n",
    "            \"main_title\": \"Generador de Casos de Uso de Databricks Agent Bricks\",\n",
    "            \"intro\": \"Este cuaderno contiene casos de uso generados por IA basados en sus esquemas. A continuación se muestra un resumen de los escenarios generados por dominio de negocio.\",\n",
    "            \"domain\": \"Dominio de Negocio\",\n",
    "            \"total\": \"Total de Casos de Uso\",\n",
    "            \"summaries\": \"Resúmenes de Casos de Uso\",\n",
    "            \"sum_id\": \"ID\",\n",
    "            \"sum_name\": \"Nombre\",\n",
    "            \"sum_value\": \"Valor Comercial\",\n",
    "            \"sum_outcome\": \"Resultado Esperado\",\n",
    "            \"warning_header\": \"ADVERTENCIA\",\n",
    "            \"warning_body\": \"No ejecute este cuaderno. Está destinado solo para demostración y catalogación. Las consultas SQL son ejemplos y pueden requerir revisión antes de la ejecución.\",\n",
    "            \"disclaimer\": \"Este contenido es generado por IA y solo con fines de demostración. Todas las consultas SQL son ejemplos y deben ser validadas por un ingeniero calificado antes de usarse en cualquier entorno de producción.\",\n",
    "            \"detailed_scenarios\": \"Detalles de Casos de Uso\",\n",
    "            \"aspect\": \"Aspecto\",\n",
    "            \"description\": \"Descripción\",\n",
    "            \"aspect_domain\": \"Dominio de Negocio\",\n",
    "            \"type\": \"Tipo\",\n",
    "            \"analytics_technique\": \"Técnica de Análisis\",\n",
    "            \"primary_table\": \"Tabla Principal\",\n",
    "            \"priority\": \"Prioridad\",\n",
    "            \"value_type_problem\": \"Problema\",\n",
    "            \"value_type_risk\": \"Riesgo\",\n",
    "            \"value_type_opportunity\": \"Oportunidad\",\n",
    "            \"value_type_improvement\": \"Mejora\",\n",
    "            \"value_priority_ultra_high\": \"Extremadamente Alta\",\n",
    "            \"value_priority_very_high\": \"Muy Alta\",\n",
    "            \"value_priority_high\": \"Alta\",\n",
    "            \"value_priority_medium\": \"Media\",\n",
    "            \"value_priority_low\": \"Baja\",\n",
    "            \"value_priority_very_low\": \"Muy Baja\",\n",
    "            \"value_priority_ultra_low\": \"Extremadamente Baja\",\n",
    "            \"statement\": \"Declaración\",\n",
    "            \"solution\": \"Solución\",\n",
    "            \"aspect_beneficiary\": \"Beneficiario\",\n",
    "            \"beneficiary\": \"Beneficiario\",\n",
    "            \"aspect_sponsor\": \"Patrocinador\",\n",
    "            \"sponsor\": \"Patrocinador\",\n",
    "            \"business_priority_alignment\": \"Alineación de Prioridad Empresarial\",\n",
    "            \"strategic_goals_alignment\": \"Alineación con Objetivos Estratégicos\",\n",
    "            \"subdomain\": \"Subdominio\",\n",
    "            \"aspect_value\": \"Valor Comercial\",\n",
    "            \"business_value\": \"Valor Comercial\",\n",
    "            \"aspect_tables\": \"Tablas Involucradas\",\n",
    "            \"aspect_ai_function\": \"Función de IA\",\n",
    "            \"aspect_analytics_technique\": \"Técnica de Análisis\",\n",
    "            \"aspect_primary_table\": \"Tabla Principal\",\n",
    "            \"aspect_priority\": \"Prioridad\",\n",
    "            \"strategic_alignment\": \"Alineación Estratégica\",\n",
    "            \"return_on_investment\": \"Retorno de Inversión\",\n",
    "            \"reusability\": \"Reusabilidad\",\n",
    "            \"time_to_value\": \"Tiempo al Valor\",\n",
    "            \"data_availability\": \"Disponibilidad de Datos\",\n",
    "            \"data_accessibility\": \"Accesibilidad de Datos\",\n",
    "            \"architecture_fitness\": \"Aptitud de Arquitectura\",\n",
    "            \"team_skills\": \"Habilidades del Equipo\",\n",
    "            \"domain_knowledge\": \"Conocimiento del Dominio\",\n",
    "            \"people_allocation\": \"Asignación de Personal\",\n",
    "            \"budget_allocation\": \"Asignación de Presupuesto\",\n",
    "            \"time_to_production\": \"Tiempo a Producción\",\n",
    "            \"value_score\": \"Puntuación de Valor\",\n",
    "            \"feasibility_score\": \"Puntuación de Viabilidad\",\n",
    "            \"priority_score\": \"Puntuación de Prioridad\",\n",
    "            \"pdf_title\": \"Casos de Uso de IA Estratégica de Databricks Agent Bricks\",\n",
    "            \"pdf_for\": \"Para\",\n",
    "            \"pdf_exec_summary\": \"Resumen Ejecutivo\",\n",
    "            \"pdf_toc_title\": \"Dominios de Casos de Uso\",\n",
    "            \"pdf_detailed_view\": \"Catálogo Detallado de Casos de Uso\",\n",
    "            \"pdf_disclaimer_title\": \"Descargo de Responsabilidad\",\n",
    "            \"pdf_fallback_summary_p1\": \"Este documento describe {total_cases} casos de uso analíticos de alto valor identificados para {business_name}.\",\n",
    "            \"pdf_fallback_summary_p2\": \"Las siguientes páginas proporcionan un desglose detallado de estas oportunidades, categorizadas por dominio de negocio.\",\n",
    "            \"pptx_main_title\": \"Casos de Uso de IA Estratégica de Databricks Agent Bricks\",\n",
    "            \"pptx_for\": \"Para\",\n",
    "            \"pptx_disclaimer_title\": \"Descargo de Responsabilidad\",\n",
    "            \"pptx_domain_suffix\": \"Casos de Uso\",\n",
    "            \"example_results\": \"Resultados de Ejemplo\",\n",
    "            \"error_no_results\": \"No se pudieron generar resultados. Verificar Cuaderno: {notebook_name} y caso de uso {use_case_id}\",\n",
    "            \"input_data_original\": \"Datos de Entrada (Valores Originales)\",\n",
    "            \"ai_generated_output\": \"Resultados Generados por IA\",\n",
    "            \"column\": \"Columna\",\n",
    "            \"value\": \"Valor\",\n",
    "            \"executive_summary_not_available\": \"Resumen ejecutivo no disponible.\",\n",
    "            \"domain_summary_not_available\": \"Resumen del dominio no disponible.\",\n",
    "            \"summary_not_available\": \"Resumen no disponible.\",\n",
    "            \"value_general_improvement\": \"Mejora General\",\n",
    "            \"value_reduce_cost\": \"Reducir Costos\",\n",
    "            \"value_increase_revenue\": \"Aumentar Ingresos\",\n",
    "            \"value_boost_productivity\": \"Impulsar Productividad\",\n",
    "            \"value_mitigate_risk\": \"Mitigar Riesgos\",\n",
    "            \"value_protect_revenue\": \"Proteger Ingresos\",\n",
    "            \"value_align_to_regulations\": \"Cumplir Regulaciones\",\n",
    "            \"value_improve_customer_experience\": \"Mejorar Experiencia del Cliente\",\n",
    "            \"value_enable_data_driven_decisions\": \"Habilitar Decisiones Basadas en Datos\",\n",
    "            \"value_optimize_operations\": \"Optimizar Operaciones\",\n",
    "            \"value_empower_talent\": \"Empoderar Talento\",\n",
    "            \"value_enhance_experience\": \"Mejorar Experiencia\",\n",
    "            \"value_drive_innovation\": \"Impulsar Innovación\",\n",
    "            \"value_achieve_esg\": \"Lograr ESG\",\n",
    "            \"value_execute_strategy\": \"Ejecutar Estrategia\",\n",
    "            \"value_forecasting\": \"Pronóstico\",\n",
    "            \"value_classification\": \"Clasificación\",\n",
    "            \"value_anomaly_detection\": \"Detección de Anomalías\",\n",
    "            \"value_cohort_analysis\": \"Análisis de Cohortes\",\n",
    "            \"value_segmentation\": \"Segmentación\",\n",
    "            \"value_sentiment_analysis\": \"Análisis de Sentimiento\",\n",
    "            \"value_trend_analysis\": \"Análisis de Tendencias\",\n",
    "            \"value_prescriptive_analytics\": \"Analítica Prescriptiva\",\n",
    "            \"value_root_cause_analysis\": \"Análisis de Causa Raíz\",\n",
    "            \"value_optimization\": \"Optimización\",\n",
    "            \"value_recommendation\": \"Recomendación\",\n",
    "            \"value_time_series_analysis\": \"Análisis de Series Temporales\",\n",
    "            \"value_predictive_analytics\": \"Analítica Predictiva\",\n",
    "            \"value_descriptive_analytics\": \"Analítica Descriptiva\",\n",
    "        },\n",
    "        \"French\": {\n",
    "            \"main_title\": \"Générateur de Cas d'Utilisation Databricks Agent Bricks\",\n",
    "            \"intro\": \"Ce carnet contient des cas d'utilisation générés par l'IA basés sur vos schémas. Voici un résumé des scénarios générés par domaine d'activité.\",\n",
    "            \"domain\": \"Domaine d'Activité\",\n",
    "            \"total\": \"Total des Cas d'Utilisation\",\n",
    "            \"summaries\": \"Résumés des Cas d'Utilisation\",\n",
    "            \"sum_id\": \"ID\",\n",
    "            \"sum_name\": \"Nom\",\n",
    "            \"sum_value\": \"Valeur Commerciale\",\n",
    "            \"sum_outcome\": \"Résultat Attendu\",\n",
    "            \"warning_header\": \"AVERTISSEMENT\",\n",
    "            \"warning_body\": \"N'exécutez pas ce carnet. Il est destiné uniquement à la démonstration et au catalogage. Les requêtes SQL sont des exemples et peuvent nécessiter une révision avant exécution.\",\n",
    "            \"disclaimer\": \"Ce contenu est généré par l'IA et à des fins de démonstration uniquement. Toutes les requêtes SQL sont des exemples et doivent être validées par un ingénieur qualifié avant utilisation en production.\",\n",
    "            \"detailed_scenarios\": \"Détails des Cas d'Utilisation\",\n",
    "            \"aspect\": \"Aspect\",\n",
    "            \"description\": \"Description\",\n",
    "            \"aspect_domain\": \"Domaine d'Activité\",\n",
    "            \"type\": \"Type\",\n",
    "            \"analytics_technique\": \"Technique d'Analyse\",\n",
    "            \"primary_table\": \"Table Principale\",\n",
    "            \"priority\": \"Priorité\",\n",
    "            \"value_type_problem\": \"Problème\",\n",
    "            \"value_type_risk\": \"Risque\",\n",
    "            \"value_type_opportunity\": \"Opportunité\",\n",
    "            \"value_type_improvement\": \"Amélioration\",\n",
    "            \"value_priority_ultra_high\": \"Extrêmement Haute\",\n",
    "            \"value_priority_very_high\": \"Très Haute\",\n",
    "            \"value_priority_high\": \"Haute\",\n",
    "            \"value_priority_medium\": \"Moyenne\",\n",
    "            \"value_priority_low\": \"Basse\",\n",
    "            \"value_priority_very_low\": \"Très Basse\",\n",
    "            \"value_priority_ultra_low\": \"Extrêmement Basse\",\n",
    "            \"statement\": \"Énoncé\",\n",
    "            \"solution\": \"Solution\",\n",
    "            \"aspect_beneficiary\": \"Bénéficiaire\",\n",
    "            \"beneficiary\": \"Bénéficiaire\",\n",
    "            \"aspect_sponsor\": \"Sponsor\",\n",
    "            \"sponsor\": \"Sponsor\",\n",
    "            \"business_priority_alignment\": \"Alignement de Priorité d'Entreprise\",\n",
    "            \"strategic_goals_alignment\": \"Alignement aux Objectifs Stratégiques\",\n",
    "            \"subdomain\": \"Sous-domaine\",\n",
    "            \"aspect_value\": \"Valeur Commerciale\",\n",
    "            \"business_value\": \"Valeur Commerciale\",\n",
    "            \"aspect_tables\": \"Tables Impliquées\",\n",
    "            \"aspect_ai_function\": \"Fonction IA\",\n",
    "            \"aspect_analytics_technique\": \"Technique d'Analyse\",\n",
    "            \"aspect_primary_table\": \"Table Principale\",\n",
    "            \"aspect_priority\": \"Priorité\",\n",
    "            \"strategic_alignment\": \"Alignement Stratégique\",\n",
    "            \"return_on_investment\": \"Retour sur Investissement\",\n",
    "            \"reusability\": \"Réutilisabilité\",\n",
    "            \"time_to_value\": \"Délai de Valorisation\",\n",
    "            \"data_availability\": \"Disponibilité des Données\",\n",
    "            \"data_accessibility\": \"Accessibilité des Données\",\n",
    "            \"architecture_fitness\": \"Adéquation Architecturale\",\n",
    "            \"team_skills\": \"Compétences de l'Équipe\",\n",
    "            \"domain_knowledge\": \"Connaissance du Domaine\",\n",
    "            \"people_allocation\": \"Allocation du Personnel\",\n",
    "            \"budget_allocation\": \"Allocation Budgétaire\",\n",
    "            \"time_to_production\": \"Délai de Production\",\n",
    "            \"value_score\": \"Score de Valeur\",\n",
    "            \"feasibility_score\": \"Score de Faisabilité\",\n",
    "            \"priority_score\": \"Score de Priorité\",\n",
    "            \"pdf_title\": \"Cas d'Utilisation IA Stratégiques Databricks Agent Bricks\",\n",
    "            \"pdf_for\": \"Pour\",\n",
    "            \"pdf_exec_summary\": \"Résumé Exécutif\",\n",
    "            \"pdf_toc_title\": \"Domaines des Cas d'Utilisation\",\n",
    "            \"pdf_detailed_view\": \"Catalogue Détaillé des Cas d'Utilisation\",\n",
    "            \"pdf_disclaimer_title\": \"Avertissement\",\n",
    "            \"pdf_fallback_summary_p1\": \"Ce document présente {total_cases} cas d'utilisation analytiques à forte valeur identifiés pour {business_name}.\",\n",
    "            \"pdf_fallback_summary_p2\": \"Les pages suivantes fournissent une répartition détaillée de ces opportunités, classées par domaine d'activité.\",\n",
    "            \"pptx_main_title\": \"Cas d'Utilisation IA Stratégiques Databricks Agent Bricks\",\n",
    "            \"pptx_for\": \"Pour\",\n",
    "            \"pptx_disclaimer_title\": \"Avertissement\",\n",
    "            \"pptx_domain_suffix\": \"Cas d'Utilisation\",\n",
    "            \"example_results\": \"Résultats d'Exemple\",\n",
    "            \"error_no_results\": \"Impossible de générer les résultats. Vérifier le Carnet: {notebook_name} et cas d'utilisation {use_case_id}\",\n",
    "            \"input_data_original\": \"Données d'Entrée (Valeurs Originales)\",\n",
    "            \"ai_generated_output\": \"Résultats Générés par l'IA\",\n",
    "            \"column\": \"Colonne\",\n",
    "            \"value\": \"Valeur\",\n",
    "            \"executive_summary_not_available\": \"Résumé exécutif non disponible.\",\n",
    "            \"domain_summary_not_available\": \"Résumé du domaine non disponible.\",\n",
    "            \"summary_not_available\": \"Résumé non disponible.\",\n",
    "            \"value_general_improvement\": \"Amélioration Générale\",\n",
    "            \"value_reduce_cost\": \"Réduire les Coûts\",\n",
    "            \"value_increase_revenue\": \"Augmenter les Revenus\",\n",
    "            \"value_boost_productivity\": \"Améliorer la Productivité\",\n",
    "            \"value_mitigate_risk\": \"Atténuer les Risques\",\n",
    "            \"value_protect_revenue\": \"Protéger les Revenus\",\n",
    "            \"value_align_to_regulations\": \"Se Conformer aux Réglementations\",\n",
    "            \"value_improve_customer_experience\": \"Améliorer l'Expérience Client\",\n",
    "            \"value_enable_data_driven_decisions\": \"Permettre les Décisions Basées sur les Données\",\n",
    "            \"value_optimize_operations\": \"Optimiser les Opérations\",\n",
    "            \"value_empower_talent\": \"Autonomiser les Talents\",\n",
    "            \"value_enhance_experience\": \"Améliorer l'Expérience\",\n",
    "            \"value_drive_innovation\": \"Stimuler l'Innovation\",\n",
    "            \"value_achieve_esg\": \"Atteindre ESG\",\n",
    "            \"value_execute_strategy\": \"Exécuter la Stratégie\",\n",
    "            \"value_forecasting\": \"Prévision\",\n",
    "            \"value_classification\": \"Classification\",\n",
    "            \"value_anomaly_detection\": \"Détection d'Anomalies\",\n",
    "            \"value_cohort_analysis\": \"Analyse de Cohortes\",\n",
    "            \"value_segmentation\": \"Segmentation\",\n",
    "            \"value_sentiment_analysis\": \"Analyse de Sentiments\",\n",
    "            \"value_trend_analysis\": \"Analyse des Tendances\",\n",
    "            \"value_prescriptive_analytics\": \"Analytique Prescriptive\",\n",
    "            \"value_root_cause_analysis\": \"Analyse des Causes Profondes\",\n",
    "            \"value_optimization\": \"Optimisation\",\n",
    "            \"value_recommendation\": \"Recommandation\",\n",
    "            \"value_time_series_analysis\": \"Analyse de Séries Temporelles\",\n",
    "            \"value_predictive_analytics\": \"Analytique Prédictive\",\n",
    "            \"value_descriptive_analytics\": \"Analytique Descriptive\",\n",
    "        },\n",
    "        \"German\": {\n",
    "            \"main_title\": \"Databricks Agent Bricks Anwendungsfall-Generator\",\n",
    "            \"intro\": \"Dieses Notebook enthält KI-generierte Anwendungsfälle basierend auf Ihren Schemas. Nachfolgend finden Sie eine Zusammenfassung der generierten Szenarien nach Geschäftsbereich.\",\n",
    "            \"domain\": \"Geschäftsbereich\",\n",
    "            \"total\": \"Gesamtzahl der Anwendungsfälle\",\n",
    "            \"summaries\": \"Zusammenfassungen der Anwendungsfälle\",\n",
    "            \"sum_id\": \"ID\",\n",
    "            \"sum_name\": \"Name\",\n",
    "            \"sum_value\": \"Geschäftswert\",\n",
    "            \"sum_outcome\": \"Erwartetes Ergebnis\",\n",
    "            \"warning_header\": \"WARNUNG\",\n",
    "            \"warning_body\": \"Führen Sie dieses Notebook nicht aus. Es dient nur zur Demonstration und Katalogisierung. Die SQL-Abfragen sind Beispiele und erfordern möglicherweise eine Überprüfung vor der Ausführung.\",\n",
    "            \"disclaimer\": \"Dieser Inhalt wurde von KI generiert und dient nur zu Demonstrationszwecken. Alle SQL-Abfragen sind Beispiele und müssen von einem qualifizierten Ingenieur validiert werden, bevor sie in einer Produktionsumgebung verwendet werden.\",\n",
    "            \"detailed_scenarios\": \"Anwendungsfall-Details\",\n",
    "            \"aspect\": \"Aspekt\",\n",
    "            \"description\": \"Beschreibung\",\n",
    "            \"aspect_domain\": \"Geschäftsbereich\",\n",
    "            \"type\": \"Typ\",\n",
    "            \"analytics_technique\": \"Analysetechnik\",\n",
    "            \"primary_table\": \"Haupttabelle\",\n",
    "            \"priority\": \"Priorität\",\n",
    "            \"value_type_problem\": \"Problem\",\n",
    "            \"value_type_risk\": \"Risiko\",\n",
    "            \"value_type_opportunity\": \"Chance\",\n",
    "            \"value_type_improvement\": \"Verbesserung\",\n",
    "            \"value_priority_ultra_high\": \"Extrem Hoch\",\n",
    "            \"value_priority_very_high\": \"Sehr Hoch\",\n",
    "            \"value_priority_high\": \"Hoch\",\n",
    "            \"value_priority_medium\": \"Mittel\",\n",
    "            \"value_priority_low\": \"Niedrig\",\n",
    "            \"value_priority_very_low\": \"Sehr Niedrig\",\n",
    "            \"value_priority_ultra_low\": \"Extrem Niedrig\",\n",
    "            \"statement\": \"Aussage\",\n",
    "            \"solution\": \"Lösung\",\n",
    "            \"aspect_beneficiary\": \"Begünstigter\",\n",
    "            \"beneficiary\": \"Begünstigter\",\n",
    "            \"aspect_sponsor\": \"Sponsor\",\n",
    "            \"sponsor\": \"Sponsor\",\n",
    "            \"business_priority_alignment\": \"Geschäftspriorität Ausrichtung\",\n",
    "            \"strategic_goals_alignment\": \"Strategische Zielausrichtung\",\n",
    "            \"subdomain\": \"Subdomäne\",\n",
    "            \"aspect_value\": \"Geschäftswert\",\n",
    "            \"business_value\": \"Geschäftswert\",\n",
    "            \"aspect_tables\": \"Beteiligte Tabellen\",\n",
    "            \"aspect_ai_function\": \"KI-Funktion\",\n",
    "            \"aspect_analytics_technique\": \"Analysetechnik\",\n",
    "            \"aspect_primary_table\": \"Haupttabelle\",\n",
    "            \"aspect_priority\": \"Priorität\",\n",
    "            \"strategic_alignment\": \"Strategische Ausrichtung\",\n",
    "            \"return_on_investment\": \"Kapitalrendite\",\n",
    "            \"reusability\": \"Wiederverwendbarkeit\",\n",
    "            \"time_to_value\": \"Zeit bis zum Wert\",\n",
    "            \"data_availability\": \"Datenverfügbarkeit\",\n",
    "            \"data_accessibility\": \"Datenzugänglichkeit\",\n",
    "            \"architecture_fitness\": \"Architektureignung\",\n",
    "            \"team_skills\": \"Teamfähigkeiten\",\n",
    "            \"domain_knowledge\": \"Fachwissen\",\n",
    "            \"people_allocation\": \"Personalzuweisung\",\n",
    "            \"budget_allocation\": \"Budgetzuweisung\",\n",
    "            \"time_to_production\": \"Zeit bis zur Produktion\",\n",
    "            \"value_score\": \"Wertpunktzahl\",\n",
    "            \"feasibility_score\": \"Machbarkeitspunktzahl\",\n",
    "            \"priority_score\": \"Prioritätspunktzahl\",\n",
    "            \"pdf_title\": \"Databricks Agent Bricks Strategische KI-Anwendungsfälle\",\n",
    "            \"pdf_for\": \"Für\",\n",
    "            \"pdf_exec_summary\": \"Zusammenfassung\",\n",
    "            \"pdf_toc_title\": \"Anwendungsfall-Bereiche\",\n",
    "            \"pdf_detailed_view\": \"Detaillierter Anwendungsfallkatalog\",\n",
    "            \"pdf_disclaimer_title\": \"Haftungsausschluss\",\n",
    "            \"pdf_fallback_summary_p1\": \"Dieses Dokument beschreibt {total_cases} hochwertige analytische Anwendungsfälle, die für {business_name} identifiziert wurden.\",\n",
    "            \"pdf_fallback_summary_p2\": \"Die folgenden Seiten bieten eine detaillierte Aufschlüsselung dieser Möglichkeiten, kategorisiert nach Geschäftsbereich.\",\n",
    "            \"pptx_main_title\": \"Databricks Agent Bricks Strategische KI-Anwendungsfälle\",\n",
    "            \"pptx_for\": \"Für\",\n",
    "            \"pptx_disclaimer_title\": \"Haftungsausschluss\",\n",
    "            \"pptx_domain_suffix\": \"Anwendungsfälle\",\n",
    "            \"example_results\": \"Beispielergebnisse\",\n",
    "            \"error_no_results\": \"Ergebnisse konnten nicht generiert werden. Prüfen Sie Notebook: {notebook_name} und Anwendungsfall {use_case_id}\",\n",
    "            \"input_data_original\": \"Eingabedaten (Originalwerte)\",\n",
    "            \"ai_generated_output\": \"KI-generierte Ergebnisse\",\n",
    "            \"column\": \"Spalte\",\n",
    "            \"value\": \"Wert\",\n",
    "            \"executive_summary_not_available\": \"Zusammenfassung nicht verfügbar.\",\n",
    "            \"domain_summary_not_available\": \"Bereichszusammenfassung nicht verfügbar.\",\n",
    "            \"summary_not_available\": \"Zusammenfassung nicht verfügbar.\",\n",
    "            \"value_general_improvement\": \"Allgemeine Verbesserung\",\n",
    "            \"value_reduce_cost\": \"Kosten Reduzieren\",\n",
    "            \"value_increase_revenue\": \"Umsatz Steigern\",\n",
    "            \"value_boost_productivity\": \"Produktivität Steigern\",\n",
    "            \"value_mitigate_risk\": \"Risiken Mindern\",\n",
    "            \"value_protect_revenue\": \"Umsatz Schützen\",\n",
    "            \"value_align_to_regulations\": \"Vorschriften Einhalten\",\n",
    "            \"value_improve_customer_experience\": \"Kundenerlebnis Verbessern\",\n",
    "            \"value_enable_data_driven_decisions\": \"Datengestützte Entscheidungen Ermöglichen\",\n",
    "            \"value_optimize_operations\": \"Betrieb Optimieren\",\n",
    "            \"value_empower_talent\": \"Talente Fördern\",\n",
    "            \"value_enhance_experience\": \"Erlebnis Verbessern\",\n",
    "            \"value_drive_innovation\": \"Innovation Vorantreiben\",\n",
    "            \"value_achieve_esg\": \"ESG Erreichen\",\n",
    "            \"value_execute_strategy\": \"Strategie Umsetzen\",\n",
    "            \"value_forecasting\": \"Prognose\",\n",
    "            \"value_classification\": \"Klassifizierung\",\n",
    "            \"value_anomaly_detection\": \"Anomalieerkennung\",\n",
    "            \"value_cohort_analysis\": \"Kohortenanalyse\",\n",
    "            \"value_segmentation\": \"Segmentierung\",\n",
    "            \"value_sentiment_analysis\": \"Stimmungsanalyse\",\n",
    "            \"value_trend_analysis\": \"Trendanalyse\",\n",
    "            \"value_prescriptive_analytics\": \"Präskriptive Analytik\",\n",
    "            \"value_root_cause_analysis\": \"Ursachenanalyse\",\n",
    "            \"value_optimization\": \"Optimierung\",\n",
    "            \"value_recommendation\": \"Empfehlung\",\n",
    "            \"value_time_series_analysis\": \"Zeitreihenanalyse\",\n",
    "            \"value_predictive_analytics\": \"Prädiktive Analytik\",\n",
    "            \"value_descriptive_analytics\": \"Deskriptive Analytik\",\n",
    "        },\n",
    "        \"Portuguese\": {\n",
    "            \"main_title\": \"Gerador de Casos de Uso Databricks Agent Bricks\",\n",
    "            \"intro\": \"Este notebook contém casos de uso gerados por IA baseados em seus esquemas. Abaixo está um resumo dos cenários gerados por domínio de negócio.\",\n",
    "            \"domain\": \"Domínio de Negócio\",\n",
    "            \"total\": \"Total de Casos de Uso\",\n",
    "            \"summaries\": \"Resumos de Casos de Uso\",\n",
    "            \"sum_id\": \"ID\",\n",
    "            \"sum_name\": \"Nome\",\n",
    "            \"sum_value\": \"Valor de Negócio\",\n",
    "            \"sum_outcome\": \"Resultado Esperado\",\n",
    "            \"warning_header\": \"AVISO\",\n",
    "            \"warning_body\": \"Não execute este notebook. É destinado apenas para demonstração e catalogação. As consultas SQL são exemplos e podem requerer revisão antes da execução.\",\n",
    "            \"disclaimer\": \"Este conteúdo foi gerado por IA e é apenas para fins de demonstração. Todas as consultas SQL são exemplos e devem ser validadas por um engenheiro qualificado antes de serem usadas em produção.\",\n",
    "            \"detailed_scenarios\": \"Detalhes dos Casos de Uso\",\n",
    "            \"aspect\": \"Aspecto\",\n",
    "            \"description\": \"Descrição\",\n",
    "            \"aspect_domain\": \"Domínio de Negócio\",\n",
    "            \"type\": \"Tipo\",\n",
    "            \"analytics_technique\": \"Técnica de Análise\",\n",
    "            \"primary_table\": \"Tabela Principal\",\n",
    "            \"priority\": \"Prioridade\",\n",
    "            \"value_type_problem\": \"Problema\",\n",
    "            \"value_type_risk\": \"Risco\",\n",
    "            \"value_type_opportunity\": \"Oportunidade\",\n",
    "            \"value_type_improvement\": \"Melhoria\",\n",
    "            \"value_priority_ultra_high\": \"Extremamente Alta\",\n",
    "            \"value_priority_very_high\": \"Muito Alta\",\n",
    "            \"value_priority_high\": \"Alta\",\n",
    "            \"value_priority_medium\": \"Média\",\n",
    "            \"value_priority_low\": \"Baixa\",\n",
    "            \"value_priority_very_low\": \"Muito Baixa\",\n",
    "            \"value_priority_ultra_low\": \"Extremamente Baixa\",\n",
    "            \"statement\": \"Declaração\",\n",
    "            \"solution\": \"Solução\",\n",
    "            \"aspect_beneficiary\": \"Beneficiário\",\n",
    "            \"beneficiary\": \"Beneficiário\",\n",
    "            \"aspect_sponsor\": \"Patrocinador\",\n",
    "            \"sponsor\": \"Patrocinador\",\n",
    "            \"business_priority_alignment\": \"Alinhamento de Prioridade de Negócio\",\n",
    "            \"strategic_goals_alignment\": \"Alinhamento com Objetivos Estratégicos\",\n",
    "            \"subdomain\": \"Subdomínio\",\n",
    "            \"aspect_value\": \"Valor de Negócio\",\n",
    "            \"business_value\": \"Valor de Negócio\",\n",
    "            \"aspect_tables\": \"Tabelas Envolvidas\",\n",
    "            \"aspect_ai_function\": \"Função de IA\",\n",
    "            \"aspect_analytics_technique\": \"Técnica de Análise\",\n",
    "            \"aspect_primary_table\": \"Tabela Principal\",\n",
    "            \"aspect_priority\": \"Prioridade\",\n",
    "            \"strategic_alignment\": \"Alinhamento Estratégico\",\n",
    "            \"return_on_investment\": \"Retorno sobre Investimento\",\n",
    "            \"reusability\": \"Reusabilidade\",\n",
    "            \"time_to_value\": \"Tempo para Valor\",\n",
    "            \"data_availability\": \"Disponibilidade de Dados\",\n",
    "            \"data_accessibility\": \"Acessibilidade de Dados\",\n",
    "            \"architecture_fitness\": \"Adequação da Arquitetura\",\n",
    "            \"team_skills\": \"Habilidades da Equipe\",\n",
    "            \"domain_knowledge\": \"Conhecimento do Domínio\",\n",
    "            \"people_allocation\": \"Alocação de Pessoas\",\n",
    "            \"budget_allocation\": \"Alocação de Orçamento\",\n",
    "            \"time_to_production\": \"Tempo para Produção\",\n",
    "            \"value_score\": \"Pontuação de Valor\",\n",
    "            \"feasibility_score\": \"Pontuação de Viabilidade\",\n",
    "            \"priority_score\": \"Pontuação de Prioridade\",\n",
    "            \"pdf_title\": \"Casos de Uso de IA Estratégica Databricks Agent Bricks\",\n",
    "            \"pdf_for\": \"Para\",\n",
    "            \"pdf_exec_summary\": \"Resumo Executivo\",\n",
    "            \"pdf_toc_title\": \"Domínios de Casos de Uso\",\n",
    "            \"pdf_detailed_view\": \"Catálogo Detalhado de Casos de Uso\",\n",
    "            \"pdf_disclaimer_title\": \"Aviso Legal\",\n",
    "            \"pdf_fallback_summary_p1\": \"Este documento descreve {total_cases} casos de uso analíticos de alto valor identificados para {business_name}.\",\n",
    "            \"pdf_fallback_summary_p2\": \"As páginas seguintes fornecem uma análise detalhada dessas oportunidades, categorizadas por domínio de negócio.\",\n",
    "            \"pptx_main_title\": \"Casos de Uso de IA Estratégica Databricks Agent Bricks\",\n",
    "            \"pptx_for\": \"Para\",\n",
    "            \"pptx_disclaimer_title\": \"Aviso Legal\",\n",
    "            \"pptx_domain_suffix\": \"Casos de Uso\",\n",
    "            \"example_results\": \"Resultados de Exemplo\",\n",
    "            \"error_no_results\": \"Não foi possível gerar resultados. Verificar Notebook: {notebook_name} e caso de uso {use_case_id}\",\n",
    "            \"input_data_original\": \"Dados de Entrada (Valores Originais)\",\n",
    "            \"ai_generated_output\": \"Resultados Gerados por IA\",\n",
    "            \"column\": \"Coluna\",\n",
    "            \"value\": \"Valor\",\n",
    "            \"executive_summary_not_available\": \"Resumo executivo não disponível.\",\n",
    "            \"domain_summary_not_available\": \"Resumo do domínio não disponível.\",\n",
    "            \"summary_not_available\": \"Resumo não disponível.\",\n",
    "            \"value_general_improvement\": \"Melhoria Geral\",\n",
    "            \"value_reduce_cost\": \"Reduzir Custos\",\n",
    "            \"value_increase_revenue\": \"Aumentar Receita\",\n",
    "            \"value_boost_productivity\": \"Aumentar Produtividade\",\n",
    "            \"value_mitigate_risk\": \"Mitigar Riscos\",\n",
    "            \"value_protect_revenue\": \"Proteger Receita\",\n",
    "            \"value_align_to_regulations\": \"Cumprir Regulamentos\",\n",
    "            \"value_improve_customer_experience\": \"Melhorar Experiência do Cliente\",\n",
    "            \"value_enable_data_driven_decisions\": \"Habilitar Decisões Baseadas em Dados\",\n",
    "            \"value_optimize_operations\": \"Otimizar Operações\",\n",
    "            \"value_empower_talent\": \"Capacitar Talentos\",\n",
    "            \"value_enhance_experience\": \"Melhorar Experiência\",\n",
    "            \"value_drive_innovation\": \"Impulsionar Inovação\",\n",
    "            \"value_achieve_esg\": \"Alcançar ESG\",\n",
    "            \"value_execute_strategy\": \"Executar Estratégia\",\n",
    "            \"value_forecasting\": \"Previsão\",\n",
    "            \"value_classification\": \"Classificação\",\n",
    "            \"value_anomaly_detection\": \"Detecção de Anomalias\",\n",
    "            \"value_cohort_analysis\": \"Análise de Coorte\",\n",
    "            \"value_segmentation\": \"Segmentação\",\n",
    "            \"value_sentiment_analysis\": \"Análise de Sentimento\",\n",
    "            \"value_trend_analysis\": \"Análise de Tendências\",\n",
    "            \"value_prescriptive_analytics\": \"Análise Prescritiva\",\n",
    "            \"value_root_cause_analysis\": \"Análise de Causa Raiz\",\n",
    "            \"value_optimization\": \"Otimização\",\n",
    "            \"value_recommendation\": \"Recomendação\",\n",
    "            \"value_time_series_analysis\": \"Análise de Séries Temporais\",\n",
    "            \"value_predictive_analytics\": \"Análise Preditiva\",\n",
    "            \"value_descriptive_analytics\": \"Análise Descritiva\",\n",
    "        },\n",
    "        \"Italian\": {\n",
    "            \"main_title\": \"Generatore di Casi d'Uso Databricks Agent Bricks\",\n",
    "            \"intro\": \"Questo notebook contiene casi d'uso generati dall'IA basati sui tuoi schemi. Di seguito è riportato un riepilogo degli scenari generati per dominio aziendale.\",\n",
    "            \"domain\": \"Dominio Aziendale\",\n",
    "            \"total\": \"Totale Casi d'Uso\",\n",
    "            \"summaries\": \"Riepiloghi dei Casi d'Uso\",\n",
    "            \"sum_id\": \"ID\",\n",
    "            \"sum_name\": \"Nome\",\n",
    "            \"sum_value\": \"Valore Aziendale\",\n",
    "            \"sum_outcome\": \"Risultato Atteso\",\n",
    "            \"warning_header\": \"AVVERTIMENTO\",\n",
    "            \"warning_body\": \"Non eseguire questo notebook. È destinato solo a scopi dimostrativi e di catalogazione. Le query SQL sono esempi e potrebbero richiedere revisione prima dell'esecuzione.\",\n",
    "            \"disclaimer\": \"Questo contenuto è generato dall'IA e solo a scopo dimostrativo. Tutte le query SQL sono esempi e devono essere validate da un ingegnere qualificato prima dell'uso in produzione.\",\n",
    "            \"detailed_scenarios\": \"Dettagli dei Casi d'Uso\",\n",
    "            \"aspect\": \"Aspetto\",\n",
    "            \"description\": \"Descrizione\",\n",
    "            \"aspect_domain\": \"Dominio Aziendale\",\n",
    "            \"type\": \"Tipo\",\n",
    "            \"analytics_technique\": \"Tecnica di Analisi\",\n",
    "            \"primary_table\": \"Tabella Principale\",\n",
    "            \"priority\": \"Priorità\",\n",
    "            \"value_type_problem\": \"Problema\",\n",
    "            \"value_type_risk\": \"Rischio\",\n",
    "            \"value_type_opportunity\": \"Opportunità\",\n",
    "            \"value_type_improvement\": \"Miglioramento\",\n",
    "            \"value_priority_ultra_high\": \"Estremamente Alta\",\n",
    "            \"value_priority_very_high\": \"Molto Alta\",\n",
    "            \"value_priority_high\": \"Alta\",\n",
    "            \"value_priority_medium\": \"Media\",\n",
    "            \"value_priority_low\": \"Bassa\",\n",
    "            \"value_priority_very_low\": \"Molto Bassa\",\n",
    "            \"value_priority_ultra_low\": \"Estremamente Bassa\",\n",
    "            \"statement\": \"Dichiarazione\",\n",
    "            \"solution\": \"Soluzione\",\n",
    "            \"aspect_beneficiary\": \"Beneficiario\",\n",
    "            \"beneficiary\": \"Beneficiario\",\n",
    "            \"aspect_sponsor\": \"Sponsor\",\n",
    "            \"sponsor\": \"Sponsor\",\n",
    "            \"business_priority_alignment\": \"Allineamento Priorità Aziendale\",\n",
    "            \"strategic_goals_alignment\": \"Allineamento agli Obiettivi Strategici\",\n",
    "            \"subdomain\": \"Sottodominio\",\n",
    "            \"aspect_value\": \"Valore Aziendale\",\n",
    "            \"business_value\": \"Valore Aziendale\",\n",
    "            \"aspect_tables\": \"Tabelle Coinvolte\",\n",
    "            \"aspect_ai_function\": \"Funzione IA\",\n",
    "            \"aspect_analytics_technique\": \"Tecnica di Analisi\",\n",
    "            \"aspect_primary_table\": \"Tabella Principale\",\n",
    "            \"aspect_priority\": \"Priorità\",\n",
    "            \"strategic_alignment\": \"Allineamento Strategico\",\n",
    "            \"return_on_investment\": \"Ritorno sull'Investimento\",\n",
    "            \"reusability\": \"Riutilizzabilità\",\n",
    "            \"time_to_value\": \"Tempo al Valore\",\n",
    "            \"data_availability\": \"Disponibilità dei Dati\",\n",
    "            \"data_accessibility\": \"Accessibilità dei Dati\",\n",
    "            \"architecture_fitness\": \"Idoneità dell'Architettura\",\n",
    "            \"team_skills\": \"Competenze del Team\",\n",
    "            \"domain_knowledge\": \"Conoscenza del Dominio\",\n",
    "            \"people_allocation\": \"Allocazione del Personale\",\n",
    "            \"budget_allocation\": \"Allocazione del Budget\",\n",
    "            \"time_to_production\": \"Tempo alla Produzione\",\n",
    "            \"value_score\": \"Punteggio di Valore\",\n",
    "            \"feasibility_score\": \"Punteggio di Fattibilità\",\n",
    "            \"priority_score\": \"Punteggio di Priorità\",\n",
    "            \"pdf_title\": \"Casi d'Uso IA Strategici Databricks Agent Bricks\",\n",
    "            \"pdf_for\": \"Per\",\n",
    "            \"pdf_exec_summary\": \"Riepilogo Esecutivo\",\n",
    "            \"pdf_toc_title\": \"Domini dei Casi d'Uso\",\n",
    "            \"pdf_detailed_view\": \"Catalogo Dettagliato dei Casi d'Uso\",\n",
    "            \"pdf_disclaimer_title\": \"Disclaimer\",\n",
    "            \"pdf_fallback_summary_p1\": \"Questo documento descrive {total_cases} casi d'uso analitici ad alto valore identificati per {business_name}.\",\n",
    "            \"pdf_fallback_summary_p2\": \"Le pagine seguenti forniscono un'analisi dettagliata di queste opportunità, categorizzate per dominio aziendale.\",\n",
    "            \"pptx_main_title\": \"Casi d'Uso IA Strategici Databricks Agent Bricks\",\n",
    "            \"pptx_for\": \"Per\",\n",
    "            \"pptx_disclaimer_title\": \"Disclaimer\",\n",
    "            \"pptx_domain_suffix\": \"Casi d'Uso\",\n",
    "            \"example_results\": \"Risultati di Esempio\",\n",
    "            \"error_no_results\": \"Impossibile generare i risultati. Verificare Notebook: {notebook_name} e caso d'uso {use_case_id}\",\n",
    "            \"input_data_original\": \"Dati di Input (Valori Originali)\",\n",
    "            \"ai_generated_output\": \"Risultati Generati dall'IA\",\n",
    "            \"column\": \"Colonna\",\n",
    "            \"value\": \"Valore\",\n",
    "            \"executive_summary_not_available\": \"Riepilogo esecutivo non disponibile.\",\n",
    "            \"domain_summary_not_available\": \"Riepilogo del dominio non disponibile.\",\n",
    "            \"summary_not_available\": \"Riepilogo non disponibile.\",\n",
    "            \"value_general_improvement\": \"Miglioramento Generale\",\n",
    "            \"value_reduce_cost\": \"Ridurre i Costi\",\n",
    "            \"value_increase_revenue\": \"Aumentare i Ricavi\",\n",
    "            \"value_boost_productivity\": \"Aumentare la Produttività\",\n",
    "            \"value_mitigate_risk\": \"Mitigare i Rischi\",\n",
    "            \"value_protect_revenue\": \"Proteggere i Ricavi\",\n",
    "            \"value_align_to_regulations\": \"Conformarsi alle Normative\",\n",
    "            \"value_improve_customer_experience\": \"Migliorare l'Esperienza del Cliente\",\n",
    "            \"value_enable_data_driven_decisions\": \"Abilitare Decisioni Basate sui Dati\",\n",
    "            \"value_optimize_operations\": \"Ottimizzare le Operazioni\",\n",
    "            \"value_empower_talent\": \"Valorizzare i Talenti\",\n",
    "            \"value_enhance_experience\": \"Migliorare l'Esperienza\",\n",
    "            \"value_drive_innovation\": \"Promuovere l'Innovazione\",\n",
    "            \"value_achieve_esg\": \"Raggiungere ESG\",\n",
    "            \"value_execute_strategy\": \"Eseguire la Strategia\",\n",
    "            \"value_forecasting\": \"Previsione\",\n",
    "            \"value_classification\": \"Classificazione\",\n",
    "            \"value_anomaly_detection\": \"Rilevamento Anomalie\",\n",
    "            \"value_cohort_analysis\": \"Analisi di Coorte\",\n",
    "            \"value_segmentation\": \"Segmentazione\",\n",
    "            \"value_sentiment_analysis\": \"Analisi del Sentimento\",\n",
    "            \"value_trend_analysis\": \"Analisi delle Tendenze\",\n",
    "            \"value_prescriptive_analytics\": \"Analisi Prescrittiva\",\n",
    "            \"value_root_cause_analysis\": \"Analisi delle Cause Profonde\",\n",
    "            \"value_optimization\": \"Ottimizzazione\",\n",
    "            \"value_recommendation\": \"Raccomandazione\",\n",
    "            \"value_time_series_analysis\": \"Analisi delle Serie Temporali\",\n",
    "            \"value_predictive_analytics\": \"Analisi Predittiva\",\n",
    "            \"value_descriptive_analytics\": \"Analisi Descrittiva\",\n",
    "        },\n",
    "        \"Chinese (Mandarin)\": {\n",
    "            \"main_title\": \"Databricks Agent Bricks 用例生成器\",\n",
    "            \"intro\": \"本笔记本包含基于您的架构由AI生成的用例。以下是按业务领域生成的场景摘要。\",\n",
    "            \"domain\": \"业务领域\",\n",
    "            \"total\": \"用例总数\",\n",
    "            \"summaries\": \"用例摘要\",\n",
    "            \"sum_id\": \"ID\",\n",
    "            \"sum_name\": \"名称\",\n",
    "            \"sum_value\": \"商业价值\",\n",
    "            \"sum_outcome\": \"预期结果\",\n",
    "            \"warning_header\": \"警告\",\n",
    "            \"warning_body\": \"请勿运行此笔记本。它仅用于演示和编目目的。SQL查询是示例，可能需要在执行前进行审核。\",\n",
    "            \"disclaimer\": \"此内容由AI生成，仅供演示目的。所有SQL查询都是示例，必须由合格工程师验证后才能在生产环境中使用。\",\n",
    "            \"detailed_scenarios\": \"用例详情\",\n",
    "            \"aspect\": \"方面\",\n",
    "            \"description\": \"描述\",\n",
    "            \"aspect_domain\": \"业务领域\",\n",
    "            \"type\": \"类型\",\n",
    "            \"analytics_technique\": \"分析技术\",\n",
    "            \"primary_table\": \"主表\",\n",
    "            \"priority\": \"优先级\",\n",
    "            \"value_type_problem\": \"问题\",\n",
    "            \"value_type_risk\": \"风险\",\n",
    "            \"value_type_opportunity\": \"机会\",\n",
    "            \"value_type_improvement\": \"改进\",\n",
    "            \"value_priority_ultra_high\": \"极高\",\n",
    "            \"value_priority_very_high\": \"非常高\",\n",
    "            \"value_priority_high\": \"高\",\n",
    "            \"value_priority_medium\": \"中等\",\n",
    "            \"value_priority_low\": \"低\",\n",
    "            \"value_priority_very_low\": \"非常低\",\n",
    "            \"value_priority_ultra_low\": \"极低\",\n",
    "            \"statement\": \"陈述\",\n",
    "            \"solution\": \"解决方案\",\n",
    "            \"aspect_beneficiary\": \"受益者\",\n",
    "            \"beneficiary\": \"受益者\",\n",
    "            \"aspect_sponsor\": \"发起人\",\n",
    "            \"sponsor\": \"发起人\",\n",
    "            \"business_priority_alignment\": \"业务优先级对齐\",\n",
    "            \"strategic_goals_alignment\": \"战略目标对齐\",\n",
    "            \"subdomain\": \"子领域\",\n",
    "            \"aspect_value\": \"商业价值\",\n",
    "            \"business_value\": \"商业价值\",\n",
    "            \"aspect_tables\": \"涉及的表\",\n",
    "            \"aspect_ai_function\": \"AI功能\",\n",
    "            \"aspect_analytics_technique\": \"分析技术\",\n",
    "            \"aspect_primary_table\": \"主表\",\n",
    "            \"aspect_priority\": \"优先级\",\n",
    "            \"strategic_alignment\": \"战略对齐\",\n",
    "            \"return_on_investment\": \"投资回报率\",\n",
    "            \"reusability\": \"可重用性\",\n",
    "            \"time_to_value\": \"价值实现时间\",\n",
    "            \"data_availability\": \"数据可用性\",\n",
    "            \"data_accessibility\": \"数据可访问性\",\n",
    "            \"architecture_fitness\": \"架构适配性\",\n",
    "            \"team_skills\": \"团队技能\",\n",
    "            \"domain_knowledge\": \"领域知识\",\n",
    "            \"people_allocation\": \"人员分配\",\n",
    "            \"budget_allocation\": \"预算分配\",\n",
    "            \"time_to_production\": \"投产时间\",\n",
    "            \"value_score\": \"价值分数\",\n",
    "            \"feasibility_score\": \"可行性分数\",\n",
    "            \"priority_score\": \"优先级分数\",\n",
    "            \"pdf_title\": \"Databricks Agent Bricks 战略AI用例\",\n",
    "            \"pdf_for\": \"为\",\n",
    "            \"pdf_exec_summary\": \"执行摘要\",\n",
    "            \"pdf_toc_title\": \"用例领域\",\n",
    "            \"pdf_detailed_view\": \"详细用例目录\",\n",
    "            \"pdf_disclaimer_title\": \"免责声明\",\n",
    "            \"pdf_fallback_summary_p1\": \"本文档概述了为{business_name}识别的{total_cases}个高价值分析用例。\",\n",
    "            \"pdf_fallback_summary_p2\": \"以下页面按业务领域分类提供这些机会的详细分析。\",\n",
    "            \"pptx_main_title\": \"Databricks Agent Bricks 战略AI用例\",\n",
    "            \"pptx_for\": \"为\",\n",
    "            \"pptx_disclaimer_title\": \"免责声明\",\n",
    "            \"pptx_domain_suffix\": \"用例\",\n",
    "            \"example_results\": \"示例结果\",\n",
    "            \"error_no_results\": \"无法生成结果。请检查笔记本：{notebook_name}和用例{use_case_id}\",\n",
    "            \"input_data_original\": \"输入数据（原始值）\",\n",
    "            \"ai_generated_output\": \"AI生成结果\",\n",
    "            \"column\": \"列\",\n",
    "            \"value\": \"值\",\n",
    "            \"executive_summary_not_available\": \"执行摘要不可用。\",\n",
    "            \"domain_summary_not_available\": \"领域摘要不可用。\",\n",
    "            \"summary_not_available\": \"摘要不可用。\",\n",
    "            \"value_general_improvement\": \"一般改进\",\n",
    "            \"value_reduce_cost\": \"降低成本\",\n",
    "            \"value_increase_revenue\": \"增加收入\",\n",
    "            \"value_boost_productivity\": \"提高生产力\",\n",
    "            \"value_mitigate_risk\": \"降低风险\",\n",
    "            \"value_protect_revenue\": \"保护收入\",\n",
    "            \"value_align_to_regulations\": \"符合法规\",\n",
    "            \"value_improve_customer_experience\": \"改善客户体验\",\n",
    "            \"value_enable_data_driven_decisions\": \"实现数据驱动决策\",\n",
    "            \"value_optimize_operations\": \"优化运营\",\n",
    "            \"value_empower_talent\": \"赋能人才\",\n",
    "            \"value_enhance_experience\": \"提升体验\",\n",
    "            \"value_drive_innovation\": \"推动创新\",\n",
    "            \"value_achieve_esg\": \"实现ESG\",\n",
    "            \"value_execute_strategy\": \"执行战略\",\n",
    "            \"value_forecasting\": \"预测\",\n",
    "            \"value_classification\": \"分类\",\n",
    "            \"value_anomaly_detection\": \"异常检测\",\n",
    "            \"value_cohort_analysis\": \"队列分析\",\n",
    "            \"value_segmentation\": \"细分\",\n",
    "            \"value_sentiment_analysis\": \"情感分析\",\n",
    "            \"value_trend_analysis\": \"趋势分析\",\n",
    "            \"value_prescriptive_analytics\": \"规范性分析\",\n",
    "            \"value_root_cause_analysis\": \"根因分析\",\n",
    "            \"value_optimization\": \"优化\",\n",
    "            \"value_recommendation\": \"推荐\",\n",
    "            \"value_time_series_analysis\": \"时间序列分析\",\n",
    "            \"value_predictive_analytics\": \"预测分析\",\n",
    "            \"value_descriptive_analytics\": \"描述性分析\",\n",
    "        },\n",
    "        \"Japanese\": {\n",
    "            \"main_title\": \"Databricks Agent Bricks ユースケースジェネレーター\",\n",
    "            \"intro\": \"このノートブックには、スキーマに基づいてAIが生成したユースケースが含まれています。以下は、ビジネスドメイン別に生成されたシナリオの概要です。\",\n",
    "            \"domain\": \"ビジネスドメイン\",\n",
    "            \"total\": \"ユースケース総数\",\n",
    "            \"summaries\": \"ユースケース概要\",\n",
    "            \"sum_id\": \"ID\",\n",
    "            \"sum_name\": \"名前\",\n",
    "            \"sum_value\": \"ビジネス価値\",\n",
    "            \"sum_outcome\": \"期待される成果\",\n",
    "            \"warning_header\": \"警告\",\n",
    "            \"warning_body\": \"このノートブックを実行しないでください。デモンストレーションとカタログ作成のみを目的としています。SQLクエリは例であり、実行前にレビューが必要な場合があります。\",\n",
    "            \"disclaimer\": \"このコンテンツはAIによって生成されており、デモンストレーション目的のみです。すべてのSQLクエリは例であり、本番環境で使用する前に資格のあるエンジニアによる検証が必要です。\",\n",
    "            \"detailed_scenarios\": \"ユースケースの詳細\",\n",
    "            \"aspect\": \"側面\",\n",
    "            \"description\": \"説明\",\n",
    "            \"aspect_domain\": \"ビジネスドメイン\",\n",
    "            \"type\": \"タイプ\",\n",
    "            \"analytics_technique\": \"分析技術\",\n",
    "            \"primary_table\": \"主要テーブル\",\n",
    "            \"priority\": \"優先度\",\n",
    "            \"value_type_problem\": \"問題\",\n",
    "            \"value_type_risk\": \"リスク\",\n",
    "            \"value_type_opportunity\": \"機会\",\n",
    "            \"value_type_improvement\": \"改善\",\n",
    "            \"value_priority_ultra_high\": \"極めて高い\",\n",
    "            \"value_priority_very_high\": \"非常に高い\",\n",
    "            \"value_priority_high\": \"高い\",\n",
    "            \"value_priority_medium\": \"中程度\",\n",
    "            \"value_priority_low\": \"低い\",\n",
    "            \"value_priority_very_low\": \"非常に低い\",\n",
    "            \"value_priority_ultra_low\": \"極めて低い\",\n",
    "            \"statement\": \"ステートメント\",\n",
    "            \"solution\": \"ソリューション\",\n",
    "            \"aspect_beneficiary\": \"受益者\",\n",
    "            \"beneficiary\": \"受益者\",\n",
    "            \"aspect_sponsor\": \"スポンサー\",\n",
    "            \"sponsor\": \"スポンサー\",\n",
    "            \"business_priority_alignment\": \"ビジネス優先度整合性\",\n",
    "            \"strategic_goals_alignment\": \"戦略目標との整合性\",\n",
    "            \"subdomain\": \"サブドメイン\",\n",
    "            \"aspect_value\": \"ビジネス価値\",\n",
    "            \"business_value\": \"ビジネス価値\",\n",
    "            \"aspect_tables\": \"関連テーブル\",\n",
    "            \"aspect_ai_function\": \"AI機能\",\n",
    "            \"aspect_analytics_technique\": \"分析技術\",\n",
    "            \"aspect_primary_table\": \"主要テーブル\",\n",
    "            \"aspect_priority\": \"優先度\",\n",
    "            \"strategic_alignment\": \"戦略的整合性\",\n",
    "            \"return_on_investment\": \"投資収益率\",\n",
    "            \"reusability\": \"再利用性\",\n",
    "            \"time_to_value\": \"価値実現までの時間\",\n",
    "            \"data_availability\": \"データ可用性\",\n",
    "            \"data_accessibility\": \"データアクセシビリティ\",\n",
    "            \"architecture_fitness\": \"アーキテクチャ適合性\",\n",
    "            \"team_skills\": \"チームスキル\",\n",
    "            \"domain_knowledge\": \"ドメイン知識\",\n",
    "            \"people_allocation\": \"人員配置\",\n",
    "            \"budget_allocation\": \"予算配分\",\n",
    "            \"time_to_production\": \"本番化までの時間\",\n",
    "            \"value_score\": \"価値スコア\",\n",
    "            \"feasibility_score\": \"実現可能性スコア\",\n",
    "            \"priority_score\": \"優先度スコア\",\n",
    "            \"pdf_title\": \"Databricks Agent Bricks 戦略的AIユースケース\",\n",
    "            \"pdf_for\": \"対象\",\n",
    "            \"pdf_exec_summary\": \"エグゼクティブサマリー\",\n",
    "            \"pdf_toc_title\": \"ユースケースドメイン\",\n",
    "            \"pdf_detailed_view\": \"詳細ユースケースカタログ\",\n",
    "            \"pdf_disclaimer_title\": \"免責事項\",\n",
    "            \"pdf_fallback_summary_p1\": \"本ドキュメントは{business_name}向けに特定された{total_cases}件の高価値分析ユースケースを概説しています。\",\n",
    "            \"pdf_fallback_summary_p2\": \"以下のページでは、ビジネスドメイン別に分類されたこれらの機会の詳細な分析を提供します。\",\n",
    "            \"pptx_main_title\": \"Databricks Agent Bricks 戦略的AIユースケース\",\n",
    "            \"pptx_for\": \"対象\",\n",
    "            \"pptx_disclaimer_title\": \"免責事項\",\n",
    "            \"pptx_domain_suffix\": \"ユースケース\",\n",
    "            \"example_results\": \"サンプル結果\",\n",
    "            \"error_no_results\": \"結果を生成できませんでした。ノートブック: {notebook_name} とユースケース {use_case_id} を確認してください\",\n",
    "            \"input_data_original\": \"入力データ（元の値）\",\n",
    "            \"ai_generated_output\": \"AI生成結果\",\n",
    "            \"column\": \"列\",\n",
    "            \"value\": \"値\",\n",
    "            \"executive_summary_not_available\": \"エグゼクティブサマリーは利用できません。\",\n",
    "            \"domain_summary_not_available\": \"ドメインサマリーは利用できません。\",\n",
    "            \"summary_not_available\": \"サマリーは利用できません。\",\n",
    "            \"value_general_improvement\": \"一般的な改善\",\n",
    "            \"value_reduce_cost\": \"コスト削減\",\n",
    "            \"value_increase_revenue\": \"収益増加\",\n",
    "            \"value_boost_productivity\": \"生産性向上\",\n",
    "            \"value_mitigate_risk\": \"リスク軽減\",\n",
    "            \"value_protect_revenue\": \"収益保護\",\n",
    "            \"value_align_to_regulations\": \"規制遵守\",\n",
    "            \"value_improve_customer_experience\": \"顧客体験の改善\",\n",
    "            \"value_enable_data_driven_decisions\": \"データ駆動型意思決定の実現\",\n",
    "            \"value_optimize_operations\": \"業務最適化\",\n",
    "            \"value_empower_talent\": \"人材育成\",\n",
    "            \"value_enhance_experience\": \"体験向上\",\n",
    "            \"value_drive_innovation\": \"イノベーション推進\",\n",
    "            \"value_achieve_esg\": \"ESG達成\",\n",
    "            \"value_execute_strategy\": \"戦略実行\",\n",
    "            \"value_forecasting\": \"予測\",\n",
    "            \"value_classification\": \"分類\",\n",
    "            \"value_anomaly_detection\": \"異常検知\",\n",
    "            \"value_cohort_analysis\": \"コホート分析\",\n",
    "            \"value_segmentation\": \"セグメンテーション\",\n",
    "            \"value_sentiment_analysis\": \"感情分析\",\n",
    "            \"value_trend_analysis\": \"トレンド分析\",\n",
    "            \"value_prescriptive_analytics\": \"処方的分析\",\n",
    "            \"value_root_cause_analysis\": \"根本原因分析\",\n",
    "            \"value_optimization\": \"最適化\",\n",
    "            \"value_recommendation\": \"レコメンデーション\",\n",
    "            \"value_time_series_analysis\": \"時系列分析\",\n",
    "            \"value_predictive_analytics\": \"予測分析\",\n",
    "            \"value_descriptive_analytics\": \"記述的分析\",\n",
    "        },\n",
    "        \"Korean\": {\n",
    "            \"main_title\": \"Databricks Agent Bricks 유스케이스 생성기\",\n",
    "            \"intro\": \"이 노트북에는 스키마를 기반으로 AI가 생성한 유스케이스가 포함되어 있습니다. 아래는 비즈니스 도메인별 생성된 시나리오 요약입니다.\",\n",
    "            \"domain\": \"비즈니스 도메인\",\n",
    "            \"total\": \"총 유스케이스\",\n",
    "            \"summaries\": \"유스케이스 요약\",\n",
    "            \"sum_id\": \"ID\",\n",
    "            \"sum_name\": \"이름\",\n",
    "            \"sum_value\": \"비즈니스 가치\",\n",
    "            \"sum_outcome\": \"예상 결과\",\n",
    "            \"warning_header\": \"경고\",\n",
    "            \"warning_body\": \"이 노트북을 실행하지 마십시오. 데모 및 카탈로그 작성 목적으로만 사용됩니다. SQL 쿼리는 예시이며 실행 전 검토가 필요할 수 있습니다.\",\n",
    "            \"disclaimer\": \"이 콘텐츠는 AI가 생성한 것이며 데모 목적으로만 사용됩니다. 모든 SQL 쿼리는 예시이며 프로덕션 환경에서 사용하기 전에 자격을 갖춘 엔지니어의 검증이 필요합니다.\",\n",
    "            \"detailed_scenarios\": \"유스케이스 세부정보\",\n",
    "            \"aspect\": \"측면\",\n",
    "            \"description\": \"설명\",\n",
    "            \"aspect_domain\": \"비즈니스 도메인\",\n",
    "            \"type\": \"유형\",\n",
    "            \"analytics_technique\": \"분석 기법\",\n",
    "            \"primary_table\": \"주요 테이블\",\n",
    "            \"priority\": \"우선순위\",\n",
    "            \"value_type_problem\": \"문제\",\n",
    "            \"value_type_risk\": \"위험\",\n",
    "            \"value_type_opportunity\": \"기회\",\n",
    "            \"value_type_improvement\": \"개선\",\n",
    "            \"value_priority_ultra_high\": \"초고\",\n",
    "            \"value_priority_very_high\": \"매우 높음\",\n",
    "            \"value_priority_high\": \"높음\",\n",
    "            \"value_priority_medium\": \"보통\",\n",
    "            \"value_priority_low\": \"낮음\",\n",
    "            \"value_priority_very_low\": \"매우 낮음\",\n",
    "            \"value_priority_ultra_low\": \"초저\",\n",
    "            \"statement\": \"설명\",\n",
    "            \"solution\": \"솔루션\",\n",
    "            \"aspect_beneficiary\": \"수혜자\",\n",
    "            \"beneficiary\": \"수혜자\",\n",
    "            \"aspect_sponsor\": \"후원자\",\n",
    "            \"sponsor\": \"후원자\",\n",
    "            \"business_priority_alignment\": \"비즈니스 우선순위 정렬\",\n",
    "            \"strategic_goals_alignment\": \"전략 목표 정렬\",\n",
    "            \"subdomain\": \"하위 도메인\",\n",
    "            \"aspect_value\": \"비즈니스 가치\",\n",
    "            \"business_value\": \"비즈니스 가치\",\n",
    "            \"aspect_tables\": \"관련 테이블\",\n",
    "            \"aspect_ai_function\": \"AI 기능\",\n",
    "            \"aspect_analytics_technique\": \"분석 기법\",\n",
    "            \"aspect_primary_table\": \"주요 테이블\",\n",
    "            \"aspect_priority\": \"우선순위\",\n",
    "            \"strategic_alignment\": \"전략적 정렬\",\n",
    "            \"return_on_investment\": \"투자 수익률\",\n",
    "            \"reusability\": \"재사용성\",\n",
    "            \"time_to_value\": \"가치 실현 시간\",\n",
    "            \"data_availability\": \"데이터 가용성\",\n",
    "            \"data_accessibility\": \"데이터 접근성\",\n",
    "            \"architecture_fitness\": \"아키텍처 적합성\",\n",
    "            \"team_skills\": \"팀 역량\",\n",
    "            \"domain_knowledge\": \"도메인 지식\",\n",
    "            \"people_allocation\": \"인력 배치\",\n",
    "            \"budget_allocation\": \"예산 배분\",\n",
    "            \"time_to_production\": \"프로덕션까지 시간\",\n",
    "            \"value_score\": \"가치 점수\",\n",
    "            \"feasibility_score\": \"실현 가능성 점수\",\n",
    "            \"priority_score\": \"우선순위 점수\",\n",
    "            \"pdf_title\": \"Databricks Agent Bricks 전략적 AI 유스케이스\",\n",
    "            \"pdf_for\": \"대상\",\n",
    "            \"pdf_exec_summary\": \"경영진 요약\",\n",
    "            \"pdf_toc_title\": \"유스케이스 도메인\",\n",
    "            \"pdf_detailed_view\": \"상세 유스케이스 카탈로그\",\n",
    "            \"pdf_disclaimer_title\": \"면책조항\",\n",
    "            \"pdf_fallback_summary_p1\": \"이 문서는 {business_name}을 위해 식별된 {total_cases}개의 고가치 분석 유스케이스를 설명합니다.\",\n",
    "            \"pdf_fallback_summary_p2\": \"다음 페이지에서는 비즈니스 도메인별로 분류된 이러한 기회의 상세 분석을 제공합니다.\",\n",
    "            \"pptx_main_title\": \"Databricks Agent Bricks 전략적 AI 유스케이스\",\n",
    "            \"pptx_for\": \"대상\",\n",
    "            \"pptx_disclaimer_title\": \"면책조항\",\n",
    "            \"pptx_domain_suffix\": \"유스케이스\",\n",
    "            \"example_results\": \"예시 결과\",\n",
    "            \"error_no_results\": \"결과를 생성할 수 없습니다. 노트북: {notebook_name} 및 유스케이스 {use_case_id}를 확인하세요\",\n",
    "            \"input_data_original\": \"입력 데이터 (원본 값)\",\n",
    "            \"ai_generated_output\": \"AI 생성 결과\",\n",
    "            \"column\": \"열\",\n",
    "            \"value\": \"값\",\n",
    "            \"executive_summary_not_available\": \"경영진 요약을 사용할 수 없습니다.\",\n",
    "            \"domain_summary_not_available\": \"도메인 요약을 사용할 수 없습니다.\",\n",
    "            \"summary_not_available\": \"요약을 사용할 수 없습니다.\",\n",
    "            \"value_general_improvement\": \"일반 개선\",\n",
    "            \"value_reduce_cost\": \"비용 절감\",\n",
    "            \"value_increase_revenue\": \"수익 증대\",\n",
    "            \"value_boost_productivity\": \"생산성 향상\",\n",
    "            \"value_mitigate_risk\": \"위험 완화\",\n",
    "            \"value_protect_revenue\": \"수익 보호\",\n",
    "            \"value_align_to_regulations\": \"규정 준수\",\n",
    "            \"value_improve_customer_experience\": \"고객 경험 개선\",\n",
    "            \"value_enable_data_driven_decisions\": \"데이터 기반 의사결정 지원\",\n",
    "            \"value_optimize_operations\": \"운영 최적화\",\n",
    "            \"value_empower_talent\": \"인재 역량 강화\",\n",
    "            \"value_enhance_experience\": \"경험 향상\",\n",
    "            \"value_drive_innovation\": \"혁신 추진\",\n",
    "            \"value_achieve_esg\": \"ESG 달성\",\n",
    "            \"value_execute_strategy\": \"전략 실행\",\n",
    "            \"value_forecasting\": \"예측\",\n",
    "            \"value_classification\": \"분류\",\n",
    "            \"value_anomaly_detection\": \"이상 탐지\",\n",
    "            \"value_cohort_analysis\": \"코호트 분석\",\n",
    "            \"value_segmentation\": \"세분화\",\n",
    "            \"value_sentiment_analysis\": \"감정 분석\",\n",
    "            \"value_trend_analysis\": \"추세 분석\",\n",
    "            \"value_prescriptive_analytics\": \"처방적 분석\",\n",
    "            \"value_root_cause_analysis\": \"근본 원인 분석\",\n",
    "            \"value_optimization\": \"최적화\",\n",
    "            \"value_recommendation\": \"추천\",\n",
    "            \"value_time_series_analysis\": \"시계열 분석\",\n",
    "            \"value_predictive_analytics\": \"예측 분석\",\n",
    "            \"value_descriptive_analytics\": \"기술적 분석\",\n",
    "        },\n",
    "        \"Hindi\": {\n",
    "            \"main_title\": \"Databricks Agent Bricks उपयोग केस जनरेटर\",\n",
    "            \"intro\": \"इस नोटबुक में आपके स्कीमा के आधार पर AI-जनित उपयोग केस शामिल हैं। नीचे व्यापार डोमेन द्वारा जनित परिदृश्यों का सारांश है।\",\n",
    "            \"domain\": \"व्यापार डोमेन\",\n",
    "            \"total\": \"कुल उपयोग केस\",\n",
    "            \"summaries\": \"उपयोग केस सारांश\",\n",
    "            \"sum_id\": \"ID\",\n",
    "            \"sum_name\": \"नाम\",\n",
    "            \"sum_value\": \"व्यापारिक मूल्य\",\n",
    "            \"sum_outcome\": \"अपेक्षित परिणाम\",\n",
    "            \"warning_header\": \"चेतावनी\",\n",
    "            \"warning_body\": \"इस नोटबुक को न चलाएं। यह केवल प्रदर्शन और कैटलॉगिंग उद्देश्यों के लिए है। SQL क्वेरी उदाहरण हैं और निष्पादन से पहले समीक्षा की आवश्यकता हो सकती है।\",\n",
    "            \"disclaimer\": \"यह सामग्री AI-जनित है और केवल प्रदर्शन उद्देश्यों के लिए है। सभी SQL क्वेरी उदाहरण हैं और उत्पादन वातावरण में उपयोग करने से पहले एक योग्य इंजीनियर द्वारा मान्य किया जाना चाहिए।\",\n",
    "            \"detailed_scenarios\": \"उपयोग केस विवरण\",\n",
    "            \"aspect\": \"पहलू\",\n",
    "            \"description\": \"विवरण\",\n",
    "            \"aspect_domain\": \"व्यापार डोमेन\",\n",
    "            \"type\": \"प्रकार\",\n",
    "            \"analytics_technique\": \"विश्लेषण तकनीक\",\n",
    "            \"primary_table\": \"प्राथमिक तालिका\",\n",
    "            \"priority\": \"प्राथमिकता\",\n",
    "            \"value_type_problem\": \"समस्या\",\n",
    "            \"value_type_risk\": \"जोखिम\",\n",
    "            \"value_type_opportunity\": \"अवसर\",\n",
    "            \"value_type_improvement\": \"सुधार\",\n",
    "            \"value_priority_ultra_high\": \"अत्यधिक उच्च\",\n",
    "            \"value_priority_very_high\": \"बहुत उच्च\",\n",
    "            \"value_priority_high\": \"उच्च\",\n",
    "            \"value_priority_medium\": \"मध्यम\",\n",
    "            \"value_priority_low\": \"कम\",\n",
    "            \"value_priority_very_low\": \"बहुत कम\",\n",
    "            \"value_priority_ultra_low\": \"अत्यधिक कम\",\n",
    "            \"statement\": \"कथन\",\n",
    "            \"solution\": \"समाधान\",\n",
    "            \"aspect_beneficiary\": \"लाभार्थी\",\n",
    "            \"beneficiary\": \"लाभार्थी\",\n",
    "            \"aspect_sponsor\": \"प्रायोजक\",\n",
    "            \"sponsor\": \"प्रायोजक\",\n",
    "            \"business_priority_alignment\": \"व्यापार प्राथमिकता संरेखण\",\n",
    "            \"strategic_goals_alignment\": \"रणनीतिक लक्ष्य संरेखण\",\n",
    "            \"subdomain\": \"उप-डोमेन\",\n",
    "            \"aspect_value\": \"व्यापारिक मूल्य\",\n",
    "            \"business_value\": \"व्यापारिक मूल्य\",\n",
    "            \"aspect_tables\": \"शामिल तालिकाएं\",\n",
    "            \"aspect_ai_function\": \"AI फ़ंक्शन\",\n",
    "            \"aspect_analytics_technique\": \"विश्लेषण तकनीक\",\n",
    "            \"aspect_primary_table\": \"प्राथमिक तालिका\",\n",
    "            \"aspect_priority\": \"प्राथमिकता\",\n",
    "            \"strategic_alignment\": \"रणनीतिक संरेखण\",\n",
    "            \"return_on_investment\": \"निवेश पर प्रतिफल\",\n",
    "            \"reusability\": \"पुन: प्रयोज्यता\",\n",
    "            \"time_to_value\": \"मूल्य तक समय\",\n",
    "            \"data_availability\": \"डेटा उपलब्धता\",\n",
    "            \"data_accessibility\": \"डेटा पहुंच\",\n",
    "            \"architecture_fitness\": \"आर्किटेक्चर उपयुक्तता\",\n",
    "            \"team_skills\": \"टीम कौशल\",\n",
    "            \"domain_knowledge\": \"डोमेन ज्ञान\",\n",
    "            \"people_allocation\": \"लोग आवंटन\",\n",
    "            \"budget_allocation\": \"बजट आवंटन\",\n",
    "            \"time_to_production\": \"उत्पादन तक समय\",\n",
    "            \"value_score\": \"मूल्य स्कोर\",\n",
    "            \"feasibility_score\": \"व्यवहार्यता स्कोर\",\n",
    "            \"priority_score\": \"प्राथमिकता स्कोर\",\n",
    "            \"pdf_title\": \"Databricks Agent Bricks रणनीतिक AI उपयोग केस\",\n",
    "            \"pdf_for\": \"के लिए\",\n",
    "            \"pdf_exec_summary\": \"कार्यकारी सारांश\",\n",
    "            \"pdf_toc_title\": \"उपयोग केस डोमेन\",\n",
    "            \"pdf_detailed_view\": \"विस्तृत उपयोग केस कैटलॉग\",\n",
    "            \"pdf_disclaimer_title\": \"अस्वीकरण\",\n",
    "            \"pdf_fallback_summary_p1\": \"यह दस्तावेज़ {business_name} के लिए पहचाने गए {total_cases} उच्च-मूल्य विश्लेषणात्मक उपयोग केस का वर्णन करता है।\",\n",
    "            \"pdf_fallback_summary_p2\": \"निम्नलिखित पृष्ठ व्यापार डोमेन द्वारा वर्गीकृत इन अवसरों का विस्तृत विश्लेषण प्रदान करते हैं।\",\n",
    "            \"pptx_main_title\": \"Databricks Agent Bricks रणनीतिक AI उपयोग केस\",\n",
    "            \"pptx_for\": \"के लिए\",\n",
    "            \"pptx_disclaimer_title\": \"अस्वीकरण\",\n",
    "            \"pptx_domain_suffix\": \"उपयोग केस\",\n",
    "            \"example_results\": \"उदाहरण परिणाम\",\n",
    "            \"error_no_results\": \"परिणाम उत्पन्न नहीं हो सके। नोटबुक: {notebook_name} और उपयोग केस {use_case_id} जांचें\",\n",
    "            \"input_data_original\": \"इनपुट डेटा (मूल मान)\",\n",
    "            \"ai_generated_output\": \"AI-जनित परिणाम\",\n",
    "            \"column\": \"कॉलम\",\n",
    "            \"value\": \"मूल्य\",\n",
    "            \"executive_summary_not_available\": \"कार्यकारी सारांश उपलब्ध नहीं है।\",\n",
    "            \"domain_summary_not_available\": \"डोमेन सारांश उपलब्ध नहीं है।\",\n",
    "            \"summary_not_available\": \"सारांश उपलब्ध नहीं है।\",\n",
    "            \"value_general_improvement\": \"सामान्य सुधार\",\n",
    "            \"value_reduce_cost\": \"लागत कम करें\",\n",
    "            \"value_increase_revenue\": \"राजस्व बढ़ाएं\",\n",
    "            \"value_boost_productivity\": \"उत्पादकता बढ़ाएं\",\n",
    "            \"value_mitigate_risk\": \"जोखिम कम करें\",\n",
    "            \"value_protect_revenue\": \"राजस्व की रक्षा करें\",\n",
    "            \"value_align_to_regulations\": \"नियमों का पालन करें\",\n",
    "            \"value_improve_customer_experience\": \"ग्राहक अनुभव सुधारें\",\n",
    "            \"value_enable_data_driven_decisions\": \"डेटा-आधारित निर्णय सक्षम करें\",\n",
    "            \"value_optimize_operations\": \"संचालन अनुकूलित करें\",\n",
    "            \"value_empower_talent\": \"प्रतिभा को सशक्त बनाएं\",\n",
    "            \"value_enhance_experience\": \"अनुभव बढ़ाएं\",\n",
    "            \"value_drive_innovation\": \"नवाचार को बढ़ावा दें\",\n",
    "            \"value_achieve_esg\": \"ESG प्राप्त करें\",\n",
    "            \"value_execute_strategy\": \"रणनीति निष्पादित करें\",\n",
    "            \"value_forecasting\": \"पूर्वानुमान\",\n",
    "            \"value_classification\": \"वर्गीकरण\",\n",
    "            \"value_anomaly_detection\": \"विसंगति पहचान\",\n",
    "            \"value_cohort_analysis\": \"समूह विश्लेषण\",\n",
    "            \"value_segmentation\": \"विभाजन\",\n",
    "            \"value_sentiment_analysis\": \"भावना विश्लेषण\",\n",
    "            \"value_trend_analysis\": \"रुझान विश्लेषण\",\n",
    "            \"value_prescriptive_analytics\": \"निर्देशात्मक विश्लेषण\",\n",
    "            \"value_root_cause_analysis\": \"मूल कारण विश्लेषण\",\n",
    "            \"value_optimization\": \"अनुकूलन\",\n",
    "            \"value_recommendation\": \"अनुशंसा\",\n",
    "            \"value_time_series_analysis\": \"समय श्रृंखला विश्लेषण\",\n",
    "            \"value_predictive_analytics\": \"भविष्यवाणी विश्लेषण\",\n",
    "            \"value_descriptive_analytics\": \"वर्णनात्मक विश्लेषण\",\n",
    "        },\n",
    "        \"Russian\": {\n",
    "            \"main_title\": \"Генератор бизнес-кейсов Databricks Agent Bricks\",\n",
    "            \"intro\": \"Эта записная книжка содержит бизнес-кейсы, созданные ИИ на основе ваших схем. Ниже приведено резюме созданных сценариев по бизнес-доменам.\",\n",
    "            \"domain\": \"Бизнес-домен\",\n",
    "            \"total\": \"Всего бизнес-кейсов\",\n",
    "            \"summaries\": \"Резюме бизнес-кейсов\",\n",
    "            \"sum_id\": \"ID\",\n",
    "            \"sum_name\": \"Название\",\n",
    "            \"sum_value\": \"Бизнес-ценность\",\n",
    "            \"sum_outcome\": \"Ожидаемый результат\",\n",
    "            \"warning_header\": \"ПРЕДУПРЕЖДЕНИЕ\",\n",
    "            \"warning_body\": \"Не запускайте эту записную книжку. Она предназначена только для демонстрации и каталогизации. SQL-запросы являются примерами и могут потребовать проверки перед выполнением.\",\n",
    "            \"disclaimer\": \"Этот контент создан ИИ и предназначен только для демонстрационных целей. Все SQL-запросы являются примерами и должны быть проверены квалифицированным инженером перед использованием в производственной среде.\",\n",
    "            \"detailed_scenarios\": \"Детали бизнес-кейсов\",\n",
    "            \"aspect\": \"Аспект\",\n",
    "            \"description\": \"Описание\",\n",
    "            \"aspect_domain\": \"Бизнес-домен\",\n",
    "            \"type\": \"Тип\",\n",
    "            \"analytics_technique\": \"Аналитическая техника\",\n",
    "            \"primary_table\": \"Основная таблица\",\n",
    "            \"priority\": \"Приоритет\",\n",
    "            \"value_type_problem\": \"Проблема\",\n",
    "            \"value_type_risk\": \"Риск\",\n",
    "            \"value_type_opportunity\": \"Возможность\",\n",
    "            \"value_type_improvement\": \"Улучшение\",\n",
    "            \"value_priority_ultra_high\": \"Крайне высокий\",\n",
    "            \"value_priority_very_high\": \"Очень высокий\",\n",
    "            \"value_priority_high\": \"Высокий\",\n",
    "            \"value_priority_medium\": \"Средний\",\n",
    "            \"value_priority_low\": \"Низкий\",\n",
    "            \"value_priority_very_low\": \"Очень низкий\",\n",
    "            \"value_priority_ultra_low\": \"Крайне низкий\",\n",
    "            \"statement\": \"Описание\",\n",
    "            \"solution\": \"Решение\",\n",
    "            \"aspect_beneficiary\": \"Выгодоприобретатель\",\n",
    "            \"beneficiary\": \"Выгодоприобретатель\",\n",
    "            \"aspect_sponsor\": \"Спонсор\",\n",
    "            \"sponsor\": \"Спонсор\",\n",
    "            \"business_priority_alignment\": \"Соответствие бизнес-приоритетам\",\n",
    "            \"strategic_goals_alignment\": \"Соответствие стратегическим целям\",\n",
    "            \"subdomain\": \"Поддомен\",\n",
    "            \"aspect_value\": \"Бизнес-ценность\",\n",
    "            \"business_value\": \"Бизнес-ценность\",\n",
    "            \"aspect_tables\": \"Связанные таблицы\",\n",
    "            \"aspect_ai_function\": \"Функция ИИ\",\n",
    "            \"aspect_analytics_technique\": \"Аналитическая техника\",\n",
    "            \"aspect_primary_table\": \"Основная таблица\",\n",
    "            \"aspect_priority\": \"Приоритет\",\n",
    "            \"strategic_alignment\": \"Стратегическое соответствие\",\n",
    "            \"return_on_investment\": \"Рентабельность инвестиций\",\n",
    "            \"reusability\": \"Возможность повторного использования\",\n",
    "            \"time_to_value\": \"Время до получения ценности\",\n",
    "            \"data_availability\": \"Доступность данных\",\n",
    "            \"data_accessibility\": \"Доступ к данным\",\n",
    "            \"architecture_fitness\": \"Соответствие архитектуре\",\n",
    "            \"team_skills\": \"Навыки команды\",\n",
    "            \"domain_knowledge\": \"Знание предметной области\",\n",
    "            \"people_allocation\": \"Распределение персонала\",\n",
    "            \"budget_allocation\": \"Распределение бюджета\",\n",
    "            \"time_to_production\": \"Время до внедрения\",\n",
    "            \"value_score\": \"Оценка ценности\",\n",
    "            \"feasibility_score\": \"Оценка реализуемости\",\n",
    "            \"priority_score\": \"Оценка приоритета\",\n",
    "            \"pdf_title\": \"Стратегические ИИ бизнес-кейсы Databricks Agent Bricks\",\n",
    "            \"pdf_for\": \"Для\",\n",
    "            \"pdf_exec_summary\": \"Резюме для руководства\",\n",
    "            \"pdf_toc_title\": \"Домены бизнес-кейсов\",\n",
    "            \"pdf_detailed_view\": \"Подробный каталог бизнес-кейсов\",\n",
    "            \"pdf_disclaimer_title\": \"Отказ от ответственности\",\n",
    "            \"pdf_fallback_summary_p1\": \"Этот документ описывает {total_cases} высокоценных аналитических бизнес-кейсов, выявленных для {business_name}.\",\n",
    "            \"pdf_fallback_summary_p2\": \"На следующих страницах представлен подробный анализ этих возможностей, классифицированных по бизнес-доменам.\",\n",
    "            \"pptx_main_title\": \"Стратегические ИИ бизнес-кейсы Databricks Agent Bricks\",\n",
    "            \"pptx_for\": \"Для\",\n",
    "            \"pptx_disclaimer_title\": \"Отказ от ответственности\",\n",
    "            \"pptx_domain_suffix\": \"Бизнес-кейсы\",\n",
    "            \"example_results\": \"Примеры результатов\",\n",
    "            \"error_no_results\": \"Не удалось создать результаты. Проверьте записную книжку: {notebook_name} и бизнес-кейс {use_case_id}\",\n",
    "            \"input_data_original\": \"Входные данные (исходные значения)\",\n",
    "            \"ai_generated_output\": \"Результаты, созданные ИИ\",\n",
    "            \"column\": \"Столбец\",\n",
    "            \"value\": \"Значение\",\n",
    "            \"executive_summary_not_available\": \"Резюме для руководства недоступно.\",\n",
    "            \"domain_summary_not_available\": \"Резюме домена недоступно.\",\n",
    "            \"summary_not_available\": \"Резюме недоступно.\",\n",
    "            \"value_general_improvement\": \"Общее Улучшение\",\n",
    "            \"value_reduce_cost\": \"Сократить Затраты\",\n",
    "            \"value_increase_revenue\": \"Увеличить Доход\",\n",
    "            \"value_boost_productivity\": \"Повысить Производительность\",\n",
    "            \"value_mitigate_risk\": \"Снизить Риски\",\n",
    "            \"value_protect_revenue\": \"Защитить Доход\",\n",
    "            \"value_align_to_regulations\": \"Соответствовать Нормативам\",\n",
    "            \"value_improve_customer_experience\": \"Улучшить Клиентский Опыт\",\n",
    "            \"value_enable_data_driven_decisions\": \"Обеспечить Решения на Основе Данных\",\n",
    "            \"value_optimize_operations\": \"Оптимизировать Операции\",\n",
    "            \"value_empower_talent\": \"Развивать Таланты\",\n",
    "            \"value_enhance_experience\": \"Улучшить Опыт\",\n",
    "            \"value_drive_innovation\": \"Стимулировать Инновации\",\n",
    "            \"value_achieve_esg\": \"Достичь ESG\",\n",
    "            \"value_execute_strategy\": \"Реализовать Стратегию\",\n",
    "            \"value_forecasting\": \"Прогнозирование\",\n",
    "            \"value_classification\": \"Классификация\",\n",
    "            \"value_anomaly_detection\": \"Обнаружение Аномалий\",\n",
    "            \"value_cohort_analysis\": \"Когортный Анализ\",\n",
    "            \"value_segmentation\": \"Сегментация\",\n",
    "            \"value_sentiment_analysis\": \"Анализ Настроений\",\n",
    "            \"value_trend_analysis\": \"Анализ Трендов\",\n",
    "            \"value_prescriptive_analytics\": \"Предписывающая Аналитика\",\n",
    "            \"value_root_cause_analysis\": \"Анализ Первопричин\",\n",
    "            \"value_optimization\": \"Оптимизация\",\n",
    "            \"value_recommendation\": \"Рекомендация\",\n",
    "            \"value_time_series_analysis\": \"Анализ Временных Рядов\",\n",
    "            \"value_predictive_analytics\": \"Прогнозная Аналитика\",\n",
    "            \"value_descriptive_analytics\": \"Описательная Аналитика\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    def _apply_translation_fallbacks(self, translations: dict, target_language: str) -> dict:\n",
    "        \"\"\"\n",
    "        Applies known fallback translations for commonly untranslated terms.\n",
    "        This fixes cases where the LLM fails to translate certain terms.\n",
    "        ALWAYS applies fallbacks for keys that are missing OR still have English values.\n",
    "        \"\"\"\n",
    "        if target_language not in self.TRANSLATION_FALLBACKS:\n",
    "            return translations\n",
    "        \n",
    "        fallbacks = self.TRANSLATION_FALLBACKS[target_language]\n",
    "        english_values = list(self.ENGLISH_TRANSLATIONS.values())\n",
    "        english_values_lower = [v.lower() for v in english_values]\n",
    "        \n",
    "        applied_count = 0\n",
    "        for key, fallback_value in fallbacks.items():\n",
    "            should_apply = False\n",
    "            current_value = translations.get(key, None)\n",
    "            \n",
    "            # Apply fallback if: key is missing, value is empty, or value is still in English\n",
    "            if key not in translations:\n",
    "                should_apply = True\n",
    "                self.logger.debug(f\"Translation MISSING for '{key}', applying fallback: '{fallback_value}'\")\n",
    "            elif not current_value or (isinstance(current_value, str) and not current_value.strip()):\n",
    "                should_apply = True\n",
    "                self.logger.debug(f\"Translation EMPTY for '{key}', applying fallback: '{fallback_value}'\")\n",
    "            elif isinstance(current_value, str) and current_value.lower() in english_values_lower:\n",
    "                should_apply = True\n",
    "                self.logger.debug(f\"Translation still ENGLISH for '{key}': '{current_value}' → '{fallback_value}'\")\n",
    "            \n",
    "            if should_apply:\n",
    "                translations[key] = fallback_value\n",
    "                applied_count += 1\n",
    "        \n",
    "        if applied_count > 0:\n",
    "            self.logger.info(f\"Applied {applied_count} fallback translations for {target_language}\")\n",
    "        \n",
    "        return translations\n",
    "\n",
    "    def _validate_translations(self, translations: dict, target_language: str) -> bool:\n",
    "        \"\"\"\n",
    "        Validates that critical fields are actually translated and not left in English.\n",
    "        Returns True if translations are valid, False otherwise.\n",
    "        \"\"\"\n",
    "        # Critical keys that MUST be translated\n",
    "        critical_keys = [\n",
    "            \"type\", \"subdomain\", \"analytics_technique\", \"primary_table\", \"priority\", \n",
    "            \"aspect_analytics_technique\", \"aspect_primary_table\", \"aspect_priority\",\n",
    "            \"statement\", \"solution\", \"business_value\", \"beneficiary\", \"sponsor\",\n",
    "            \"strategic_goal_alignment\",\n",
    "            \"pdf_detailed_view\", \"pptx_domain_suffix\", \"domain\",\n",
    "            \"value_type_opportunity\", \"value_type_problem\", \"value_type_risk\", \"value_type_improvement\",\n",
    "            \"value_priority_very_high\", \"value_priority_high\", \"value_priority_medium\", \"value_priority_low\", \"value_priority_very_low\",\n",
    "            \"example_results\", \"column\", \"value\"\n",
    "        ]\n",
    "        \n",
    "        # Expected English values that should NOT appear in translations\n",
    "        english_values = [\n",
    "            \"Type\", \"Subdomain\", \"Analytics Technique\", \"Primary Table\", \"Priority\",\n",
    "            \"Statement\", \"Solution\", \"Business Value\", \"Beneficiary\", \"Sponsor\",\n",
    "            \"Business Priority Alignment\", \"AI Confidence\", \"AI Justification\",\n",
    "            \"Detailed Use Case Catalog\", \"Use Cases\", \"Business Domain\",\n",
    "            \"Opportunity\", \"Problem\", \"Risk\", \"Improvement\",\n",
    "            \"Simple\", \"Medium\", \"Complex\", \"Very Complex\",\n",
    "            \"High\", \"Very High\", \"Low\", \"Very Low\",\n",
    "            \"Example Results\", \"Column\", \"Value\"\n",
    "        ]\n",
    "        \n",
    "        # Check if any critical key still has an English value (case-insensitive)\n",
    "        english_values_lower = [v.lower() for v in english_values]\n",
    "        for key in critical_keys:\n",
    "            if key in translations:\n",
    "                value = translations[key]\n",
    "                if isinstance(value, str) and value.lower() in english_values_lower:\n",
    "                    self.logger.warning(f\"Translation validation FAILED for {target_language}: Key '{key}' still has English value '{value}'\")\n",
    "                    return False\n",
    "        \n",
    "        self.logger.info(f\"Translation validation PASSED for {target_language}\")\n",
    "        return True\n",
    "\n",
    "    def get_translations(self, target_language: str) -> dict:\n",
    "        \"\"\"\n",
    "        Gets translations for UI elements for a given language.\n",
    "        Uses AI agent and caches the result.\n",
    "        Supports retry logic (max 2 attempts).\n",
    "        \"\"\"\n",
    "        if target_language == \"English\":\n",
    "            self.logger.info(\"Using default English UI translations.\")\n",
    "            return self.ENGLISH_TRANSLATIONS\n",
    "        \n",
    "        if target_language in self.translation_cache:\n",
    "            cached = self.translation_cache[target_language]\n",
    "            # Check if all keys from ENGLISH_TRANSLATIONS are present in cache\n",
    "            missing_keys = set(self.ENGLISH_TRANSLATIONS.keys()) - set(cached.keys())\n",
    "            if not missing_keys:\n",
    "                # Validate that translations are actually in target language\n",
    "                if self._validate_translations(cached, target_language):\n",
    "                    self.logger.info(f\"Using cached UI translations for {target_language}.\")\n",
    "                    return cached\n",
    "                else:\n",
    "                    self.logger.warning(f\"Cached translations for {target_language} contain English values. Forcing re-translation...\")\n",
    "                    del self.translation_cache[target_language]\n",
    "            else:\n",
    "                self.logger.info(f\"Cache for {target_language} is outdated (missing {len(missing_keys)} keys). Refreshing...\")\n",
    "                # Remove from cache to force re-translation\n",
    "                del self.translation_cache[target_language]\n",
    "\n",
    "        self.logger.debug(f\"Calling LLM to get UI translations for {target_language}...\")\n",
    "        \n",
    "        # Retry loop: up to 2 attempts\n",
    "        for attempt in range(1, 3):\n",
    "            try:\n",
    "                if attempt > 1:\n",
    "                    self.logger.info(f\"Retrying UI translation for {target_language} (Attempt {attempt}/2)...\")\n",
    "                \n",
    "                # Escape braces in JSON payload so they don't interfere with .format()\n",
    "                json_str = json.dumps(self.ENGLISH_TRANSLATIONS, indent=2)\n",
    "                # Replace { with {{ and } with }} to escape them for .format()\n",
    "                json_str_escaped = json_str.replace('{', '{{').replace('}', '}}')\n",
    "                \n",
    "                prompt_vars = {\n",
    "                    \"json_payload\": json_str_escaped,\n",
    "                    \"target_language\": target_language\n",
    "                }\n",
    "                \n",
    "                self.logger.info(f\"⏳ Waiting for LLM response (translating UI to {target_language})...\")\n",
    "                response_raw = self.ai_agent.run_worker(\n",
    "                    step_name=f\"Translate_UI_{target_language}_Attempt{attempt}\",\n",
    "                    worker_prompt_path=\"KEYWORDS_TRANSLATE_PROMPT\", # Use key\n",
    "                    prompt_vars=prompt_vars,\n",
    "                    response_schema=None\n",
    "                )\n",
    "                self.logger.info(f\"✅ Received LLM response, parsing translations...\")\n",
    "                \n",
    "                response_clean = clean_json_response(response_raw)\n",
    "                translated_dict = json.loads(response_clean)\n",
    "                \n",
    "                final_translations = self.ENGLISH_TRANSLATIONS.copy()\n",
    "                final_translations.update(translated_dict) \n",
    "                \n",
    "                # Apply known fallback fixes for commonly untranslated terms\n",
    "                final_translations = self._apply_translation_fallbacks(final_translations, target_language)\n",
    "                \n",
    "                # Validate translations before caching\n",
    "                if not self._validate_translations(final_translations, target_language):\n",
    "                    if attempt < 2:\n",
    "                        self.logger.warning(f\"Translation validation failed on attempt {attempt}. Will retry...\")\n",
    "                        continue  # Retry\n",
    "                    else:\n",
    "                        # Apply fallbacks one more time before giving up\n",
    "                        final_translations = self._apply_translation_fallbacks(final_translations, target_language)\n",
    "                        if self._validate_translations(final_translations, target_language):\n",
    "                            self.logger.info(f\"Fallback translations fixed the issue. Using fallbacks.\")\n",
    "                            self.translation_cache[target_language] = final_translations\n",
    "                            return final_translations\n",
    "                        self.logger.error(f\"Translation validation failed on final attempt. Falling back to English.\")\n",
    "                        return self.ENGLISH_TRANSLATIONS\n",
    "                \n",
    "                self.logger.info(f\"Successfully fetched and cached UI translations for {target_language} on attempt {attempt}.\")\n",
    "                self.translation_cache[target_language] = final_translations\n",
    "                return final_translations\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to get UI translations for {target_language} (Attempt {attempt}/2): {e}\")\n",
    "                if attempt == 2:\n",
    "                    self.logger.error(f\"All retry attempts exhausted for UI translations. Falling back to English.\")\n",
    "                    return self.ENGLISH_TRANSLATIONS\n",
    "\n",
    "    def translate_use_case_list(self, english_use_cases: list, target_language: str, max_parallelism: int = 20, enable_parallelization: bool = True) -> list:\n",
    "        \"\"\"\n",
    "        Translates the 9 key fields for a list of use case dictionaries.\n",
    "        Uses dynamic batch sizing to maximize context utilization.\n",
    "        \n",
    "        Args:\n",
    "            english_use_cases: List of use case dictionaries to translate\n",
    "            target_language: Target language name\n",
    "            max_parallelism: Maximum parallel workers (only used if enable_parallelization=True)\n",
    "            enable_parallelization: If False, processes batches sequentially to avoid nested ThreadPoolExecutors\n",
    "        \"\"\"\n",
    "        if target_language == \"English\":\n",
    "            return english_use_cases\n",
    "        \n",
    "        if not english_use_cases:\n",
    "             self.logger.warning(\"No use cases provided to translate.\")\n",
    "             return []\n",
    "\n",
    "        self.logger.info(f\"Starting translation for {target_language}... (parallelization: {'enabled' if enable_parallelization else 'disabled'})\")\n",
    "        \n",
    "        # FIXED BATCH SIZE: 3 use cases per batch for ALL languages\n",
    "        # This prevents LLM output truncation issues with long SQL queries (8-10K chars each)\n",
    "        # Optimal balance between translation speed and reliability\n",
    "        batch_size = 3\n",
    "        \n",
    "        self.logger.info(f\"Translation batch sizing: {batch_size} use cases per batch (FIXED for all languages to prevent truncation)\")\n",
    "        \n",
    "        batches = [english_use_cases[i:i + batch_size] for i in range(0, len(english_use_cases), batch_size)]\n",
    "        translated_use_cases = []\n",
    "        \n",
    "        if enable_parallelization:\n",
    "            # Parallel processing (used when NOT called from within another ThreadPoolExecutor)\n",
    "            with ThreadPoolExecutor(max_workers=max_parallelism) as executor:\n",
    "                futures = [executor.submit(self._translate_batch, batch, target_language, i) for i, batch in enumerate(batches)]\n",
    "                \n",
    "                # Add timeout: 10 minutes per batch * number of batches / parallelism\n",
    "                total_timeout = (len(batches) * 600) // max_parallelism + 300\n",
    "                self.logger.info(f\"Translation timeout set to {total_timeout}s for {len(batches)} batches\")\n",
    "                \n",
    "                try:\n",
    "                    for future in concurrent.futures.as_completed(futures, timeout=total_timeout):\n",
    "                        try:\n",
    "                            translated_batch = future.result(timeout=600)\n",
    "                            if translated_batch:\n",
    "                                translated_use_cases.extend(translated_batch)\n",
    "                        except concurrent.futures.TimeoutError:\n",
    "                            self.logger.error(f\"Translation batch timed out after 10 minutes\")\n",
    "                        except Exception as e:\n",
    "                            self.logger.error(f\"A translation future failed unexpectedly: {e}\")\n",
    "                except concurrent.futures.TimeoutError:\n",
    "                    self.logger.error(f\"Overall translation timeout reached ({total_timeout}s). Proceeding with {len(translated_use_cases)} translated use cases.\")\n",
    "        else:\n",
    "            # Sequential processing (used when called from within another ThreadPoolExecutor to avoid nesting)\n",
    "            self.logger.info(f\"Processing {len(batches)} translation batches sequentially to avoid nested ThreadPoolExecutors...\")\n",
    "            for i, batch in enumerate(batches):\n",
    "                try:\n",
    "                    translated_batch = self._translate_batch(batch, target_language, i)\n",
    "                    if translated_batch:\n",
    "                        translated_use_cases.extend(translated_batch)\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Translation batch {i} failed: {e}\")\n",
    "        \n",
    "        if len(translated_use_cases) != len(english_use_cases):\n",
    "            self.logger.warning(f\"Translation mismatch: expected {len(english_use_cases)} use cases, translated {len(translated_use_cases)}. Some batches may have failed and reverted to English.\")\n",
    "        \n",
    "        self.logger.info(f\"Translation completed for {target_language}.\")\n",
    "        return translated_use_cases\n",
    "\n",
    "    def _translate_batch(self, use_case_batch: list, target_language: str, batch_num: int, attempt: int = 1, split_attempt: int = 0) -> list:\n",
    "        \"\"\"\n",
    "        Private method to translate a single batch of use cases.\n",
    "        Returns the original English batch on any failure.\n",
    "        Supports retry logic with automatic batch splitting on truncation (up to 3 split attempts).\n",
    "        \n",
    "        Args:\n",
    "            use_case_batch: List of use case dictionaries to translate\n",
    "            target_language: Target language for translation\n",
    "            batch_num: Batch number for logging\n",
    "            attempt: Current attempt number (1 or 2)\n",
    "            split_attempt: Number of times batch has been split (0-3)\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"Translating batch #{batch_num} (Attempt {attempt}/2, Split {split_attempt}/3) ({len(use_case_batch)} use cases) to {target_language}...\")\n",
    "        \n",
    "        try:\n",
    "            # OPTIMIZATION: Extract SQL to reduce payload size and prevent truncation\n",
    "            # SQL doesn't need to be translated, so we remove it from the batch and add it back after\n",
    "            sql_mapping = {}\n",
    "            batch_without_sql = []\n",
    "            for uc in use_case_batch:\n",
    "                uc_copy = uc.copy()\n",
    "                uc_id = uc_copy.get('No', '')\n",
    "                # Store SQL separately\n",
    "                sql_mapping[uc_id] = uc_copy.pop('SQL', '')\n",
    "                batch_without_sql.append(uc_copy)\n",
    "            \n",
    "            # Escape braces in JSON payload so they don't interfere with .format()\n",
    "            json_str = json.dumps(batch_without_sql, indent=2)\n",
    "            json_str_escaped = json_str.replace('{', '{{').replace('}', '}}')\n",
    "            \n",
    "            prompt_vars = {\n",
    "                \"json_payload\": json_str_escaped,\n",
    "                \"target_language\": target_language\n",
    "            }\n",
    "            \n",
    "            self.logger.info(f\"⏳ [Batch {batch_num}] Waiting for LLM response (translating {len(use_case_batch)} use cases to {target_language})...\")\n",
    "            response_raw = self.ai_agent.run_worker(\n",
    "                step_name=f\"Translate_UseCases_{target_language}_Batch_{batch_num}_Attempt{attempt}_Split{split_attempt}\",\n",
    "                worker_prompt_path=\"USE_CASE_TRANSLATE_PROMPT\",\n",
    "                prompt_vars=prompt_vars,\n",
    "                response_schema=None\n",
    "            )\n",
    "            self.logger.info(f\"✅ [Batch {batch_num}] Received LLM response, parsing translated use cases...\")\n",
    "            \n",
    "            # Parse CSV response\n",
    "            translated_batch = self._parse_translation_csv(response_raw, batch_without_sql, batch_num, target_language)\n",
    "            \n",
    "            # Add SQL back to translated use cases\n",
    "            if translated_batch:\n",
    "                for uc in translated_batch:\n",
    "                    uc_id = uc.get('No', '')\n",
    "                    if uc_id in sql_mapping:\n",
    "                        uc['SQL'] = sql_mapping[uc_id]\n",
    "            \n",
    "            if translated_batch and len(translated_batch) == len(use_case_batch):\n",
    "                self.logger.debug(f\"Successfully translated batch #{batch_num} on attempt {attempt}, split {split_attempt}.\")\n",
    "                return translated_batch\n",
    "            else:\n",
    "                raise ValueError(f\"Translation returned {len(translated_batch) if translated_batch else 0} rows, expected {len(use_case_batch)}\")\n",
    "\n",
    "        except InputTooLongError as e:\n",
    "            # Handle context limit exceeded - split batch recursively\n",
    "            if len(use_case_batch) <= 1:\n",
    "                # Cannot split further - single use case is too large\n",
    "                self.logger.error(f\"Batch #{batch_num} has single use case that's too large for translation. Reverting to English.\")\n",
    "                return use_case_batch\n",
    "            \n",
    "            # Split into 2 sub-batches\n",
    "            self.logger.warning(f\"Batch #{batch_num} exceeds context limit ({str(e)[:100]}). Splitting into smaller sub-batches...\")\n",
    "            mid = len(use_case_batch) // 2\n",
    "            sub_batch_1 = use_case_batch[:mid]\n",
    "            sub_batch_2 = use_case_batch[mid:]\n",
    "            \n",
    "            # Recursively translate sub-batches (with attempt=1 for fresh retries, increment split_attempt)\n",
    "            translated_1 = self._translate_batch(sub_batch_1, target_language, f\"{batch_num}a\", attempt=1, split_attempt=split_attempt + 1)\n",
    "            translated_2 = self._translate_batch(sub_batch_2, target_language, f\"{batch_num}b\", attempt=1, split_attempt=split_attempt + 1)\n",
    "            \n",
    "            # Combine results\n",
    "            combined = translated_1 + translated_2\n",
    "            if len(combined) == len(use_case_batch):\n",
    "                self.logger.info(f\"Successfully translated batch #{batch_num} after splitting into sub-batches\")\n",
    "                return combined\n",
    "            else:\n",
    "                self.logger.warning(f\"Sub-batch splitting failed for batch #{batch_num}. Reverting to English.\")\n",
    "                return use_case_batch\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            is_truncation = \"TRUNCATED\" in error_msg\n",
    "            \n",
    "            self.logger.error(f\"Failed to translate use case batch #{batch_num} (Attempt {attempt}/2, Split {split_attempt}/3) for {target_language}: {e}\")\n",
    "            \n",
    "            # If truncation detected and batch has more than 1 item, try splitting it (up to 3 times)\n",
    "            if is_truncation and len(use_case_batch) > 1 and split_attempt < 3:\n",
    "                self.logger.warning(f\"Batch #{batch_num} appears truncated. Reducing batch size (split attempt {split_attempt + 1}/3)...\")\n",
    "                \n",
    "                # Calculate smaller batch size (split into 2 or 3 pieces depending on batch size)\n",
    "                if len(use_case_batch) >= 3:\n",
    "                    # Split into 3 smaller pieces for better chance of success\n",
    "                    third = len(use_case_batch) // 3\n",
    "                    sub_batch_1 = use_case_batch[:third]\n",
    "                    sub_batch_2 = use_case_batch[third:2*third]\n",
    "                    sub_batch_3 = use_case_batch[2*third:]\n",
    "                    \n",
    "                    # Recursively translate sub-batches with incremented split_attempt\n",
    "                    translated_1 = self._translate_batch(sub_batch_1, target_language, f\"{batch_num}a\", attempt=1, split_attempt=split_attempt + 1)\n",
    "                    translated_2 = self._translate_batch(sub_batch_2, target_language, f\"{batch_num}b\", attempt=1, split_attempt=split_attempt + 1)\n",
    "                    translated_3 = self._translate_batch(sub_batch_3, target_language, f\"{batch_num}c\", attempt=1, split_attempt=split_attempt + 1)\n",
    "                    \n",
    "                    # Combine results\n",
    "                    combined = translated_1 + translated_2 + translated_3\n",
    "                else:\n",
    "                    # Split into 2 pieces\n",
    "                    mid = len(use_case_batch) // 2\n",
    "                    sub_batch_1 = use_case_batch[:mid]\n",
    "                    sub_batch_2 = use_case_batch[mid:]\n",
    "                    \n",
    "                    translated_1 = self._translate_batch(sub_batch_1, target_language, f\"{batch_num}a\", attempt=1, split_attempt=split_attempt + 1)\n",
    "                    translated_2 = self._translate_batch(sub_batch_2, target_language, f\"{batch_num}b\", attempt=1, split_attempt=split_attempt + 1)\n",
    "                    \n",
    "                    combined = translated_1 + translated_2\n",
    "                \n",
    "                if len(combined) == len(use_case_batch):\n",
    "                    self.logger.info(f\"Successfully translated batch #{batch_num} after reducing batch size (split {split_attempt + 1})\")\n",
    "                    return combined\n",
    "                else:\n",
    "                    self.logger.warning(f\"Batch size reduction failed for batch #{batch_num}. Trying standard retry...\")\n",
    "                    # Fall through to standard retry logic\n",
    "            \n",
    "            # Standard retry logic: Try one more time if this is the first attempt and we haven't exceeded split attempts\n",
    "            if attempt < 2 and not (is_truncation and split_attempt >= 3):\n",
    "                self.logger.info(f\"Retrying batch #{batch_num} for {target_language} (Attempt 2/2)...\")\n",
    "                return self._translate_batch(use_case_batch, target_language, batch_num, attempt=2, split_attempt=split_attempt)\n",
    "            else:\n",
    "                # All attempts exhausted\n",
    "                if is_truncation and split_attempt >= 3:\n",
    "                    self.logger.error(f\"Batch #{batch_num} still truncated after {split_attempt} split attempts. Proceeding with English.\")\n",
    "                else:\n",
    "                    self.logger.warning(f\"All retry attempts exhausted for batch #{batch_num}. Reverting to English for this batch.\")\n",
    "                return use_case_batch\n",
    "    \n",
    "    def _parse_translation_csv(self, csv_response: str, original_batch: list, batch_num: int, target_language: str) -> list:\n",
    "        \"\"\"\n",
    "        Parse CSV translation response and return list of translated use case dictionaries.\n",
    "        Only returns rows that match the original batch by 'No' field to prevent incorrect row counts.\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        try:\n",
    "            # VALIDATION 1: Check if response is empty or too short\n",
    "            if not csv_response or len(csv_response.strip()) < 100:\n",
    "                raise ValueError(f\"Response is empty or too short (len={len(csv_response) if csv_response else 0})\")\n",
    "            \n",
    "            # VALIDATION 2: Check if response contains obvious non-CSV content\n",
    "            first_100_chars = csv_response[:100].lower()\n",
    "            forbidden_starts = ['here is', 'i have', 'i\\'ve', 'the translation', 'below is', 'sure,', 'of course']\n",
    "            if any(first_100_chars.startswith(phrase) for phrase in forbidden_starts):\n",
    "                self.logger.error(f\"Batch #{batch_num}: Response starts with conversational text instead of CSV header\")\n",
    "                raise ValueError(\"Response contains conversational text instead of pure CSV\")\n",
    "            \n",
    "            # Clean response - remove markdown fences if present\n",
    "            csv_clean = csv_response.strip()\n",
    "            if csv_clean.startswith('```'):\n",
    "                csv_clean = re.sub(r'^```[a-z]*\\n', '', csv_clean)\n",
    "                csv_clean = re.sub(r'\\n```$', '', csv_clean)\n",
    "            \n",
    "            # VALIDATION 3: Check for SQL code blocks that shouldn't be there\n",
    "            # Note: This validation is informational only and doesn't block processing\n",
    "            sql_pattern_count = csv_clean.count('SELECT ') + csv_clean.count('WITH ') + csv_clean.count('FROM ')\n",
    "            # SQL should only appear in the SQL column, so count should be reasonable (approx. number of rows)\n",
    "            expected_sql_mentions = len(original_batch)\n",
    "            if sql_pattern_count > (expected_sql_mentions * 3):  # Allow some margin\n",
    "                # Debug-level logging instead of warning (reduces noise)\n",
    "                self.logger.debug(f\"Batch #{batch_num}: Response contains {sql_pattern_count} SQL keywords (expected ~{expected_sql_mentions}). This is normal for complex queries.\")\n",
    "            \n",
    "            # Find header line (14 columns - SQL is NOT included in translation to prevent truncation)\n",
    "            # Support both quoted and unquoted headers from LLM\n",
    "            # Also support legacy formats for backwards compatibility\n",
    "            header_pattern_quoted = r'\"No\",\"Name\",\"Business Domain\",\"Subdomain\",\"type\",\"Analytics Technique\",\"Statement\",\"Solution\",\"Business Value\",\"Beneficiary\",\"Sponsor\",\"Business Priority Alignment\",\"Tables Involved\",\"Priority\"'\n",
    "            header_pattern_unquoted = r'No,Name,Business Domain,Subdomain,type,Analytics Technique,Statement,Solution,Business Value,Beneficiary,Sponsor,Business Priority Alignment,Tables Involved,Priority'\n",
    "            # Legacy patterns (with Complexity instead of Analytics Technique - for backwards compatibility)\n",
    "            legacy_header_quoted = r'\"No\",\"Name\",\"Business Domain\",\"Subdomain\",\"type\",\"Statement\",\"Solution\",\"Business Value\",\"Beneficiary\",\"Sponsor\",\"Business Priority Alignment\",\"Tables Involved\",\"Complexity\",\"Priority\"'\n",
    "            legacy_header_unquoted = r'No,Name,Business Domain,Subdomain,type,Statement,Solution,Business Value,Beneficiary,Sponsor,Business Priority Alignment,Tables Involved,Complexity,Priority'\n",
    "            header_match = re.search(header_pattern_quoted, csv_clean)\n",
    "            if not header_match:\n",
    "                header_match = re.search(header_pattern_unquoted, csv_clean)\n",
    "            if not header_match:\n",
    "                # Try legacy format (13 columns without Business Priority Alignment)\n",
    "                header_match = re.search(legacy_header_quoted, csv_clean)\n",
    "                if header_match:\n",
    "                    self.logger.debug(f\"Batch #{batch_num}: Using legacy 13-column format (Business Priority Alignment not in translation)\")\n",
    "            if not header_match:\n",
    "                header_match = re.search(legacy_header_unquoted, csv_clean)\n",
    "                if header_match:\n",
    "                    self.logger.debug(f\"Batch #{batch_num}: Using legacy 13-column format (unquoted)\")\n",
    "            if not header_match:\n",
    "                # Try to show what we found instead\n",
    "                first_line = csv_clean.split('\\n')[0][:200] if csv_clean else \"Empty\"\n",
    "                self.logger.error(f\"Batch #{batch_num}: CSV header not found. First line: {first_line}\")\n",
    "                raise ValueError(f\"Could not find CSV header. First line was: {first_line}\")\n",
    "            \n",
    "            # Extract CSV starting from header\n",
    "            csv_data = csv_clean[header_match.start():]\n",
    "            \n",
    "            # Build a set of expected IDs from the original batch\n",
    "            expected_ids = {uc.get('No', '').strip() for uc in original_batch}\n",
    "            self.logger.debug(f\"Expected IDs for batch #{batch_num}: {expected_ids}\")\n",
    "            \n",
    "            # Parse CSV using centralized utility\n",
    "            parsed_rows = CSVParser.parse_csv_string(\n",
    "                csv_data,\n",
    "                logger=self.logger,\n",
    "                context=f\"Batch #{batch_num}\",\n",
    "                quoting=csv.QUOTE_ALL\n",
    "            )\n",
    "            all_parsed_rows = []\n",
    "            translated_rows = []\n",
    "            \n",
    "            for row_dict in parsed_rows:\n",
    "                # Clean up field values - handle both string and list values\n",
    "                cleaned_row = {}\n",
    "                for k, v in row_dict.items():\n",
    "                    # Ensure k is a string (handle edge cases where CSV parser returns unexpected types)\n",
    "                    key = str(k) if not isinstance(k, str) else k\n",
    "                    \n",
    "                    # Handle different value types robustly\n",
    "                    if v is None:\n",
    "                        cleaned_row[key] = \"\"\n",
    "                    elif isinstance(v, list):\n",
    "                        # If value is a list (shouldn't happen but handle it), join it\n",
    "                        cleaned_row[key] = ', '.join(str(item) for item in v)\n",
    "                    elif isinstance(v, str):\n",
    "                        cleaned_row[key] = v.strip()\n",
    "                    else:\n",
    "                        cleaned_row[key] = str(v)\n",
    "                \n",
    "                row_id = cleaned_row.get('No', '').strip()\n",
    "                \n",
    "                if not row_id:\n",
    "                    self.logger.debug(f\"Batch #{batch_num}: Skipping row with empty ID\")\n",
    "                    continue\n",
    "                \n",
    "                all_parsed_rows.append(cleaned_row)\n",
    "                \n",
    "                if expected_ids and row_id not in expected_ids:\n",
    "                    row_id_preview = row_id[:50] if len(row_id) > 50 else row_id\n",
    "                    self.logger.warning(f\"Batch #{batch_num}: Skipping unexpected row with ID '{row_id_preview}' (not in batch)\")\n",
    "                    continue\n",
    "                \n",
    "                translated_rows.append(cleaned_row)\n",
    "            \n",
    "            # Log if we got more rows than expected\n",
    "            if len(all_parsed_rows) > len(original_batch):\n",
    "                self.logger.warning(f\"Batch #{batch_num}: CSV response contained {len(all_parsed_rows)} rows, but only {len(translated_rows)} matched the expected batch IDs. \"\n",
    "                                  f\"LLM may have returned extra rows - filtered to correct batch.\")\n",
    "            \n",
    "            # Verify we got all expected rows\n",
    "            translated_ids = {row.get('No', '').strip() for row in translated_rows}\n",
    "            missing_ids = expected_ids - translated_ids\n",
    "            if missing_ids:\n",
    "                self.logger.error(f\"Batch #{batch_num}: Missing translations for IDs: {missing_ids}\")\n",
    "                \n",
    "                # Check if response appears truncated (ends mid-sentence or mid-field)\n",
    "                last_100_chars = csv_response[-100:].strip() if csv_response else \"\"\n",
    "                is_truncated = (\n",
    "                    not last_100_chars.endswith('\"') or  # Doesn't end with closing quote\n",
    "                    '\",\"' in last_100_chars[-20:] or  # Ends mid-field\n",
    "                    len(last_100_chars) < 50  # Response is suspiciously short\n",
    "                )\n",
    "                \n",
    "                if is_truncated:\n",
    "                    self.logger.error(f\"Batch #{batch_num}: Response appears TRUNCATED. Last 100 chars: ...{last_100_chars}\")\n",
    "                    raise ValueError(f\"Translation response was TRUNCATED - missing {len(missing_ids)} rows. Reduce batch size or simplify content.\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Translation missing {len(missing_ids)} expected rows: {missing_ids}\")\n",
    "            \n",
    "            self.logger.debug(f\"Parsed {len(translated_rows)} translated rows from CSV for batch #{batch_num} (filtered from {len(all_parsed_rows)} total rows)\")\n",
    "            return translated_rows\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to parse translation CSV for batch #{batch_num}: {e}\")\n",
    "            # Show snippet for debugging\n",
    "            snippet = csv_response[:500] if csv_response else \"Empty response\"\n",
    "            self.logger.error(f\"CSV response snippet: {snippet}\")\n",
    "            return []\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Inspire\n",
    "# ==============================================================================\n",
    "# 2. MAIN APPLICATION CLASS (MODIFIED FOR STRICT ORDERING)\n",
    "# ==============================================================================\n",
    "\n",
    "# NOTE: Global dependency checks have been removed per user request.\n",
    "# Dependencies will be checked and installed on-demand.\n",
    "\n",
    "# ==============================================================================\n",
    "# MEMORY-EFFICIENT STORAGE MANAGER\n",
    "# ==============================================================================\n",
    "class IntermediateStorageManager:\n",
    "    \"\"\"\n",
    "    Manages file-based intermediate storage for large datasets to prevent memory explosion.\n",
    "    Stores batch results on disk and provides memory-efficient iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_path=\"/tmp\", logger=None):\n",
    "        \"\"\"\n",
    "        Initialize the storage manager.\n",
    "        \n",
    "        Args:\n",
    "            base_path: Base path for temporary storage (default: /tmp)\n",
    "            logger: Logger instance\n",
    "        \"\"\"\n",
    "        self.logger = logger or logging.getLogger(self.__class__.__name__)\n",
    "        self.base_path = base_path\n",
    "        self.temp_dir = None\n",
    "        self.batch_files = []\n",
    "        self.initialized = False\n",
    "        \n",
    "    def initialize(self):\n",
    "        \"\"\"Create temporary directory for intermediate storage.\"\"\"\n",
    "        if not self.initialized:\n",
    "            self.temp_dir = tempfile.mkdtemp(prefix=\"inspire_\", dir=self.base_path)\n",
    "            self.logger.info(f\"\uD83D\uDCC1 Initialized intermediate storage at: {self.temp_dir}\")\n",
    "            self.initialized = True\n",
    "            \n",
    "    def save_batch(self, batch_num, data):\n",
    "        \"\"\"\n",
    "        Save batch data to disk.\n",
    "        \n",
    "        Args:\n",
    "            batch_num: Batch identifier (can be int or str)\n",
    "            data: List of use case dictionaries\n",
    "        \"\"\"\n",
    "        if not self.initialized:\n",
    "            self.initialize()\n",
    "            \n",
    "        # Handle both int and string batch_num\n",
    "        if isinstance(batch_num, int):\n",
    "            batch_file = os.path.join(self.temp_dir, f\"batch_{batch_num:04d}.json\")\n",
    "        else:\n",
    "            batch_file = os.path.join(self.temp_dir, f\"batch_{batch_num}.json\")\n",
    "        with open(batch_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        self.batch_files.append(batch_file)\n",
    "        file_size = os.path.getsize(batch_file) / (1024 * 1024)  # Size in MB\n",
    "        self.logger.debug(f\"\uD83D\uDCBE Saved batch {batch_num} to disk ({len(data)} use cases, {file_size:.2f} MB)\")\n",
    "        \n",
    "    def load_batch(self, batch_file):\n",
    "        \"\"\"Load a single batch from disk.\"\"\"\n",
    "        with open(batch_file, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "            \n",
    "    def iter_batches(self):\n",
    "        \"\"\"\n",
    "        Iterator over all batches.\n",
    "        Memory-efficient: loads one batch at a time.\n",
    "        \"\"\"\n",
    "        for batch_file in self.batch_files:\n",
    "            yield self.load_batch(batch_file)\n",
    "            \n",
    "    def iter_all_use_cases(self):\n",
    "        \"\"\"\n",
    "        Iterator over all use cases across all batches.\n",
    "        Memory-efficient: loads one batch at a time and yields individual use cases.\n",
    "        \"\"\"\n",
    "        for batch in self.iter_batches():\n",
    "            for use_case in batch:\n",
    "                yield use_case\n",
    "                \n",
    "    def load_all_use_cases(self):\n",
    "        \"\"\"\n",
    "        Load all use cases into memory.\n",
    "        Use this only when necessary (e.g., for deduplication).\n",
    "        \"\"\"\n",
    "        all_use_cases = []\n",
    "        for batch in self.iter_batches():\n",
    "            all_use_cases.extend(batch)\n",
    "        return all_use_cases\n",
    "        \n",
    "    def get_total_count(self):\n",
    "        \"\"\"Get total count of use cases without loading all into memory.\"\"\"\n",
    "        count = 0\n",
    "        for batch in self.iter_batches():\n",
    "            count += len(batch)\n",
    "        return count\n",
    "        \n",
    "    def cleanup(self):\n",
    "        \"\"\"Remove temporary directory and all files.\"\"\"\n",
    "        if self.temp_dir and os.path.exists(self.temp_dir):\n",
    "            try:\n",
    "                shutil.rmtree(self.temp_dir)\n",
    "                self.logger.info(f\"\uD83E\uDDF9 Cleaned up intermediate storage: {self.temp_dir}\")\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Failed to cleanup temp directory {self.temp_dir}: {e}\")\n",
    "            finally:\n",
    "                self.temp_dir = None\n",
    "                self.batch_files = []\n",
    "                self.initialized = False\n",
    "                \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get storage statistics.\"\"\"\n",
    "        if not self.temp_dir or not os.path.exists(self.temp_dir):\n",
    "            return {\"initialized\": False}\n",
    "            \n",
    "        total_size = 0\n",
    "        for batch_file in self.batch_files:\n",
    "            if os.path.exists(batch_file):\n",
    "                total_size += os.path.getsize(batch_file)\n",
    "                \n",
    "        return {\n",
    "            \"initialized\": True,\n",
    "            \"temp_dir\": self.temp_dir,\n",
    "            \"num_batches\": len(self.batch_files),\n",
    "            \"total_size_mb\": total_size / (1024 * 1024),\n",
    "            \"use_case_count\": self.get_total_count()\n",
    "        }\n",
    "    \n",
    "    def save_column_tracking(self, fqtn: str, column_names: list):\n",
    "        \"\"\"\n",
    "        Save column tracking for a table to disk.\n",
    "        \n",
    "        Args:\n",
    "            fqtn: Fully qualified table name (catalog.schema.table)\n",
    "            column_names: List of column names that were kept\n",
    "        \"\"\"\n",
    "        if not self.initialized:\n",
    "            self.initialize()\n",
    "        \n",
    "        # Create column_tracking subdirectory if it doesn't exist\n",
    "        tracking_dir = os.path.join(self.temp_dir, \"column_tracking\")\n",
    "        os.makedirs(tracking_dir, exist_ok=True)\n",
    "        \n",
    "        # Use a safe filename (replace dots and special chars)\n",
    "        safe_filename = fqtn.replace('.', '_').replace('`', '') + \".json\"\n",
    "        tracking_file = os.path.join(tracking_dir, safe_filename)\n",
    "        \n",
    "        with open(tracking_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                \"fqtn\": fqtn,\n",
    "                \"columns\": column_names,\n",
    "                \"timestamp\": datetime.datetime.now().isoformat()\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        self.logger.debug(f\"\uD83D\uDCBE Saved column tracking for {fqtn}: {len(column_names)} columns\")\n",
    "    \n",
    "    def load_column_tracking(self, fqtn: str) -> list:\n",
    "        \"\"\"\n",
    "        Load column tracking for a table from disk.\n",
    "        \n",
    "        Args:\n",
    "            fqtn: Fully qualified table name (catalog.schema.table)\n",
    "            \n",
    "        Returns:\n",
    "            List of column names that were kept, or None if no tracking exists\n",
    "        \"\"\"\n",
    "        if not self.temp_dir or not os.path.exists(self.temp_dir):\n",
    "            return None\n",
    "        \n",
    "        tracking_dir = os.path.join(self.temp_dir, \"column_tracking\")\n",
    "        if not os.path.exists(tracking_dir):\n",
    "            return None\n",
    "        \n",
    "        # Use a safe filename (replace dots and special chars)\n",
    "        safe_filename = fqtn.replace('.', '_').replace('`', '') + \".json\"\n",
    "        tracking_file = os.path.join(tracking_dir, safe_filename)\n",
    "        \n",
    "        if not os.path.exists(tracking_file):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            with open(tracking_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                return data.get(\"columns\", None)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Failed to load column tracking for {fqtn}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def has_column_tracking(self, fqtn: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if column tracking exists for a table.\n",
    "        \n",
    "        Args:\n",
    "            fqtn: Fully qualified table name (catalog.schema.table)\n",
    "            \n",
    "        Returns:\n",
    "            True if tracking exists, False otherwise\n",
    "        \"\"\"\n",
    "        if not self.temp_dir or not os.path.exists(self.temp_dir):\n",
    "            return False\n",
    "        \n",
    "        tracking_dir = os.path.join(self.temp_dir, \"column_tracking\")\n",
    "        if not os.path.exists(tracking_dir):\n",
    "            return False\n",
    "        \n",
    "        safe_filename = fqtn.replace('.', '_').replace('`', '') + \".json\"\n",
    "        tracking_file = os.path.join(tracking_dir, safe_filename)\n",
    "    \n",
    "    def save_pass1_ids(self, use_case_ids: list):\n",
    "        \"\"\"Save PASS 1 use case IDs to disk for memory-efficient comparison.\"\"\"\n",
    "        if not self.initialized:\n",
    "            self.initialize()\n",
    "        ids_file = os.path.join(self.temp_dir, \"pass1_ids.json\")\n",
    "        with open(ids_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(use_case_ids, f)\n",
    "        self.logger.debug(f\"\uD83D\uDCBE Saved {len(use_case_ids)} PASS 1 IDs to disk\")\n",
    "    \n",
    "    def load_pass1_ids(self) -> set:\n",
    "        \"\"\"Load PASS 1 use case IDs from disk as a set for O(1) lookup.\"\"\"\n",
    "        if not self.temp_dir or not os.path.exists(self.temp_dir):\n",
    "            return set()\n",
    "        ids_file = os.path.join(self.temp_dir, \"pass1_ids.json\")\n",
    "        if not os.path.exists(ids_file):\n",
    "            return set()\n",
    "        try:\n",
    "            with open(ids_file, 'r', encoding='utf-8') as f:\n",
    "                return set(json.load(f))\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Failed to load PASS 1 IDs: {e}\")\n",
    "            return set()\n",
    "    \n",
    "    def save_feedback_file(self, feedback_lines: list):\n",
    "        \"\"\"Save feedback string to disk to avoid keeping large string in memory.\"\"\"\n",
    "        if not self.initialized:\n",
    "            self.initialize()\n",
    "        feedback_file = os.path.join(self.temp_dir, \"pass1_feedback.txt\")\n",
    "        with open(feedback_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"\\n\".join(feedback_lines))\n",
    "        self.logger.debug(f\"\uD83D\uDCBE Saved feedback to disk ({len(feedback_lines)} lines)\")\n",
    "    \n",
    "    def load_feedback_file(self) -> str:\n",
    "        \"\"\"Load feedback string from disk.\"\"\"\n",
    "        if not self.temp_dir or not os.path.exists(self.temp_dir):\n",
    "            return \"\"\n",
    "        feedback_file = os.path.join(self.temp_dir, \"pass1_feedback.txt\")\n",
    "        if not os.path.exists(feedback_file):\n",
    "            return \"\"\n",
    "        try:\n",
    "            with open(feedback_file, 'r', encoding='utf-8') as f:\n",
    "                return f.read()\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Failed to load feedback file: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def iter_pass1_use_cases_for_feedback(self, limit: int = 200):\n",
    "        \"\"\"\n",
    "        Memory-efficient iterator for building feedback.\n",
    "        Yields (idx, name, tables) tuples from PASS 1 batches without loading all into memory.\n",
    "        \"\"\"\n",
    "        idx = 0\n",
    "        for batch in self.iter_batches():\n",
    "            for uc in batch:\n",
    "                if idx >= limit:\n",
    "                    return\n",
    "                idx += 1\n",
    "                name = str(uc.get('Name', '')).replace('|', '-')[:80]\n",
    "                tables = str(uc.get('Tables Involved', '')).replace('|', '-')[:60]\n",
    "                yield (idx, name, tables)\n",
    "    \n",
    "    def save_id_maps(self, column_id_map: dict, id_column_map: dict, table_id_map: dict, id_table_map: dict):\n",
    "        \"\"\"Save column/table ID maps to disk to reduce memory footprint.\"\"\"\n",
    "        if not self.initialized:\n",
    "            self.initialize()\n",
    "        maps_file = os.path.join(self.temp_dir, \"id_maps.json\")\n",
    "        data = {\n",
    "            \"column_id_map\": column_id_map,\n",
    "            \"id_column_map\": id_column_map,\n",
    "            \"table_id_map\": table_id_map,\n",
    "            \"id_table_map\": id_table_map\n",
    "        }\n",
    "        with open(maps_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f)\n",
    "        self.logger.debug(f\"\uD83D\uDCBE Saved ID maps to disk (columns: {len(column_id_map)}, tables: {len(table_id_map)})\")\n",
    "    \n",
    "    def load_id_maps(self) -> tuple:\n",
    "        \"\"\"Load ID maps from disk. Returns (column_id_map, id_column_map, table_id_map, id_table_map).\"\"\"\n",
    "        if not self.temp_dir or not os.path.exists(self.temp_dir):\n",
    "            return {}, {}, {}, {}\n",
    "        maps_file = os.path.join(self.temp_dir, \"id_maps.json\")\n",
    "        if not os.path.exists(maps_file):\n",
    "            return {}, {}, {}, {}\n",
    "        try:\n",
    "            with open(maps_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            return (data.get(\"column_id_map\", {}), data.get(\"id_column_map\", {}),\n",
    "                    data.get(\"table_id_map\", {}), data.get(\"id_table_map\", {}))\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Failed to load ID maps: {e}\")\n",
    "            return {}, {}, {}, {}\n",
    "        \n",
    "        return os.path.exists(tracking_file)\n",
    "\n",
    "class DatabricksInspire:\n",
    "    \n",
    "    @staticmethod\n",
    "    def _extract_primary_table(tables_involved: str) -> str:\n",
    "        \"\"\"\n",
    "        Extracts the primary (first) table from the Tables Involved field.\n",
    "        \n",
    "        Args:\n",
    "            tables_involved: Comma-separated string of table names\n",
    "            \n",
    "        Returns:\n",
    "            The first table name, or 'N/A' if empty\n",
    "        \"\"\"\n",
    "        if not tables_involved or not isinstance(tables_involved, str):\n",
    "            return \"N/A\"\n",
    "        \n",
    "        # Split by comma and get the first table\n",
    "        tables = [t.strip() for t in tables_involved.split(',') if t.strip()]\n",
    "        if tables:\n",
    "            return tables[0]\n",
    "        return \"N/A\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.spark = spark\n",
    "        self.dbutils = dbutils\n",
    "        # Initialize workspace client with timeout configuration to prevent indefinite hangs\n",
    "        from databricks.sdk.config import Config\n",
    "        config = Config()\n",
    "        config.retry_timeout_seconds = 300  # 5 minute timeout for individual API calls\n",
    "        self.workspace = WorkspaceClient(config=config)  # Changed from w_client to workspace\n",
    "        self.w_client = self.workspace  # Keep w_client as alias for backward compatibility\n",
    "\n",
    "        # --- Store Widget Values ---\n",
    "        self.business_name = kwargs.get(\"business\", \"gen\")\n",
    "        \n",
    "        # --- Business Priorities (user-provided, multi-select) ---\n",
    "        business_priorities_raw = kwargs.get(\"business_priorities\", \"\")\n",
    "        self.user_business_priorities = [goal.strip() for goal in business_priorities_raw.split(',') if goal.strip()]\n",
    "        \n",
    "        # --- Strategic Goals (user-provided, comma-separated) ---\n",
    "        strategic_goals_raw = kwargs.get(\"strategic_goals\", \"\")\n",
    "        self.user_strategic_goals = [goal.strip() for goal in strategic_goals_raw.split(',') if goal.strip()]\n",
    "        \n",
    "        # --- Business Domains (user-provided, comma-separated) ---\n",
    "        business_domains_raw = kwargs.get(\"business_domains\", \"\")\n",
    "        self.user_business_domains = [domain.strip() for domain in business_domains_raw.split(',') if domain.strip()]\n",
    "        \n",
    "        self.additional_context = \"\"\n",
    "        \n",
    "        self.catalogs_str = kwargs.get(\"catalogs\", \"\")\n",
    "        self.schemas_str = kwargs.get(\"schemas\", \"\")\n",
    "        self.tables_str = kwargs.get(\"tables\", \"\")\n",
    "        self.generate_choices = [opt.strip() for opt in kwargs.get(\"generate\", \"use cases\").split(',')]\n",
    "        \n",
    "        self.generation_path = kwargs.get(\"generation_path\", \"./\")\n",
    "        \n",
    "        self.output_languages = [lang.strip() for lang in kwargs.get(\"output_language\", \"English\").split(',') if lang.strip()]\n",
    "        if not self.output_languages:\n",
    "            self.output_languages = [\"English\"]\n",
    "            \n",
    "        raw_max_parallelism = kwargs.get(\"max_parallelism\", None)\n",
    "        if raw_max_parallelism is None or str(raw_max_parallelism).strip() == \"\":\n",
    "            self.max_parallelism = 0\n",
    "            self.auto_parallelism = True\n",
    "        else:\n",
    "            self.max_parallelism = min(int(raw_max_parallelism), 100)\n",
    "            self.auto_parallelism = False\n",
    "        self.scan_parallelism = int(kwargs.get(\"scan_parallelism\", 5) or 5)\n",
    "        if self.scan_parallelism < 1:\n",
    "            self.scan_parallelism = 5\n",
    "        if self.scan_parallelism > 20:\n",
    "            self.scan_parallelism = 20\n",
    "        self.cluster_memory_gb = int(kwargs.get(\"cluster_memory_gb\", 32) or 32)\n",
    "        self.cluster_worker_count = int(kwargs.get(\"cluster_worker_count\", 2) or 2)\n",
    "        \n",
    "        # === Use unstructured data flag (from Generation Options) ===\n",
    "        self.use_unstructured_data = kwargs.get(\"use_unstructured_data\", \"yes\").lower() == \"yes\"\n",
    "        \n",
    "        # === Technical exclusion strategy (from Generation Options) ===\n",
    "        raw_technical_exclusion_strategy = kwargs.get(\"technical_exclusion_strategy\", \"Aggressive\")\n",
    "        if not raw_technical_exclusion_strategy or str(raw_technical_exclusion_strategy).strip().lower() == \"none\":\n",
    "            raw_technical_exclusion_strategy = \"Aggressive\"\n",
    "        self.technical_exclusion_strategy = raw_technical_exclusion_strategy\n",
    "        \n",
    "        # === Operation Mode (NEW - controls main operation) ===\n",
    "        self.operation_mode = kwargs.get(\"operation_mode\", \"Discover Usecases\")\n",
    "        \n",
    "        # === SQL Code Generation Flag (from Generation Options) ===\n",
    "        self.generate_sql_code = \"SQL Code\" in self.generate_choices\n",
    "        if not self.generate_sql_code:\n",
    "            log_print(\"ℹ️ SQL Code generation DISABLED - notebooks will have placeholder SQL\")\n",
    "        \n",
    "        # === SQL Model Serving (model endpoint for ai_query in generated SQL) ===\n",
    "        self.sql_model_serving = kwargs.get(\"sql_model_serving\", \"databricks-gpt-oss-120b\").strip()\n",
    "        if not self.sql_model_serving:\n",
    "            self.sql_model_serving = \"databricks-gpt-oss-120b\"\n",
    "\n",
    "        # === NEW: Global LLM timeout and retry controls ===\n",
    "        # User requested explicit global timeout of 300 seconds\n",
    "        self.llm_timeout_seconds = 300 \n",
    "        # Base timeout for SQL generation - will be adjusted adaptively based on CTE count\n",
    "        self.sql_generation_base_timeout = 180  # Base: 3 minutes\n",
    "        self.sql_generation_per_cte_timeout = 30  # Add 30s per CTE\n",
    "        self.sql_generation_max_timeout = 360  # Cap at 6 minutes\n",
    "        # max_retry represents how many times to retry after the first attempt\n",
    "        self.max_retry_attempts = max(0, int(kwargs.get(\"max_retry\", 1)))\n",
    "        \n",
    "        # === NEW: Initialize merged business context ===\n",
    "        self.merged_business_context = {}\n",
    "        \n",
    "        # === NEW: JSON file path for docs-only mode ===\n",
    "        self.json_file_path = kwargs.get(\"json_file_path\", None)\n",
    "\n",
    "        # --- Setup Logging & Output ---\n",
    "        self.sanitized_customer_name = self._sanitize_name(self.business_name)\n",
    "        \n",
    "        # Keep log in /tmp during execution (always writable), will copy to output dir at end\n",
    "        self.local_log_output_dir = f\"/tmp/{self.sanitized_customer_name}\"\n",
    "\n",
    "        resolved_generation_path = self.generation_path\n",
    "        if resolved_generation_path.startswith(\"./\"):\n",
    "            try:\n",
    "                logical_notebook_path = self.dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "                current_notebook_dir = os.path.dirname(logical_notebook_path)\n",
    "                relative_path = resolved_generation_path[2:]\n",
    "                resolved_generation_path = os.path.join(current_notebook_dir, relative_path)\n",
    "                log_print(f\"Resolved relative generation path to: {resolved_generation_path}\")\n",
    "            except Exception:\n",
    "                default_path = f\"/Shared/{self.sanitized_customer_name}_output\"\n",
    "                log_print(f\"Could not determine notebook path for './'. Defaulting to {default_path}.\", level=\"WARNING\")\n",
    "                resolved_generation_path = default_path\n",
    "\n",
    "        self.base_output_dir = os.path.join(resolved_generation_path, self.sanitized_customer_name)\n",
    "        self.notebook_output_dir = os.path.join(self.base_output_dir, \"notebooks\")\n",
    "        self.docs_output_dir = os.path.join(self.base_output_dir, \"docs\")\n",
    "\n",
    "        setup_logging(self.local_log_output_dir) \n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        \n",
    "        # Initialize memory-efficient storage manager\n",
    "        self.storage_manager = IntermediateStorageManager(base_path=\"/tmp\", logger=self.logger)\n",
    "        \n",
    "        # Initialize honesty tracking for reporting what happened during processing\n",
    "        self.processing_honesty = {\n",
    "            'total_tables_discovered': 0,\n",
    "            'total_tables_processed': 0,\n",
    "            'total_batches_created': 0,\n",
    "            'total_batch_splits': 0,\n",
    "            'tables_with_columns_dropped': [],\n",
    "            'batch_split_history': [],\n",
    "            'tables_completely_processed': [],\n",
    "            'tables_partially_processed': [],\n",
    "            'tables_skipped': []\n",
    "        }\n",
    "        self.validation_timeouts_discarded = []\n",
    "        \n",
    "        max_parallelism_display = self.max_parallelism if not self.auto_parallelism else \"auto\"\n",
    "        self.logger.info(f\"DatabricksInspire initialized for business: {self.business_name}. Target Language(s): {self.output_languages}. Base Output Dir: {self.base_output_dir}. Notebooks Dir: {self.notebook_output_dir}. Docs Dir: {self.docs_output_dir}. Scan parallelism: {self.scan_parallelism}. Max parallelism: {max_parallelism_display}\")\n",
    "        \n",
    "        # Skip cleanup for modes that work on existing notebooks\n",
    "        is_preserve_mode = self.operation_mode in [\"Re-generate SQL\", \"Generate Sample Result\"]\n",
    "        \n",
    "        try:\n",
    "            if is_preserve_mode:\n",
    "                self.logger.info(f\"ℹ️ {self.operation_mode.upper()} MODE: Skipping directory cleanup to preserve existing files\")\n",
    "                log_print(f\"ℹ️ {self.operation_mode} mode: Preserving existing files (expected behavior)\")\n",
    "            else:\n",
    "                self.logger.info(f\"Cleaning up existing output directory...\")\n",
    "                try:\n",
    "                    self.w_client.workspace.delete(self.base_output_dir, recursive=True)\n",
    "                    self.logger.info(f\"Successfully deleted existing output directory: {self.base_output_dir}\")\n",
    "                except Exception as delete_error:\n",
    "                    self.logger.debug(f\"No existing directory to delete or deletion failed: {delete_error}\")\n",
    "                \n",
    "                self.logger.info(f\"Creating fresh workspace directories...\")\n",
    "                self.w_client.workspace.mkdirs(self.base_output_dir)\n",
    "                self.w_client.workspace.mkdirs(self.notebook_output_dir)\n",
    "                self.w_client.workspace.mkdirs(self.docs_output_dir)\n",
    "                self.logger.info(f\"Successfully created all output directories.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create workspace directories: {e}\")\n",
    "        \n",
    "        if 'PROMPT_TEMPLATES' not in globals():\n",
    "            self.logger.critical(\"CRITICAL ERROR: 'PROMPT_TEMPLATES' dictionary is not defined. Please run the cell defining it.\")\n",
    "            raise NameError(\"PROMPT_TEMPLATES not found. Please run the cell that defines the prompt dictionary.\")\n",
    "\n",
    "        self.ai_agent = AIAgent(\n",
    "            spark=self.spark,\n",
    "            logger=self.logger,\n",
    "            worker_llm_config=AI_MODEL_NAME,\n",
    "            judge_llm_config=AI_MODEL_NAME,\n",
    "            prompt_templates=PROMPT_TEMPLATES,\n",
    "            default_timeout_seconds=self.llm_timeout_seconds,\n",
    "            max_retry_attempts=self.max_retry_attempts\n",
    "        )\n",
    "        \n",
    "        self.translation_service = TranslationService(ai_agent=self.ai_agent, logger=self.logger)\n",
    "        \n",
    "        # === NEW: Column Registry (Bitmap Technique) ===\n",
    "        self.column_id_map = {}   # FQN -> ID\n",
    "        self.id_column_map = {}   # ID -> {fqn, description}\n",
    "        self.next_column_id = 1\n",
    "        # === NEW: Table Registry (Bitmap Technique) ===\n",
    "        self.table_id_map = {}    # Table FQN -> ID\n",
    "        self.id_table_map = {}    # ID -> table FQN\n",
    "        self.next_table_id = 1\n",
    "        self.registry_lock = threading.Lock()\n",
    "        \n",
    "        self.data_loader = None\n",
    "        # Skip data loader if we're in docs-only mode (JSON file path provided)\n",
    "        # Note: \"use cases\" is always generated (removed from widget options)\n",
    "        if not self.json_file_path:\n",
    "            if (\"PDF Catalog\" in self.generate_choices or \n",
    "                \"Use Cases Catalog PDF\" in self.generate_choices or\n",
    "                \"Presentation\" in self.generate_choices or\n",
    "                \"SQL Regeneration\" not in self.generate_choices):\n",
    "                \n",
    "                if not (self.catalogs_str or self.schemas_str or self.tables_str):\n",
    "                    self.logger.error(\"For 'use cases', 'PDF', or 'Presentation' you must provide at least one catalog, schema, or table.\")\n",
    "                else:\n",
    "                    # === MEMORY-OPTIMIZED DATA LOADER ===\n",
    "                    # Enable two-pass mode for intelligent batching based on table sizes\n",
    "                    # Enable column sampling for tables with >250 columns\n",
    "                    # Use streaming for schemas with >10K tables\n",
    "                    self.data_loader = DataLoader(\n",
    "                        catalogs=self.catalogs_str,\n",
    "                        schemas=self.schemas_str,\n",
    "                        tables=self.tables_str,\n",
    "                        logger=self.logger,\n",
    "                        enable_two_pass=True,           # Enable intelligent batching\n",
    "                        enable_column_sampling=True,    # Sample wide tables\n",
    "                        streaming_batch_size=1000,      # Stream in chunks of 1000 tables\n",
    "                        max_parallelism=self.scan_parallelism,\n",
    "                        schema_timeout_seconds=900      # 15 min timeout per schema\n",
    "                    )\n",
    "        \n",
    "    def _calculate_dynamic_parallelism(self, total_tables, total_schema_chars, safe_context_limit, base_prompt_size):\n",
    "        memory_pool = max(1, self.cluster_memory_gb * max(self.cluster_worker_count, 1))\n",
    "        max_by_memory = max(2, min(8, int(memory_pool // 8)))\n",
    "        if total_tables <= 0:\n",
    "            return max_by_memory, 0, 0, 0, max_by_memory\n",
    "        avg_table_chars = max(1, int(total_schema_chars / total_tables))\n",
    "        available_chars = max(1, safe_context_limit - base_prompt_size)\n",
    "        tables_per_batch = max(1, int(available_chars // avg_table_chars))\n",
    "        est_batches = int((total_tables + tables_per_batch - 1) // tables_per_batch) if tables_per_batch else total_tables\n",
    "        size_factor = 1.0\n",
    "        if avg_table_chars >= 24000:\n",
    "            size_factor = 0.5\n",
    "        elif avg_table_chars >= 18000:\n",
    "            size_factor = 0.7\n",
    "        elif avg_table_chars >= 12000:\n",
    "            size_factor = 0.85\n",
    "        recommended = int(max_by_memory * size_factor)\n",
    "        if recommended < 2:\n",
    "            recommended = 2\n",
    "        if recommended > max_by_memory:\n",
    "            recommended = max_by_memory\n",
    "        if est_batches > 0 and recommended > est_batches:\n",
    "            recommended = est_batches\n",
    "        if recommended < 1:\n",
    "            recommended = 1\n",
    "        return recommended, tables_per_batch, est_batches, avg_table_chars, max_by_memory\n",
    "        \n",
    "    def _get_translations(self, language: str) -> dict:\n",
    "        return self.translation_service.get_translations(language)\n",
    "\n",
    "    def _report_processing_honesty(self):\n",
    "        \"\"\"\n",
    "        Generates and displays a detailed report of what happened during processing.\n",
    "        Reports on batch splits, column drops, and whether all data was processed completely.\n",
    "        \"\"\"\n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"\uD83D\uDCCA PROCESSING HONESTY REPORT\")\n",
    "        log_print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        h = self.processing_honesty\n",
    "        \n",
    "        log_print(f\"\uD83D\uDCC8 Overall Statistics:\")\n",
    "        log_print(f\"   • Total tables discovered: {h['total_tables_discovered']}\")\n",
    "        log_print(f\"   • Total tables processed: {h['total_tables_processed']}\")\n",
    "        log_print(f\"   • Total batches created: {h['total_batches_created']}\")\n",
    "        log_print(f\"   • Total batch splits performed: {h['total_batch_splits']}\")\n",
    "        \n",
    "        if h['batch_split_history']:\n",
    "            proactive_splits = [s for s in h['batch_split_history'] if s.get('split_type') == 'Proactive']\n",
    "            reactive_splits = [s for s in h['batch_split_history'] if s.get('split_type') == 'Reactive']\n",
    "            \n",
    "            log_print(f\"\\n⚡ Batch Splitting Details:\")\n",
    "            log_print(f\"   {h['total_batch_splits']} batch(es) were split to fit LLM context limits:\")\n",
    "            log_print(f\"   • Proactive splits (detected before LLM call): {len(proactive_splits)}\")\n",
    "            log_print(f\"   • Reactive splits (after LLM error): {len(reactive_splits)}\")\n",
    "            \n",
    "            for idx, split in enumerate(h['batch_split_history'], 1):\n",
    "                split_icon = \"\uD83D\uDD2E\" if split.get('split_type') == 'Proactive' else \"⚡\"\n",
    "                log_print(f\"   {idx}. {split_icon} {split.get('split_type', 'Unknown')} | Batch '{split['batch']}': {split['original_tables']} tables → {split['split_into']} sub-batches\")\n",
    "                log_print(f\"      - Sub-batch 1: {split['sub_batch_1_tables']} tables\")\n",
    "                log_print(f\"      - Sub-batch 2: {split['sub_batch_2_tables']} tables\")\n",
    "        else:\n",
    "            log_print(f\"\\n✅ No batch splitting was required - all batches fit within LLM context limits\")\n",
    "        \n",
    "        if h['tables_with_columns_dropped']:\n",
    "            log_print(f\"\\n⚠️  Column Dropping Details:\", level=\"WARNING\")\n",
    "            log_print(f\"   {len(h['tables_with_columns_dropped'])} table(s) had columns dropped to fit LLM limits:\")\n",
    "            for idx, drop_info in enumerate(h['tables_with_columns_dropped'], 1):\n",
    "                business_tag = \"\uD83D\uDD35 BUSINESS\" if drop_info['is_business'] else \"⚪ non-business\"\n",
    "                log_print(f\"   {idx}. {business_tag} | {drop_info['table']}\")\n",
    "                log_print(f\"      - Original columns: {drop_info['original_columns']}\")\n",
    "                log_print(f\"      - Kept columns: {drop_info['kept_columns']} ({drop_info['drop_percentage']:.1f}% dropped)\")\n",
    "                log_print(f\"      - Dropped columns: {drop_info['dropped_columns']}\")\n",
    "        else:\n",
    "            log_print(f\"\\n✅ No columns were dropped - all tables processed with full schema\")\n",
    "        \n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"\uD83C\uDFAF HONESTY ASSESSMENT:\")\n",
    "        log_print(f\"{'='*80}\")\n",
    "        \n",
    "        honesty_percentage = 100.0\n",
    "        issues = []\n",
    "        \n",
    "        if h['batch_split_history']:\n",
    "            issues.append(f\"• {h['total_batch_splits']} batch split(s) occurred (but all tables were still processed)\")\n",
    "        \n",
    "        if h['tables_with_columns_dropped']:\n",
    "            total_columns_dropped = sum(d['dropped_columns'] for d in h['tables_with_columns_dropped'])\n",
    "            total_columns_original = sum(d['original_columns'] for d in h['tables_with_columns_dropped'])\n",
    "            drop_percentage = (total_columns_dropped / total_columns_original * 100) if total_columns_original > 0 else 0\n",
    "            \n",
    "            issues.append(f\"• {len(h['tables_with_columns_dropped'])} table(s) had columns dropped ({drop_percentage:.1f}% of columns from affected tables)\")\n",
    "            \n",
    "            honesty_percentage -= min(drop_percentage / 2, 30)\n",
    "        \n",
    "        if not issues:\n",
    "            log_print(f\"\\n✅ 100% HONEST - All tables processed completely with all columns\")\n",
    "            log_print(f\"   No compromises were made during processing.\")\n",
    "        else:\n",
    "            log_print(f\"\\n\uD83D\uDCCA Honesty Score: {honesty_percentage:.1f}%\")\n",
    "            log_print(f\"\\n   Processing required the following compromises:\")\n",
    "            for issue in issues:\n",
    "                log_print(f\"   {issue}\")\n",
    "            log_print(f\"\\n   ℹ️  Note: Batch splits don't affect completeness - all tables are still processed.\")\n",
    "            log_print(f\"   ⚠️  Column drops DO affect completeness - some schema details were omitted.\", level=\"WARNING\")\n",
    "        \n",
    "        log_print(f\"\\n{'='*80}\\n\")\n",
    "    \n",
    "    def _get_lang_abbr(self, language: str) -> str:\n",
    "        \"\"\"Returns a 2-letter abbreviation for a language.\"\"\"\n",
    "        lang_map = {\n",
    "            \"english\": \"en\",\n",
    "            \"arabic\": \"ar\",\n",
    "            \"french\": \"fr\",\n",
    "            \"spanish\": \"es\",\n",
    "            \"german\": \"de\",\n",
    "            \"portuguese\": \"pt\",\n",
    "            \"italian\": \"it\",\n",
    "            \"japanese\": \"ja\",\n",
    "            \"korean\": \"ko\",\n",
    "            \"chinese\": \"zh\"\n",
    "        }\n",
    "        return lang_map.get(language.lower(), language.lower()[:2])\n",
    "\n",
    "    # === BUSINESS CONTEXT AND SPONSOR MAPPING ===\n",
    "    def _get_business_context_from_llm(self) -> dict:\n",
    "        \"\"\"\n",
    "        Calls the BUSINESS_CONTEXT_WORKER_PROMPT to get comprehensive business context.\n",
    "        Returns a dict with 15 business context fields.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"\uD83D\uDD0D Calling Business Context Worker for: {self.business_name}\")\n",
    "        \n",
    "        try:\n",
    "            # First, determine the industry using a simple analysis\n",
    "            industry = self.business_name  # Default to business name\n",
    "            \n",
    "            prompt_vars = {\n",
    "                \"name\": self.business_name,\n",
    "                \"industry\": industry,\n",
    "                \"type_description\": \"a business entity or organization\",\n",
    "                \"type_label\": \"organization\"\n",
    "            }\n",
    "            \n",
    "            self.logger.info(f\"⏳ Waiting for LLM response (Business Context extraction)...\")\n",
    "            response_json = self.ai_agent.run_worker(\n",
    "                step_name=\"Business_Context_Extraction\",\n",
    "                worker_prompt_path=\"BUSINESS_CONTEXT_WORKER_PROMPT\",\n",
    "                prompt_vars=prompt_vars,\n",
    "                response_schema=None\n",
    "            )\n",
    "            self.logger.info(f\"✅ Received LLM response, parsing business context...\")\n",
    "            \n",
    "            # Parse JSON response\n",
    "            response_clean = clean_json_response(response_json)\n",
    "            context_data = json.loads(response_clean)\n",
    "            \n",
    "            self.logger.info(f\"✅ Business Context Worker extracted {len(context_data)} fields\")\n",
    "            return context_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to get business context from LLM: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _merge_business_contexts(self, llm_context: dict, user_context_str: str) -> dict:\n",
    "        \"\"\"\n",
    "        Merges LLM-generated business context with user-provided context.\n",
    "        User context ALWAYS takes precedence and overrides LLM context.\n",
    "        \n",
    "        Args:\n",
    "            llm_context: Dictionary from Business Context Worker (15 fields)\n",
    "            user_context_str: Comma-separated string from user (Business Context widget)\n",
    "            \n",
    "        Returns:\n",
    "            Merged dictionary with user values taking precedence\n",
    "        \"\"\"\n",
    "        merged_context = llm_context.copy()\n",
    "        \n",
    "        if not user_context_str or not user_context_str.strip():\n",
    "            self.logger.info(\"No user-provided business context. Using LLM context only.\")\n",
    "            return merged_context\n",
    "        \n",
    "        # Parse user context - assume it's comma-separated values that override specific fields\n",
    "        # For simplicity, we'll treat the entire user context as additional focus areas\n",
    "        user_focus_areas = [area.strip() for area in user_context_str.split(',') if area.strip()]\n",
    "        \n",
    "        if user_focus_areas:\n",
    "            self.logger.info(f\"✅ User provided {len(user_focus_areas)} business context items - these will take precedence\")\n",
    "            \n",
    "            # Store user focus areas separately for later use\n",
    "            # We'll use them to override or extend LLM context\n",
    "            merged_context['user_focus_areas'] = ', '.join(user_focus_areas)\n",
    "            \n",
    "            # Also update relevant fields with user context\n",
    "            if 'business_units_divisions_domains_subdomains' in merged_context:\n",
    "                # Prepend user areas to ensure they're used\n",
    "                existing = merged_context['business_units_divisions_domains_subdomains']\n",
    "                merged_context['business_units_divisions_domains_subdomains'] = ', '.join(user_focus_areas) + ', ' + existing\n",
    "            else:\n",
    "                merged_context['business_units_divisions_domains_subdomains'] = ', '.join(user_focus_areas)\n",
    "        \n",
    "        return merged_context\n",
    "    \n",
    "    # === USER DOMAIN ASSIGNMENT ===\n",
    "    def _assign_to_user_domains(self, use_cases: list, user_domains: list, language: str) -> list:\n",
    "        \"\"\"\n",
    "        Assigns use cases to user-provided business domains using LLM, \n",
    "        then discovers subdomains within each domain.\n",
    "        \n",
    "        This method is called when user provides specific business domains.\n",
    "        The user provides TOP-LEVEL DOMAINS ONLY - Inspire discovers subdomains automatically.\n",
    "        \n",
    "        Args:\n",
    "            use_cases: List of use case dictionaries\n",
    "            user_domains: List of user-provided domain names (top-level only)\n",
    "            language: Output language\n",
    "            \n",
    "        Returns:\n",
    "            List of use cases with Business Domain and Subdomain properly assigned\n",
    "        \"\"\"\n",
    "        import io\n",
    "        import csv\n",
    "        from collections import defaultdict, Counter\n",
    "        from concurrent.futures import ThreadPoolExecutor\n",
    "        import concurrent.futures\n",
    "        \n",
    "        self.logger.info(f\"\uD83D\uDCCD Assigning {len(use_cases)} use cases to {len(user_domains)} user-provided domains...\")\n",
    "        self.logger.info(f\"\uD83D\uDCCD User provided TOP-LEVEL domains only: {', '.join(user_domains)}\")\n",
    "        self.logger.info(f\"\uD83D\uDCCD Inspire will discover SUBDOMAINS within each domain automatically\")\n",
    "        \n",
    "        if not use_cases or not user_domains:\n",
    "            return use_cases\n",
    "        \n",
    "        # Build a simple prompt for domain assignment\n",
    "        domain_list_str = \", \".join([f'\"{d}\"' for d in user_domains])\n",
    "        \n",
    "        # Convert use cases to CSV for LLM\n",
    "        output = io.StringIO()\n",
    "        fieldnames = ['No', 'Name', 'type', 'Analytics Technique', 'Statement', 'Solution', \n",
    "                     'Business Value', 'Beneficiary', 'Sponsor', 'Tables Involved']\n",
    "        writer = csv.DictWriter(output, fieldnames=fieldnames, extrasaction='ignore')\n",
    "        writer.writeheader()\n",
    "        writer.writerows(use_cases)\n",
    "        use_cases_csv = output.getvalue()\n",
    "        \n",
    "        # Build custom prompt for user domain assignment\n",
    "        assignment_prompt = f\"\"\"You are an expert business analyst. Your task is to assign each use case to ONE of the following EXACT business domains:\n",
    "\n",
    "**ALLOWED DOMAINS (USE EXACTLY AS PROVIDED)**: {domain_list_str}\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL RULES \uD83D\uDEA8**:\n",
    "1. You MUST assign EVERY use case to EXACTLY ONE domain from the list above\n",
    "2. You MUST use the domain names EXACTLY as provided - no modifications\n",
    "3. You MUST NOT create or invent any new domain names\n",
    "4. Every use case MUST have a domain assigned\n",
    "\n",
    "**USE CASES TO ASSIGN**:\n",
    "{use_cases_csv}\n",
    "\n",
    "**OUTPUT FORMAT** (CSV only, no explanation):\n",
    "```csv\n",
    "use_case_id,domain\n",
    "```\n",
    "\n",
    "For each use case, output ONE row with the use case ID and the assigned domain.\n",
    "Start your response with the CSV header line: use_case_id,domain\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Call LLM for domain assignment using _call_ai_query directly\n",
    "            response_raw = self.ai_agent._call_ai_query(\n",
    "                prompt=assignment_prompt,\n",
    "                prompt_name=f\"Assign_User_Domains_{language}\",\n",
    "                response_schema=None,\n",
    "                display_name=f\"Assign_User_Domains_{language}\"\n",
    "            )\n",
    "            \n",
    "            # Clean response\n",
    "            response_clean = clean_json_response(response_raw)\n",
    "            \n",
    "            # Parse CSV response\n",
    "            csv_rows = CSVParser.parse_csv_string(\n",
    "                response_clean,\n",
    "                logger=self.logger,\n",
    "                context=\"User domain assignment\"\n",
    "            )\n",
    "            \n",
    "            # Build domain assignment map\n",
    "            domain_map = {}\n",
    "            for row in csv_rows:\n",
    "                uc_id = row.get('use_case_id', '').strip()\n",
    "                domain = row.get('domain', '').strip()\n",
    "                if uc_id and domain:\n",
    "                    # Validate domain is in user list\n",
    "                    if domain in user_domains:\n",
    "                        domain_map[uc_id] = domain\n",
    "                    else:\n",
    "                        # Try case-insensitive match\n",
    "                        for ud in user_domains:\n",
    "                            if ud.lower() == domain.lower():\n",
    "                                domain_map[uc_id] = ud\n",
    "                                break\n",
    "                        else:\n",
    "                            # Assign to first domain as fallback\n",
    "                            domain_map[uc_id] = user_domains[0]\n",
    "                            self.logger.warning(f\"Use case {uc_id} assigned invalid domain '{domain}', defaulting to '{user_domains[0]}'\")\n",
    "            \n",
    "            # Apply domain assignments (only domain, NOT subdomain yet)\n",
    "            assigned_count = 0\n",
    "            for uc in use_cases:\n",
    "                uc_id = uc.get('No', '')\n",
    "                if uc_id in domain_map:\n",
    "                    uc['Business Domain'] = domain_map[uc_id]\n",
    "                    assigned_count += 1\n",
    "                else:\n",
    "                    # Fallback: assign to first user domain\n",
    "                    uc['Business Domain'] = user_domains[0]\n",
    "                    self.logger.warning(f\"Use case {uc_id} not in LLM response, defaulting to '{user_domains[0]}'\")\n",
    "            \n",
    "            self.logger.info(f\"✅ Successfully assigned {assigned_count}/{len(use_cases)} use cases to user domains\")\n",
    "            \n",
    "            # Log domain distribution\n",
    "            domain_counts = Counter(uc.get('Business Domain', 'Unknown') for uc in use_cases)\n",
    "            for domain, count in sorted(domain_counts.items(), key=lambda x: -x[1]):\n",
    "                self.logger.info(f\"   \uD83D\uDCC1 {domain}: {count} use cases\")\n",
    "            \n",
    "            # === NOW DISCOVER SUBDOMAINS WITHIN EACH DOMAIN ===\n",
    "            self.logger.info(f\"\uD83D\uDCCD STEP 2: Discovering subdomains within each user-provided domain...\")\n",
    "            \n",
    "            # Group use cases by domain\n",
    "            domain_usecases_map = defaultdict(list)\n",
    "            for uc in use_cases:\n",
    "                domain = uc.get('Business Domain', '').strip()\n",
    "                if domain:\n",
    "                    domain_usecases_map[domain].append(uc)\n",
    "            \n",
    "            # ADAPTIVE PARALLELISM: Calculate based on number of domains and use cases\n",
    "            subdomain_parallelism, reason = calculate_adaptive_parallelism(\n",
    "                \"subdomain_detection\", self.max_parallelism,\n",
    "                num_items=len(use_cases),\n",
    "                num_domains=len(domain_usecases_map),\n",
    "                is_llm_operation=True, logger=self.logger\n",
    "            )\n",
    "            log_adaptive_parallelism_decision(\"subdomain_detection\", subdomain_parallelism, self.max_parallelism, reason)\n",
    "            self.logger.info(f\"Processing {len(domain_usecases_map)} domains for subdomain discovery...\")\n",
    "            \n",
    "            # Process each domain in parallel for subdomain detection\n",
    "            final_use_cases_with_subdomains = []\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=subdomain_parallelism, \n",
    "                                   thread_name_prefix=\"SubdomainDetect\") as executor:\n",
    "                # Submit subdomain detection for each domain\n",
    "                future_to_domain = {}\n",
    "                for domain_name, domain_use_cases in domain_usecases_map.items():\n",
    "                    future = executor.submit(\n",
    "                        self._detect_subdomains_for_domain,\n",
    "                        domain_name,\n",
    "                        domain_use_cases,\n",
    "                        language\n",
    "                    )\n",
    "                    future_to_domain[future] = domain_name\n",
    "                    self.logger.debug(f\"✓ Submitted subdomain detection for domain '{domain_name}' ({len(domain_use_cases)} use cases)\")\n",
    "                \n",
    "                # Collect results as they complete\n",
    "                for future in concurrent.futures.as_completed(future_to_domain):\n",
    "                    domain_name = future_to_domain[future]\n",
    "                    try:\n",
    "                        use_cases_with_subdomains = future.result()\n",
    "                        if use_cases_with_subdomains:\n",
    "                            self.logger.info(f\"✅ Domain '{domain_name}': Subdomain discovery complete ({len(use_cases_with_subdomains)} use cases)\")\n",
    "                            final_use_cases_with_subdomains.extend(use_cases_with_subdomains)\n",
    "                        else:\n",
    "                            # CRITICAL FIX: Assign default subdomains when discovery returns empty\n",
    "                            self.logger.warning(f\"⚠️ Domain '{domain_name}': Subdomain discovery returned no use cases - assigning default subdomains\")\n",
    "                            domain_use_cases = domain_usecases_map.get(domain_name, [])\n",
    "                            for uc in domain_use_cases:\n",
    "                                if not uc.get('Subdomain'):\n",
    "                                    uc['Subdomain'] = f\"General {domain_name}\"\n",
    "                            final_use_cases_with_subdomains.extend(domain_use_cases)\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"❌ Domain '{domain_name}': Subdomain discovery failed: {e}\")\n",
    "                        # Fall back to use cases without subdomains for this domain\n",
    "                        domain_use_cases = domain_usecases_map.get(domain_name, [])\n",
    "                        # Set default subdomain as \"General [Domain]\" when subdomain detection fails\n",
    "                        for uc in domain_use_cases:\n",
    "                            if not uc.get('Subdomain'):\n",
    "                                uc['Subdomain'] = f\"General {domain_name}\"\n",
    "                        self.logger.warning(f\"Using default subdomains for domain '{domain_name}'\")\n",
    "                        final_use_cases_with_subdomains.extend(domain_use_cases)\n",
    "            \n",
    "            self.logger.info(f\"✅ Domain assignment and subdomain discovery complete! {len(final_use_cases_with_subdomains)} use cases processed\")\n",
    "            \n",
    "            # Log subdomain distribution\n",
    "            subdomain_counts = Counter(uc.get('Subdomain', 'Unknown') for uc in final_use_cases_with_subdomains)\n",
    "            self.logger.info(f\"\uD83D\uDCCA Subdomain distribution:\")\n",
    "            for subdomain, count in sorted(subdomain_counts.items(), key=lambda x: -x[1]):\n",
    "                self.logger.info(f\"   \uD83D\uDCC1 {subdomain}: {count} use cases\")\n",
    "            \n",
    "            return final_use_cases_with_subdomains\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to assign user domains via LLM: {e}. Using fallback distribution with subdomain discovery.\")\n",
    "            \n",
    "            # Fallback: distribute use cases evenly across user domains\n",
    "            for idx, uc in enumerate(use_cases):\n",
    "                domain_idx = idx % len(user_domains)\n",
    "                uc['Business Domain'] = user_domains[domain_idx]\n",
    "            \n",
    "            # Even on fallback, try to discover subdomains\n",
    "            self.logger.info(f\"\uD83D\uDCCD Fallback: Attempting subdomain discovery for distributed domains...\")\n",
    "            \n",
    "            domain_usecases_map = defaultdict(list)\n",
    "            for uc in use_cases:\n",
    "                domain = uc.get('Business Domain', '').strip()\n",
    "                if domain:\n",
    "                    domain_usecases_map[domain].append(uc)\n",
    "            \n",
    "            final_use_cases = []\n",
    "            for domain_name, domain_use_cases in domain_usecases_map.items():\n",
    "                try:\n",
    "                    use_cases_with_subdomains = self._detect_subdomains_for_domain(\n",
    "                        domain_name, domain_use_cases, language\n",
    "                    )\n",
    "                    final_use_cases.extend(use_cases_with_subdomains)\n",
    "                except Exception as sub_e:\n",
    "                    self.logger.error(f\"Subdomain detection failed for domain '{domain_name}': {sub_e}\")\n",
    "                    # Last resort: set subdomain = \"General [Domain]\"\n",
    "                    for uc in domain_use_cases:\n",
    "                        uc['Subdomain'] = f\"General {domain_name}\"\n",
    "                    final_use_cases.extend(domain_use_cases)\n",
    "            \n",
    "            return final_use_cases if final_use_cases else use_cases\n",
    "    \n",
    "    # === HELPER FUNCTIONS ===\n",
    "    def _apply_domain_mapping_flat(self, all_use_cases: list, domain_mapping: dict) -> list:\n",
    "        \"\"\"Applies a domain mapping to a flat list[dict] of use cases.\"\"\"\n",
    "        modified_use_cases = []\n",
    "        for uc in all_use_cases:\n",
    "            original_domain = uc.get('Business Domain', 'Other')\n",
    "            new_domain = domain_mapping.get(original_domain, original_domain)\n",
    "            uc['Business Domain'] = new_domain\n",
    "            modified_use_cases.append(uc)\n",
    "        return modified_use_cases\n",
    "    \n",
    "    def _apply_subdomain_mapping(self, all_use_cases: list, subdomain_mapping: dict) -> list:\n",
    "        \"\"\"\n",
    "        Applies subdomain mapping to consolidate overlapping subdomains.\n",
    "        \n",
    "        The subdomain_mapping contains the FINAL list of subdomains per domain.\n",
    "        This function maps use cases' current subdomains to the consolidated ones\n",
    "        by finding the best matching subdomain from the mapping.\n",
    "        \n",
    "        Args:\n",
    "            all_use_cases: List of use case dictionaries\n",
    "            subdomain_mapping: Dict mapping domain names to lists of consolidated subdomain names\n",
    "            \n",
    "        Returns:\n",
    "            Modified list of use cases with updated Subdomain field\n",
    "        \"\"\"\n",
    "        modified_use_cases = []\n",
    "        \n",
    "        for uc in all_use_cases:\n",
    "            domain = uc.get('Business Domain', '')\n",
    "            current_subdomain = uc.get('Subdomain', '')\n",
    "            \n",
    "            # If this domain has a subdomain mapping, apply it\n",
    "            if domain in subdomain_mapping and isinstance(subdomain_mapping[domain], list):\n",
    "                target_subdomains = subdomain_mapping[domain]\n",
    "                \n",
    "                # Find the best matching subdomain from the consolidated list\n",
    "                # Strategy: Match by word overlap\n",
    "                best_match = None\n",
    "                best_overlap_score = 0\n",
    "                \n",
    "                current_words = set(current_subdomain.lower().split())\n",
    "                \n",
    "                for target_subdomain in target_subdomains:\n",
    "                    target_words = set(target_subdomain.lower().split())\n",
    "                    overlap = len(current_words & target_words)\n",
    "                    \n",
    "                    # If we have word overlap, this is a candidate\n",
    "                    if overlap > best_overlap_score:\n",
    "                        best_overlap_score = overlap\n",
    "                        best_match = target_subdomain\n",
    "                    # If no word overlap yet, check if current subdomain contains target words\n",
    "                    elif best_overlap_score == 0:\n",
    "                        # Check if any target word is contained in current subdomain\n",
    "                        for target_word in target_words:\n",
    "                            if target_word in current_subdomain.lower():\n",
    "                                best_match = target_subdomain\n",
    "                                break\n",
    "                \n",
    "                # Apply the best match if found\n",
    "                if best_match:\n",
    "                    uc['Subdomain'] = best_match\n",
    "                    self.logger.debug(f\"Mapped subdomain '{current_subdomain}' → '{best_match}' in domain '{domain}'\")\n",
    "            \n",
    "            modified_use_cases.append(uc)\n",
    "        \n",
    "        return modified_use_cases\n",
    "    \n",
    "    def _enforce_subdomain_constraints(self, use_cases: list) -> list:\n",
    "        \"\"\"\n",
    "        Enforces subdomain constraints:\n",
    "        1. Each domain has at most 8 subdomains\n",
    "        2. Each subdomain has at least 3 use cases\n",
    "        3. No overlapping subdomain names within a domain\n",
    "        \n",
    "        Args:\n",
    "            use_cases: List of use case dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            Modified list of use cases with enforced constraints\n",
    "        \"\"\"\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        # Group use cases by domain and subdomain\n",
    "        domain_subdomain_cases = defaultdict(lambda: defaultdict(list))\n",
    "        for uc in use_cases:\n",
    "            domain = uc.get('Business Domain', 'Other')\n",
    "            subdomain = uc.get('Subdomain', 'General')\n",
    "            domain_subdomain_cases[domain][subdomain].append(uc)\n",
    "        \n",
    "        # Process each domain\n",
    "        for domain, subdomains_dict in domain_subdomain_cases.items():\n",
    "            # STEP 1: Check for overlapping subdomain names and merge them\n",
    "            subdomain_list = list(subdomains_dict.keys())\n",
    "            merged_mapping = {}  # Maps old subdomain -> new subdomain\n",
    "            \n",
    "            for subdomain in subdomain_list:\n",
    "                if subdomain in merged_mapping:\n",
    "                    continue  # Already processed\n",
    "                \n",
    "                # Find all other subdomains that share words with this one\n",
    "                subdomain_words = set(subdomain.lower().split())\n",
    "                merge_targets = [subdomain]\n",
    "                \n",
    "                for other_subdomain in subdomain_list:\n",
    "                    if other_subdomain == subdomain or other_subdomain in merged_mapping:\n",
    "                        continue\n",
    "                    \n",
    "                    other_words = set(other_subdomain.lower().split())\n",
    "                    # If they share any words, merge them\n",
    "                    if subdomain_words & other_words:\n",
    "                        merge_targets.append(other_subdomain)\n",
    "                \n",
    "                # Choose the shortest name as the merged name\n",
    "                if len(merge_targets) > 1:\n",
    "                    merged_name = min(merge_targets, key=len)\n",
    "                    for target in merge_targets:\n",
    "                        merged_mapping[target] = merged_name\n",
    "                    self.logger.info(f\"Domain '{domain}': Merging overlapping subdomains {merge_targets} → '{merged_name}'\")\n",
    "                else:\n",
    "                    merged_mapping[subdomain] = subdomain\n",
    "            \n",
    "            # Apply merging\n",
    "            new_subdomains_dict = defaultdict(list)\n",
    "            for old_subdomain, cases in subdomains_dict.items():\n",
    "                new_subdomain = merged_mapping.get(old_subdomain, old_subdomain)\n",
    "                new_subdomains_dict[new_subdomain].extend(cases)\n",
    "            \n",
    "            # Update use cases with merged subdomain names\n",
    "            for new_subdomain, cases in new_subdomains_dict.items():\n",
    "                for uc in cases:\n",
    "                    uc['Subdomain'] = new_subdomain\n",
    "            \n",
    "            # STEP 2: Check subdomain count (min 2, max 8 per domain)\n",
    "            if len(new_subdomains_dict) < 2:\n",
    "                self.logger.warning(f\"Domain '{domain}' has only {len(new_subdomains_dict)} subdomain(s) (<2). Creating additional subdomains...\")\n",
    "                # If less than 2 subdomains, split the largest subdomain into 2\n",
    "                if len(new_subdomains_dict) == 1:\n",
    "                    subdomain_name = list(new_subdomains_dict.keys())[0]\n",
    "                    cases = new_subdomains_dict[subdomain_name]\n",
    "                    if len(cases) >= 6:  # Only split if we have enough cases\n",
    "                        mid = len(cases) // 2\n",
    "                        new_subdomains_dict[f\"{subdomain_name} A\"] = cases[:mid]\n",
    "                        new_subdomains_dict[f\"{subdomain_name} B\"] = cases[mid:]\n",
    "                        del new_subdomains_dict[subdomain_name]\n",
    "                        self.logger.info(f\"Domain '{domain}': Split '{subdomain_name}' into 2 subdomains to meet minimum requirement\")\n",
    "            \n",
    "            if len(new_subdomains_dict) > 8:\n",
    "                self.logger.warning(f\"Domain '{domain}' has {len(new_subdomains_dict)} subdomains (>8). Merging smallest ones...\")\n",
    "                \n",
    "                # Sort by use case count\n",
    "                sorted_subdomains = sorted(new_subdomains_dict.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "                \n",
    "                # Keep top 7 subdomains, merge the rest into the 8th\n",
    "                keep_subdomains = sorted_subdomains[:7]\n",
    "                merge_subdomains = sorted_subdomains[7:]\n",
    "                \n",
    "                # Create a merged subdomain name (use \"Other Services\" as catch-all - must be 2 words)\n",
    "                merged_subdomain_name = \"Other Services\"\n",
    "                merged_cases = []\n",
    "                for subdomain, cases in merge_subdomains:\n",
    "                    merged_cases.extend(cases)\n",
    "                \n",
    "                # Update use cases\n",
    "                for uc in merged_cases:\n",
    "                    uc['Subdomain'] = merged_subdomain_name\n",
    "                \n",
    "                self.logger.info(f\"Domain '{domain}': Merged {len(merge_subdomains)} small subdomains into '{merged_subdomain_name}'\")\n",
    "                \n",
    "                # Rebuild subdomains dict\n",
    "                new_subdomains_dict = dict(keep_subdomains)\n",
    "                new_subdomains_dict[merged_subdomain_name] = merged_cases\n",
    "            \n",
    "            # STEP 3: Check minimum use cases per subdomain (at least 3)\n",
    "            small_subdomains = [(sd, cases) for sd, cases in new_subdomains_dict.items() if len(cases) < 3]\n",
    "            \n",
    "            if small_subdomains:\n",
    "                self.logger.warning(f\"Domain '{domain}' has {len(small_subdomains)} subdomains with <3 use cases. Merging...\")\n",
    "                \n",
    "                # Merge small subdomains into the largest subdomain\n",
    "                sorted_by_size = sorted(new_subdomains_dict.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "                \n",
    "                if len(sorted_by_size) > 0:\n",
    "                    target_subdomain = sorted_by_size[0][0]\n",
    "                    \n",
    "                    for subdomain, cases in small_subdomains:\n",
    "                        if subdomain != target_subdomain:  # Don't merge into itself\n",
    "                            self.logger.info(f\"Domain '{domain}': Merging subdomain '{subdomain}' ({len(cases)} cases) into '{target_subdomain}'\")\n",
    "                            for uc in cases:\n",
    "                                uc['Subdomain'] = target_subdomain\n",
    "        \n",
    "        return use_cases\n",
    "\n",
    "    def _group_use_cases_by_domain_flat(self, all_use_cases: list) -> dict:\n",
    "        \"\"\"\n",
    "        Groups a flat list[dict] of use cases by their 'Business Domain'.\n",
    "        Use cases within each domain are sorted by priority descending (Very High first).\n",
    "        \n",
    "        NOTE: This sorting is used for PDF/PPT/XLS outputs. Notebooks use ID-based sorting instead.\n",
    "        \"\"\"\n",
    "        # Sort by priority first (for PDF/PPT/XLS outputs)\n",
    "        all_use_cases = sorted(all_use_cases, key=self._priority_sort_key)\n",
    "        grouped_by_domain = defaultdict(list)\n",
    "        for uc in all_use_cases:\n",
    "            domain = uc.get('Business Domain') or 'Other'\n",
    "            grouped_by_domain[domain].append(uc)\n",
    "        return grouped_by_domain\n",
    "\n",
    "    def _align_translated_data(self, master_grouped_data: dict, translated_flat_list: list) -> dict:\n",
    "        \"\"\"\n",
    "        Creates a grouped dictionary for the target language that strictly follows \n",
    "        the ordering and categorization of the master (English) data.\n",
    "        \"\"\"\n",
    "        translated_map = {uc['No']: uc for uc in translated_flat_list}\n",
    "        aligned_data = {}\n",
    "        for en_domain in master_grouped_data.keys(): \n",
    "            english_cases = master_grouped_data[en_domain]\n",
    "            translated_domain_name = en_domain\n",
    "            if english_cases:\n",
    "                first_id = english_cases[0]['No']\n",
    "                if first_id in translated_map:\n",
    "                    translated_domain_name = translated_map[first_id].get('Business Domain', en_domain)\n",
    "            \n",
    "            translated_group = []\n",
    "            for en_uc in english_cases:\n",
    "                uc_id = en_uc['No']\n",
    "                if uc_id in translated_map:\n",
    "                    translated_group.append(translated_map[uc_id])\n",
    "                else:\n",
    "                    translated_group.append(en_uc)\n",
    "            aligned_data[translated_domain_name] = translated_group\n",
    "        return aligned_data\n",
    "    # === END HELPER FUNCTIONS ===\n",
    "\n",
    "\n",
    "    \n",
    "    def _validate_sql_locally_with_spark(self, sql_query: str, use_case_id: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Validate SQL syntax using Databricks SQL Statement Execution API.\n",
    "        Uses REST API with minimal wait time and external links disposition for quick validation.\n",
    "        Creates a fresh workspace client for each call to avoid token expiry issues.\n",
    "        \n",
    "        Args:\n",
    "            sql_query: The SQL query to validate\n",
    "            use_case_id: Use case ID for logging\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (is_valid: bool, error_message: str or None)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import requests\n",
    "            import json\n",
    "            from databricks.sdk import WorkspaceClient\n",
    "            from databricks.sdk.config import Config\n",
    "            \n",
    "            # Create a FRESH workspace client with new token for each call\n",
    "            self.logger.debug(f\"[{use_case_id}] \uD83D\uDD04 Creating fresh workspace client for API validation...\")\n",
    "            \n",
    "            # Get current config\n",
    "            original_workspace = self.workspace\n",
    "            \n",
    "            # Create new config - this will force token refresh\n",
    "            config = Config(\n",
    "                host=original_workspace.config.host,\n",
    "                token=original_workspace.config.token\n",
    "            )\n",
    "            \n",
    "            # Create fresh workspace client\n",
    "            fresh_workspace = WorkspaceClient(config=config)\n",
    "            \n",
    "            # Get fresh token and URL\n",
    "            workspace_url = fresh_workspace.config.host\n",
    "            token = fresh_workspace.config.token\n",
    "            \n",
    "            # API endpoint\n",
    "            api_url = f\"{workspace_url}/api/2.0/sql/statements\"\n",
    "            \n",
    "            # Get SQL warehouse ID\n",
    "            sql_warehouse_id = getattr(self, 'sql_warehouse_id', None)\n",
    "            if not sql_warehouse_id:\n",
    "                self.logger.debug(f\"[{use_case_id}] No SQL warehouse configured for validation\")\n",
    "                return (True, None)\n",
    "            \n",
    "            # Request payload - using REST API validation pattern\n",
    "            # Setting wait_timeout to 50s and limiting rows to validate syntax without executing\n",
    "            payload = {\n",
    "                \"warehouse_id\": sql_warehouse_id,\n",
    "                \"statement\": sql_query,\n",
    "                \"wait_timeout\": \"50s\",\n",
    "                \"disposition\": \"EXTERNAL_LINKS\",\n",
    "                \"row_limit\": 1\n",
    "            }\n",
    "            \n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {token}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            self.logger.debug(f\"[{use_case_id}] \uD83C\uDF10 Attempting SQL Warehouse API validation with fresh token...\")\n",
    "            \n",
    "            # Make API call with fresh token\n",
    "            response = requests.post(api_url, headers=headers, json=payload, timeout=300)\n",
    "            \n",
    "            # Check response status\n",
    "            if response.status_code == 200:\n",
    "                # Query was accepted and syntax is valid\n",
    "                self.logger.info(f\"[{use_case_id}] ✅ API validation passed - SQL syntax is valid\")\n",
    "                return (True, None)\n",
    "                    \n",
    "            elif response.status_code == 400:\n",
    "                # 400 Bad Request indicates syntax error\n",
    "                try:\n",
    "                    error_data = response.json()\n",
    "                    error_code = error_data.get('error_code', '')\n",
    "                    error_msg = error_data.get('message', 'Syntax error')\n",
    "                    \n",
    "                    # Common error codes: SYNTAX_ERROR, UNRESOLVED_COLUMN, TABLE_OR_VIEW_NOT_FOUND, etc.\n",
    "                    if error_code:\n",
    "                        error_msg = f\"[{error_code}] {error_msg}\"\n",
    "                    \n",
    "                    self.logger.warning(f\"[{use_case_id}] ❌ API validation failed: {error_msg}\")\n",
    "                    return (False, error_msg)\n",
    "                except Exception as parse_error:\n",
    "                    error_msg = f\"Syntax error (status 400): {response.text[:200]}\"\n",
    "                    self.logger.warning(f\"[{use_case_id}] ❌ API validation failed: {error_msg}\")\n",
    "                    return (False, error_msg)\n",
    "                \n",
    "            elif response.status_code in [401, 403]:\n",
    "                try:\n",
    "                    error_data = response.json()\n",
    "                    error_detail = error_data.get('message', 'Unauthorized/Forbidden')\n",
    "                except:\n",
    "                    error_detail = 'Unauthorized/Forbidden'\n",
    "                \n",
    "                self.logger.debug(\n",
    "                    f\"[{use_case_id}] ⚠️  API authentication failed (status {response.status_code}): {error_detail}\"\n",
    "                )\n",
    "                return (True, None)\n",
    "                \n",
    "            else:\n",
    "                self.logger.debug(f\"[{use_case_id}] ⚠️  API returned status {response.status_code}\")\n",
    "                return (True, None)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Exception during API call\n",
    "            error_msg = str(e)\n",
    "            self.logger.debug(f\"[{use_case_id}] ❌ API validation failed (exception): {error_msg[:100]}...\")\n",
    "            return (False, error_msg)\n",
    "    \n",
    "    def _validate_sql_remotely_with_fresh_client(self, sql_query: str, use_case_id: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Validate SQL syntax using SQL Warehouse REST API (retry with fresh client).\n",
    "        Uses the SQL Statement Execution API with minimal wait time and external links disposition.\n",
    "        Creates a fresh workspace client for each call to avoid token expiry issues.\n",
    "        \n",
    "        Args:\n",
    "            sql_query: The SQL query to validate\n",
    "            use_case_id: Use case ID for logging\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (is_valid: bool, error_message: str or None)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import requests\n",
    "            import json\n",
    "            from databricks.sdk import WorkspaceClient\n",
    "            from databricks.sdk.config import Config\n",
    "            \n",
    "            # Create a FRESH workspace client with new token (retry attempt)\n",
    "            self.logger.debug(f\"[{use_case_id}] \uD83D\uDD04 Creating fresh workspace client for retry validation...\")\n",
    "            \n",
    "            # Get current config\n",
    "            original_workspace = self.workspace\n",
    "            \n",
    "            # Create new config - this will force token refresh\n",
    "            config = Config(\n",
    "                host=original_workspace.config.host,\n",
    "                token=original_workspace.config.token\n",
    "            )\n",
    "            \n",
    "            # Create fresh workspace client\n",
    "            fresh_workspace = WorkspaceClient(config=config)\n",
    "            \n",
    "            # Get fresh token\n",
    "            workspace_url = fresh_workspace.config.host\n",
    "            token = fresh_workspace.config.token\n",
    "            \n",
    "            # API endpoint\n",
    "            api_url = f\"{workspace_url}/api/2.0/sql/statements\"\n",
    "            \n",
    "            # Get SQL warehouse ID\n",
    "            sql_warehouse_id = getattr(self, 'sql_warehouse_id', None)\n",
    "            if not sql_warehouse_id:\n",
    "                self.logger.debug(f\"[{use_case_id}] No SQL warehouse configured for remote validation\")\n",
    "                return (True, None)\n",
    "            \n",
    "            # Request payload - using REST API validation pattern\n",
    "            # Setting wait_timeout to 50s and limiting rows to validate syntax without executing\n",
    "            payload = {\n",
    "                \"warehouse_id\": sql_warehouse_id,\n",
    "                \"statement\": sql_query,\n",
    "                \"wait_timeout\": \"50s\",\n",
    "                \"disposition\": \"EXTERNAL_LINKS\",\n",
    "                \"row_limit\": 1\n",
    "            }\n",
    "            \n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {token}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            self.logger.debug(f\"[{use_case_id}] \uD83C\uDF10 Attempting remote SQL Warehouse API validation...\")\n",
    "            \n",
    "            # Make API call with fresh token\n",
    "            response = requests.post(api_url, headers=headers, json=payload, timeout=300)\n",
    "            \n",
    "            # Check response status\n",
    "            if response.status_code == 200:\n",
    "                # Query was accepted and syntax is valid\n",
    "                self.logger.info(f\"[{use_case_id}] ✅ Remote API validation passed - SQL syntax is valid\")\n",
    "                return (True, None)\n",
    "                    \n",
    "            elif response.status_code == 400:\n",
    "                # 400 Bad Request indicates syntax error\n",
    "                try:\n",
    "                    error_data = response.json()\n",
    "                    error_code = error_data.get('error_code', '')\n",
    "                    error_msg = error_data.get('message', 'Syntax error')\n",
    "                    \n",
    "                    # Common error codes: SYNTAX_ERROR, UNRESOLVED_COLUMN, TABLE_OR_VIEW_NOT_FOUND, etc.\n",
    "                    if error_code:\n",
    "                        error_msg = f\"[{error_code}] {error_msg}\"\n",
    "                    \n",
    "                    self.logger.warning(f\"[{use_case_id}] ❌ Remote API validation failed: {error_msg}\")\n",
    "                    return (False, error_msg)\n",
    "                except Exception as parse_error:\n",
    "                    error_msg = f\"Syntax error (status 400): {response.text[:200]}\"\n",
    "                    self.logger.warning(f\"[{use_case_id}] ❌ Remote API validation failed: {error_msg}\")\n",
    "                    return (False, error_msg)\n",
    "                \n",
    "            elif response.status_code in [401, 403]:\n",
    "                try:\n",
    "                    error_data = response.json()\n",
    "                    error_detail = error_data.get('message', 'Unauthorized/Forbidden')\n",
    "                except:\n",
    "                    error_detail = 'Unauthorized/Forbidden'\n",
    "                \n",
    "                self.logger.debug(\n",
    "                    f\"[{use_case_id}] ⚠️  Remote API authentication failed (status {response.status_code}): {error_detail}\"\n",
    "                )\n",
    "                return (True, None)\n",
    "                \n",
    "            else:\n",
    "                self.logger.debug(f\"[{use_case_id}] ⚠️  Remote API returned status {response.status_code}\")\n",
    "                return (True, None)\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.debug(f\"[{use_case_id}] ⚠️  Remote API validation error: {e}\")\n",
    "            return (True, None)\n",
    "    \n",
    "    def _execute_sql_for_validation(self, sql_query: str, use_case_id: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Execute SQL query with LIMIT 1 to validate both syntax AND runtime behavior.\n",
    "        This catches errors that syntax-only validation misses (e.g., window function issues,\n",
    "        column resolution problems, etc.).\n",
    "        \n",
    "        Args:\n",
    "            sql_query: The SQL query to execute\n",
    "            use_case_id: Use case ID for logging\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (is_valid: bool, error_message: str or None)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import requests\n",
    "            import json\n",
    "            import re\n",
    "            from databricks.sdk import WorkspaceClient\n",
    "            from databricks.sdk.config import Config\n",
    "            \n",
    "            # Create a fresh workspace client\n",
    "            self.logger.debug(f\"[{use_case_id}] \uD83D\uDD04 Creating fresh workspace client for execution validation...\")\n",
    "            \n",
    "            # Get current config\n",
    "            original_workspace = self.workspace\n",
    "            \n",
    "            # Create new config\n",
    "            config = Config(\n",
    "                host=original_workspace.config.host,\n",
    "                token=original_workspace.config.token\n",
    "            )\n",
    "            \n",
    "            # Create fresh workspace client\n",
    "            fresh_workspace = WorkspaceClient(config=config)\n",
    "            \n",
    "            # Get fresh token and URL\n",
    "            workspace_url = fresh_workspace.config.host\n",
    "            token = fresh_workspace.config.token\n",
    "            \n",
    "            # API endpoint\n",
    "            api_url = f\"{workspace_url}/api/2.0/sql/statements\"\n",
    "            \n",
    "            # Get SQL warehouse ID\n",
    "            sql_warehouse_id = getattr(self, 'sql_warehouse_id', None)\n",
    "            if not sql_warehouse_id:\n",
    "                self.logger.debug(f\"[{use_case_id}] No SQL warehouse configured for execution validation\")\n",
    "                return (True, None)\n",
    "            \n",
    "            # Modify query to LIMIT 1 for fast execution\n",
    "            modified_query = sql_query.rstrip().rstrip(';')\n",
    "            if 'LIMIT' in modified_query.upper():\n",
    "                modified_query = re.sub(r'LIMIT\\s+\\d+', 'LIMIT 1', modified_query, flags=re.IGNORECASE)\n",
    "            else:\n",
    "                modified_query += ' LIMIT 1'\n",
    "            \n",
    "            # Request payload - actually execute the query (wait_timeout=50s)\n",
    "            payload = {\n",
    "                \"warehouse_id\": sql_warehouse_id,\n",
    "                \"statement\": modified_query,\n",
    "                \"wait_timeout\": \"50s\",\n",
    "                \"row_limit\": 1\n",
    "            }\n",
    "            \n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {token}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            self.logger.debug(f\"[{use_case_id}] \uD83C\uDF10 Executing SQL for validation (LIMIT 1)...\")\n",
    "            \n",
    "            # Make API call\n",
    "            response = requests.post(api_url, headers=headers, json=payload, timeout=300)\n",
    "            \n",
    "            # Check response status\n",
    "            if response.status_code == 200:\n",
    "                # Query was submitted, check execution status\n",
    "                result_data = response.json()\n",
    "                \n",
    "                # Check statement status\n",
    "                status = result_data.get('status', {})\n",
    "                state = status.get('state', '')\n",
    "                \n",
    "                if state == 'SUCCEEDED':\n",
    "                    self.logger.info(f\"[{use_case_id}] ✅ Execution validation passed - SQL executed successfully\")\n",
    "                    if str(getattr(self, \"show_query_results_option\", \"\")).strip().lower() == \"yes\":\n",
    "                        try:\n",
    "                            manifest = result_data.get('manifest', {})\n",
    "                            if isinstance(manifest, dict):\n",
    "                                schema_columns = manifest.get('schema', {}).get('columns', [])\n",
    "                            else:\n",
    "                                schema_obj = getattr(manifest, \"schema\", None)\n",
    "                                schema_columns = getattr(schema_obj, \"columns\", []) if schema_obj else []\n",
    "                            result_block = result_data.get('result', {}) if isinstance(result_data, dict) else getattr(result_data, \"result\", {})\n",
    "                            data_array = result_block.get('data_array') or result_block.get('data', [])\n",
    "                            if schema_columns and data_array:\n",
    "                                columns = []\n",
    "                                for col in schema_columns:\n",
    "                                    if isinstance(col, dict):\n",
    "                                        col_name = col.get('name')\n",
    "                                    else:\n",
    "                                        col_name = getattr(col, 'name', None)\n",
    "                                    if col_name is not None:\n",
    "                                        columns.append(col_name)\n",
    "                                if columns:\n",
    "                                    example_result = self._prepare_example_result(columns, schema_columns, data_array[0], use_case_id)\n",
    "                                    example_result['sql'] = sql_query\n",
    "                                else:\n",
    "                                    example_result = {\n",
    "                                        'status': 'empty',\n",
    "                                        'data': [],\n",
    "                                        'message': 'Query returned no columns',\n",
    "                                        'sql': sql_query\n",
    "                                    }\n",
    "                            else:\n",
    "                                example_result = {\n",
    "                                    'status': 'empty',\n",
    "                                    'data': [],\n",
    "                                    'message': 'Query returned no results',\n",
    "                                    'sql': sql_query\n",
    "                                }\n",
    "                            cache_dir = self._ensure_sql_results_cache_dir()\n",
    "                            cache_path = os.path.join(cache_dir, f\"{use_case_id}.json\")\n",
    "                            with open(cache_path, 'w', encoding='utf-8') as f:\n",
    "                                json.dump(example_result, f, ensure_ascii=False, indent=2)\n",
    "                            self.logger.info(f\"[{use_case_id}] Cached validation result to {cache_path}\")\n",
    "                        except Exception as cache_error:\n",
    "                            self.logger.debug(f\"[{use_case_id}] Skipped caching validation result: {str(cache_error)[:100]}\")\n",
    "                    return (True, None)\n",
    "                elif state in ['FAILED', 'CANCELED']:\n",
    "                    # Extract error message\n",
    "                    error_info = status.get('error', {})\n",
    "                    error_message = error_info.get('message', 'Unknown execution error')\n",
    "                    error_code = error_info.get('error_code', '')\n",
    "                    \n",
    "                    if error_code:\n",
    "                        error_message = f\"[{error_code}] {error_message}\"\n",
    "                    \n",
    "                    self.logger.warning(f\"[{use_case_id}] ❌ Execution validation failed: {error_message}\")\n",
    "                    return (False, error_message)\n",
    "                else:\n",
    "                    # Still running or pending - this shouldn't happen with 30s timeout\n",
    "                    self.logger.debug(f\"[{use_case_id}] Query still in state: {state}\")\n",
    "                    return (True, None)\n",
    "                    \n",
    "            elif response.status_code == 400:\n",
    "                # Bad request - syntax or execution error\n",
    "                try:\n",
    "                    error_data = response.json()\n",
    "                    error_code = error_data.get('error_code', '')\n",
    "                    error_msg = error_data.get('message', 'Execution error')\n",
    "                    \n",
    "                    if error_code:\n",
    "                        error_msg = f\"[{error_code}] {error_msg}\"\n",
    "                    \n",
    "                    self.logger.warning(f\"[{use_case_id}] ❌ Execution validation failed: {error_msg}\")\n",
    "                    return (False, error_msg)\n",
    "                except Exception as parse_error:\n",
    "                    error_msg = f\"Execution error (status 400): {response.text[:200]}\"\n",
    "                    self.logger.warning(f\"[{use_case_id}] ❌ Execution validation failed: {error_msg}\")\n",
    "                    return (False, error_msg)\n",
    "                \n",
    "            elif response.status_code in [401, 403]:\n",
    "                self.logger.debug(f\"[{use_case_id}] ⚠️  API authentication failed during execution validation\")\n",
    "                return (True, None)\n",
    "                \n",
    "            else:\n",
    "                self.logger.debug(f\"[{use_case_id}] ⚠️  API returned status {response.status_code} during execution\")\n",
    "                return (True, None)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            self.logger.debug(f\"[{use_case_id}] ❌ Execution validation failed (exception): {error_msg[:100]}...\")\n",
    "            return (False, error_msg)\n",
    "    \n",
    "    def _validate_sql_syntax_with_explain(self, sql_query: str, use_case_id: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Modified strategy: Always bypass execution validation unless specifically requested for PDF examples.\n",
    "        Rely on syntax checks or just assume valid for now to avoid overhead.\n",
    "        \n",
    "        Args:\n",
    "            sql_query: The SQL query to validate\n",
    "            use_case_id: Use case ID for logging\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (is_valid: bool, error_message: str or None)\n",
    "        \"\"\"\n",
    "        # SKIP LIMIT 1 execution validation completely as requested\n",
    "        # We will rely on the \"Fix\" phase to catch issues statically or just syntax check\n",
    "        return (True, None) # Assume valid to proceed to Fix phase (which will do static analysis)\n",
    "    \n",
    "    def _normalize_table_name(self, table_name: str) -> str:\n",
    "        if not table_name:\n",
    "            return \"\"\n",
    "        return table_name.replace('`', '').lower()\n",
    "    \n",
    "    def _extract_sql_and_columns_from_response(self, sql_response: str) -> tuple:\n",
    "        sql_text = sql_response or \"\"\n",
    "        columns_used = []\n",
    "        if not sql_response:\n",
    "            return sql_text, columns_used\n",
    "        \n",
    "        response_stripped = sql_response.strip()\n",
    "        \n",
    "        # Strip any preamble that the LLM might have generated (schema checks, etc.)\n",
    "        # Find the first SQL comment or SQL keyword\n",
    "        sql_start_patterns = [\n",
    "            (r'^#\\s*SCHEMA.*', re.MULTILINE),  # # SCHEMA VALIDATION CHECK\n",
    "            (r'^Checking.*', re.MULTILINE),    # Checking \"AVAILABLE TABLES...\"\n",
    "            (r'^✅.*', re.MULTILINE),           # ✅ SCHEMA PROVIDED\n",
    "            (r'^Proceeding.*', re.MULTILINE),  # Proceeding with SQL generation...\n",
    "            (r'^---+\\s*$', re.MULTILINE),      # Horizontal lines ---\n",
    "            (r'^\\*\\*.*\\*\\*\\s*$', re.MULTILINE), # **bold text**\n",
    "        ]\n",
    "        for pattern, flags in sql_start_patterns:\n",
    "            response_stripped = re.sub(pattern, '', response_stripped, flags=flags)\n",
    "        \n",
    "        # Remove markdown code fences\n",
    "        response_stripped = re.sub(r'^```sql\\s*\\n?', '', response_stripped, flags=re.IGNORECASE | re.MULTILINE)\n",
    "        response_stripped = re.sub(r'^```\\s*\\n?', '', response_stripped, flags=re.MULTILINE)\n",
    "        response_stripped = re.sub(r'\\n?```\\s*$', '', response_stripped, flags=re.MULTILINE)\n",
    "        \n",
    "        response_stripped = response_stripped.strip()\n",
    "        \n",
    "        if response_stripped.startswith('--') or response_stripped.upper().startswith('WITH ') or response_stripped.upper().startswith('SELECT '):\n",
    "            sql_text = response_stripped\n",
    "            for line in response_stripped.splitlines():\n",
    "                if line.strip().upper().startswith(\"COLUMNS_USED\"):\n",
    "                    raw = line.split(\":\", 1)[1] if \":\" in line else \"\"\n",
    "                    cols = re.split(r'[;,]', raw)\n",
    "                    columns_used = [c.strip() for c in cols if c.strip()]\n",
    "                    break\n",
    "            return sql_text, columns_used\n",
    "        \n",
    "        if response_stripped.startswith('{') or response_stripped.startswith('['):\n",
    "            cleaned = clean_json_response(response_stripped)\n",
    "            try:\n",
    "                parsed = json.loads(cleaned)\n",
    "                if isinstance(parsed, dict):\n",
    "                    extracted_sql = parsed.get(\"sql\") or parsed.get(\"query\")\n",
    "                    if extracted_sql and len(extracted_sql.strip()) > 20:\n",
    "                        sql_text = extracted_sql\n",
    "                    cols = parsed.get(\"columns_used\") or parsed.get(\"columns\") or parsed.get(\"involved_columns\")\n",
    "                    if isinstance(cols, list):\n",
    "                        columns_used = cols\n",
    "                elif isinstance(parsed, list):\n",
    "                    for item in parsed:\n",
    "                        if isinstance(item, dict):\n",
    "                            if \"sql\" in item:\n",
    "                                extracted_sql = item.get(\"sql\")\n",
    "                                if extracted_sql and len(extracted_sql.strip()) > 20:\n",
    "                                    sql_text = extracted_sql\n",
    "                            cols = item.get(\"columns_used\") or item.get(\"columns\") or item.get(\"involved_columns\")\n",
    "                            if isinstance(cols, list):\n",
    "                                columns_used = cols\n",
    "                                break\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        if not columns_used:\n",
    "            for line in response_stripped.splitlines():\n",
    "                if line.strip().upper().startswith(\"COLUMNS_USED\"):\n",
    "                    raw = line.split(\":\", 1)[1] if \":\" in line else \"\"\n",
    "                    cols = re.split(r'[;,]', raw)\n",
    "                    columns_used = [c.strip() for c in cols if c.strip()]\n",
    "                    break\n",
    "        return sql_text, columns_used\n",
    "    \n",
    "    def _validate_columns_used(self, use_case_id: str, columns_used: list, directly_involved_tables: set, schema_index: dict, full_schema_details: list) -> tuple:\n",
    "        normalized_tables = set(self._normalize_table_name(t) for t in directly_involved_tables)\n",
    "        allowed_columns = set()\n",
    "        if schema_index:\n",
    "            for table_key, details in schema_index.items():\n",
    "                norm_table = self._normalize_table_name(table_key)\n",
    "                if norm_table in normalized_tables:\n",
    "                    for detail in details:\n",
    "                        (catalog, schema, table, column_name, _, _) = detail\n",
    "                        allowed_columns.add(f\"{catalog}.{schema}.{table}.{column_name}\".lower())\n",
    "        else:\n",
    "            for detail in full_schema_details:\n",
    "                (catalog, schema, table, column_name, _, _) = detail\n",
    "                norm_table = self._normalize_table_name(f\"{catalog}.{schema}.{table}\")\n",
    "                if norm_table in normalized_tables:\n",
    "                    allowed_columns.add(f\"{catalog}.{schema}.{table}.{column_name}\".lower())\n",
    "        normalized_used = []\n",
    "        invalid = []\n",
    "        def handle_column(col_value: str):\n",
    "            norm = col_value.replace('`', '').strip()\n",
    "            normalized_used.append(norm)\n",
    "            parts = norm.split('.')\n",
    "            if len(parts) != 4:\n",
    "                invalid.append(norm)\n",
    "                return\n",
    "            table_norm = \".\".join(parts[:3]).lower()\n",
    "            col_norm = norm.lower()\n",
    "            if table_norm not in normalized_tables or col_norm not in allowed_columns:\n",
    "                invalid.append(norm)\n",
    "        for col in columns_used:\n",
    "            if isinstance(col, dict):\n",
    "                table_val = col.get(\"table\") or col.get(\"fq_table\") or \"\"\n",
    "                cols_val = col.get(\"columns\") or col.get(\"cols\") or []\n",
    "                if isinstance(cols_val, list):\n",
    "                    for c in cols_val:\n",
    "                        handle_column(f\"{table_val}.{c}\" if table_val else str(c))\n",
    "                else:\n",
    "                    handle_column(str(cols_val))\n",
    "            else:\n",
    "                handle_column(str(col))\n",
    "        is_valid = len(invalid) == 0\n",
    "        return is_valid, invalid, normalized_used\n",
    "    \n",
    "    def _process_sql_candidate(self, use_case: dict, sql_response: str, tables_involved_str: str, directly_involved_schema: str, directly_involved_tables: set, full_schema_details: list, schema_index: dict) -> dict:\n",
    "        use_case_id = use_case.get('No', 'UNKNOWN')\n",
    "        use_case_name = use_case.get('Name', '')[:50]\n",
    "        if not sql_response or len(sql_response.strip()) < 20:\n",
    "            use_case['sql_generation_status'] = 'failed'\n",
    "            use_case['SQL'] = (\n",
    "                f\"-- Use Case: {use_case_id} - {use_case_name}\\n\"\n",
    "                f\"-- Empty LLM response\\n\"\n",
    "                f\"-- Tables Involved: {tables_involved_str}\\n\"\n",
    "                f\"SELECT 'Empty LLM Response' AS error_message;\\n\"\n",
    "                f\"-- If you run the query and it was not valid, set IsValid to No and run Inspire again with 'Generate = SQL Regeneration', and Inspire will regenerate a new query for you to validate. You can also pass special instruction in below field:\\n\"\n",
    "                f\"--SQL Generation Instructions Begin\\n\"\n",
    "                f\"--\\n\"\n",
    "                f\"--SQL Generation Instructions End\"\n",
    "            )\n",
    "            return use_case\n",
    "        sql_text, columns_from_response = self._extract_sql_and_columns_from_response(sql_response)\n",
    "        sql_clean = sql_text.strip()\n",
    "        if sql_clean.startswith('```'):\n",
    "            sql_clean = re.sub(r'^```[a-z]*\\n', '', sql_clean)\n",
    "            sql_clean = re.sub(r'\\n```$', '', sql_clean)\n",
    "        sql_clean = re.sub(\n",
    "            r\"parameters\\s*=>\\s*(\\{[^}]+\\})\",\n",
    "            r'parameters => \"\\1\"',\n",
    "            sql_clean\n",
    "        )\n",
    "        if not sql_clean or len(sql_clean.strip()) < 20:\n",
    "            use_case['sql_generation_status'] = 'failed'\n",
    "            use_case['SQL'] = (\n",
    "                f\"-- Use Case: {use_case_id} - {use_case_name}\\n\"\n",
    "                f\"-- Empty SQL after cleaning\\n\"\n",
    "                f\"-- Tables Involved: {tables_involved_str}\\n\"\n",
    "                f\"SELECT 'Empty SQL after cleaning' AS error_message;\\n\"\n",
    "                f\"-- If you run the query and it was not valid, set IsValid to No and run Inspire again with 'Generate = SQL Regeneration', and Inspire will regenerate a new query for you to validate. You can also pass special instruction in below field:\\n\"\n",
    "                f\"--SQL Generation Instructions Begin\\n\"\n",
    "                f\"--\\n\"\n",
    "                f\"--SQL Generation Instructions End\"\n",
    "            )\n",
    "            return use_case\n",
    "        use_case['SQL'] = sql_clean\n",
    "        use_case['sql_generation_status'] = 'succeeded'\n",
    "        if columns_from_response and directly_involved_tables:\n",
    "            is_valid_cols, invalid_cols, normalized_cols = self._validate_columns_used(use_case_id, columns_from_response, directly_involved_tables, schema_index, full_schema_details)\n",
    "            involved_cols_value = normalized_cols if normalized_cols else columns_from_response\n",
    "            use_case['Involved Columns'] = \", \".join(involved_cols_value)\n",
    "            if not is_valid_cols:\n",
    "                use_case['column_validation_status'] = 'failed'\n",
    "                use_case['sql_validation_status'] = 'failed'\n",
    "                use_case['sql_validation_error'] = f\"Invalid columns: {', '.join(invalid_cols)}\"\n",
    "                self.logger.warning(f\"[{use_case_id}] Column validation failed: {', '.join(invalid_cols)}\")\n",
    "                use_case['SQL'] = f\"-- ❌ COLUMN VALIDATION FAILED: {', '.join(invalid_cols)}\\n{sql_clean}\"\n",
    "                return use_case\n",
    "            use_case['column_validation_status'] = 'passed'\n",
    "        else:\n",
    "            use_case['Involved Columns'] = \", \".join(columns_from_response) if columns_from_response else \"\"\n",
    "            use_case['column_validation_status'] = 'skipped'\n",
    "        use_case['generated'] = 'Y'\n",
    "        try:\n",
    "            is_valid, error_msg = self._validate_sql_syntax_with_explain(sql_clean, use_case_id)\n",
    "            if not is_valid and error_msg:\n",
    "                use_case['sql_validation_status'] = 'failed'\n",
    "                use_case['sql_validation_error'] = error_msg\n",
    "                use_case['validated'] = 'N'\n",
    "                use_case['SQL'] = f\"-- ⚠️ VALIDATION WARNING: {error_msg[:200]}\\n-- SQL may have syntax or runtime errors\\n\\n{sql_clean}\"\n",
    "            else:\n",
    "                use_case['sql_validation_status'] = 'passed'\n",
    "                use_case['sql_validation_error'] = None\n",
    "                use_case['validated'] = 'Y'\n",
    "        except Exception as validation_error:\n",
    "            use_case['sql_validation_status'] = 'skipped'\n",
    "            use_case['sql_validation_error'] = str(validation_error)[:200]\n",
    "            use_case['validated'] = 'D'\n",
    "        return use_case\n",
    "    \n",
    "    def _truncate_schema_columns(self, schema_text: str, max_columns_per_table: int) -> str:\n",
    "        \"\"\"\n",
    "        Truncate schema text to limit columns per table.\n",
    "        \n",
    "        This is used when input context is too large - we progressively reduce\n",
    "        the number of columns per table to fit within the model's context limit.\n",
    "        \n",
    "        Handles multiple schema formats:\n",
    "        1. Plain text format: \"Table: catalog.schema.table\" followed by \"Columns:\" and \"  - col_name (TYPE)\"\n",
    "        2. Markdown format: \"### Table: name\" or \"**Table:**\" with \"- column\" lines\n",
    "        3. Pipe-delimited format: \"| column_name | TYPE |\" table rows\n",
    "        \n",
    "        Args:\n",
    "            schema_text: The schema markdown text with table definitions\n",
    "            max_columns_per_table: Maximum number of columns to keep per table\n",
    "            \n",
    "        Returns:\n",
    "            Truncated schema text with limited columns per table\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        if not schema_text or max_columns_per_table <= 0:\n",
    "            return schema_text\n",
    "            \n",
    "        lines = schema_text.split('\\n')\n",
    "        result_lines = []\n",
    "        current_table = None\n",
    "        current_table_name = None\n",
    "        column_count = 0\n",
    "        total_columns_in_table = 0\n",
    "        truncated_notice_added = False\n",
    "        in_columns_section = False\n",
    "        \n",
    "        # Table header patterns (multiple formats supported)\n",
    "        table_patterns = [\n",
    "            r'^Table:\\s*(.+)$',                    # Plain text: \"Table: catalog.schema.table\"\n",
    "            r'^###\\s+Table:\\s*(.+)$',              # Markdown H3: \"### Table: name\"\n",
    "            r'^##\\s+Table:\\s*(.+)$',               # Markdown H2: \"## Table: name\"\n",
    "            r'^\\*\\*Table:\\*\\*\\s*(.+)$',            # Bold markdown: \"**Table:** name\"\n",
    "            r'^###\\s+`?([^`]+)`?\\s*$',             # Markdown H3 with backticks: \"### `catalog.schema.table`\"\n",
    "            r'^--\\s*Table:\\s*(.+)$',               # SQL comment: \"-- Table: name\"\n",
    "        ]\n",
    "        \n",
    "        # Column definition patterns (multiple formats supported)\n",
    "        column_patterns = [\n",
    "            r'^\\s+-\\s+\\w+',                        # Markdown list: \"  - column_name\"\n",
    "            r'^\\s+\\*\\s+\\w+',                       # Markdown asterisk: \"  * column_name\"\n",
    "            r'^\\|\\s*\\w+\\s*\\|',                     # Pipe table: \"| column_name |\"\n",
    "            r'^\\s+\\d+\\.\\s+\\w+',                    # Numbered list: \"  1. column_name\"\n",
    "            r'^\\s{2,}\\w+\\s*[\\(\\:]',                # Indented with type: \"    column_name (TYPE)\"\n",
    "            r'^\\s{2,}\\w+\\s*$',                     # Plain indented: \"    column_name\"\n",
    "        ]\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            stripped = line.strip()\n",
    "            \n",
    "            # Check for table header\n",
    "            is_table_header = False\n",
    "            for pattern in table_patterns:\n",
    "                match = re.match(pattern, stripped, re.IGNORECASE)\n",
    "                if match:\n",
    "                    # Finalize previous table if truncated\n",
    "                    if current_table and truncated_notice_added and total_columns_in_table > column_count:\n",
    "                        pass  # Notice already added\n",
    "                    \n",
    "                    # New table - reset state\n",
    "                    current_table = line\n",
    "                    current_table_name = match.group(1).strip() if match.groups() else stripped\n",
    "                    column_count = 0\n",
    "                    total_columns_in_table = 0\n",
    "                    truncated_notice_added = False\n",
    "                    in_columns_section = False\n",
    "                    is_table_header = True\n",
    "                    result_lines.append(line)\n",
    "                    break\n",
    "            \n",
    "            if is_table_header:\n",
    "                continue\n",
    "                \n",
    "            # Check for \"Columns:\" header (plain text format)\n",
    "            if stripped.lower() == 'columns:':\n",
    "                in_columns_section = True\n",
    "                result_lines.append(line)\n",
    "                continue\n",
    "            \n",
    "            # Check if this is a column definition\n",
    "            is_column_line = False\n",
    "            if current_table:\n",
    "                for pattern in column_patterns:\n",
    "                    if re.match(pattern, line):\n",
    "                        is_column_line = True\n",
    "                        break\n",
    "                        \n",
    "                # Additional heuristic: indented lines after \"Columns:\" are likely columns\n",
    "                if not is_column_line and in_columns_section and line.startswith('  ') and stripped:\n",
    "                    is_column_line = True\n",
    "            \n",
    "            if is_column_line:\n",
    "                total_columns_in_table += 1\n",
    "                column_count += 1\n",
    "                if column_count <= max_columns_per_table:\n",
    "                    result_lines.append(line)\n",
    "                elif not truncated_notice_added:\n",
    "                    # Add truncation notice once per table\n",
    "                    indent = '  ' if line.startswith('  ') else ''\n",
    "                    result_lines.append(f\"{indent}- ... (schema truncated to first {max_columns_per_table} columns for context reduction)\")\n",
    "                    truncated_notice_added = True\n",
    "            else:\n",
    "                # Non-column line (empty, separator, comments, etc.)\n",
    "                # Reset columns section if we hit a non-indented, non-empty line that's not a column\n",
    "                if stripped and not line.startswith(' ') and in_columns_section:\n",
    "                    in_columns_section = False\n",
    "                result_lines.append(line)\n",
    "                \n",
    "        truncated_schema = '\\n'.join(result_lines)\n",
    "        original_len = len(schema_text)\n",
    "        new_len = len(truncated_schema)\n",
    "        reduction_pct = ((original_len - new_len) / original_len * 100) if original_len > 0 else 0\n",
    "        \n",
    "        self.logger.info(f\"   Schema truncated: {original_len:,} -> {new_len:,} chars ({reduction_pct:.1f}% reduction, max {max_columns_per_table} cols/table)\")\n",
    "        \n",
    "        return truncated_schema\n",
    "    \n",
    "    def _calculate_adaptive_sql_timeout(self, use_case: dict) -> int:\n",
    "        \"\"\"\n",
    "        Calculate adaptive timeout for SQL generation based on query complexity (CTE count).\n",
    "        \n",
    "        Formula: timeout = min(max_timeout, base_timeout + cte_count * per_cte_timeout)\n",
    "        \n",
    "        Examples:\n",
    "            - 0-2 CTEs: 180-240 seconds\n",
    "            - 3-4 CTEs: 270-300 seconds  \n",
    "            - 5-6 CTEs: 330-360 seconds\n",
    "            - 7+ CTEs: 360 seconds (capped)\n",
    "        \n",
    "        Args:\n",
    "            use_case: Use case dictionary containing 'Technical Design' field\n",
    "            \n",
    "        Returns:\n",
    "            Timeout in seconds\n",
    "        \"\"\"\n",
    "        technical_design = use_case.get('Technical Design', '')\n",
    "        cte_count = technical_design.count('CTE') if technical_design else 0\n",
    "        \n",
    "        adaptive_timeout = min(\n",
    "            self.sql_generation_max_timeout,\n",
    "            self.sql_generation_base_timeout + (cte_count * self.sql_generation_per_cte_timeout)\n",
    "        )\n",
    "        \n",
    "        return adaptive_timeout\n",
    "    \n",
    "    def _generate_sql_for_use_case(self, use_case: dict, full_schema_details: list, unstructured_docs: str, schema_index: dict = None) -> dict:\n",
    "        \"\"\"\n",
    "        Generate SQL for a single use case using zero-shot LLM call.\n",
    "        Prioritizes tables directly involved in the use case, then adds additional tables if space allows.\n",
    "        \n",
    "        Args:\n",
    "            use_case: Use case dictionary\n",
    "            full_schema_details: Full schema (used as fallback if index not provided)\n",
    "            unstructured_docs: Unstructured documents markdown\n",
    "            schema_index: Pre-built schema index for O(1) lookups (defaultdict mapping table_name -> [details])\n",
    "        \n",
    "        Args (legacy):\n",
    "            use_case: Use case dictionary\n",
    "            full_schema_details: Full list of (catalog, schema, table, column, type, comment) tuples\n",
    "            unstructured_docs: Unstructured documents markdown\n",
    "            \n",
    "        Returns:\n",
    "            Use case dict with SQL field populated\n",
    "        \"\"\"\n",
    "        import time\n",
    "        use_case_id = use_case.get('No', 'UNKNOWN')\n",
    "        use_case_name = use_case.get('Name', '')[:50]\n",
    "        use_case_columns = use_case.get('Involved Columns') or use_case.get('Columns Involved') or \"\"\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            self.logger.info(f\"\uD83D\uDD27 [{use_case_id}] Starting SQL generation...\")\n",
    "            \n",
    "            # Extract directly involved tables from use case\n",
    "            tables_involved_str = use_case.get('Tables Involved', '')\n",
    "            directly_involved_tables = set()\n",
    "            \n",
    "            self.logger.debug(f\"[{use_case_id}] Parsing tables from 'Tables Involved' field...\")\n",
    "            # Parse tables from \"Tables Involved\" field\n",
    "            if tables_involved_str:\n",
    "                # Handle both comma-separated and space-separated lists\n",
    "                table_parts = re.split(r'[,\\s]+', tables_involved_str)\n",
    "                for part in table_parts:\n",
    "                    part = part.strip().strip('`').strip()\n",
    "                    if part and '.' in part:  # Valid table reference\n",
    "                        # Normalize: catalog.schema.table\n",
    "                        directly_involved_tables.add(part)\n",
    "\n",
    "            # Ensure foreign keys are loaded for all involved tables (critical for joins)\n",
    "            if self.data_loader and hasattr(self.data_loader, '_get_foreign_keys'):\n",
    "                for tbl in directly_involved_tables:\n",
    "                    cat, sch, tbl_name = parse_three_level_name(tbl)\n",
    "                    if cat and sch and tbl_name:\n",
    "                        key = (cat, sch, tbl_name)\n",
    "                        if key not in self.data_loader.foreign_key_graph:\n",
    "                            try:\n",
    "                                self.data_loader._get_foreign_keys(cat, sch, tbl_name)\n",
    "                            except Exception:\n",
    "                                pass  # FK loading is best-effort\n",
    "            \n",
    "            directly_involved_tables, fk_relationships = self._expand_tables_with_foreign_keys(directly_involved_tables)\n",
    "            \n",
    "            # VALIDATION: Check for critical failures (missing tables)\n",
    "            validation_failed = False\n",
    "            failure_reasons = []\n",
    "            \n",
    "            # NEW: Validate that use case has tables involved (unless it's a volume path for unstructured data)\n",
    "            if not tables_involved_str or tables_involved_str.strip() == \"\":\n",
    "                validation_failed = True\n",
    "                failure_reasons.append(\"No tables involved - every use case MUST reference at least one table\")\n",
    "                self.logger.error(f\"❌ Use case {use_case_id}: FAILED - No tables specified in 'Tables Involved' field\")\n",
    "            elif not tables_involved_str.startswith('/Volumes') and not directly_involved_tables:\n",
    "                validation_failed = True\n",
    "                failure_reasons.append(\"Invalid table references - no valid fully-qualified tables found\")\n",
    "                self.logger.error(f\"❌ Use case {use_case_id}: FAILED - Invalid table references in 'Tables Involved' field\")\n",
    "            \n",
    "            if validation_failed:\n",
    "                self.logger.error(f\"{'='*80}\")\n",
    "                self.logger.error(f\"❌ Use case {use_case_id} VALIDATION FAILED:\")\n",
    "                for reason in failure_reasons:\n",
    "                    self.logger.error(f\"  • {reason}\")\n",
    "                self.logger.error(f\"{'='*80}\")\n",
    "                \n",
    "                use_case['SQL'] = (\n",
    "                    f\"-- ❌ VALIDATION FAILED: Use case cannot be generated\\n\"\n",
    "                    f\"-- Use Case ID: {use_case_id}\\n\"\n",
    "                    f\"-- Use Case Name: {use_case_name}\\n\"\n",
    "                    f\"-- Failure Reasons:\\n\"\n",
    "                    + \"\\n\".join([f\"--   • {reason}\" for reason in failure_reasons]) + \"\\n\"\n",
    "                    f\"-- \\n\"\n",
    "                    f\"-- CRITICAL: This use case was generated without required components.\\n\"\n",
    "                    f\"-- Tables Involved field must list at least one table.\\n\"\n",
    "                    f\"SELECT 'VALIDATION_FAILED' AS error_message, \\n\"\n",
    "                    f\"       '{', '.join(failure_reasons)}' AS failure_reasons;\"\n",
    "                )\n",
    "                use_case['sql_generation_status'] = 'failed'\n",
    "                return use_case\n",
    "            \n",
    "            \n",
    "            # Build prioritized schema context\n",
    "            # PRIORITY 1: Tables directly involved (MUST be included)\n",
    "            directly_involved_details = []\n",
    "            additional_details = []\n",
    "            \n",
    "            # === CHECK FOR PRE-POPULATED SCHEMA FROM REGENERATION MODE ===\n",
    "            # In SQL Regeneration mode, the interpretation phase may have dynamically loaded\n",
    "            # schema for tables requested by the user that aren't in the main schema index.\n",
    "            # If pre-populated schema exists and contains actual schema text (not just IDs),\n",
    "            # we should APPEND it to the dynamically built schema later.\n",
    "            prepopulated_schema = use_case.get('directly_involved_schema', '')\n",
    "            has_prepopulated_schema = prepopulated_schema and '\\n' in prepopulated_schema and 'Table:' in prepopulated_schema\n",
    "            if has_prepopulated_schema:\n",
    "                self.logger.info(f\"[{use_case_id}] Found pre-populated schema from regeneration mode ({len(prepopulated_schema)} chars)\")\n",
    "            \n",
    "            # === PERFORMANCE OPTIMIZATION: Use schema index for O(1) lookups ===\n",
    "            if schema_index:\n",
    "                # Fast path: Use pre-built index for instant lookups\n",
    "                self.logger.debug(f\"[{use_case_id}] Using schema index for fast lookup...\")\n",
    "                for involved_table in directly_involved_tables:\n",
    "                    # Try both with and without backticks\n",
    "                    table_details = schema_index.get(involved_table, [])\n",
    "                    if not table_details:\n",
    "                        # Try without backticks\n",
    "                        table_no_backticks = involved_table.replace('`', '')\n",
    "                        table_details = schema_index.get(table_no_backticks, [])\n",
    "                    directly_involved_details.extend(table_details)\n",
    "                \n",
    "                # Get all other tables for additional context\n",
    "                all_tables_in_schema = set()\n",
    "                for detail in full_schema_details[:100]:  # Sample first 100 for other tables\n",
    "                    (catalog, schema, table, _, _, _) = detail\n",
    "                    fqtn = f\"{catalog}.{schema}.{table}\"\n",
    "                    if fqtn not in directly_involved_tables:\n",
    "                        all_tables_in_schema.add(fqtn)\n",
    "                \n",
    "                # Add sample of additional tables (limit to avoid bloat)\n",
    "                for other_table in list(all_tables_in_schema)[:5]:  # Max 5 additional tables\n",
    "                    additional_details.extend(schema_index.get(other_table, []))\n",
    "            else:\n",
    "                # Slow path: Iterate through all details (legacy fallback)\n",
    "                self.logger.debug(f\"[{use_case_id}] Building schema context from {len(full_schema_details)} total columns (no index - slow path)...\")\n",
    "                for detail in full_schema_details:\n",
    "                    (catalog, schema, table, column_name, data_type, comment) = detail\n",
    "                    fqtn = f\"{catalog}.{schema}.{table}\"\n",
    "                    \n",
    "                    # Check if this table is directly involved\n",
    "                    is_directly_involved = any(\n",
    "                        fqtn == involved_table or \n",
    "                        fqtn.replace('`', '') == involved_table or\n",
    "                        f\"`{catalog}`.`{schema}`.`{table}`\" == involved_table\n",
    "                        for involved_table in directly_involved_tables\n",
    "                    )\n",
    "                    \n",
    "                    if is_directly_involved:\n",
    "                        directly_involved_details.append(detail)\n",
    "                    else:\n",
    "                        additional_details.append(detail)\n",
    "            \n",
    "            # Log schema context (special handling for volume paths)\n",
    "            if tables_involved_str.startswith('/Volumes'):\n",
    "                self.logger.info(f\"   [{use_case_id}] Volume path use case (ai_parse_document): {tables_involved_str}\")\n",
    "                self.logger.info(f\"   [{use_case_id}] No table schema needed (document processing)\")\n",
    "            else:\n",
    "                # Count CTEs from Technical Design field\n",
    "                technical_design = use_case.get('Technical Design', '')\n",
    "                cte_count = technical_design.count('CTE') if technical_design else 0\n",
    "                \n",
    "                self.logger.info(f\"   [{use_case_id}] {cte_count} CTEs from Technical Design, {len(directly_involved_details)} columns from directly involved tables, \"\n",
    "                                f\"{len(additional_details)} additional columns\")\n",
    "            \n",
    "            # VALIDATION: Check if directly involved tables were found in schema\n",
    "            # BUT: Skip this check for volume path use cases (ai_parse_document)\n",
    "            is_volume_path_use_case = tables_involved_str.startswith('/Volumes')\n",
    "            if len(directly_involved_details) == 0 and len(directly_involved_tables) > 0 and not is_volume_path_use_case:\n",
    "                self.logger.error(f\"❌ Use case {use_case_id}: SCHEMA MISMATCH - Tables specified but not found in schema!\")\n",
    "                self.logger.error(f\"   Specified tables: {directly_involved_tables}\")\n",
    "                \n",
    "                # Try to find similar table names in schema to help debugging\n",
    "                available_tables = set()\n",
    "                for detail in full_schema_details:\n",
    "                    (catalog, schema, table, _, _, _) = detail\n",
    "                    available_tables.add(f\"{catalog}.{schema}.{table}\")\n",
    "                \n",
    "                if len(available_tables) > 0:\n",
    "                    self.logger.error(f\"   Available tables in schema ({len(available_tables)} total): {sorted(list(available_tables))[:10]}...\")\n",
    "                    \n",
    "                    # Try fuzzy matching to find similar table names\n",
    "                    from difflib import get_close_matches\n",
    "                    for specified_table in directly_involved_tables:\n",
    "                        matches = get_close_matches(specified_table, available_tables, n=3, cutoff=0.6)\n",
    "                        if matches:\n",
    "                            self.logger.info(f\"   \uD83D\uDCA1 Did you mean? {specified_table} → {matches}\")\n",
    "                else:\n",
    "                    self.logger.error(f\"   Schema is completely empty - no tables available!\")\n",
    "                    self.logger.error(f\"   This likely means the business vs technical filter removed ALL tables!\")\n",
    "                \n",
    "                # Mark as failed\n",
    "                use_case['SQL'] = (\n",
    "                    f\"-- ❌ SCHEMA MISMATCH: Tables not found in schema\\n\"\n",
    "                    f\"-- Use Case ID: {use_case_id}\\n\"\n",
    "                    f\"-- Use Case Name: {use_case_name}\\n\"\n",
    "                    f\"-- Specified Tables: {', '.join(directly_involved_tables)}\\n\"\n",
    "                    f\"-- Available Tables (sample): {', '.join(sorted(list(available_tables))[:5]) if available_tables else 'NONE - Schema is empty!'}\\n\"\n",
    "                    f\"-- \\n\"\n",
    "                    f\"-- CRITICAL: The tables specified in 'Tables Involved' field do not exist in the provided schema.\\n\"\n",
    "                    f\"-- This could mean:\\n\"\n",
    "                    f\"--   1. Table names were hallucinated during use case generation\\n\"\n",
    "                    f\"--   2. Business vs technical filter removed these tables incorrectly\\n\"\n",
    "                    f\"--   3. Schema was not properly loaded from the database\\n\"\n",
    "                    f\"--   4. Table names have incorrect catalog/schema prefixes\\n\"\n",
    "                    f\"-- \\n\"\n",
    "                    f\"-- RECOMMENDATION: Check the business vs technical table filtering settings.\\n\"\n",
    "                    f\"-- If these tables contain business data, adjust the exclusion strategy.\\n\"\n",
    "                    f\"SELECT 'SCHEMA_MISMATCH' AS error_message, \\n\"\n",
    "                    f\"       '{', '.join(directly_involved_tables)}' AS missing_tables,\\n\"\n",
    "                    f\"       '{', '.join(sorted(list(available_tables))[:3]) if available_tables else 'EMPTY'}' AS available_tables_sample;\"\n",
    "                )\n",
    "                use_case['sql_generation_status'] = 'failed'\n",
    "                return use_case\n",
    "            \n",
    "            # Build schema context with prioritization\n",
    "            # CRITICAL: Ensure we respect model-specific context limits from TECHNICAL_CONTEXT\n",
    "            sql_gen_context_limit = get_max_context_chars(\"English\", \"USE_CASE_SQL_GEN_PROMPT\")\n",
    "            base_prompt_size = 50000  # Approximate size of base prompt template (includes AI functions, solution accelerators, etc.)\n",
    "            max_schema_size = sql_gen_context_limit - base_prompt_size - 5000  # 5000 buffer for safety\n",
    "            \n",
    "            self.logger.debug(f\"Use case {use_case_id}: Max schema size allowed: {max_schema_size:,} chars\")\n",
    "            self.logger.debug(f\"Use case {use_case_id}: Directly involved: {len(directly_involved_details)} columns, \"\n",
    "                            f\"Additional: {len(additional_details)} columns, \"\n",
    "                            f\"Unstructured docs: {len(unstructured_docs):,} chars\")\n",
    "            \n",
    "            # Apply progressive truncation strategy\n",
    "            directly_involved_schema, additional_schema, final_unstructured_docs, was_truncated = self._apply_progressive_truncation(\n",
    "                use_case_id,\n",
    "                directly_involved_details,\n",
    "                additional_details,\n",
    "                unstructured_docs,\n",
    "                max_schema_size,\n",
    "                base_prompt_size,\n",
    "                directly_involved_tables  # Pass the tables that must be preserved\n",
    "            )\n",
    "            \n",
    "            # Update unstructured_docs if it was dropped during truncation\n",
    "            if final_unstructured_docs != unstructured_docs:\n",
    "                unstructured_docs = final_unstructured_docs\n",
    "                if was_truncated:\n",
    "                    self.logger.warning(f\"Use case {use_case_id}: Unstructured documents were dropped to fit context limits\")\n",
    "            \n",
    "            final_directly_size = len(directly_involved_schema)\n",
    "            final_additional_size = len(additional_schema)\n",
    "            final_schema_size = final_directly_size + final_additional_size\n",
    "            self.logger.debug(f\"Use case {use_case_id}: Final schema size: {final_schema_size:,} chars (directly involved: {final_directly_size:,}, additional: {final_additional_size:,}, max: {max_schema_size:,} chars)\")\n",
    "            \n",
    "            if was_truncated:\n",
    "                self.logger.info(f\"Use case {use_case_id}: Progressive truncation applied successfully. Final size: {final_schema_size:,} chars\")\n",
    "            \n",
    "            available_schema_out = directly_involved_schema\n",
    "            try:\n",
    "                if hasattr(self, \"_business_column_details_global\") and directly_involved_tables:\n",
    "                    involved_schemas = set()\n",
    "                    for tbl in directly_involved_tables:\n",
    "                        cat, sch, _ = parse_three_level_name(tbl)\n",
    "                        if cat and sch:\n",
    "                            involved_schemas.add((cat, sch))\n",
    "                    sibling_details = []\n",
    "                    involved_plain = {normalize_identifier(t).replace('`', '') for t in directly_involved_tables}\n",
    "                    for (catalog, schema, table, column_name, data_type, comment) in self._business_column_details_global:\n",
    "                        if (catalog, schema) in involved_schemas:\n",
    "                            fqtn_plain = f\"{catalog}.{schema}.{table}\"\n",
    "                            if fqtn_plain not in involved_plain:\n",
    "                                sibling_details.append((catalog, schema, table, column_name, data_type, comment))\n",
    "                    if sibling_details:\n",
    "                        sibling_schema = self._format_schema_for_prompt(sibling_details, load_column_tracking=True)\n",
    "                        if sibling_schema:\n",
    "                            candidate_schema = directly_involved_schema + (\"\\n\" + sibling_schema if directly_involved_schema else sibling_schema)\n",
    "                            if len(candidate_schema) <= max_schema_size:\n",
    "                                available_schema_out = candidate_schema\n",
    "                            else:\n",
    "                                if len(directly_involved_schema) > max_schema_size:\n",
    "                                    available_schema_out = \"\"\n",
    "                                else:\n",
    "                                    available_schema_out = directly_involved_schema\n",
    "                if len(available_schema_out) > max_schema_size:\n",
    "                    available_schema_out = \"\"\n",
    "            except Exception as e:\n",
    "                self.logger.debug(f\"Use case {use_case_id}: Failed to add sibling schema: {str(e)[:120]}\")\n",
    "                available_schema_out = directly_involved_schema\n",
    "            \n",
    "            fk_relationships_md = \"\"\n",
    "            if fk_relationships:\n",
    "                unique_fk = sorted(set(fk_relationships))\n",
    "                fk_relationships_md = \"\\n\".join([f\"- {rel}\" for rel in unique_fk])\n",
    "            else:\n",
    "                fk_relationships_md = \"None\"\n",
    "            \n",
    "            # === MERGE PRE-POPULATED SCHEMA FROM REGENERATION MODE ===\n",
    "            # If the interpretation phase dynamically loaded schema for user-requested tables,\n",
    "            # append it to the available schema to prevent hallucination\n",
    "            if has_prepopulated_schema:\n",
    "                # Check if prepopulated schema contains tables not already in available_schema_out\n",
    "                # Parse table names from prepopulated schema\n",
    "                prepopulated_tables = set()\n",
    "                for line in prepopulated_schema.split('\\n'):\n",
    "                    if line.strip().startswith('Table:'):\n",
    "                        tbl_name = line.replace('Table:', '').strip()\n",
    "                        prepopulated_tables.add(tbl_name)\n",
    "                \n",
    "                # Check which tables are NOT in the schema we already built\n",
    "                new_tables = []\n",
    "                for tbl in prepopulated_tables:\n",
    "                    tbl_normalized = tbl.replace('`', '')\n",
    "                    if tbl_normalized not in available_schema_out:\n",
    "                        new_tables.append(tbl)\n",
    "                \n",
    "                if new_tables:\n",
    "                    self.logger.info(f\"[{use_case_id}] Appending dynamically loaded schema for tables: {new_tables}\")\n",
    "                    # Append the prepopulated schema to include user-requested tables\n",
    "                    if available_schema_out:\n",
    "                        available_schema_out = available_schema_out + \"\\n\\n-- DYNAMICALLY LOADED TABLES (user-requested in regeneration instructions) --\\n\" + prepopulated_schema\n",
    "                    else:\n",
    "                        available_schema_out = prepopulated_schema\n",
    "                    self.logger.info(f\"[{use_case_id}] Total schema size after merge: {len(available_schema_out)} chars\")\n",
    "            \n",
    "            # Prepare prompt variables\n",
    "            # CRITICAL: We only provide directly_involved_schema to prevent hallucination\n",
    "            # No additional tables are provided - LLM can ONLY use tables explicitly involved in the use case\n",
    "            user_instructions = use_case.get('_user_instructions', '')\n",
    "            previous_feedback = \"\"\n",
    "            if user_instructions:\n",
    "                previous_feedback = f\"**USER INSTRUCTIONS (MUST FOLLOW):**\\nThe user has provided the following specific instructions for generating this SQL query. You MUST follow these instructions:\\n\\n{user_instructions}\\n\"\n",
    "                self.logger.info(f\"[{use_case_id}] Passing SQL Generation Instructions to LLM: {user_instructions[:200]}...\")\n",
    "                log_print(f\"   \uD83D\uDCDD [{use_case_id}] Including user SQL instructions in prompt\")\n",
    "            \n",
    "            # Get interpreted regeneration context if present (only populated during SQL Regeneration mode)\n",
    "            interpreted_regeneration_context = use_case.get('_interpreted_regeneration_context', '')\n",
    "            if interpreted_regeneration_context:\n",
    "                self.logger.info(f\"[{use_case_id}] Including interpreted regeneration context in SQL generation prompt\")\n",
    "            \n",
    "            # Get enriched business context from merged_business_context\n",
    "            enriched_ctx = getattr(self, 'merged_business_context', {})\n",
    "            prompt_vars = {\n",
    "                \"use_case_id\": use_case_id,\n",
    "                \"use_case_name\": use_case.get('Name', ''),\n",
    "                \"business_domain\": use_case.get('Business Domain', ''),\n",
    "                \"statement\": use_case.get('Statement', ''),\n",
    "                \"solution\": use_case.get('Solution', ''),\n",
    "                \"tables_involved\": tables_involved_str,\n",
    "                \"directly_involved_schema\": available_schema_out,\n",
    "                \"use_case_columns\": use_case_columns,\n",
    "                \"foreign_key_relationships\": fk_relationships_md,\n",
    "                \"unstructured_docs\": unstructured_docs,\n",
    "                \"previous_feedback\": previous_feedback,\n",
    "                \"interpreted_regeneration_context\": interpreted_regeneration_context,\n",
    "                \"ai_functions_summary\": generate_ai_functions_doc(\"summary\"),\n",
    "                \"statistical_functions_detailed\": generate_statistical_functions_doc(\"table\"),\n",
    "                \"business_name\": self.business_name,  # Pass business context to SQL generation\n",
    "                \"sql_model_serving\": self.sql_model_serving,  # User-configurable model for ai_query in generated SQL\n",
    "                # Enriched business context for persona enrichment in ai_query prompts\n",
    "                \"enriched_business_context\": enriched_ctx.get('business_context', 'General business operations'),\n",
    "                \"enriched_strategic_goals\": enriched_ctx.get('strategic_goals', 'Operational excellence and customer satisfaction') if isinstance(enriched_ctx.get('strategic_goals'), str) else ', '.join(enriched_ctx.get('strategic_goals', ['Operational excellence'])),\n",
    "                \"enriched_business_priorities\": enriched_ctx.get('business_priorities', 'Digital transformation and cost optimization') if isinstance(enriched_ctx.get('business_priorities'), str) else ', '.join(enriched_ctx.get('business_priorities', ['Digital transformation'])),\n",
    "                \"enriched_strategic_initiative\": enriched_ctx.get('strategic_initiative', 'Data-driven decision making'),\n",
    "                \"enriched_value_chain\": enriched_ctx.get('value_chain', 'Standard business operations'),\n",
    "                \"enriched_revenue_model\": enriched_ctx.get('revenue_model', 'Diverse revenue streams')\n",
    "            }\n",
    "            use_case['_directly_involved_schema'] = available_schema_out\n",
    "            use_case['_directly_involved_tables'] = list(directly_involved_tables)\n",
    "            \n",
    "            try:\n",
    "                # Final pre-flight check: Estimate actual prompt size\n",
    "                test_prompt = self.ai_agent._load_and_format_prompt(\"USE_CASE_SQL_GEN_PROMPT\", prompt_vars)\n",
    "                estimated_prompt_size = len(test_prompt)\n",
    "                \n",
    "                if estimated_prompt_size > sql_gen_context_limit:\n",
    "                    self.logger.error(\n",
    "                        f\"Use case {use_case_id}: Estimated prompt size ({estimated_prompt_size:,} chars) STILL exceeds model limit ({sql_gen_context_limit:,}). \"\n",
    "                        f\"Tables involved: {tables_involved_str}. \"\n",
    "                        f\"Schema size: {final_schema_size:,} chars. \"\n",
    "                        f\"Involved tables have {len(directly_involved_details)} columns total.\"\n",
    "                    )\n",
    "                    # Set error SQL and return use_case\n",
    "                    use_case['SQL'] = (\n",
    "                        f\"-- ERROR: Context too large for AI SQL generation\\n\"\n",
    "                        f\"-- Use Case: {use_case_id}\\n\"\n",
    "                        f\"-- Tables: {tables_involved_str}\\n\"\n",
    "                        f\"-- Estimated prompt: {estimated_prompt_size:,} chars (limit: {sql_gen_context_limit:,})\\n\"\n",
    "                        f\"-- Schema size: {final_schema_size:,} chars\\n\"\n",
    "                        f\"-- Total columns: {len(directly_involved_details)}\\n\"\n",
    "                        f\"-- RESOLUTION: Manually write SQL or reduce tables/columns involved\\n\"\n",
    "                        f\"SELECT 'Context too large - manual SQL required' AS error_message;\"\n",
    "                    )\n",
    "                    use_case['sql_generation_status'] = 'failed'\n",
    "                    return use_case\n",
    "                \n",
    "                self.logger.debug(f\"Use case {use_case_id}: Estimated prompt size: {estimated_prompt_size:,} chars (OK)\")\n",
    "                \n",
    "                # Check if schema is empty (but allow for volume paths in ai_parse_document use cases)\n",
    "                is_volume_path = tables_involved_str.startswith('/Volumes')\n",
    "                if (not directly_involved_schema or directly_involved_schema.strip() == \"\") and not is_volume_path:\n",
    "                    self.logger.error(f\"Use case {use_case_id}: Schema is EMPTY! Cannot generate SQL without table definitions.\")\n",
    "                    self.logger.error(f\"Use case {use_case_id}: Tables involved: {tables_involved_str}\")\n",
    "                    self.logger.error(f\"Use case {use_case_id}: Directly involved tables: {directly_involved_tables}\")\n",
    "                    self.logger.error(f\"Use case {use_case_id}: Directly involved details count: {len(directly_involved_details)}\")\n",
    "                    \n",
    "                    # Check if tables exist in full schema\n",
    "                    available_tables = set()\n",
    "                    for detail in full_schema_details:\n",
    "                        (catalog, schema, table, _, _, _) = detail\n",
    "                        available_tables.add(f\"{catalog}.{schema}.{table}\")\n",
    "                    \n",
    "                    if available_tables:\n",
    "                        self.logger.error(f\"Use case {use_case_id}: Full schema has {len(available_tables)} tables available\")\n",
    "                        self.logger.error(f\"Use case {use_case_id}: This suggests the business vs technical filter may be too aggressive\")\n",
    "                    else:\n",
    "                        self.logger.error(f\"Use case {use_case_id}: Full schema is also empty - database schema not loaded!\")\n",
    "                    \n",
    "                    use_case['SQL'] = (\n",
    "                        f\"-- CRITICAL ERROR: Schema is empty\\n\"\n",
    "                        f\"-- Use Case: {use_case_id}\\n\"\n",
    "                        f\"-- Tables Involved: {tables_involved_str}\\n\"\n",
    "                        f\"-- CRITICAL: Required tables are NOT provided in the schema\\n\"\n",
    "                        f\"-- \\n\"\n",
    "                        f\"-- DIAGNOSIS:\\n\"\n",
    "                        f\"-- - Directly involved details: {len(directly_involved_details)}\\n\"\n",
    "                        f\"-- - Full schema tables: {len(available_tables)}\\n\"\n",
    "                        f\"-- \\n\"\n",
    "                        f\"-- POSSIBLE CAUSES:\\n\"\n",
    "                        f\"-- 1. Business vs technical table filter removed these tables\\n\"\n",
    "                        f\"-- 2. Tables don't exist in the database\\n\"\n",
    "                        f\"-- 3. Schema loading failed\\n\"\n",
    "                        f\"-- 4. Table names in use case are incorrect\\n\"\n",
    "                        f\"-- \\n\"\n",
    "                        f\"-- RECOMMENDATION: Review the table filtering settings and verify table names.\\n\"\n",
    "                        f\"SELECT 'Schema Empty Error' AS error_message,\\n\"\n",
    "                        f\"       {len(available_tables)} AS total_schema_tables,\\n\"\n",
    "                        f\"       '{tables_involved_str}' AS requested_tables;\"\n",
    "                    )\n",
    "                    use_case['sql_generation_status'] = 'failed'\n",
    "                    return use_case\n",
    "                \n",
    "                adaptive_timeout = self._calculate_adaptive_sql_timeout(use_case)\n",
    "                self.logger.info(f\"⏳ [{use_case_id}] Waiting for LLM response (SQL generation, timeout={adaptive_timeout}s)...\")\n",
    "                \n",
    "                # MAIN CALL: Try with full context first (no reduction)\n",
    "                sql_response = None\n",
    "                needs_retry = False\n",
    "                retry_error = None\n",
    "                \n",
    "                try:\n",
    "                    sql_response = self.ai_agent.run_worker(\n",
    "                        step_name=f\"Generate_SQL_{use_case_id}_Wave\",\n",
    "                        worker_prompt_path=\"USE_CASE_SQL_GEN_PROMPT\",\n",
    "                        prompt_vars=prompt_vars,\n",
    "                        response_schema=None,\n",
    "                        timeout_override=adaptive_timeout,\n",
    "                        max_retries_override=0\n",
    "                    )\n",
    "                except (InputTooLongError, TruncatedResponseError) as e:\n",
    "                    needs_retry = True\n",
    "                    retry_error = e\n",
    "                    error_type = \"Input too long\" if isinstance(e, InputTooLongError) else \"Response truncated\"\n",
    "                    self.logger.warning(f\"⚠️  [{use_case_id}] {error_type} on main call - will retry with reduced context: {str(e)[:200]}\")\n",
    "                except Exception as e:\n",
    "                    error_msg_lower = str(e).lower()\n",
    "                    is_context_too_long = any(kw in error_msg_lower for kw in [\n",
    "                        'input is too long', 'too long for requested model', 'input length',\n",
    "                        'exceeds context limit', 'context window', 'token limit exceeded',\n",
    "                        'maximum context length', 'bad_request', '400'\n",
    "                    ]) and ('input' in error_msg_lower or 'length' in error_msg_lower or 'model' in error_msg_lower)\n",
    "                    \n",
    "                    is_timeout = any(kw in error_msg_lower for kw in ['timeout', 'timed out', 'deadline'])\n",
    "                    \n",
    "                    if is_context_too_long:\n",
    "                        needs_retry = True\n",
    "                        retry_error = InputTooLongError(str(e))\n",
    "                        self.logger.warning(f\"⚠️  [{use_case_id}] Context too long on main call - will retry with reduced context: {str(e)[:200]}\")\n",
    "                    elif is_timeout:\n",
    "                        self.logger.warning(f\"⏱️  Use case {use_case_id}: SQL generation timed out\")\n",
    "                        use_case['SQL'] = (\n",
    "                            f\"-- Use Case: {use_case_id} - {use_case_name}\\n\"\n",
    "                            f\"-- SQL generation timed out\\n\"\n",
    "                            f\"-- Tables Involved: {tables_involved_str}\\n\"\n",
    "                            f\"SELECT 'SQL Generation Timeout' AS error_message;\\n\"\n",
    "                            f\"--END OF GENERATED SQL\"\n",
    "                        )\n",
    "                        use_case['sql_generation_status'] = 'timeout'\n",
    "                        use_case['generated'] = 'N'\n",
    "                        use_case['validated'] = 'D'\n",
    "                        return use_case\n",
    "                    else:\n",
    "                        raise e\n",
    "                \n",
    "                # RETRY LOOP: Only if main call failed with context/truncation error\n",
    "                # Use percentage-based column reduction on retries\n",
    "                if needs_retry:\n",
    "                    # Progressive retry strategy with PERCENTAGE-based column reduction\n",
    "                    # Retry 1: Remove additional tables, keep 100% of columns\n",
    "                    # Retry 2: Keep 75% of columns per table\n",
    "                    # Retry 3: Keep 50% of columns per table\n",
    "                    # Retry 4: Keep 25% of columns per table\n",
    "                    MAX_RETRIES = 4\n",
    "                    column_reduction_percentages = [1.0, 0.75, 0.50, 0.25]  # Percentage of columns to KEEP\n",
    "                    \n",
    "                    # Count total columns in schema to calculate percentage-based limits\n",
    "                    original_schema = directly_involved_schema\n",
    "                    total_columns = original_schema.count('\\n  -') + original_schema.count('\\n- ')  # Rough column count\n",
    "                    \n",
    "                    for retry_attempt in range(MAX_RETRIES):\n",
    "                        try:\n",
    "                            reduction_pct = column_reduction_percentages[retry_attempt]\n",
    "                            \n",
    "                            if retry_attempt == 0:\n",
    "                                # First retry: Remove additional tables only, keep all columns\n",
    "                                self.logger.warning(f\"⚠️  [{use_case_id}] Retry {retry_attempt+1}/{MAX_RETRIES}: Removing additional tables (keeping 100% columns)\")\n",
    "                                prompt_vars[\"additional_schema\"] = \"\"\n",
    "                            else:\n",
    "                                # Subsequent retries: Also reduce columns by percentage\n",
    "                                # Calculate max columns as percentage of estimated columns per table\n",
    "                                # Assume average of 100 columns per table as baseline\n",
    "                                max_cols_per_table = max(10, int(100 * reduction_pct))\n",
    "                                self.logger.warning(f\"⚠️  [{use_case_id}] Retry {retry_attempt+1}/{MAX_RETRIES}: Keeping {int(reduction_pct*100)}% columns (max {max_cols_per_table} per table)\")\n",
    "                                truncated_schema = self._truncate_schema_columns(original_schema, max_cols_per_table)\n",
    "                                prompt_vars[\"directly_involved_schema\"] = truncated_schema\n",
    "                                prompt_vars[\"additional_schema\"] = \"\"\n",
    "                            \n",
    "                            sql_response = self.ai_agent.run_worker(\n",
    "                                step_name=f\"Generate_SQL_{use_case_id}_Wave_Retry{retry_attempt+1}_{int(reduction_pct*100)}pct\",\n",
    "                                worker_prompt_path=\"USE_CASE_SQL_GEN_PROMPT\",\n",
    "                                prompt_vars=prompt_vars,\n",
    "                                response_schema=None,\n",
    "                                timeout_override=adaptive_timeout,\n",
    "                                max_retries_override=0\n",
    "                            )\n",
    "                            \n",
    "                            # Success!\n",
    "                            self.logger.info(f\"✅ [{use_case_id}] Retry {retry_attempt+1} succeeded with {int(reduction_pct*100)}% columns\")\n",
    "                            break\n",
    "                            \n",
    "                        except (InputTooLongError, TruncatedResponseError) as e:\n",
    "                            error_type = \"Input too long\" if isinstance(e, InputTooLongError) else \"Response truncated\"\n",
    "                            self.logger.warning(f\"⚠️  [{use_case_id}] {error_type} on retry {retry_attempt+1}/{MAX_RETRIES}: {str(e)[:200]}\")\n",
    "                            \n",
    "                            if retry_attempt >= MAX_RETRIES - 1:\n",
    "                                # All retries exhausted\n",
    "                                self.logger.error(f\"❌ [{use_case_id}] All {MAX_RETRIES} retries exhausted\")\n",
    "                                use_case['SQL'] = (\n",
    "                                    f\"-- ❌ SQL GENERATION FAILED ({error_type.upper()})\\n\"\n",
    "                                    f\"-- Use Case: {use_case_id}\\n\"\n",
    "                                    f\"-- Tables Involved: {tables_involved_str}\\n\"\n",
    "                                    f\"-- Error: {error_type} even after {MAX_RETRIES} retries with progressive column reduction (100%→75%→50%→25%)\\n\"\n",
    "                                    f\"-- RESOLUTION: Manually write SQL or reduce tables/columns involved\\n\"\n",
    "                                    f\"SELECT 'Context too large - manual SQL required' AS error_message;\\n\"\n",
    "                                    f\"--END OF GENERATED SQL\"\n",
    "                                )\n",
    "                                use_case['sql_generation_status'] = 'failed'\n",
    "                                use_case['generated'] = 'N'\n",
    "                                use_case['validated'] = 'D'\n",
    "                                return use_case\n",
    "                            continue\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            error_msg_lower = str(e).lower()\n",
    "                            is_context_too_long = any(kw in error_msg_lower for kw in [\n",
    "                                'input is too long', 'too long for requested model', 'input length',\n",
    "                                'exceeds context limit', 'context window', 'token limit exceeded',\n",
    "                                'maximum context length', 'bad_request', '400'\n",
    "                            ]) and ('input' in error_msg_lower or 'length' in error_msg_lower or 'model' in error_msg_lower)\n",
    "                            \n",
    "                            if is_context_too_long:\n",
    "                                self.logger.warning(f\"⚠️  [{use_case_id}] Context too long on retry {retry_attempt+1}/{MAX_RETRIES}: {str(e)[:200]}\")\n",
    "                                if retry_attempt >= MAX_RETRIES - 1:\n",
    "                                    use_case['SQL'] = (\n",
    "                                        f\"-- ❌ SQL GENERATION FAILED (CONTEXT TOO LARGE)\\n\"\n",
    "                                        f\"-- Use Case: {use_case_id}\\n\"\n",
    "                                        f\"-- Tables Involved: {tables_involved_str}\\n\"\n",
    "                                        f\"-- Error: Input exceeds model's context limit even after {MAX_RETRIES} retries\\n\"\n",
    "                                        f\"SELECT 'Context too large - manual SQL required' AS error_message;\\n\"\n",
    "                                        f\"--END OF GENERATED SQL\"\n",
    "                                    )\n",
    "                                    use_case['sql_generation_status'] = 'failed'\n",
    "                                    return use_case\n",
    "                                continue\n",
    "                            else:\n",
    "                                raise e\n",
    "\n",
    "                self.logger.info(f\"✅ [{use_case_id}] Received LLM response ({len(sql_response) if sql_response else 0} chars)\")\n",
    "                if sql_response and (\"STATUS: FAILED\" in sql_response or \"Schema missing\" in sql_response):\n",
    "                    self.logger.error(f\"Use case {use_case_id}: SQL generation returned FAILED status.\")\n",
    "                    use_case['SQL'] = (\n",
    "                        f\"-- Use Case: {use_case_id} - {use_case_name}\\n\"\n",
    "                        f\"-- SQL generation failed\\n\"\n",
    "                        f\"-- Tables Involved: {tables_involved_str}\\n\"\n",
    "                        f\"SELECT 'SQL Generation Failed' AS error_message;\\n\"\n",
    "                        f\"--END OF GENERATED SQL\"\n",
    "                    )\n",
    "                    use_case['sql_generation_status'] = 'failed'\n",
    "                    use_case['generated'] = 'N'\n",
    "                    use_case['validated'] = 'D'\n",
    "                    return use_case\n",
    "            except (InputTooLongError, TruncatedResponseError) as inner_e:\n",
    "                # Re-raise context/truncation errors to be handled by outer exception handler\n",
    "                self.logger.error(f\"Use case {use_case_id}: Inner exception: {str(inner_e)[:200]}\")\n",
    "                use_case['SQL'] = (\n",
    "                    f\"-- ❌ SQL GENERATION FAILED\\n\"\n",
    "                    f\"-- Use Case: {use_case_id}\\n\"\n",
    "                    f\"-- Tables Involved: {tables_involved_str}\\n\"\n",
    "                    f\"-- Error: {str(inner_e)[:200]}\\n\"\n",
    "                    f\"SELECT 'SQL Generation Failed' AS error_message;\\n\"\n",
    "                    f\"--END OF GENERATED SQL\"\n",
    "                )\n",
    "                use_case['sql_generation_status'] = 'failed'\n",
    "                use_case['generated'] = 'N'\n",
    "                use_case['validated'] = 'D'\n",
    "                return use_case\n",
    "            \n",
    "            # Validate that we got a response\n",
    "            processed_use_case = self._process_sql_candidate(\n",
    "                use_case,\n",
    "                sql_response,\n",
    "                tables_involved_str,\n",
    "                directly_involved_schema,\n",
    "                directly_involved_tables,\n",
    "                full_schema_details,\n",
    "                schema_index\n",
    "            )\n",
    "            elapsed = time.time() - start_time\n",
    "            self.logger.info(f\"✓ SQL generated for {use_case_id} in {elapsed:.1f}s\")\n",
    "            return processed_use_case\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            elapsed = time.time() - start_time if 'start_time' in locals() else 0\n",
    "            error_msg = str(e)[:200]\n",
    "            error_lower = error_msg.lower()\n",
    "            stack_trace = traceback.format_exc()[:500]\n",
    "            self.logger.error(f\"✗ Failed to generate SQL for {use_case_id} after {elapsed:.1f}s: {error_msg}\")\n",
    "            self.logger.debug(f\"Stack trace for {use_case_id}: {stack_trace}\")\n",
    "            \n",
    "            is_timeout = any(kw in error_lower for kw in ['timeout', 'timed out', 'deadline'])\n",
    "            \n",
    "            if is_timeout:\n",
    "                self.logger.warning(f\"⏱️  Use case {use_case_id}: SQL generation timed out (outer)\")\n",
    "                use_case['SQL'] = (\n",
    "                    f\"-- Use Case: {use_case_id} - {use_case_name}\\n\"\n",
    "                    f\"-- SQL generation timed out\\n\"\n",
    "                    f\"-- Tables Involved: {tables_involved_str}\\n\"\n",
    "                    f\"SELECT 'SQL Generation Timeout' AS error_message;\\n\"\n",
    "                    f\"-- If you run the query and it was not valid, set IsValid to No and run Inspire again with 'Generate = SQL Regeneration', and Inspire will regenerate a new query for you to validate. You can also pass special instruction in below field:\\n\"\n",
    "                    f\"--SQL Generation Instructions Begin\\n\"\n",
    "                    f\"--\\n\"\n",
    "                    f\"--SQL Generation Instructions End\"\n",
    "                )\n",
    "                use_case['sql_generation_status'] = 'timeout'\n",
    "                use_case['generated'] = 'N'\n",
    "                use_case['validated'] = 'D'\n",
    "            else:\n",
    "                use_case['SQL'] = (\n",
    "                    f\"-- Use Case: {use_case_id} - {use_case_name}\\n\"\n",
    "                    f\"-- SQL generation exception: {error_msg[:100]}\\n\"\n",
    "                    f\"-- Tables Involved: {tables_involved_str}\\n\"\n",
    "                    f\"SELECT 'SQL Generation Exception' AS error_message;\\n\"\n",
    "                    f\"-- If you run the query and it was not valid, set IsValid to No and run Inspire again with 'Generate = SQL Regeneration', and Inspire will regenerate a new query for you to validate. You can also pass special instruction in below field:\\n\"\n",
    "                    f\"--SQL Generation Instructions Begin\\n\"\n",
    "                    f\"--\\n\"\n",
    "                    f\"--SQL Generation Instructions End\"\n",
    "                )\n",
    "                use_case['sql_generation_status'] = 'failed'\n",
    "                use_case['generated'] = 'N'\n",
    "                use_case['validated'] = 'D'\n",
    "                \n",
    "            return use_case\n",
    "\n",
    "    def _fix_sql_after_validation_failure(self, use_case: dict, full_schema_details: list, unstructured_docs_markdown: str, schema_index: dict) -> dict:\n",
    "        use_case_id = use_case.get('No', 'UNKNOWN')\n",
    "        tables_involved_str = use_case.get('Tables Involved', '')\n",
    "        directly_involved_schema = use_case.get('_directly_involved_schema', '')\n",
    "        directly_involved_tables = set(use_case.get('_directly_involved_tables') or [])\n",
    "        \n",
    "        # Check if this is a \"force static check\" mode (no execution error)\n",
    "        is_static_check = use_case.get('_force_static_check', False)\n",
    "        explain_error_msg = use_case.get('sql_validation_error') or \"SQL validation failed\"\n",
    "        \n",
    "        if is_static_check:\n",
    "            explain_error_msg = \"Please perform a static code analysis on the query. Check for: 1) Syntax errors 2) References to columns that do not exist in the schema provided 3) Logic issues. Return the FIXED query.\"\n",
    "\n",
    "        reviewer_prompt_vars = {\n",
    "            \"use_case_id\": use_case_id,\n",
    "            \"use_case_name\": use_case.get('Name', ''),\n",
    "            \"business_domain\": use_case.get('Business Domain', ''),\n",
    "            \"statement\": use_case.get('Statement', ''),\n",
    "            \"tables_involved\": tables_involved_str,\n",
    "            \"directly_involved_schema\": directly_involved_schema,\n",
    "            \"original_sql\": use_case.get('SQL', ''),\n",
    "            \"explain_error\": explain_error_msg,\n",
    "            \"use_case_columns\": use_case.get('Involved Columns') or use_case.get('Columns Involved') or \"\"\n",
    "        }\n",
    "        adaptive_timeout = self._calculate_adaptive_sql_timeout(use_case)\n",
    "        fixed_sql = self.ai_agent.run_worker(\n",
    "            step_name=f\"Fix_SQL_Execution_{use_case_id}_WaveRetry\",\n",
    "            worker_prompt_path=\"USE_CASE_SQL_FIX_PROMPT\",\n",
    "            prompt_vars=reviewer_prompt_vars,\n",
    "            response_schema=None,\n",
    "            timeout_override=adaptive_timeout,\n",
    "            max_retries_override=self.max_retry_attempts\n",
    "        )\n",
    "        return self._process_sql_candidate(\n",
    "            use_case,\n",
    "            fixed_sql,\n",
    "            tables_involved_str,\n",
    "            directly_involved_schema,\n",
    "            directly_involved_tables,\n",
    "            full_schema_details,\n",
    "            schema_index\n",
    "        )\n",
    "\n",
    "    def _run_sql_task_wrapper(self, use_case: dict, full_schema_details: list, unstructured_docs_markdown: str, schema_index: dict) -> dict:\n",
    "        if use_case.get('_needs_fix'):\n",
    "            return self._fix_sql_after_validation_failure(use_case, full_schema_details, unstructured_docs_markdown, schema_index)\n",
    "        \n",
    "        # 1. Generate Initial SQL\n",
    "        uc_with_sql = self._generate_sql_for_use_case(use_case, full_schema_details, unstructured_docs_markdown, schema_index)\n",
    "        \n",
    "        return uc_with_sql\n",
    "\n",
    "    def _run_sql_wave(self, wave_id: int, use_cases: list, full_schema_details: list, unstructured_docs_markdown: str, schema_index: dict, parallelism: int) -> tuple:\n",
    "        use_cases_with_sql = []\n",
    "        timed_out = []\n",
    "        validation_failed = []\n",
    "        with ThreadPoolExecutor(max_workers=parallelism, thread_name_prefix=f\"SQLWave{wave_id}\") as executor:\n",
    "            future_to_uc = {}\n",
    "            for uc in use_cases:\n",
    "                future = executor.submit(self._run_sql_task_wrapper, uc, full_schema_details, unstructured_docs_markdown, schema_index)\n",
    "                future_to_uc[future] = uc\n",
    "            for future in concurrent.futures.as_completed(future_to_uc, timeout=None):\n",
    "                uc_ref = future_to_uc[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result is None:\n",
    "                        result = uc_ref\n",
    "                    use_cases_with_sql.append(result)\n",
    "                    \n",
    "                    status = result.get('sql_generation_status')\n",
    "                    if status == 'timeout':\n",
    "                        timed_out.append(result)\n",
    "                        use_case_id = result.get('No', 'UNKNOWN')\n",
    "                        result['generated'] = 'N'\n",
    "                        result['validated'] = 'D'\n",
    "                        self.logger.warning(f\"⏱️ [{use_case_id}] SQL generation timed out - marked for Queries regeneration\")\n",
    "                    elif status == 'failed':\n",
    "                        result['generated'] = 'N'\n",
    "                        result['validated'] = 'D'\n",
    "                        timed_out.append(result)\n",
    "                    elif result.get('sql_validation_status') == 'failed' or result.get('column_validation_status') == 'failed':\n",
    "                        result['generated'] = 'Y'\n",
    "                        result['validated'] = 'N'\n",
    "                        validation_failed.append(result)\n",
    "                    else:\n",
    "                        if result.get('generated') != 'Y':\n",
    "                            result['generated'] = 'Y'\n",
    "                        if result.get('validated') not in ['Y', 'N']:\n",
    "                            result['validated'] = 'D'\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    msg = str(e)\n",
    "                    is_timeout = 'timeout' in msg.lower() or 'timed out' in msg.lower()\n",
    "                    use_case_id = uc_ref.get('No', 'UNKNOWN')\n",
    "                    tables_involved_str = uc_ref.get('Tables Involved', '')\n",
    "                    use_case_name = uc_ref.get('Name', '')[:50]\n",
    "                    \n",
    "                    if is_timeout:\n",
    "                        self.logger.warning(f\"⏱️ [{use_case_id}] SQL generation timed out in wave executor\")\n",
    "                        uc_ref['SQL'] = (\n",
    "                            f\"-- Use Case: {use_case_id} - {use_case_name}\\n\"\n",
    "                            f\"-- SQL generation timed out\\n\"\n",
    "                            f\"-- Tables Involved: {tables_involved_str}\\n\"\n",
    "                            f\"SELECT 'SQL Generation Timeout' AS error_message;\\n\"\n",
    "                            f\"-- If you run the query and it was not valid, set IsValid to No and run Inspire again with 'Generate = SQL Regeneration', and Inspire will regenerate a new query for you to validate. You can also pass special instruction in below field:\\n\"\n",
    "                            f\"--SQL Generation Instructions Begin\\n\"\n",
    "                            f\"--\\n\"\n",
    "                            f\"--SQL Generation Instructions End\"\n",
    "                        )\n",
    "                        uc_ref['sql_generation_status'] = 'timeout'\n",
    "                    else:\n",
    "                        uc_ref['sql_generation_status'] = 'failed'\n",
    "                        uc_ref['SQL'] = (\n",
    "                            f\"-- Use Case: {use_case_id} - {use_case_name}\\n\"\n",
    "                            f\"-- SQL generation failed: {msg[:100]}\\n\"\n",
    "                            f\"-- Tables Involved: {tables_involved_str}\\n\"\n",
    "                            f\"SELECT 'SQL Generation Error' AS error_message;\\n\"\n",
    "                            f\"-- If you run the query and it was not valid, set IsValid to No and run Inspire again with 'Generate = SQL Regeneration', and Inspire will regenerate a new query for you to validate. You can also pass special instruction in below field:\\n\"\n",
    "                            f\"--SQL Generation Instructions Begin\\n\"\n",
    "                            f\"--\\n\"\n",
    "                            f\"--SQL Generation Instructions End\"\n",
    "                        )\n",
    "                    uc_ref['generated'] = 'N'\n",
    "                    uc_ref['validated'] = 'D'\n",
    "                    timed_out.append(uc_ref)\n",
    "                    use_cases_with_sql.append(uc_ref)\n",
    "        return use_cases_with_sql, timed_out, validation_failed\n",
    "\n",
    "    def _generate_sql_sequential(self, use_cases: list, full_schema_details: list, unstructured_docs_markdown: str) -> list:\n",
    "        \"\"\"\n",
    "        Generate SQL for all use cases SEQUENTIALLY (no parallelism).\n",
    "        Used within domain processing where the domain itself is already running in parallel.\n",
    "        \n",
    "        Args:\n",
    "            use_cases: List of use case dictionaries  \n",
    "            full_schema_details: Full list of (catalog, schema, table, column, type, comment) tuples\n",
    "            unstructured_docs_markdown: Unstructured documents markdown\n",
    "        \"\"\"\n",
    "        total_use_cases = len(use_cases)\n",
    "        \n",
    "        self.logger.info(f\"Starting sequential SQL generation for {total_use_cases} use cases...\")\n",
    "        \n",
    "        # Build schema index ONCE for fast lookup\n",
    "        self.logger.debug(f\"Building schema index from {len(full_schema_details)} columns...\")\n",
    "        schema_by_table = defaultdict(list)\n",
    "        for detail in full_schema_details:\n",
    "            (catalog, schema, table, column_name, data_type, comment) = detail\n",
    "            fqtn = f\"{catalog}.{schema}.{table}\"\n",
    "            fqtn_backticks = f\"`{catalog}`.`{schema}`.`{table}`\"\n",
    "            schema_by_table[fqtn].append(detail)\n",
    "            schema_by_table[fqtn_backticks].append(detail)\n",
    "        \n",
    "        # Process use cases sequentially\n",
    "        use_cases_with_sql = []\n",
    "        completed_count = 0\n",
    "        failed_count = 0\n",
    "        deferred_timeouts = []\n",
    "        \n",
    "        for idx, uc in enumerate(use_cases, 1):\n",
    "            use_case_id = uc.get('No', 'UNKNOWN')\n",
    "            use_case_name = uc.get('Name', '')[:40]\n",
    "            \n",
    "            try:\n",
    "                result = self._generate_sql_for_use_case(uc, full_schema_details, unstructured_docs_markdown, schema_by_table)\n",
    "                use_cases_with_sql.append(result)\n",
    "                completed_count += 1\n",
    "                \n",
    "                if idx % 5 == 0 or idx == total_use_cases:\n",
    "                    self.logger.debug(f\"SQL generation progress: {idx}/{total_use_cases}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                failed_count += 1\n",
    "                error_msg = str(e)[:150]\n",
    "                self.logger.error(f\"SQL generation failed for {use_case_id}: {error_msg}\")\n",
    "                uc['SQL'] = f\"-- SQL generation failed for {use_case_id}\\n-- Error: {error_msg}\\nSELECT 'SQL Generation Error' as error;\"\n",
    "                use_cases_with_sql.append(uc)\n",
    "        \n",
    "        success_count = completed_count - failed_count\n",
    "        self.logger.info(f\"✅ Sequential SQL generation complete: {success_count} succeeded, {failed_count} failed\")\n",
    "        \n",
    "        return use_cases_with_sql\n",
    "    \n",
    "    def _generate_sql_parallel(self, use_cases: list, full_schema_details: list, unstructured_docs_markdown: str, is_retry: bool = False) -> list:\n",
    "        \"\"\"\n",
    "        Generate SQL for all use cases in parallel using max_parallelism.\n",
    "        Note: Uses lower parallelism (max 10) to avoid overwhelming the LLM service and cluster.\n",
    "        \n",
    "        Args:\n",
    "            use_cases: List of use case dictionaries\n",
    "            full_schema_details: Full list of (catalog, schema, table, column, type, comment) tuples\n",
    "            unstructured_docs_markdown: Unstructured documents markdown\n",
    "        \"\"\"\n",
    "        # === CHECK IF SQL CODE GENERATION IS DISABLED ===\n",
    "        if not self.generate_sql_code:\n",
    "            self.logger.info(\"⚠️ SQL Code generation is DISABLED - using placeholder SQL for all use cases\")\n",
    "            log_print(f\"\\n⚠️ SQL Code generation DISABLED - using placeholder SQL\")\n",
    "            \n",
    "            for uc in use_cases:\n",
    "                tables_involved = uc.get('Tables Involved', 'your_table')\n",
    "                first_table = tables_involved.split(',')[0].strip() if tables_involved else 'your_table'\n",
    "                placeholder_sql = (\n",
    "                    f\"-- TODO: SQL Code generation was disabled\\n\"\n",
    "                    f\"-- To generate SQL: Run 'Re-generate SQL' operation mode\\n\"\n",
    "                    f\"-- Tables Involved: {tables_involved}\\n\"\n",
    "                    f\"SELECT * FROM {first_table} LIMIT 10;\"\n",
    "                )\n",
    "                uc['SQL'] = placeholder_sql\n",
    "                uc['generated'] = 'N'\n",
    "                uc['validated'] = 'N'\n",
    "            \n",
    "            return use_cases\n",
    "        \n",
    "        total_use_cases = len(use_cases)\n",
    "        total_columns = len(full_schema_details)\n",
    "        avg_prompt_chars = total_columns * 50 + len(unstructured_docs_markdown)  # Estimate based on schema + docs\n",
    "        \n",
    "        # ADAPTIVE PARALLELISM: Calculate based on use cases, columns, and prompt size\n",
    "        sql_parallelism, reason = calculate_adaptive_parallelism(\n",
    "            \"sql_generation\", self.max_parallelism,\n",
    "            num_items=total_use_cases,\n",
    "            total_columns=total_columns,\n",
    "            avg_prompt_chars=avg_prompt_chars,\n",
    "            is_llm_operation=True, logger=self.logger\n",
    "        )\n",
    "        \n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"\uD83D\uDD04 SQL GENERATION: {total_use_cases} use cases\")\n",
    "        log_print(f\"{'='*80}\")\n",
    "        log_adaptive_parallelism_decision(\"sql_generation\", sql_parallelism, self.max_parallelism, reason)\n",
    "        log_print(f\"Estimated time per wave: {(total_use_cases * 5 / sql_parallelism / 60):.1f} minutes\")\n",
    "        log_print(f\"{'='*80}\\n\")\n",
    "        self.logger.info(f\"\uD83D\uDD27 Building schema index from {len(full_schema_details)} columns for fast lookup...\")\n",
    "        schema_by_table = defaultdict(list)\n",
    "        for detail in full_schema_details:\n",
    "            (catalog, schema, table, column_name, data_type, comment) = detail\n",
    "            # Create multiple index keys for flexible matching\n",
    "            fqtn = f\"{catalog}.{schema}.{table}\"\n",
    "            fqtn_backticks = f\"`{catalog}`.`{schema}`.`{table}`\"\n",
    "            schema_by_table[fqtn].append(detail)\n",
    "            schema_by_table[fqtn_backticks].append(detail)\n",
    "        self.logger.info(f\"   ✓ Schema index built with {len(schema_by_table)} table entries\")\n",
    "        priority_order = {\n",
    "            \"ultra high\": 0,\n",
    "            \"very high\": 1,\n",
    "            \"high\": 2,\n",
    "            \"medium\": 3,\n",
    "            \"low\": 4,\n",
    "            \"very low\": 5,\n",
    "            \"ultra low\": 6\n",
    "        }\n",
    "        def sort_backlog(items):\n",
    "            return [uc for _, uc in sorted(\n",
    "                enumerate(items),\n",
    "                key=lambda pair: (priority_order.get(str(pair[1].get('Priority', '')).strip().lower(), len(priority_order)), pair[0])\n",
    "            )]\n",
    "        wave_parallelism = [\n",
    "            (1, sql_parallelism),\n",
    "            (2, sql_parallelism),\n",
    "            (3, max(1, (sql_parallelism + 1) // 2)),\n",
    "            (4, max(1, (sql_parallelism + 2) // 3)),\n",
    "            (5, max(1, (sql_parallelism + 2) // 3))\n",
    "        ]\n",
    "        final_results = {}\n",
    "        backlog = sort_backlog(use_cases)\n",
    "        for wave_id, wave_workers in wave_parallelism:\n",
    "            if not backlog:\n",
    "                break\n",
    "            backlog = sort_backlog(backlog)\n",
    "            import time\n",
    "            wave_start_time = time.time()\n",
    "            self.logger.info(f\"\uD83D\uDD01 Wave {wave_id}: processing {len(backlog)} use cases with parallelism {wave_workers} (priority-ordered)\")\n",
    "            log_print(f\"   ▶️ Wave {wave_id}: {len(backlog)} use cases, parallelism {wave_workers} (priority-ordered)\")\n",
    "            results, timed_out, validation_failed = self._run_sql_wave(wave_id, backlog, full_schema_details, unstructured_docs_markdown, schema_by_table, wave_workers)\n",
    "            \n",
    "            wave_end_time = time.time()\n",
    "            wave_duration = wave_end_time - wave_start_time\n",
    "            \n",
    "            wave_succeeded = 0\n",
    "            wave_failed = 0\n",
    "            for uc in results:\n",
    "                if uc.get('generated') == 'Y' and uc.get('validated') in ['Y', 'D']:\n",
    "                    wave_succeeded += 1\n",
    "                else:\n",
    "                    wave_failed += 1\n",
    "            \n",
    "            self.logger.info(f\"\uD83D\uDCCA Wave {wave_id} Report:\")\n",
    "            self.logger.info(f\"   • Duration: {wave_duration:.1f}s\")\n",
    "            self.logger.info(f\"   • Start: {time.strftime('%H:%M:%S', time.localtime(wave_start_time))}\")\n",
    "            self.logger.info(f\"   • End: {time.strftime('%H:%M:%S', time.localtime(wave_end_time))}\")\n",
    "            self.logger.info(f\"   • Processed: {len(results)}\")\n",
    "            self.logger.info(f\"   • Succeeded: {wave_succeeded}\")\n",
    "            self.logger.info(f\"   • Failed: {wave_failed}\")\n",
    "            self.logger.info(f\"      - ⏱️ Timeouts/Errors (will retry in next wave): {len(timed_out)}\")\n",
    "            self.logger.info(f\"      - \uD83D\uDD0D Validation Failures (will retry in next wave): {len(validation_failed)}\")\n",
    "            \n",
    "            if len(timed_out) > 0:\n",
    "                if wave_id < 5:\n",
    "                    self.logger.info(f\"   ⏱️ TIMED OUT SQL DETAILS (will retry in wave {wave_id + 1}):\")\n",
    "                else:\n",
    "                    self.logger.info(f\"   ⏱️ FAILED SQL DETAILS (run 'SQL Regeneration' to regenerate):\")\n",
    "                for uc in timed_out:\n",
    "                    uc_id = uc.get('No', 'UNKNOWN')\n",
    "                    uc_name = uc.get('Name', '')[:40]\n",
    "                    retry_count = uc.get('_retry_attempt', 0)\n",
    "                    self.logger.info(f\"      • [{uc_id}] {uc_name} → generated={uc.get('generated', 'N')}, validated={uc.get('validated', 'D')}, retries={retry_count}\")\n",
    "            \n",
    "            log_print(f\"\\n================================================================================\")\n",
    "            log_print(f\"\uD83D\uDCCA WAVE {wave_id} REPORT\")\n",
    "            log_print(f\"================================================================================\")\n",
    "            log_print(f\"   • Time: {time.strftime('%H:%M:%S', time.localtime(wave_start_time))} - {time.strftime('%H:%M:%S', time.localtime(wave_end_time))} ({wave_duration:.1f}s)\")\n",
    "            log_print(f\"   • Total Processed: {len(results)}\")\n",
    "            log_print(f\"   • ✅ Succeeded: {wave_succeeded}\")\n",
    "            log_print(f\"   • ❌ Failed: {wave_failed}\")\n",
    "            if len(timed_out) > 0:\n",
    "                if wave_id < 5:\n",
    "                    log_print(f\"      - ⏱️  Timeouts/Errors: {len(timed_out)} (will retry in wave {wave_id + 1})\")\n",
    "                else:\n",
    "                    log_print(f\"      - ⏱️  Timeouts/Errors: {len(timed_out)} (run 'SQL Regeneration' to regenerate)\")\n",
    "            if len(validation_failed) > 0:\n",
    "                log_print(f\"      - \uD83D\uDD0D Validation Failures: {len(validation_failed)} (will retry in next wave)\")\n",
    "            log_print(f\"================================================================================\\n\")\n",
    "            \n",
    "            log_print(f\"   ✅ Wave {wave_id} done in {wave_duration:.1f}s: {wave_succeeded} OK, {wave_failed} Failed\")\n",
    "            \n",
    "            for uc in results:\n",
    "                final_results[uc.get('No', 'UNKNOWN')] = uc\n",
    "            \n",
    "            for uc in validation_failed:\n",
    "                if uc.get('_needs_fix'):\n",
    "                    del uc['_needs_fix']\n",
    "                else:\n",
    "                    uc['_needs_fix'] = True\n",
    "\n",
    "            # Build backlog for next wave: include BOTH validation failures AND timeouts for retry\n",
    "            # Reset timeout status for timed_out items so they get retried\n",
    "            for uc in timed_out:\n",
    "                # Clear timeout status to allow retry in next wave\n",
    "                if uc.get('sql_generation_status') == 'timeout':\n",
    "                    uc['sql_generation_status'] = 'pending_retry'\n",
    "                    uc['_retry_attempt'] = uc.get('_retry_attempt', 0) + 1\n",
    "                    self.logger.debug(f\"   \uD83D\uDD04 [{uc.get('No', 'UNKNOWN')}] Marked for retry (attempt {uc['_retry_attempt']})\")\n",
    "            \n",
    "            # Combine validation failures and timed out items for next wave retry\n",
    "            retry_items = validation_failed + timed_out\n",
    "            backlog = sort_backlog(retry_items)\n",
    "            \n",
    "            if len(timed_out) > 0 and wave_id < 5:\n",
    "                self.logger.info(f\"   \uD83D\uDD04 {len(timed_out)} timed out use cases will be retried in wave {wave_id + 1}\")\n",
    "                log_print(f\"   \uD83D\uDD04 {len(timed_out)} timed out use cases will be retried in wave {wave_id + 1}\")\n",
    "            \n",
    "            completed = len(results)\n",
    "            progress_pct = (completed / total_use_cases) * 100 if total_use_cases else 0\n",
    "            self.logger.info(f\"Wave {wave_id} completed {completed} items ({progress_pct:.1f}%) - {len(timed_out)} timeouts/errors, {len(validation_failed)} validation failures (will retry)\")\n",
    "        for uc in backlog:\n",
    "            if uc.get('sql_generation_status') == 'timeout':\n",
    "                uc['SQL'] = (\n",
    "                    f\"-- SQL generation timed out for {uc.get('No', 'UNKNOWN')}\\n\"\n",
    "                    f\"-- Timeout: {self.llm_timeout_seconds} seconds (final attempt)\\n\"\n",
    "                    f\"SELECT 'SQL Generation Timeout' as error;\"\n",
    "                )\n",
    "            elif uc.get('sql_validation_status') == 'failed' or uc.get('column_validation_status') == 'failed':\n",
    "                if not uc.get('SQL'):\n",
    "                    uc['SQL'] = f\"-- SQL validation failed for {uc.get('No', 'UNKNOWN')}\\nSELECT 'SQL Validation Failed' as error;\"\n",
    "            final_results[uc.get('No', 'UNKNOWN')] = uc\n",
    "        ordered_results = []\n",
    "        for uc in use_cases:\n",
    "            ordered_results.append(final_results.get(uc.get('No', 'UNKNOWN'), uc))\n",
    "        success_count = sum(1 for uc in ordered_results if uc.get('sql_generation_status') == 'succeeded' and uc.get('sql_validation_status') != 'failed' and uc.get('column_validation_status') != 'failed')\n",
    "        failed_count = len(ordered_results) - success_count\n",
    "        self.logger.info(f\"✅ SQL generation complete across waves: {success_count} succeeded, {failed_count} failed (Total: {len(ordered_results)}/{total_use_cases})\")\n",
    "        log_print(f\"✅ SQL generation finished across waves: {success_count} succeeded, {failed_count} failed\")\n",
    "        \n",
    "        # Show REST API validation statistics\n",
    "        if hasattr(self, '_explain_stats'):\n",
    "            stats = self._explain_stats\n",
    "            self.logger.info(\"\")\n",
    "            self.logger.info(\"=\" * 80)\n",
    "            self.logger.info(\"\uD83D\uDCCA SQL VALIDATION SUMMARY (REST API with Fresh Clients)\")\n",
    "            self.logger.info(\"=\" * 80)\n",
    "            self.logger.info(f\"   Total queries attempted: {stats['attempted']}\")\n",
    "            self.logger.info(f\"   ✅ Validations succeeded: {stats['succeeded']}\")\n",
    "            \n",
    "            # Show breakdown of primary vs retry attempts\n",
    "            local_count = stats.get('local_succeeded', 0)\n",
    "            remote_count = stats.get('remote_succeeded', 0)\n",
    "            if local_count > 0 or remote_count > 0:\n",
    "                self.logger.info(f\"      ├─ \uD83C\uDF10 Primary attempt: {local_count}\")\n",
    "                self.logger.info(f\"      └─ \uD83D\uDD04 Retry attempt: {remote_count}\")\n",
    "            \n",
    "            self.logger.info(f\"   ❌ Validations failed (syntax errors): {stats['failed']}\")\n",
    "            self.logger.info(f\"   ⚠️  Authentication/permission errors: {stats['auth_errors']}\")\n",
    "            self.logger.info(f\"   ⏭️  Skipped (no warehouse): {stats['skipped']}\")\n",
    "            \n",
    "            if stats['auth_errors'] > 0:\n",
    "                self.logger.info(\"\")\n",
    "                self.logger.info(\"   ⚠️  Note: Authentication errors prevent SQL validation.\")\n",
    "                self.logger.info(\"   Consider checking warehouse permissions.\")\n",
    "            \n",
    "            if stats['failed'] > 0:\n",
    "                self.logger.info(\"\")\n",
    "                self.logger.info(f\"   ⚠️  {stats['failed']} queries had syntax errors and were attempted to be fixed.\")\n",
    "            \n",
    "            self.logger.info(\"=\" * 80)\n",
    "            self.logger.info(\"\")\n",
    "            \n",
    "            log_print(f\"\\n{'=' * 80}\")\n",
    "            log_print(f\"\uD83D\uDCCA SQL VALIDATION SUMMARY (REST API with Fresh Clients)\")\n",
    "            log_print(f\"{'=' * 80}\")\n",
    "            log_print(f\"   ✅ Succeeded: {stats['succeeded']} (\uD83C\uDF10 Primary: {local_count}, \uD83D\uDD04 Retry: {remote_count})\")\n",
    "            log_print(f\"   ❌ Failed: {stats['failed']}\")\n",
    "            log_print(f\"   ⚠️  Auth errors: {stats['auth_errors']}\")\n",
    "            log_print(f\"   ⏭️  Skipped: {stats['skipped']}\")\n",
    "            if local_count > 0 or remote_count > 0:\n",
    "                log_print(f\"   \uD83D\uDCA1 Using REST API validation with fresh workspace clients per call\")\n",
    "                log_print(f\"   \uD83D\uDCA1 Configuration: wait_timeout=50s, disposition=EXTERNAL_LINKS, row_limit=1\")\n",
    "            if stats['auth_errors'] > 0:\n",
    "                log_print(f\"   Note: {stats['auth_errors']} validation attempts failed due to auth errors.\")\n",
    "            log_print(f\"{'=' * 80}\\n\")\n",
    "        \n",
    "        if len(ordered_results) < total_use_cases:\n",
    "            missing = total_use_cases - len(ordered_results)\n",
    "            self.logger.warning(f\"⚠️ Missing {missing} use cases from SQL generation results\")\n",
    "        \n",
    "        return ordered_results\n",
    "\n",
    "    def _generate_sql_and_notebooks_by_domain(self, all_use_cases: list, full_schema_details: list, \n",
    "                                               unstructured_docs_markdown: str, translations: dict, \n",
    "                                               summary_dict: dict = None) -> list:\n",
    "        \"\"\"\n",
    "        Generate SQL queries domain-by-domain, creating each notebook immediately after\n",
    "        its domain's SQL generation is complete. Domains are processed in order of \n",
    "        use case count (smallest first) to enable quick testing during demos.\n",
    "        \n",
    "        Args:\n",
    "            all_use_cases: All use cases to process\n",
    "            full_schema_details: Schema details for SQL generation\n",
    "            unstructured_docs_markdown: Unstructured documentation\n",
    "            translations: Translation dictionary\n",
    "            summary_dict: Optional domain summaries\n",
    "            \n",
    "        Returns:\n",
    "            list: All use cases with SQL generated\n",
    "        \"\"\"\n",
    "        import time\n",
    "        \n",
    "        if not all_use_cases:\n",
    "            self.logger.warning(\"No use cases provided for domain-by-domain SQL generation\")\n",
    "            return []\n",
    "        \n",
    "        # === CHECK IF SQL CODE GENERATION IS DISABLED ===\n",
    "        if not self.generate_sql_code:\n",
    "            self.logger.info(\"⚠️ SQL Code generation is DISABLED - using placeholder SQL for all use cases\")\n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"⚠️ SQL CODE GENERATION DISABLED\")\n",
    "            log_print(f\"{'='*80}\")\n",
    "            log_print(f\"Notebooks will be generated with placeholder SQL.\")\n",
    "            log_print(f\"To generate SQL, set regenerate_sql:Yes in notebook and run 'Re-generate SQL' mode.\")\n",
    "            log_print(f\"{'='*80}\\n\")\n",
    "            \n",
    "            # Set placeholder SQL for all use cases\n",
    "            for uc in all_use_cases:\n",
    "                tables_involved = uc.get('Tables Involved', 'your_table')\n",
    "                first_table = tables_involved.split(',')[0].strip() if tables_involved else 'your_table'\n",
    "                placeholder_sql = (\n",
    "                    f\"-- TODO: SQL Code generation was disabled\\n\"\n",
    "                    f\"-- To generate SQL: Run 'Re-generate SQL' operation mode\\n\"\n",
    "                    f\"-- Tables Involved: {tables_involved}\\n\"\n",
    "                    f\"SELECT * FROM {first_table} LIMIT 10;\"\n",
    "                )\n",
    "                uc['SQL'] = placeholder_sql\n",
    "                uc['generated'] = 'N'\n",
    "                uc['validated'] = 'N'  # Mark as needing regeneration (regenerate_sql:Yes)\n",
    "            \n",
    "            # Still need to create notebooks - proceed to notebook assembly\n",
    "            grouped_by_domain = self._group_use_cases_by_domain_flat(all_use_cases)\n",
    "            \n",
    "            # Build domain prefix map (same logic as normal flow)\n",
    "            domain_prefix_map = {}\n",
    "            for domain, use_cases in grouped_by_domain.items():\n",
    "                if use_cases:\n",
    "                    first_id = use_cases[0].get('No', 'N99-ZZ99')\n",
    "                    try:\n",
    "                        prefix = first_id.split('-')[0]\n",
    "                        prefix_num = int(prefix[1:])\n",
    "                        domain_prefix_map[domain] = (prefix_num, prefix)\n",
    "                    except (ValueError, IndexError):\n",
    "                        domain_prefix_map[domain] = (999, 'N99')\n",
    "            \n",
    "            sorted_domains = sorted(grouped_by_domain.keys(), \n",
    "                                   key=lambda d: len(grouped_by_domain[d]))\n",
    "            \n",
    "            # Create notebooks for all domains (with placeholder SQL)\n",
    "            for domain_idx, domain_name in enumerate(sorted_domains, start=1):\n",
    "                domain_use_cases = grouped_by_domain[domain_name]\n",
    "                actual_prefix = domain_prefix_map.get(domain_name, (domain_idx, f\"N{domain_idx:02d}\"))[1]\n",
    "                sanitized_domain = self._sanitize_name(domain_name)\n",
    "                notebook_name = f\"{actual_prefix}-{sanitized_domain}\"\n",
    "                \n",
    "                log_print(f\"\uD83D\uDCD3 Creating notebook: {notebook_name} ({len(domain_use_cases)} use cases with placeholder SQL)\")\n",
    "                \n",
    "                domain_summary = summary_dict.get(domain_name) if summary_dict else None\n",
    "                sorted_cases = sorted(domain_use_cases, key=self._natural_sort_key)\n",
    "                \n",
    "                try:\n",
    "                    self._assemble_notebook_for_db(\n",
    "                        db_name=domain_name, use_cases=sorted_cases, translations=translations,\n",
    "                        db_prefix=actual_prefix, filename_override=notebook_name, domain_summary=domain_summary\n",
    "                    )\n",
    "                    log_print(f\"   ✅ {notebook_name}.ipynb created\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Failed to create notebook for domain '{domain_name}': {e}\")\n",
    "                    log_print(f\"   ❌ Failed to create notebook: {str(e)[:100]}\")\n",
    "            \n",
    "            log_print(f\"\\n✅ All notebooks created with placeholder SQL\")\n",
    "            log_print(f\"   \uD83D\uDCCC To generate SQL: Set regenerate_sql:Yes in notebooks and run 'Re-generate SQL' mode\")\n",
    "            \n",
    "            return all_use_cases\n",
    "        \n",
    "        grouped_by_domain = self._group_use_cases_by_domain_flat(all_use_cases)\n",
    "        \n",
    "        # CRITICAL FIX: Extract domain PREFIX from USE CASE IDs - this ensures notebook names match use case IDs\n",
    "        # The ID assignment phase already determined domain prefixes (N01, N02, etc.).\n",
    "        # We sort by USE CASE COUNT (smallest first for quick testing) but use PREFIX from IDs for notebook naming.\n",
    "        domain_prefix_map = {}  # domain_name -> (prefix_number, prefix_string)\n",
    "        for domain, use_cases in grouped_by_domain.items():\n",
    "            if use_cases:\n",
    "                # Extract prefix from first use case ID (e.g., \"N15-AI01\" -> \"N15\" -> 15)\n",
    "                first_id = use_cases[0].get('No', 'N99-ZZ99')\n",
    "                try:\n",
    "                    prefix = first_id.split('-')[0]  # \"N15\"\n",
    "                    prefix_num = int(prefix[1:])  # 15\n",
    "                    domain_prefix_map[domain] = (prefix_num, prefix)\n",
    "                except (ValueError, IndexError):\n",
    "                    domain_prefix_map[domain] = (999, 'N99')  # Fallback for malformed IDs\n",
    "        \n",
    "        # Sort domains by USE CASE COUNT (smallest first) - enables quick testing of smaller domains\n",
    "        # NOTE: Notebook prefix comes from use case IDs (domain_prefix_map), NOT from this sort order\n",
    "        sorted_domains = sorted(grouped_by_domain.keys(), \n",
    "                               key=lambda d: len(grouped_by_domain[d]))\n",
    "        \n",
    "        # For logging, still calculate impact scores\n",
    "        domain_impact_scores = {domain: self._calculate_domain_impact_score(use_cases) \n",
    "                               for domain, use_cases in grouped_by_domain.items()}\n",
    "        \n",
    "        total_domains = len(sorted_domains)\n",
    "        total_use_cases = len(all_use_cases)\n",
    "        \n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"\uD83C\uDFAF DOMAIN-BY-DOMAIN SQL GENERATION & NOTEBOOK CREATION\")\n",
    "        log_print(f\"{'='*80}\")\n",
    "        log_print(f\"\uD83D\uDCCA Total: {total_use_cases} use cases across {total_domains} domains\")\n",
    "        log_print(f\"\uD83D\uDCCB Processing order (smallest domains first for quick testing):\")\n",
    "        for idx, domain in enumerate(sorted_domains, 1):\n",
    "            uc_count = len(grouped_by_domain[domain])\n",
    "            prefix = domain_prefix_map.get(domain, (idx, f\"N{idx:02d}\"))[1]\n",
    "            impact = domain_impact_scores[domain]\n",
    "            log_print(f\"   {idx}. {prefix}-{domain}: {uc_count} use cases (impact: {impact:.1f})\")\n",
    "        log_print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        self.logger.info(f\"Starting domain-by-domain SQL generation for {total_use_cases} use cases across {total_domains} domains\")\n",
    "        self.logger.info(f\"Domain order (by size): {[(len(grouped_by_domain[d]), domain_prefix_map.get(d, (0, 'N00'))[1], d) for d in sorted_domains]}\")\n",
    "        \n",
    "        self.logger.info(f\"\uD83D\uDD27 Building schema index from {len(full_schema_details)} columns for fast lookup...\")\n",
    "        schema_by_table = defaultdict(list)\n",
    "        for detail in full_schema_details:\n",
    "            (catalog, schema, table, column_name, data_type, comment) = detail\n",
    "            fqtn = f\"{catalog}.{schema}.{table}\"\n",
    "            fqtn_backticks = f\"`{catalog}`.`{schema}`.`{table}`\"\n",
    "            schema_by_table[fqtn].append(detail)\n",
    "            schema_by_table[fqtn_backticks].append(detail)\n",
    "        self.logger.info(f\"   ✓ Schema index built with {len(schema_by_table)} table entries\")\n",
    "        \n",
    "        final_results_map = {}\n",
    "        notebooks_created = []\n",
    "        overall_start_time = time.time()\n",
    "        cumulative_use_cases_done = 0\n",
    "        \n",
    "        for domain_idx, domain_name in enumerate(sorted_domains, start=1):\n",
    "            domain_use_cases = grouped_by_domain[domain_name]\n",
    "            domain_uc_count = len(domain_use_cases)\n",
    "            # Get the actual prefix from use case IDs (matches notebook name)\n",
    "            actual_prefix = domain_prefix_map.get(domain_name, (domain_idx, f\"N{domain_idx:02d}\"))[1]\n",
    "            \n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"\uD83C\uDFE2 DOMAIN {domain_idx}/{total_domains}: {domain_name.upper()} (Notebook: {actual_prefix})\")\n",
    "            log_print(f\"{'='*80}\")\n",
    "            log_print(f\"   \uD83D\uDCCA Use cases in this domain: {domain_uc_count}\")\n",
    "            log_print(f\"   \uD83D\uDD04 Progress: {cumulative_use_cases_done}/{total_use_cases} use cases completed so far\")\n",
    "            \n",
    "            domain_start_time = time.time()\n",
    "            self.logger.info(f\"\\n\uD83C\uDFE2 [{domain_idx}/{total_domains}] Starting domain: {domain_name} ({actual_prefix}, {domain_uc_count} use cases)\")\n",
    "            \n",
    "            log_print(f\"\\n   \uD83D\uDCDD PHASE 1: Generating SQL for {domain_uc_count} use cases (wave pattern)...\")\n",
    "            \n",
    "            # ADAPTIVE PARALLELISM: Calculate based on domain use cases and schema size\n",
    "            sql_parallelism, reason = calculate_adaptive_parallelism(\n",
    "                \"sql_generation\", self.max_parallelism,\n",
    "                num_items=domain_uc_count,\n",
    "                total_columns=len(full_schema_details),\n",
    "                avg_prompt_chars=len(full_schema_details) * 50,\n",
    "                is_llm_operation=True, logger=self.logger\n",
    "            )\n",
    "            log_adaptive_parallelism_decision(\"sql_generation\", sql_parallelism, self.max_parallelism, reason)\n",
    "            \n",
    "            priority_order = {\n",
    "                \"ultra high\": 0, \"very high\": 1, \"high\": 2, \"medium\": 3,\n",
    "                \"low\": 4, \"very low\": 5, \"ultra low\": 6\n",
    "            }\n",
    "            \n",
    "            def sort_backlog(items):\n",
    "                return [uc for _, uc in sorted(\n",
    "                    enumerate(items),\n",
    "                    key=lambda pair: (priority_order.get(str(pair[1].get('Priority', '')).strip().lower(), len(priority_order)), pair[0])\n",
    "                )]\n",
    "            \n",
    "            wave_parallelism = [\n",
    "                (1, sql_parallelism),\n",
    "                (2, sql_parallelism),\n",
    "                (3, max(1, (sql_parallelism + 1) // 2)),\n",
    "                (4, max(1, (sql_parallelism + 2) // 3)),\n",
    "                (5, max(1, (sql_parallelism + 2) // 3))\n",
    "            ]\n",
    "            \n",
    "            domain_final_results = {}\n",
    "            backlog = sort_backlog(domain_use_cases)\n",
    "            \n",
    "            for wave_id, wave_workers in wave_parallelism:\n",
    "                if not backlog:\n",
    "                    break\n",
    "                backlog = sort_backlog(backlog)\n",
    "                wave_start_time = time.time()\n",
    "                \n",
    "                self.logger.info(f\"   \uD83D\uDD01 Wave {wave_id}: processing {len(backlog)} use cases with parallelism {wave_workers}\")\n",
    "                log_print(f\"      ▶️ Wave {wave_id}: {len(backlog)} use cases, parallelism {wave_workers}\")\n",
    "                \n",
    "                results, timed_out, validation_failed = self._run_sql_wave(\n",
    "                    wave_id, backlog, full_schema_details, unstructured_docs_markdown, schema_by_table, wave_workers\n",
    "                )\n",
    "                \n",
    "                wave_end_time = time.time()\n",
    "                wave_duration = wave_end_time - wave_start_time\n",
    "                \n",
    "                wave_succeeded = sum(1 for uc in results if uc.get('generated') == 'Y' and uc.get('validated') in ['Y', 'D'])\n",
    "                wave_failed = len(results) - wave_succeeded\n",
    "                \n",
    "                log_print(f\"      ✅ Wave {wave_id} done in {wave_duration:.1f}s: {wave_succeeded} OK, {wave_failed} Failed\")\n",
    "                \n",
    "                for uc in results:\n",
    "                    domain_final_results[uc.get('No', 'UNKNOWN')] = uc\n",
    "                \n",
    "                for uc in validation_failed:\n",
    "                    if uc.get('_needs_fix'):\n",
    "                        del uc['_needs_fix']\n",
    "                    else:\n",
    "                        uc['_needs_fix'] = True\n",
    "                \n",
    "                for uc in timed_out:\n",
    "                    if uc.get('sql_generation_status') == 'timeout':\n",
    "                        uc['sql_generation_status'] = 'pending_retry'\n",
    "                        uc['_retry_attempt'] = uc.get('_retry_attempt', 0) + 1\n",
    "                \n",
    "                retry_items = validation_failed + timed_out\n",
    "                backlog = sort_backlog(retry_items)\n",
    "            \n",
    "            for uc in backlog:\n",
    "                if uc.get('sql_generation_status') == 'timeout':\n",
    "                    uc['SQL'] = (\n",
    "                        f\"-- SQL generation timed out for {uc.get('No', 'UNKNOWN')}\\n\"\n",
    "                        f\"-- Timeout: {self.llm_timeout_seconds} seconds (final attempt)\\n\"\n",
    "                        f\"SELECT 'SQL Generation Timeout' as error;\"\n",
    "                    )\n",
    "                elif uc.get('sql_validation_status') == 'failed' or uc.get('column_validation_status') == 'failed':\n",
    "                    if not uc.get('SQL'):\n",
    "                        uc['SQL'] = f\"-- SQL validation failed for {uc.get('No', 'UNKNOWN')}\\nSELECT 'SQL Validation Failed' as error;\"\n",
    "                domain_final_results[uc.get('No', 'UNKNOWN')] = uc\n",
    "            \n",
    "            domain_ordered_results = []\n",
    "            for uc in domain_use_cases:\n",
    "                domain_ordered_results.append(domain_final_results.get(uc.get('No', 'UNKNOWN'), uc))\n",
    "            \n",
    "            for uc in domain_ordered_results:\n",
    "                final_results_map[uc.get('No', 'UNKNOWN')] = uc\n",
    "            \n",
    "            domain_success = sum(1 for uc in domain_ordered_results \n",
    "                               if uc.get('sql_generation_status') == 'succeeded' \n",
    "                               and uc.get('sql_validation_status') != 'failed' \n",
    "                               and uc.get('column_validation_status') != 'failed')\n",
    "            domain_failed = len(domain_ordered_results) - domain_success\n",
    "            \n",
    "            sql_duration = time.time() - domain_start_time\n",
    "            log_print(f\"\\n   ✅ SQL Generation Complete: {domain_success} succeeded, {domain_failed} failed ({sql_duration:.1f}s)\")\n",
    "            \n",
    "            log_print(f\"\\n   \uD83D\uDCD3 PHASE 2: Creating notebook for domain '{domain_name}'...\")\n",
    "            \n",
    "            notebook_start_time = time.time()\n",
    "            sorted_cases = sorted(domain_ordered_results, key=self._natural_sort_key)\n",
    "            # CRITICAL FIX: Use prefix from use case IDs, not from loop index\n",
    "            # This ensures N15-AI01 use cases go into N15-xxx.ipynb, not N06-xxx.ipynb\n",
    "            domain_prefix = domain_prefix_map.get(domain_name, (domain_idx, f\"N{domain_idx:02d}\"))[1]\n",
    "            notebook_name = f\"{domain_prefix}-{self._sanitize_name(domain_name)}\"\n",
    "            \n",
    "            domain_summary = None\n",
    "            if summary_dict:\n",
    "                domain_summary = summary_dict.get(domain_name, None)\n",
    "            \n",
    "            try:\n",
    "                self._assemble_notebook_for_db(\n",
    "                    db_name=domain_name, use_cases=sorted_cases, translations=translations,\n",
    "                    db_prefix=domain_prefix, filename_override=notebook_name, domain_summary=domain_summary\n",
    "                )\n",
    "                notebooks_created.append((domain_idx, notebook_name, True))\n",
    "                notebook_duration = time.time() - notebook_start_time\n",
    "                \n",
    "                log_print(f\"\\n{'*'*80}\")\n",
    "                log_print(f\"\uD83C\uDF89 DOMAIN '{domain_name.upper()}' COMPLETE!\")\n",
    "                log_print(f\"{'*'*80}\")\n",
    "                log_print(f\"   \uD83D\uDCD3 Notebook: {notebook_name}.ipynb\")\n",
    "                log_print(f\"   \uD83D\uDCCA Use cases: {domain_uc_count} ({domain_success} SQL OK, {domain_failed} SQL Failed)\")\n",
    "                total_domain_time = time.time() - domain_start_time\n",
    "                log_print(f\"   ⏱️  Total time: {total_domain_time:.1f}s (SQL: {sql_duration:.1f}s, Notebook: {notebook_duration:.1f}s)\")\n",
    "                log_print(f\"   ✅ READY FOR TESTING!\")\n",
    "                log_print(f\"{'*'*80}\\n\")\n",
    "                \n",
    "                self.logger.info(f\"\uD83C\uDF89 [{domain_idx}/{total_domains}] Domain '{domain_name}' notebook '{notebook_name}.ipynb' READY FOR INSPECTION\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                notebooks_created.append((domain_idx, notebook_name, False))\n",
    "                self.logger.error(f\"❌ [{domain_idx}/{total_domains}] Failed to create notebook for domain '{domain_name}': {e}\")\n",
    "                log_print(f\"   ❌ Notebook creation failed: {str(e)[:100]}\")\n",
    "            \n",
    "            cumulative_use_cases_done += domain_uc_count\n",
    "            \n",
    "            remaining_domains = total_domains - domain_idx\n",
    "            if remaining_domains > 0:\n",
    "                avg_time_per_uc = (time.time() - overall_start_time) / cumulative_use_cases_done if cumulative_use_cases_done > 0 else 0\n",
    "                remaining_ucs = total_use_cases - cumulative_use_cases_done\n",
    "                eta_seconds = avg_time_per_uc * remaining_ucs\n",
    "                log_print(f\"   \uD83D\uDCC8 Progress: {cumulative_use_cases_done}/{total_use_cases} use cases ({cumulative_use_cases_done*100//total_use_cases}%)\")\n",
    "                log_print(f\"   ⏳ Estimated time remaining: {eta_seconds/60:.1f} minutes ({remaining_domains} domains left)\")\n",
    "        \n",
    "        overall_duration = time.time() - overall_start_time\n",
    "        \n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"\uD83C\uDFC1 ALL DOMAINS PROCESSED\")\n",
    "        log_print(f\"{'='*80}\")\n",
    "        log_print(f\"   \uD83D\uDCCA Total use cases: {total_use_cases}\")\n",
    "        log_print(f\"   \uD83D\uDCD3 Notebooks created: {sum(1 for _, _, success in notebooks_created if success)}/{total_domains}\")\n",
    "        log_print(f\"   ⏱️  Total time: {overall_duration/60:.1f} minutes\")\n",
    "        log_print(f\"\\n   \uD83D\uDCD3 Notebooks ready for testing:\")\n",
    "        for idx, name, success in notebooks_created:\n",
    "            status = \"✅\" if success else \"❌\"\n",
    "            log_print(f\"      {status} {name}.ipynb\")\n",
    "        log_print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        self.logger.info(f\"✅ Domain-by-domain SQL generation complete: {total_use_cases} use cases, {total_domains} notebooks in {overall_duration:.1f}s\")\n",
    "        \n",
    "        ordered_results = []\n",
    "        for uc in all_use_cases:\n",
    "            ordered_results.append(final_results_map.get(uc.get('No', 'UNKNOWN'), uc))\n",
    "        \n",
    "        return ordered_results\n",
    "\n",
    "    def _deduplicate_use_cases(self, all_use_cases: list) -> list:\n",
    "        \"\"\"\n",
    "        Calls an LLM to perform AGGRESSIVE global deduplication on ALL use cases.\n",
    "        \n",
    "        \uD83D\uDEA8 ENHANCED: Now includes Business Value assessment to filter out low-value use cases.\n",
    "        \n",
    "        Deduplication criteria:\n",
    "        1. Semantic similarity of Names\n",
    "        2. Duplicate or trivial use cases\n",
    "        3. Low business value relative to industry/business context\n",
    "        4. Use cases with insufficient distinctiveness\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting AGGRESSIVE global deduplication for {len(all_use_cases)} use cases...\")\n",
    "        self.logger.info(f\"Deduplication will analyze: Name similarity + Business Value + Distinctiveness\")\n",
    "        \n",
    "        if len(all_use_cases) < 2:\n",
    "            self.logger.debug(\"Skipping deduplication, not enough use cases to compare.\")\n",
    "            return all_use_cases\n",
    "        \n",
    "        try:\n",
    "            # Create markdown table with ID, Name, and Business Value\n",
    "            md_parts = [\"| ID | Name | Business Value | Tables |\\n|---|---|---|---|\\n\"]\n",
    "            for uc in all_use_cases:\n",
    "                name = str(uc.get('Name', '')).replace('|', r'\\|')\n",
    "                business_value = str(uc.get('Business Value', ''))[:100].replace('|', r'\\|')\n",
    "                tables = str(uc.get('Tables Involved', ''))[:50].replace('|', r'\\|')\n",
    "                md_parts.append(f\"| {uc['No']} | {name} | {business_value} | {tables} |\\n\")\n",
    "            use_case_markdown = \"\".join(md_parts)\n",
    "            \n",
    "            self.logger.debug(f\"Created deduplication markdown table with {len(all_use_cases)} use cases\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create markdown for deduplication: {e}\")\n",
    "            return all_use_cases\n",
    "        \n",
    "        try:\n",
    "            # Check if the use_case_markdown might exceed context limits (using model-specific limits)\n",
    "            review_context_limit = get_max_context_chars(\"English\", \"REVIEW_USE_CASES_PROMPT\")\n",
    "            markdown_size = len(use_case_markdown)\n",
    "            prompt_template = self.ai_agent.prompt_templates.get(\"REVIEW_USE_CASES_PROMPT\", \"\")\n",
    "            estimated_prompt_size = len(prompt_template) + markdown_size + 1000  # +1000 for other vars\n",
    "            \n",
    "            if estimated_prompt_size > review_context_limit:\n",
    "                self.logger.warning(\n",
    "                    f\"Deduplication prompt size ({estimated_prompt_size:,} chars) exceeds model limit ({review_context_limit:,}). \"\n",
    "                    f\"Falling back to domain-level parallel deduplication...\"\n",
    "                )\n",
    "                return self._deduplicate_use_cases_by_domain_parallel(all_use_cases)\n",
    "            \n",
    "            prompt_vars = {\n",
    "                \"use_case_markdown\": use_case_markdown,\n",
    "                \"total_count\": len(all_use_cases)\n",
    "            }\n",
    "            \n",
    "            self.logger.info(f\"⏳ Waiting for LLM response (deduplicating {len(all_use_cases)} use cases)...\")\n",
    "            \n",
    "            response_raw = self.ai_agent.run_worker(\n",
    "                step_name=\"Deduplicate_Use_Cases\",\n",
    "                worker_prompt_path=\"REVIEW_USE_CASES_PROMPT\",\n",
    "                prompt_vars=prompt_vars,\n",
    "                response_schema=None\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f\"✅ Received LLM response, parsing deduplication results...\")\n",
    "            \n",
    "            # Clean response (remove markdown fences if present)\n",
    "            response_clean = clean_json_response(response_raw)\n",
    "            \n",
    "            # Parse CSV\n",
    "            try:\n",
    "                # Parse CSV using centralized utility\n",
    "                csv_rows = CSVParser.parse_csv_string(\n",
    "                    response_clean,\n",
    "                    logger=self.logger,\n",
    "                    context=\"Deduplication\"\n",
    "                )\n",
    "                ids_to_keep = []\n",
    "                \n",
    "                for row in csv_rows:\n",
    "                    # Handle column name\n",
    "                    uc_id = row.get('use_case_id', '').strip()\n",
    "                    if uc_id:\n",
    "                        ids_to_keep.append(uc_id)\n",
    "                \n",
    "                if not ids_to_keep:\n",
    "                    raise ValueError(\"CSV contains no use case IDs\")\n",
    "                \n",
    "                ids_to_keep_set = set(ids_to_keep)\n",
    "                    \n",
    "            except Exception as csv_err:\n",
    "                self.logger.error(f\"CSV parsing failed: {csv_err}. Raw response (first 500 chars): {response_raw[:500]}\")\n",
    "                raise\n",
    "            \n",
    "            # \uD83D\uDEA8 NEW: AGGRESSIVE COVERAGE - Ensure at least 1 use case per business table\n",
    "            # Log which use cases were removed\n",
    "            removed_count = len(all_use_cases) - len(ids_to_keep_set)\n",
    "            removal_pct = (removed_count / len(all_use_cases)) * 100 if all_use_cases else 0\n",
    "            \n",
    "            self.logger.info(f\"Deduplication complete: Retained {len(ids_to_keep_set)} use cases, removed {removed_count} ({removal_pct:.1f}%)\")\n",
    "            \n",
    "            # Log count of removed use cases per domain\n",
    "            if removed_count > 0:\n",
    "                removed_use_cases = [uc for uc in all_use_cases if uc['No'] not in ids_to_keep_set]\n",
    "                \n",
    "                # Group removed use cases by domain\n",
    "                from collections import defaultdict\n",
    "                domain_removal_counts = defaultdict(int)\n",
    "                for uc in removed_use_cases:\n",
    "                    domain = uc.get('Business Domain', 'Unknown')\n",
    "                    domain_removal_counts[domain] += 1\n",
    "                \n",
    "                # Log counts per domain\n",
    "                self.logger.info(f\"Removed use cases by domain:\")\n",
    "                for domain in sorted(domain_removal_counts.keys()):\n",
    "                    count = domain_removal_counts[domain]\n",
    "                    self.logger.info(f\"  - {domain}: {count} use case(s) removed\")\n",
    "            \n",
    "            unique_use_cases = [uc for uc in all_use_cases if uc['No'] in ids_to_keep_set]\n",
    "            return unique_use_cases\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Global use case deduplication failed: {e}. Proceeding with the full list of use cases.\")\n",
    "            return all_use_cases\n",
    "\n",
    "    def _deduplicate_use_cases_by_domain_parallel(self, all_use_cases: list) -> list:\n",
    "        \"\"\"\n",
    "        Deduplicate use cases at domain level in parallel using scores for intelligent selection.\n",
    "        \n",
    "        Args:\n",
    "            all_use_cases: List of all use case dictionaries (should be scored first)\n",
    "            \n",
    "        Returns:\n",
    "            List of deduplicated use cases\n",
    "        \"\"\"\n",
    "        from collections import defaultdict\n",
    "        from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "        import concurrent.futures\n",
    "        import time\n",
    "        \n",
    "        self.logger.info(f\"\uD83D\uDD04 Starting intelligent domain-level deduplication for {len(all_use_cases)} scored use cases...\")\n",
    "        \n",
    "        # Group use cases by domain\n",
    "        domain_use_cases = defaultdict(list)\n",
    "        for uc in all_use_cases:\n",
    "            domain = uc.get('Business Domain', 'Unknown')\n",
    "            domain_use_cases[domain].append(uc)\n",
    "        \n",
    "        self.logger.info(f\"\uD83D\uDCCA Grouped use cases into {len(domain_use_cases)} domains\")\n",
    "        \n",
    "        # Deduplicate each domain in parallel\n",
    "        deduplicated_results = []\n",
    "        \n",
    "        def dedupe_domain(domain_name, domain_ucs):\n",
    "            \"\"\"Deduplicate a single domain's use cases.\"\"\"\n",
    "            try:\n",
    "                self.logger.info(f\"[{domain_name}] Deduplicating {len(domain_ucs)} use cases...\")\n",
    "                \n",
    "                # Create markdown table for this domain including scores\n",
    "                md_parts = [\"| ID | Name | Business Value | Tables | ROI | Strat. Align |\\n|---|---|---|---|---|---|\\n\"]\n",
    "                for uc in domain_ucs:\n",
    "                    name = str(uc.get('Name', '')).replace('|', r'\\|')\n",
    "                    business_value = str(uc.get('Business Value', ''))[:100].replace('|', r'\\|')\n",
    "                    tables = str(uc.get('Tables Involved', ''))[:50].replace('|', r'\\|')\n",
    "                    roi = str(uc.get('Return on Investment', 'N/A'))\n",
    "                    strat_align = str(uc.get('Strategic Alignment', 'N/A'))\n",
    "                    \n",
    "                    md_parts.append(f\"| {uc['No']} | {name} | {business_value} | {tables} | {roi} | {strat_align} |\\n\")\n",
    "                use_case_markdown = \"\".join(md_parts)\n",
    "                \n",
    "                # Check size (using model-specific limits from TECHNICAL_CONTEXT)\n",
    "                review_context_limit = get_max_context_chars(\"English\", \"REVIEW_USE_CASES_PROMPT\")\n",
    "                prompt_template = self.ai_agent.prompt_templates.get(\"REVIEW_USE_CASES_PROMPT\", \"\")\n",
    "                estimated_size = len(prompt_template) + len(use_case_markdown) + 1000\n",
    "                \n",
    "                if estimated_size > review_context_limit:\n",
    "                    self.logger.warning(f\"[{domain_name}] Domain still too large ({estimated_size:,} chars). Keeping all {len(domain_ucs)} use cases without deduplication.\")\n",
    "                    return domain_ucs\n",
    "                \n",
    "                # Append explicit instructions about score-based selection to the markdown context\n",
    "                context_notes = \"\"\"\n",
    "                **CRITICAL DEDUPLICATION RULES**:\n",
    "                1. If two use cases are DUPLICATES (same intent/logic):\n",
    "                   - Keep the one with higher 'ROI' and 'Strat. Align'.\n",
    "                   - If scores are similar, keep the one with better detail.\n",
    "                2. If two use cases use IDENTICAL tables and have similar logic -> Treat as DUPLICATE.\n",
    "                3. If two use cases are similar but use DIFFERENT tables -> KEEP BOTH.\n",
    "                   - In this case, mark them as distinct.\n",
    "                   - Ensure they have IDENTICAL scores if logic is same.\n",
    "                   - Add note to Justification: \"Very Similar to [Other_ID]\"\n",
    "                \n",
    "                **HIGH PRIORITY**:\n",
    "                - Pay special attention to complex, high-value use cases that involve \"Root Cause Analysis\", \"Predictive\", \"Optimization\", \"Anomaly Detection\", or \"Forecasting\".\n",
    "                - DO NOT REMOVE high-value use cases unless they are EXACT duplicates.\n",
    "                \"\"\"\n",
    "                \n",
    "                prompt_vars = {\n",
    "                    \"use_case_markdown\": use_case_markdown + \"\\n\" + context_notes,\n",
    "                    \"total_count\": len(domain_ucs)\n",
    "                }\n",
    "                \n",
    "                self.logger.info(f\"⏳ [{domain_name}] Waiting for LLM response...\")\n",
    "                \n",
    "                response_raw = self.ai_agent.run_worker(\n",
    "                    step_name=f\"Deduplicate_Domain_{domain_name}\",\n",
    "                    worker_prompt_path=\"REVIEW_USE_CASES_PROMPT\",\n",
    "                    prompt_vars=prompt_vars,\n",
    "                    response_schema=None\n",
    "                )\n",
    "                \n",
    "                # Parse response\n",
    "                response_clean = clean_json_response(response_raw)\n",
    "                \n",
    "                # Parse CSV using centralized utility\n",
    "                csv_rows = CSVParser.parse_csv_string(\n",
    "                    response_clean,\n",
    "                    logger=self.logger,\n",
    "                    context=f\"Domain deduplication for {domain_name}\"\n",
    "                )\n",
    "                ids_to_keep = set()\n",
    "                \n",
    "                for row in csv_rows:\n",
    "                    uc_id = row.get('use_case_id', '').strip()\n",
    "                    if uc_id:\n",
    "                        ids_to_keep.add(uc_id)\n",
    "                \n",
    "                if not ids_to_keep:\n",
    "                    self.logger.warning(f\"[{domain_name}] No IDs returned. Keeping all use cases.\")\n",
    "                    return domain_ucs\n",
    "                \n",
    "                deduplicated = [uc for uc in domain_ucs if uc['No'] in ids_to_keep]\n",
    "                \n",
    "                # Logic to handle the \"Same Logic, Different Tables\" case (sync scores & justification)\n",
    "                # This requires parsing the FULL response if the LLM provided metadata, \n",
    "                # but currently REVIEW_USE_CASES_PROMPT typically returns just a list of IDs.\n",
    "                # Since we can't easily sync scores without the LLM telling us which pairs match,\n",
    "                # we rely on the LLM's selection in the ID list for now.\n",
    "                \n",
    "                removed = len(domain_ucs) - len(deduplicated)\n",
    "                \n",
    "                self.logger.info(f\"✅ [{domain_name}] Retained {len(deduplicated)} use cases, removed {removed}\")\n",
    "                \n",
    "                return deduplicated\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"[{domain_name}] Deduplication failed: {e}. Keeping all {len(domain_ucs)} use cases.\")\n",
    "                return domain_ucs\n",
    "        \n",
    "        # ADAPTIVE PARALLELISM: Calculate based on domains and total use cases\n",
    "        total_use_cases = sum(len(ucs) for ucs in domain_use_cases.values())\n",
    "        num_domains = len(domain_use_cases)\n",
    "        \n",
    "        dedup_parallelism, reason = calculate_adaptive_parallelism(\n",
    "            \"deduplication\", self.max_parallelism,\n",
    "            num_items=total_use_cases,\n",
    "            num_domains=num_domains,\n",
    "            is_llm_operation=True, logger=self.logger\n",
    "        )\n",
    "        \n",
    "        # Calculate timeout: 5 minutes per domain + 5 minutes buffer\n",
    "        overall_timeout = (num_domains * 300 // dedup_parallelism) + 300\n",
    "        \n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"\uD83D\uDD04 DEDUPLICATION: Processing {num_domains} domains ({total_use_cases} total use cases)\")\n",
    "        log_print(f\"{'='*80}\")\n",
    "        log_adaptive_parallelism_decision(\"deduplication\", dedup_parallelism, self.max_parallelism, reason)\n",
    "        log_print(f\"Overall timeout: {overall_timeout}s ({overall_timeout//60} min)\")\n",
    "        log_print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=dedup_parallelism, thread_name_prefix=\"DomainDedupe\") as executor:\n",
    "            future_to_domain = {}\n",
    "            for domain, domain_ucs in domain_use_cases.items():\n",
    "                future = executor.submit(dedupe_domain, domain, domain_ucs)\n",
    "                future_to_domain[future] = domain\n",
    "            \n",
    "            completed_count = 0\n",
    "            completed_domains = set()\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                for future in as_completed(future_to_domain, timeout=overall_timeout):\n",
    "                    domain = future_to_domain[future]\n",
    "                    elapsed = time.time() - start_time\n",
    "                    try:\n",
    "                        domain_deduplicated = future.result(timeout=30)\n",
    "                        deduplicated_results.extend(domain_deduplicated)\n",
    "                        completed_count += 1\n",
    "                        completed_domains.add(domain)\n",
    "                        log_print(f\"[Deduplication] ✓ Domain {completed_count}/{len(domain_use_cases)} complete: {domain} ({elapsed:.1f}s elapsed)\")\n",
    "                    except concurrent.futures.TimeoutError:\n",
    "                        self.logger.error(f\"[{domain}] Result collection timed out - keeping original use cases\")\n",
    "                        deduplicated_results.extend(domain_use_cases.get(domain, []))\n",
    "                        completed_domains.add(domain)\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"[{domain}] Failed to collect results: {e} - keeping original use cases\")\n",
    "                        deduplicated_results.extend(domain_use_cases.get(domain, []))\n",
    "                        completed_domains.add(domain)\n",
    "            except concurrent.futures.TimeoutError:\n",
    "                self.logger.error(f\"⚠️  Overall deduplication timeout reached ({overall_timeout}s). {completed_count}/{len(domain_use_cases)} domains completed.\")\n",
    "                log_print(f\"[Deduplication] ⚠️  TIMEOUT - keeping original use cases for incomplete domains\", level=\"WARNING\")\n",
    "                for domain, domain_ucs in domain_use_cases.items():\n",
    "                    if domain not in completed_domains:\n",
    "                        self.logger.warning(f\"[{domain}] Timed out - keeping all {len(domain_ucs)} original use cases\")\n",
    "                        deduplicated_results.extend(domain_ucs)\n",
    "        \n",
    "        total_removed = len(all_use_cases) - len(deduplicated_results)\n",
    "        removal_pct = (total_removed / len(all_use_cases)) * 100 if all_use_cases else 0\n",
    "        \n",
    "        self.logger.info(f\"✅ Domain-level deduplication complete: Retained {len(deduplicated_results)} use cases, removed {total_removed} ({removal_pct:.1f}%)\")\n",
    "        \n",
    "        return deduplicated_results\n",
    "\n",
    "    def _score_use_cases_global(self, all_use_cases: list, business_context: str = \"\",\n",
    "                                strategic_goals: list = None, business_priorities: list = None,\n",
    "                                strategic_initiative: str = \"\", value_chain: str = \"\",\n",
    "                                revenue_model: str = \"\") -> list:\n",
    "        \"\"\"\n",
    "        Try to score ALL use cases together in one prompt using minimal context (ID, Name, Statement, Business Value).\n",
    "        Returns None on failure so callers can fall back to domain-based scoring.\n",
    "        \"\"\"\n",
    "        if not all_use_cases:\n",
    "            return all_use_cases\n",
    "\n",
    "        try:\n",
    "            md_parts = [\"| No | Name | Statement | Business Value |\\n|---|---|---|---|\\n\"]\n",
    "            pipe_escape = r'\\|'\n",
    "            for uc in all_use_cases:\n",
    "                no_val = str(uc.get('No', '')).replace('|', pipe_escape)\n",
    "                name_val = str(uc.get('Name', '')).replace('|', pipe_escape)\n",
    "                stmt_val = str(uc.get('Statement', '')).replace('|', pipe_escape)\n",
    "                bv_val = str(uc.get('Business Value', '')).replace('|', pipe_escape)\n",
    "                md_parts.append(f\"| {no_val} | {name_val} | {stmt_val} | {bv_val} |\\n\")\n",
    "            use_case_markdown = \"\".join(md_parts)\n",
    "\n",
    "            prompt_vars = {\n",
    "                \"use_case_markdown\": use_case_markdown,\n",
    "                \"business_context\": business_context or \"General business operations\",\n",
    "                \"strategic_goals\": \"\\n\".join([f\"- {goal}\" for goal in (strategic_goals or [])]) or \"- Improve operational efficiency\",\n",
    "                \"business_priorities\": \"\\n\".join([f\"- {priority}\" for priority in (business_priorities or [])]) or \"- Optimize costs\",\n",
    "                \"strategic_initiative\": strategic_initiative or \"Data-driven transformation program\",\n",
    "                \"value_chain\": value_chain or \"Standard operations\",\n",
    "                \"revenue_model\": revenue_model or \"Products and services\"\n",
    "            }\n",
    "\n",
    "            self.logger.info(f\"✅ Attempting GLOBAL scoring for {len(all_use_cases)} use cases (minimal fields to fit context)\")\n",
    "            response_raw = self.ai_agent.run_worker(\n",
    "                step_name=\"Score_All_Use_Cases_Global\",\n",
    "                worker_prompt_path=\"SCORE_USE_CASES_PROMPT\",\n",
    "                prompt_vars=prompt_vars,\n",
    "                response_schema=None\n",
    "            )\n",
    "\n",
    "            response_clean = clean_json_response(response_raw)\n",
    "            scoring_data = CSVParser.parse_csv_string(\n",
    "                response_clean,\n",
    "                logger=self.logger,\n",
    "                context=\"GLOBAL scoring\"\n",
    "            )\n",
    "\n",
    "            scoring_by_no = {s.get('No'): s for s in scoring_data}\n",
    "            scored_use_cases = []\n",
    "            for uc in all_use_cases:\n",
    "                no = uc.get('No')\n",
    "                scores = scoring_by_no.get(no, {})\n",
    "                uc_copy = uc.copy()\n",
    "                for key, value in scores.items():\n",
    "                    if key != 'No':\n",
    "                        uc_copy[key] = value\n",
    "\n",
    "                try:\n",
    "                    strategic_alignment = float(uc_copy.get('Strategic Alignment', 3.5))\n",
    "                    roi = float(uc_copy.get('Return on Investment', 3.5))\n",
    "                    reusability = float(uc_copy.get('Reusability', 3.5))\n",
    "                    time_to_value = float(uc_copy.get('Time to Value', 3.5))\n",
    "\n",
    "                    data_availability = float(uc_copy.get('Data Availability', 3.5))\n",
    "                    data_accessibility = float(uc_copy.get('Data Accessibility', 3.5))\n",
    "                    architecture_fitness = float(uc_copy.get('Architecture Fitness', 3.5))\n",
    "                    team_skills = float(uc_copy.get('Team Skills', 3.5))\n",
    "                    domain_knowledge = float(uc_copy.get('Domain Knowledge', 3.5))\n",
    "                    people_allocation = float(uc_copy.get('People Allocation', 3.5))\n",
    "                    budget_allocation = float(uc_copy.get('Budget Allocation', 3.5))\n",
    "                    time_to_production = float(uc_copy.get('Time to Production', 3.5))\n",
    "\n",
    "                    value_score = (\n",
    "                        (roi * 0.60)\n",
    "                        + (strategic_alignment * 0.25)\n",
    "                        + (time_to_value * 0.075)\n",
    "                        + (reusability * 0.075)\n",
    "                    )\n",
    "\n",
    "                    feasibility_inputs = [\n",
    "                        data_availability,\n",
    "                        data_accessibility,\n",
    "                        architecture_fitness,\n",
    "                        team_skills,\n",
    "                        domain_knowledge,\n",
    "                        people_allocation,\n",
    "                        budget_allocation,\n",
    "                        time_to_production,\n",
    "                    ]\n",
    "                    feasibility_score = sum(feasibility_inputs) / len(feasibility_inputs)\n",
    "\n",
    "                    priority_score = (value_score * 1.5) + (feasibility_score * 0.5)\n",
    "\n",
    "                    uc_copy['Value'] = round(value_score, 2)\n",
    "                    uc_copy['Feasibility'] = round(feasibility_score, 2)\n",
    "                    uc_copy['Priority Score'] = round(priority_score, 2)\n",
    "                    \n",
    "                    # Ensure AI_Confidence and AI_Feedback are set with defaults if missing\n",
    "                    if 'AI_Confidence' not in uc_copy or not uc_copy.get('AI_Confidence'):\n",
    "                        uc_copy['AI_Confidence'] = 0.5\n",
    "                    if 'AI_Feedback' not in uc_copy or not uc_copy.get('AI_Feedback'):\n",
    "                        uc_copy['AI_Feedback'] = 'No feedback provided by AI scoring.'\n",
    "\n",
    "                    if priority_score >= 9.5:\n",
    "                        priority_label = \"Ultra High\"\n",
    "                    elif priority_score >= 8.5:\n",
    "                        priority_label = \"Very High\"\n",
    "                    elif priority_score >= 7.5:\n",
    "                        priority_label = \"High\"\n",
    "                    elif priority_score >= 5.5:\n",
    "                        priority_label = \"Medium\"\n",
    "                    elif priority_score >= 4.5:\n",
    "                        priority_label = \"Low\"\n",
    "                    elif priority_score >= 2.5:\n",
    "                        priority_label = \"Very Low\"\n",
    "                    else:\n",
    "                        priority_label = \"Ultra Low\"\n",
    "                    uc_copy['Priority'] = priority_label\n",
    "                except Exception:\n",
    "                    uc_copy['Priority'] = uc_copy.get('Priority', 'Medium')\n",
    "                    uc_copy['AI_Confidence'] = uc_copy.get('AI_Confidence', 0.5)\n",
    "                    uc_copy['AI_Feedback'] = uc_copy.get('AI_Feedback', 'Scoring exception occurred.')\n",
    "                scored_use_cases.append(uc_copy)\n",
    "\n",
    "            self.logger.info(f\"✅ GLOBAL scoring succeeded for {len(scored_use_cases)} use cases\")\n",
    "            return scored_use_cases\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Global scoring failed, will fall back to domain-based scoring: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _score_per_domain_parallel(self, all_use_cases: list, business_context: str = \"\",\n",
    "                                   strategic_goals: list = None, business_priorities: list = None,\n",
    "                                   strategic_initiative: str = \"\", value_chain: str = \"\",\n",
    "                                   revenue_model: str = \"\") -> list:\n",
    "        \"\"\"\n",
    "        Score use cases per domain in parallel (Phase 1).\n",
    "        \n",
    "        Each domain is scored in its own thread, all domains run in parallel.\n",
    "        After this, ALL scored use cases are returned for SQL generation.\n",
    "        \n",
    "        STABILITY FIX: Uses adaptive parallelism to prevent LLM API rate limiting \n",
    "        and adds heartbeat + total timeout to prevent hangs.\n",
    "        \"\"\"\n",
    "        import time\n",
    "        from collections import defaultdict\n",
    "        from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError as FuturesTimeoutError\n",
    "        \n",
    "        # Group use cases by domain first to calculate adaptive parallelism\n",
    "        domain_groups = defaultdict(list)\n",
    "        for uc in all_use_cases:\n",
    "            domain = uc.get('Business Domain', 'Other')\n",
    "            domain_groups[domain].append(uc)\n",
    "        \n",
    "        num_use_cases = len(all_use_cases)\n",
    "        num_domains = len(domain_groups)\n",
    "        \n",
    "        # ADAPTIVE PARALLELISM: Calculate based on use cases and domains\n",
    "        scoring_parallelism, reason = calculate_adaptive_parallelism(\n",
    "            \"scoring\", self.max_parallelism,\n",
    "            num_items=num_use_cases,\n",
    "            num_domains=num_domains,\n",
    "            is_llm_operation=True, logger=self.logger\n",
    "        )\n",
    "        \n",
    "        # Dynamic timeouts based on workload\n",
    "        # More use cases = more time needed\n",
    "        base_timeout_per_uc = 30  # seconds per use case\n",
    "        TOTAL_SCORING_TIMEOUT = max(1800, min(3600, num_use_cases * base_timeout_per_uc))  # 30-60 min\n",
    "        HEARTBEAT_INTERVAL = 60  # Log progress every 60 seconds\n",
    "        PER_DOMAIN_TIMEOUT = max(600, min(1200, (num_use_cases // num_domains) * 60))  # 10-20 min per domain\n",
    "        \n",
    "        self.logger.info(f\"\uD83D\uDCCA PHASE 1: Scoring {num_use_cases} use cases per domain in parallel\")\n",
    "        self.logger.info(f\"\uD83D\uDCCA Grouped into {num_domains} domains\")\n",
    "        \n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"\uD83D\uDCCA PHASE 1: SCORING PER DOMAIN (PARALLEL)\")\n",
    "        log_print(f\"{'='*80}\")\n",
    "        log_print(f\"Total use cases: {num_use_cases}\")\n",
    "        log_print(f\"Total domains: {num_domains}\")\n",
    "        log_adaptive_parallelism_decision(\"scoring\", scoring_parallelism, self.max_parallelism, reason)\n",
    "        log_print(f\"Total timeout: {TOTAL_SCORING_TIMEOUT}s ({TOTAL_SCORING_TIMEOUT//60} min)\")\n",
    "        log_print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        for domain, use_cases in sorted(domain_groups.items()):\n",
    "            self.logger.info(f\"   - {domain}: {len(use_cases)} use cases\")\n",
    "        \n",
    "        def score_domain(domain_name, domain_use_cases):\n",
    "            \"\"\"Score one domain's use cases\"\"\"\n",
    "            try:\n",
    "                self.logger.info(f\"\uD83D\uDCCA [{domain_name}] Scoring {len(domain_use_cases)} use cases...\")\n",
    "                \n",
    "                scored = self._score_use_cases(\n",
    "                    domain_use_cases,\n",
    "                    business_context=business_context,\n",
    "                    strategic_goals=strategic_goals,\n",
    "                    business_priorities=business_priorities,\n",
    "                    strategic_initiative=strategic_initiative,\n",
    "                    value_chain=value_chain,\n",
    "                    revenue_model=revenue_model\n",
    "                )\n",
    "                \n",
    "                self.logger.info(f\"✅ [{domain_name}] Scoring complete\")\n",
    "                return (domain_name, scored)\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"❌ [{domain_name}] Scoring failed: {e}\")\n",
    "                # Return with default scores - fix bug where 'Pending' priority was kept\n",
    "                for uc in domain_use_cases:\n",
    "                    if uc.get('Priority') in (None, '', 'Pending'):\n",
    "                        uc['Priority'] = 'Medium'\n",
    "                        uc['Priority Score'] = 5.0\n",
    "                        uc['Value'] = 3.5\n",
    "                        uc['Feasibility'] = 3.5\n",
    "                return (domain_name, domain_use_cases)\n",
    "        \n",
    "        # Score all domains in parallel with reduced parallelism\n",
    "        all_scored_use_cases = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=scoring_parallelism, thread_name_prefix=\"DomainScoring\") as executor:\n",
    "            future_to_domain = {}\n",
    "            for domain, domain_use_cases in domain_groups.items():\n",
    "                future = executor.submit(score_domain, domain, domain_use_cases)\n",
    "                future_to_domain[future] = domain\n",
    "            \n",
    "            completed_domains = 0\n",
    "            failed_domains = 0\n",
    "            total_domains = len(domain_groups)\n",
    "            start_time = time.time()\n",
    "            last_heartbeat = start_time\n",
    "            \n",
    "            # SOLUTION 1: Add total timeout to as_completed to prevent infinite hangs\n",
    "            try:\n",
    "                for future in as_completed(future_to_domain, timeout=TOTAL_SCORING_TIMEOUT):\n",
    "                    domain = future_to_domain[future]\n",
    "                    current_time = time.time()\n",
    "                    \n",
    "                    # SOLUTION 1: Heartbeat logging to show progress\n",
    "                    if current_time - last_heartbeat >= HEARTBEAT_INTERVAL:\n",
    "                        elapsed = current_time - start_time\n",
    "                        pending = total_domains - completed_domains - failed_domains\n",
    "                        log_print(f\"⏳ Scoring progress: {completed_domains}/{total_domains} done, {pending} pending ({elapsed:.0f}s elapsed)\")\n",
    "                        self.logger.info(f\"⏳ Scoring heartbeat: {completed_domains}/{total_domains} domains complete, {pending} pending\")\n",
    "                        last_heartbeat = current_time\n",
    "                    \n",
    "                    try:\n",
    "                        domain_name, scored_use_cases = future.result(timeout=PER_DOMAIN_TIMEOUT)\n",
    "                        all_scored_use_cases.extend(scored_use_cases)\n",
    "                        completed_domains += 1\n",
    "                        \n",
    "                        log_print(f\"✓ Scored domain {completed_domains}/{total_domains}: {domain_name} ({len(scored_use_cases)} use cases)\")\n",
    "                        self.logger.info(f\"✓ Domain {completed_domains}/{total_domains} scoring complete: {domain_name}\")\n",
    "                        \n",
    "                    except FuturesTimeoutError:\n",
    "                        failed_domains += 1\n",
    "                        self.logger.error(f\"❌ [{domain}] Scoring timed out after {PER_DOMAIN_TIMEOUT}s\")\n",
    "                        log_print(f\"✗ Timeout domain {completed_domains + failed_domains}/{total_domains}: {domain}\", level=\"ERROR\")\n",
    "                    except Exception as e:\n",
    "                        failed_domains += 1\n",
    "                        self.logger.error(f\"❌ [{domain}] Failed to collect scoring results: {e}\")\n",
    "                        log_print(f\"✗ Failed domain {completed_domains + failed_domains}/{total_domains}: {domain}\", level=\"ERROR\")\n",
    "                        \n",
    "            except FuturesTimeoutError:\n",
    "                # SOLUTION 1: Handle total timeout - proceed with what we have\n",
    "                elapsed = time.time() - start_time\n",
    "                pending = total_domains - completed_domains - failed_domains\n",
    "                self.logger.error(f\"⚠️ TOTAL SCORING TIMEOUT reached after {elapsed:.0f}s. {completed_domains}/{total_domains} domains completed, {pending} still pending.\")\n",
    "                log_print(f\"⚠️ SCORING TIMEOUT: {completed_domains}/{total_domains} domains completed after {elapsed:.0f}s\", level=\"WARNING\")\n",
    "                log_print(f\"   Proceeding with {len(all_scored_use_cases)} scored use cases\", level=\"WARNING\")\n",
    "        \n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"✅ PHASE 1 COMPLETE: ALL DOMAINS SCORED\")\n",
    "        log_print(f\"{'='*80}\")\n",
    "        log_print(f\"Total scored use cases: {len(all_scored_use_cases)}\")\n",
    "        log_print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        self.logger.info(f\"✅ Phase 1 complete: {len(all_scored_use_cases)} use cases scored across all domains\")\n",
    "        \n",
    "        normalized_use_cases = self._normalize_priority_scores(all_scored_use_cases)\n",
    "        self.logger.info(f\"✅ Phase 1 normalized across {len(normalized_use_cases)} use cases\")\n",
    "        \n",
    "        return normalized_use_cases\n",
    "\n",
    "    def _normalize_priority_scores(self, use_cases: list) -> list:\n",
    "        if not use_cases:\n",
    "            return use_cases\n",
    "        \n",
    "        scores = []\n",
    "        for uc in use_cases:\n",
    "            try:\n",
    "                scores.append(float(uc.get('Priority Score', 0)))\n",
    "            except (TypeError, ValueError):\n",
    "                continue\n",
    "        \n",
    "        max_score = max(scores) if scores else 0.0\n",
    "        if max_score <= 0:\n",
    "            return use_cases\n",
    "        \n",
    "        target_max = random.uniform(9.5, 9.95)\n",
    "        scale = target_max / max_score\n",
    "        \n",
    "        for uc in use_cases:\n",
    "            try:\n",
    "                priority_score = float(uc.get('Priority Score', 0))\n",
    "            except (TypeError, ValueError):\n",
    "                priority_score = 0.0\n",
    "            try:\n",
    "                value_score = float(uc.get('Value', 0))\n",
    "            except (TypeError, ValueError):\n",
    "                value_score = 0.0\n",
    "            try:\n",
    "                feasibility_score = float(uc.get('Feasibility', 0))\n",
    "            except (TypeError, ValueError):\n",
    "                feasibility_score = 0.0\n",
    "            \n",
    "            priority_scaled = min(priority_score * scale, target_max)\n",
    "            value_scaled = min(value_score * scale, target_max)\n",
    "            feasibility_scaled = min(feasibility_score * scale, target_max)\n",
    "            \n",
    "            uc['Priority Score'] = round(priority_scaled, 2)\n",
    "            uc['Value'] = round(value_scaled, 2)\n",
    "            uc['Feasibility'] = round(feasibility_scaled, 2)\n",
    "            \n",
    "            if priority_scaled >= 9.5:\n",
    "                priority_label = \"Ultra High\"\n",
    "            elif priority_scaled >= 8.5:\n",
    "                priority_label = \"Very High\"\n",
    "            elif priority_scaled >= 7.5:\n",
    "                priority_label = \"High\"\n",
    "            elif priority_scaled >= 5.5:\n",
    "                priority_label = \"Medium\"\n",
    "            elif priority_scaled >= 4.5:\n",
    "                priority_label = \"Low\"\n",
    "            elif priority_scaled >= 2.5:\n",
    "                priority_label = \"Very Low\"\n",
    "            else:\n",
    "                priority_label = \"Ultra Low\"\n",
    "            \n",
    "            uc['Priority'] = priority_label\n",
    "        \n",
    "        self.logger.info(f\"Applied priority normalization scale factor {scale:.2f} with target max {target_max:.2f}\")\n",
    "        return use_cases\n",
    "    \n",
    "    def _score_use_cases(self, all_use_cases: list, business_context: str = \"\", strategic_goals: list = None,\n",
    "                         business_priorities: list = None, strategic_initiative: str = \"\",\n",
    "                         value_chain: str = \"\", revenue_model: str = \"\") -> list:\n",
    "        \"\"\"\n",
    "        Calls an LLM to score use cases across 13 different factors.\n",
    "        \n",
    "        \uD83D\uDEA8 NEW SCORING STRATEGY (PRIORITIZED):\n",
    "        1. PRIMARY APPROACH: Score ALL use cases in ONE prompt (preferred for consistency)\n",
    "        2. FALLBACK APPROACH: If all use cases don't fit in one prompt, score by domain in parallel\n",
    "        \n",
    "        \uD83D\uDEA8 WEIGHTING: Priority Score = (Value × 1.5) + (Feasibility × 0.5)\n",
    "        - Value = (ROI × 0.60) + (Strategic Alignment × 0.25) + (Time to Value × 0.075) + (Reusability × 0.075)\n",
    "        \n",
    "        \uD83D\uDEA8 NEW: If use cases exceed 2048, implements 2-pass scoring:\n",
    "        - Pass 1: Score all use cases and select top 2048 by priority\n",
    "        - Pass 2: Re-score the top 2048 for final ranking\n",
    "        \n",
    "        Args:\n",
    "            all_use_cases: List of use case dictionaries to score\n",
    "            business_context: Business context description\n",
    "            strategic_goals: List of strategic goals\n",
    "            business_priorities: List of business priorities\n",
    "            strategic_initiative: Description of strategic initiative\n",
    "            value_chain: Description of value chain\n",
    "            revenue_model: Description of revenue model\n",
    "            \n",
    "        Returns:\n",
    "            List of scored use cases (top 2048 if input >2048, otherwise all)\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting LLM-based scoring for {len(all_use_cases)} use cases...\")\n",
    "        \n",
    "        if not all_use_cases:\n",
    "            self.logger.warning(\"No use cases to score.\")\n",
    "            return all_use_cases\n",
    "        \n",
    "        # Check if we need 2-pass scoring\n",
    "        needs_two_pass = len(all_use_cases) > 2048\n",
    "        \n",
    "        if needs_two_pass:\n",
    "            self.logger.warning(f\"⚠️ Use case count ({len(all_use_cases)}) exceeds 2048. Implementing 2-PASS SCORING process...\")\n",
    "            log_print(f\"\\n{'='*80}\", level=\"WARNING\")\n",
    "            log_print(f\"⚠️  LARGE USE CASE SET DETECTED: {len(all_use_cases)} use cases\", level=\"WARNING\")\n",
    "            log_print(f\"{'='*80}\")\n",
    "            log_print(f\"Implementing 2-PASS SCORING to select top 2048 use cases:\")\n",
    "            log_print(f\"  PASS 1: Score all {len(all_use_cases)} use cases → Select top 2048\")\n",
    "            log_print(f\"  PASS 2: Re-score top 2048 for final ranking\")\n",
    "            log_print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Format business context variables for the prompt\n",
    "        strategic_goals_text = \"\\n\".join([f\"- {goal}\" for goal in (strategic_goals or [])])\n",
    "        business_priorities_text = \"\\n\".join([f\"- {priority}\" for priority in (business_priorities or [])])\n",
    "        \n",
    "        domain_name = all_use_cases[0].get('Business Domain', 'Domain') if all_use_cases else \"Domain\"\n",
    "        \n",
    "        try:\n",
    "            # Create markdown table for this domain (minimal fields to fit context)\n",
    "            md_parts = [\"| No | Name | Business Value |\\n|---|---|---|\\n\"]\n",
    "            for uc in all_use_cases:\n",
    "                no = str(uc.get('No', '')).replace('|', r'\\|')\n",
    "                name = str(uc.get('Name', '')).replace('|', r'\\|')\n",
    "                business_value = str(uc.get('Business Value', '')).replace('|', r'\\|')\n",
    "                md_parts.append(f\"| {no} | {name} | {business_value} |\\n\")\n",
    "            use_case_markdown = \"\".join(md_parts)\n",
    "            \n",
    "            prompt_vars = {\n",
    "                \"use_case_markdown\": use_case_markdown,\n",
    "                \"business_context\": business_context or \"General business operations\",\n",
    "                \"strategic_goals\": strategic_goals_text or \"- Maximize operational efficiency\\n- Improve customer satisfaction\",\n",
    "                \"business_priorities\": business_priorities_text or \"- Digital transformation\\n- Cost optimization\",\n",
    "                \"strategic_initiative\": strategic_initiative or \"Data-driven transformation program\",\n",
    "                \"value_chain\": value_chain or \"Standard business operations\",\n",
    "                \"revenue_model\": revenue_model or \"Product and service sales\"\n",
    "            }\n",
    "            \n",
    "            self.logger.info(f\"⏳ [{domain_name}] Waiting for LLM response (scoring {len(all_use_cases)} use cases)...\")\n",
    "            response_raw = self.ai_agent.run_worker(\n",
    "                step_name=f\"Score_Use_Cases_{domain_name}\",\n",
    "                worker_prompt_path=\"SCORE_USE_CASES_PROMPT\",\n",
    "                prompt_vars=prompt_vars,\n",
    "                response_schema=None\n",
    "            )\n",
    "            self.logger.info(f\"✅ [{domain_name}] Received LLM response, processing results...\")\n",
    "            \n",
    "            # Parse CSV response using centralized utility\n",
    "            response_clean = clean_json_response(response_raw)\n",
    "            scoring_data = CSVParser.parse_csv_string(\n",
    "                response_clean,\n",
    "                logger=self.logger,\n",
    "                context=f\"Scoring for domain {domain_name}\"\n",
    "            )\n",
    "            \n",
    "            scoring_map = {item['No']: item for item in scoring_data}\n",
    "            \n",
    "            self.logger.info(f\"Received scoring for {len(scoring_map)} use cases from LLM for domain '{domain_name}'\")\n",
    "            \n",
    "            # Check for missing scores and retry with progressive batch splitting\n",
    "            missing_ids = [uc['No'] for uc in all_use_cases if uc['No'] not in scoring_map]\n",
    "            MAX_RETRY_ROUNDS = 3  # Maximum number of retry rounds\n",
    "            BATCH_SIZE_FOR_RETRY = 15  # Split into smaller batches for reliability\n",
    "            \n",
    "            retry_round = 0\n",
    "            while missing_ids and retry_round < MAX_RETRY_ROUNDS:\n",
    "                retry_round += 1\n",
    "                missing_ucs = [uc for uc in all_use_cases if uc['No'] in missing_ids]\n",
    "                \n",
    "                # If many missing, split into smaller batches for better success rate\n",
    "                if len(missing_ucs) > BATCH_SIZE_FOR_RETRY:\n",
    "                    self.logger.warning(f\"⚠️ [{domain_name}] Round {retry_round}: {len(missing_ids)} use cases missing scores. Splitting into batches of {BATCH_SIZE_FOR_RETRY}...\")\n",
    "                    batches = [missing_ucs[i:i + BATCH_SIZE_FOR_RETRY] for i in range(0, len(missing_ucs), BATCH_SIZE_FOR_RETRY)]\n",
    "                else:\n",
    "                    self.logger.warning(f\"⚠️ [{domain_name}] Round {retry_round}: Retrying {len(missing_ids)} missing use cases...\")\n",
    "                    batches = [missing_ucs]\n",
    "                \n",
    "                for batch_idx, batch_ucs in enumerate(batches):\n",
    "                    # Create a smaller prompt for this batch\n",
    "                    retry_md_parts = [\"| No | Name | Business Value |\\n|---|---|---|\\n\"]\n",
    "                    for uc in batch_ucs:\n",
    "                        no = str(uc.get('No', '')).replace('|', r'\\|')\n",
    "                        name = str(uc.get('Name', '')).replace('|', r'\\|')\n",
    "                        business_value = str(uc.get('Business Value', '')).replace('|', r'\\|')\n",
    "                        retry_md_parts.append(f\"| {no} | {name} | {business_value} |\\n\")\n",
    "                    retry_use_case_markdown = \"\".join(retry_md_parts)\n",
    "                    \n",
    "                    retry_prompt_vars = {\n",
    "                        \"use_case_markdown\": retry_use_case_markdown,\n",
    "                        \"business_context\": business_context or \"General business operations\",\n",
    "                        \"strategic_goals\": strategic_goals_text or \"- Maximize operational efficiency\\n- Improve customer satisfaction\",\n",
    "                        \"business_priorities\": business_priorities_text or \"- Digital transformation\\n- Cost optimization\",\n",
    "                        \"strategic_initiative\": strategic_initiative or \"Data-driven transformation program\",\n",
    "                        \"value_chain\": value_chain or \"Standard business operations\",\n",
    "                        \"revenue_model\": revenue_model or \"Product and service sales\"\n",
    "                    }\n",
    "                    \n",
    "                    try:\n",
    "                        batch_label = f\"Batch {batch_idx+1}/{len(batches)}\" if len(batches) > 1 else \"\"\n",
    "                        self.logger.info(f\"⏳ [{domain_name}] Round {retry_round} {batch_label}: Scoring {len(batch_ucs)} use cases...\")\n",
    "                        retry_response_raw = self.ai_agent.run_worker(\n",
    "                            step_name=f\"Score_Use_Cases_{domain_name}_Retry{retry_round}_B{batch_idx+1}\",\n",
    "                            worker_prompt_path=\"SCORE_USE_CASES_PROMPT\",\n",
    "                            prompt_vars=retry_prompt_vars,\n",
    "                            response_schema=None\n",
    "                        )\n",
    "                        retry_response_clean = clean_json_response(retry_response_raw)\n",
    "                        retry_scoring_data = CSVParser.parse_csv_string(\n",
    "                            retry_response_clean,\n",
    "                            logger=self.logger,\n",
    "                            context=f\"Retry scoring for domain {domain_name} round {retry_round} batch {batch_idx+1}\"\n",
    "                        )\n",
    "                        \n",
    "                        # Merge retry results into main scoring map\n",
    "                        new_scores = 0\n",
    "                        for item in retry_scoring_data:\n",
    "                            if item.get('No') and item['No'] not in scoring_map:\n",
    "                                scoring_map[item['No']] = item\n",
    "                                new_scores += 1\n",
    "                        \n",
    "                        if new_scores > 0:\n",
    "                            self.logger.info(f\"✅ [{domain_name}] Round {retry_round} {batch_label}: Got {new_scores} new scores (total: {len(scoring_map)})\")\n",
    "                    except Exception as retry_err:\n",
    "                        self.logger.warning(f\"[{domain_name}] Round {retry_round} {batch_label} failed: {str(retry_err)[:100]}\")\n",
    "                \n",
    "                # Update missing_ids for next round\n",
    "                missing_ids = [uc['No'] for uc in all_use_cases if uc['No'] not in scoring_map]\n",
    "                if missing_ids:\n",
    "                    self.logger.info(f\"[{domain_name}] After round {retry_round}: Still missing {len(missing_ids)} scores\")\n",
    "            \n",
    "            if missing_ids:\n",
    "                self.logger.warning(f\"⚠️ [{domain_name}] After {MAX_RETRY_ROUNDS} retry rounds, {len(missing_ids)} use cases still missing scores. Using defaults.\")\n",
    "            \n",
    "            # Add scoring data to use cases and compute Value/Feasibility/Priority in code\n",
    "            scored_use_cases = []\n",
    "            for uc in all_use_cases:\n",
    "                uc_id = uc['No']\n",
    "                if uc_id in scoring_map:\n",
    "                    scores = scoring_map[uc_id]\n",
    "                    \n",
    "                    uc['Strategic Alignment'] = float(scores.get('Strategic Alignment', 3.5))\n",
    "                    uc['Return on Investment'] = float(scores.get('Return on Investment', 3.5))\n",
    "                    uc['Reusability'] = float(scores.get('Reusability', 3.5))\n",
    "                    uc['Time to Value'] = float(scores.get('Time to Value', 3.5))\n",
    "                    uc['Data Availability'] = float(scores.get('Data Availability', 3.5))\n",
    "                    uc['Data Accessibility'] = float(scores.get('Data Accessibility', 3.5))\n",
    "                    uc['Architecture Fitness'] = float(scores.get('Architecture Fitness', 3.5))\n",
    "                    uc['Team Skills'] = float(scores.get('Team Skills', 3.5))\n",
    "                    uc['Domain Knowledge'] = float(scores.get('Domain Knowledge', 3.5))\n",
    "                    uc['People Allocation'] = float(scores.get('People Allocation', 3.5))\n",
    "                    uc['Budget Allocation'] = float(scores.get('Budget Allocation', 3.5))\n",
    "                    uc['Time to Production'] = float(scores.get('Time to Production', 3.5))\n",
    "                    uc['Business Priority Alignment'] = scores.get('Business Priority Alignment', 'General Improvement')\n",
    "                    uc['Strategic Goals Alignment'] = scores.get('Strategic Goals Alignment', 'General Improvement')\n",
    "\n",
    "                    value_score = (\n",
    "                        (uc['Return on Investment'] * 0.60)\n",
    "                        + (uc['Strategic Alignment'] * 0.25)\n",
    "                        + (uc['Time to Value'] * 0.075)\n",
    "                        + (uc['Reusability'] * 0.075)\n",
    "                    )\n",
    "\n",
    "                    feasibility_inputs = [\n",
    "                        uc['Data Availability'],\n",
    "                        uc['Data Accessibility'],\n",
    "                        uc['Architecture Fitness'],\n",
    "                        uc['Team Skills'],\n",
    "                        uc['Domain Knowledge'],\n",
    "                        uc['People Allocation'],\n",
    "                        uc['Budget Allocation'],\n",
    "                        uc['Time to Production'],\n",
    "                    ]\n",
    "                    feasibility_score = sum(feasibility_inputs) / len(feasibility_inputs)\n",
    "\n",
    "                    priority_score = (value_score * 1.5) + (feasibility_score * 0.5)\n",
    "\n",
    "                    uc['Value'] = round(value_score, 2)\n",
    "                    uc['Feasibility'] = round(feasibility_score, 2)\n",
    "                    uc['Priority Score'] = round(priority_score, 2)\n",
    "                    \n",
    "                    if 'Justification' in scores:\n",
    "                        uc['Justification'] = scores.get('Justification', '')\n",
    "                    \n",
    "                    uc['AI_Confidence'] = scores.get('AI_Confidence', 0.5)\n",
    "                    uc['AI_Feedback'] = scores.get('AI_Feedback', 'No feedback provided by AI scoring.')\n",
    "                    \n",
    "                    if priority_score >= 9.5:\n",
    "                        priority_label = \"Ultra High\"\n",
    "                    elif priority_score >= 8.5:\n",
    "                        priority_label = \"Very High\"\n",
    "                    elif priority_score >= 7.5:\n",
    "                        priority_label = \"High\"\n",
    "                    elif priority_score >= 5.5:\n",
    "                        priority_label = \"Medium\"\n",
    "                    elif priority_score >= 4.5:\n",
    "                        priority_label = \"Low\"\n",
    "                    elif priority_score >= 2.5:\n",
    "                        priority_label = \"Very Low\"\n",
    "                    else:\n",
    "                        priority_label = \"Ultra Low\"\n",
    "                    uc['Priority'] = priority_label\n",
    "                    \n",
    "                    self.logger.debug(f\"Scored {uc_id}: Value={uc['Value']}, Feasibility={uc['Feasibility']}, Priority Score={priority_score}, Priority={priority_label}\")\n",
    "                else:\n",
    "                    self.logger.warning(f\"No scoring data received for use case {uc_id}, using defaults\")\n",
    "                    uc['Strategic Alignment'] = 3.5\n",
    "                    uc['Return on Investment'] = 3.5\n",
    "                    uc['Reusability'] = 3.5\n",
    "                    uc['Time to Value'] = 3.5\n",
    "                    uc['Data Availability'] = 3.5\n",
    "                    uc['Data Accessibility'] = 3.5\n",
    "                    uc['Architecture Fitness'] = 3.5\n",
    "                    uc['Team Skills'] = 3.5\n",
    "                    uc['Domain Knowledge'] = 3.5\n",
    "                    uc['People Allocation'] = 3.5\n",
    "                    uc['Budget Allocation'] = 3.5\n",
    "                    uc['Time to Production'] = 3.5\n",
    "                    uc['Value'] = 3.5\n",
    "                    uc['Feasibility'] = 3.5\n",
    "                    uc['Priority Score'] = 7.0\n",
    "                    uc['Priority'] = \"Medium\"\n",
    "                    uc['AI_Confidence'] = 0.5\n",
    "                    uc['AI_Feedback'] = 'Default scoring applied - no LLM scoring data received.'\n",
    "                \n",
    "                scored_use_cases.append(uc)\n",
    "            \n",
    "            self.logger.info(f\"Pass 1 scoring complete for {len(scored_use_cases)} use cases in domain '{domain_name}'\")\n",
    "            \n",
    "            if needs_two_pass and len(scored_use_cases) > 2048:\n",
    "                self.logger.warning(f\"\uD83D\uDD04 Starting PASS 2: Selecting top 2048 use cases and re-scoring...\")\n",
    "                log_print(f\"\\n{'='*80}\")\n",
    "                log_print(f\"\uD83D\uDD04 PASS 1 COMPLETE: {len(scored_use_cases)} use cases scored\")\n",
    "                log_print(f\"{'='*80}\")\n",
    "                log_print(f\"Selecting top 2048 use cases by Priority Score for PASS 2...\")\n",
    "                \n",
    "                scored_use_cases.sort(key=lambda x: x.get('Priority Score', 0), reverse=True)\n",
    "                top_2048 = scored_use_cases[:2048]\n",
    "                excluded_count = len(scored_use_cases) - 2048\n",
    "                \n",
    "                self.logger.info(f\"Selected top 2048 use cases. Excluded {excluded_count} lower-priority use cases.\")\n",
    "                log_print(f\"✓ Selected top 2048 use cases\")\n",
    "                log_print(f\"✗ Excluded {excluded_count} lower-priority use cases\")\n",
    "                log_print(f\"\\nStarting PASS 2: Re-scoring top 2048 use cases for final ranking...\")\n",
    "                log_print(f\"{'='*80}\\n\")\n",
    "                \n",
    "                final_scored = self._score_use_cases(\n",
    "                    top_2048,\n",
    "                    business_context=business_context,\n",
    "                    strategic_goals=strategic_goals,\n",
    "                    business_priorities=business_priorities,\n",
    "                    strategic_initiative=strategic_initiative,\n",
    "                    value_chain=value_chain,\n",
    "                    revenue_model=revenue_model\n",
    "                )\n",
    "                \n",
    "                self.logger.info(f\"✅ PASS 2 complete. Final set: {len(final_scored)} use cases\")\n",
    "                log_print(f\"\\n{'='*80}\")\n",
    "                log_print(f\"✅ 2-PASS SCORING COMPLETE\")\n",
    "                log_print(f\"{'='*80}\")\n",
    "                log_print(f\"Final use case count: {len(final_scored)}\")\n",
    "                log_print(f\"{'='*80}\\n\")\n",
    "                \n",
    "                return final_scored\n",
    "            \n",
    "            self.logger.info(f\"Scoring complete for {len(scored_use_cases)} use cases in domain '{domain_name}'\")\n",
    "            return scored_use_cases\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Use case scoring failed: {e}. Proceeding without LLM scoring.\")\n",
    "            for uc in all_use_cases:\n",
    "                uc['Strategic Alignment'] = 3.5\n",
    "                uc['Return on Investment'] = 3.5\n",
    "                uc['Reusability'] = 3.5\n",
    "                uc['Time to Value'] = 3.5\n",
    "                uc['Data Availability'] = 3.5\n",
    "                uc['Data Accessibility'] = 3.5\n",
    "                uc['Architecture Fitness'] = 3.5\n",
    "                uc['Team Skills'] = 3.5\n",
    "                uc['Domain Knowledge'] = 3.5\n",
    "                uc['People Allocation'] = 3.5\n",
    "                uc['Budget Allocation'] = 3.5\n",
    "                uc['Time to Production'] = 3.5\n",
    "                uc['Value'] = 3.5\n",
    "                uc['Feasibility'] = 3.5\n",
    "                uc['Priority Score'] = 7.0\n",
    "                uc['Priority'] = \"Medium\"\n",
    "            return all_use_cases\n",
    "\n",
    "    def _translate_and_prepare_language_pack(self, lang: str, flat_english_use_cases: list, english_grouped_data: dict, business_name: str) -> tuple:\n",
    "        \"\"\"\n",
    "        A single-function wrapper to run all data-gathering for a language.\n",
    "        Designed to be run in a ThreadPoolExecutor.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Set language for context limit calculations\n",
    "            self.ai_agent.set_language(lang)\n",
    "            \n",
    "            self.logger.info(f\"Starting translation & summary pack for {lang}...\")\n",
    "            lang_abbr = self._get_lang_abbr(lang)\n",
    "            lang_translations = self.translation_service.get_translations(lang)\n",
    "            # Disable parallelization to avoid nested ThreadPoolExecutors (this function is already called in parallel)\n",
    "            lang_use_cases_translated = self.translation_service.translate_use_case_list([uc.copy() for uc in flat_english_use_cases], lang, max_parallelism=self.max_parallelism, enable_parallelization=False)\n",
    "            lang_grouped_data = self._align_translated_data(english_grouped_data, lang_use_cases_translated)\n",
    "            (lang_summary_dict, transliterated_name) = self._get_salesy_summary(lang_grouped_data, business_name, lang, lang_translations)\n",
    "            \n",
    "            self.logger.debug(f\"Successfully processed all data for {lang}.\")\n",
    "            return (lang, lang_abbr, lang_translations, lang_grouped_data, lang_summary_dict, transliterated_name)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to process translation artifacts for {lang}: {e}\")\n",
    "            import traceback\n",
    "            self.logger.error(f\"Full traceback for {lang}: {traceback.format_exc()}\")\n",
    "            return (lang, lang_abbr, None, None, None, None) # Return Nones to signal failure\n",
    "\n",
    "    def _generate_documents_for_all_languages(self, final_consolidated_use_cases: list, english_grouped_data: dict = None, summary_dict: dict = None, languages: list = None, skip_excel_langs: list = None):\n",
    "        \"\"\"\n",
    "        Generates PDF/PPTX/Excel documents for all languages.\n",
    "        Can be called from normal path (after use case generation) or docs-only path (from JSON).\n",
    "        \n",
    "        Args:\n",
    "            final_consolidated_use_cases: List of use cases (flat)\n",
    "            english_grouped_data: Optional, will be computed if not provided\n",
    "            summary_dict: Optional, will be computed if not provided\n",
    "            languages: Optional list of languages to generate (default: self.output_languages)\n",
    "        \"\"\"\n",
    "        target_languages = languages if languages else self.output_languages\n",
    "        skip_excel_langs = set(skip_excel_langs or [])\n",
    "        self.logger.info(f\"--- Starting Document Generation for Languages: {target_languages} ---\")\n",
    "        \n",
    "        # CRITICAL: Install all required dependencies BEFORE starting translations\n",
    "        # This prevents wasting time on translations if dependencies are missing\n",
    "        self.logger.info(\"Checking and installing required dependencies before starting translations...\")\n",
    "        dependencies_ok = True\n",
    "        \n",
    "        if \"PDF Catalog\" in self.generate_choices or \"Use Cases Catalog PDF\" in self.generate_choices:\n",
    "            self.logger.info(\"Checking PDF dependencies (weasyprint)...\")\n",
    "            try:\n",
    "                import weasyprint\n",
    "                self.logger.info(\"✓ PDF package (weasyprint) already installed.\")\n",
    "            except ImportError:\n",
    "                self.logger.info(\"PDF package (weasyprint) not found. Installing...\")\n",
    "                try:\n",
    "                    import subprocess, sys\n",
    "                    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"weasyprint\"])\n",
    "                    import weasyprint\n",
    "                    self.logger.info(\"✓ PDF package (weasyprint) installed successfully.\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"✗ Failed to install PDF dependencies: {e}\")\n",
    "                    dependencies_ok = False\n",
    "        \n",
    "        if \"Presentation\" in self.generate_choices:\n",
    "            self.logger.info(\"Checking PPTX dependencies (python-pptx)...\")\n",
    "            try:\n",
    "                import pptx\n",
    "                self.logger.info(\"✓ PPTX package (python-pptx) already installed.\")\n",
    "            except ImportError:\n",
    "                self.logger.info(\"PPTX package (python-pptx) not found. Installing...\")\n",
    "                try:\n",
    "                    import subprocess, sys\n",
    "                    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"python-pptx\"])\n",
    "                    import pptx\n",
    "                    self.logger.info(\"✓ PPTX package (python-pptx) installed successfully.\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"✗ Failed to install PPTX dependencies: {e}\")\n",
    "                    dependencies_ok = False\n",
    "        \n",
    "        # Always check Excel dependencies as they're used for all artifact generation\n",
    "        self.logger.info(\"Checking Excel dependencies (pandas, openpyxl)...\")\n",
    "        try:\n",
    "            import pandas, openpyxl\n",
    "            self.logger.info(\"✓ Excel packages (pandas, openpyxl) already installed.\")\n",
    "        except ImportError:\n",
    "            self.logger.info(\"Excel packages not found. Installing...\")\n",
    "            try:\n",
    "                import subprocess, sys\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\", \"openpyxl\"])\n",
    "                import pandas, openpyxl\n",
    "                self.logger.info(\"✓ Excel packages (pandas, openpyxl) installed successfully.\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"✗ Failed to install Excel dependencies: {e}\")\n",
    "                dependencies_ok = False\n",
    "        \n",
    "        if not dependencies_ok:\n",
    "            self.logger.warning(\"⚠️ Some dependencies failed to install. PDF/Excel generation may fail, but .md and .csv files will still be generated.\")\n",
    "        \n",
    "        self.logger.info(\"✓ Proceeding with translations and artifact generation (fallback to .md/.csv if needed)...\")\n",
    "        \n",
    "        # Prepare English data if not provided\n",
    "        flat_english_use_cases = final_consolidated_use_cases\n",
    "        if english_grouped_data is None:\n",
    "            _unsorted_grouped = self._group_use_cases_by_domain_flat(flat_english_use_cases)\n",
    "            english_grouped_data = {k: _unsorted_grouped[k] for k in sorted(_unsorted_grouped.keys())}\n",
    "        \n",
    "        # Get English translations\n",
    "        english_translations = self.translation_service.get_translations(\"English\")\n",
    "        \n",
    "        # Get summary if not provided\n",
    "        if summary_dict is None:\n",
    "            (summary_dict, transliterated_name) = self._get_salesy_summary(english_grouped_data, self.business_name, \"English\", english_translations)\n",
    "        \n",
    "        # STEP 1: Run translations in parallel (no nesting)\n",
    "        # ADAPTIVE PARALLELISM: Calculate based on languages and use cases\n",
    "        num_languages = len([l for l in target_languages if l != \"English\"])\n",
    "        num_use_cases = len(flat_english_use_cases)\n",
    "        \n",
    "        translation_parallelism, reason = calculate_adaptive_parallelism(\n",
    "            \"translation\", self.max_parallelism,\n",
    "            num_items=num_use_cases,\n",
    "            num_domains=num_languages,  # Each language is like a domain\n",
    "            is_llm_operation=True, logger=self.logger\n",
    "        )\n",
    "        log_adaptive_parallelism_decision(\"translation\", translation_parallelism, self.max_parallelism, reason)\n",
    "        \n",
    "        translation_futures = []\n",
    "        translation_results = {}\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=translation_parallelism, thread_name_prefix=\"Translator\") as translation_executor:\n",
    "            for lang in target_languages:\n",
    "                if lang == \"English\":\n",
    "                    # No translation needed for English\n",
    "                    self.logger.info(\"Preparing English artifacts (no translation needed).\")\n",
    "                    (summary_dict_en, transliterated_name_en) = self._get_salesy_summary(english_grouped_data, self.business_name, \"English\", english_translations)\n",
    "                    lang_abbr = self._get_lang_abbr(\"English\")\n",
    "                    translation_results[\"English\"] = (\"English\", lang_abbr, english_translations, english_grouped_data, summary_dict_en, transliterated_name_en)\n",
    "                    continue\n",
    "\n",
    "                self.logger.info(f\"Submitting translation & summary pack job for {lang}...\")\n",
    "                f = translation_executor.submit(\n",
    "                    self._translate_and_prepare_language_pack,\n",
    "                    lang, flat_english_use_cases, english_grouped_data, self.business_name\n",
    "                )\n",
    "                translation_futures.append((f, lang))\n",
    "\n",
    "            self.logger.info(f\"Waiting for {len(translation_futures)} language packs to complete...\")\n",
    "            # Add timeout: 30 minutes per language pack\n",
    "            total_timeout = len(translation_futures) * 1800\n",
    "            self.logger.info(f\"Language pack processing timeout set to {total_timeout}s ({total_timeout//60} minutes)\")\n",
    "            \n",
    "            try:\n",
    "                for future, lang in translation_futures:\n",
    "                    try:\n",
    "                        result = future.result(timeout=1800)\n",
    "                        (lang, lang_abbr, lang_translations, lang_grouped_data, lang_summary_dict, transliterated_name) = result\n",
    "                        \n",
    "                        if lang_translations is None:\n",
    "                            self.logger.warning(f\"Skipping artifact generation for {lang} due to translation/summary failure.\")\n",
    "                            continue\n",
    "\n",
    "                        self.logger.info(f\"Translation pack for {lang} complete.\")\n",
    "                        translation_results[lang] = result\n",
    "                    except concurrent.futures.TimeoutError:\n",
    "                        self.logger.error(f\"Language pack processing timed out after 30 minutes for {lang}\")\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Language pack processing failed for {lang}: {e}\")\n",
    "            except concurrent.futures.TimeoutError:\n",
    "                self.logger.error(f\"Overall language pack processing timeout reached ({total_timeout}s)\")\n",
    "        \n",
    "        # STEP 2: Run artifact writing in parallel (separate, not nested)\n",
    "        # ADAPTIVE PARALLELISM: Calculate based on number of artifacts to write\n",
    "        num_artifacts = len(translation_results) * 3  # Roughly: PDF, PPTX, Excel per language\n",
    "        \n",
    "        writing_parallelism, reason = calculate_adaptive_parallelism(\n",
    "            \"artifact_writing\", self.max_parallelism,\n",
    "            num_items=num_artifacts,\n",
    "            is_llm_operation=False, logger=self.logger\n",
    "        )\n",
    "        log_adaptive_parallelism_decision(\"artifact_writing\", writing_parallelism, self.max_parallelism, reason)\n",
    "        self.logger.info(f\"Translations complete. Starting artifact generation for {len(translation_results)} languages...\")\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=writing_parallelism, thread_name_prefix=\"Writer\") as writer_executor:\n",
    "            writing_futures = []\n",
    "            \n",
    "            for lang, result in translation_results.items():\n",
    "                (lang, lang_abbr, lang_translations, lang_grouped_data, lang_summary_dict, transliterated_name) = result\n",
    "                \n",
    "                self.logger.info(f\"Submitting writing jobs for {lang}...\")\n",
    "                \n",
    "                # ALWAYS generate .md and .csv files for English (fallback artifacts)\n",
    "                if lang == \"English\":\n",
    "                    f = writer_executor.submit(self._generate_markdown_catalog, lang, lang_abbr, lang_grouped_data, lang_summary_dict, transliterated_name)\n",
    "                    writing_futures.append((f, f\"{lang} Markdown\"))\n",
    "                    f = writer_executor.submit(self._generate_csv_catalog, lang, lang_abbr, lang_grouped_data)\n",
    "                    writing_futures.append((f, f\"{lang} CSV\"))\n",
    "                \n",
    "                if \"PDF Catalog\" in self.generate_choices or \"Use Cases Catalog PDF\" in self.generate_choices:\n",
    "                    f = writer_executor.submit(self.generate_catalog_pdf, lang, lang_abbr, lang_translations, lang_summary_dict, lang_grouped_data, transliterated_name)\n",
    "                    writing_futures.append((f, f\"{lang} PDF\"))\n",
    "                if \"Presentation\" in self.generate_choices:\n",
    "                    f = writer_executor.submit(self.generate_presentation_pptx, lang, lang_abbr, lang_translations, lang_summary_dict, lang_grouped_data, transliterated_name)\n",
    "                    writing_futures.append((f, f\"{lang} PPTX\"))\n",
    "                if lang == \"English\" and lang not in skip_excel_langs:\n",
    "                    f = writer_executor.submit(self._generate_use_case_excel, lang, lang_abbr, lang_grouped_data)\n",
    "                    writing_futures.append((f, f\"{lang} Excel\"))\n",
    "                elif lang == \"English\":\n",
    "                    self.logger.info(f\"Skipping Excel generation for {lang} (already generated).\")\n",
    "                else:\n",
    "                    self.logger.info(f\"Skipping Excel generation for {lang} (English only).\")\n",
    "            \n",
    "            # Wait for all writing jobs to complete\n",
    "            for future, job_name in writing_futures:\n",
    "                try:\n",
    "                    future.result(timeout=600)\n",
    "                    self.logger.info(f\"✓ {job_name} completed\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"✗ {job_name} failed: {e}\")\n",
    "        \n",
    "        self.logger.info(\"All artifact writing jobs completed.\")\n",
    "\n",
    "    def run(self):\n",
    "        self.logger.info(f\"Starting tasks: {self.generate_choices}, Operation Mode: {self.operation_mode}\")\n",
    "        \n",
    "        if 'PROMPT_TEMPLATES' not in globals():\n",
    "            self.logger.critical(\"CRITICAL ERROR: 'PROMPT_TEMPLATES' dictionary is not defined. Please run the cell defining it.\")\n",
    "            log_print(\"CRITICAL ERROR: 'PROMPT_TEMPLATES' dictionary is not defined. Please run the cell defining it.\", level=\"CRITICAL\")\n",
    "            return\n",
    "        \n",
    "        if self.auto_parallelism and self.max_parallelism <= 0:\n",
    "            (recommended, tables_per_batch, est_batches, avg_table_chars, max_by_memory) = self._calculate_dynamic_parallelism(\n",
    "                0,\n",
    "                0,\n",
    "                0,\n",
    "                0\n",
    "            )\n",
    "            self.max_parallelism = recommended\n",
    "            self.logger.info(f\"Dynamic parallelism set to {self.max_parallelism} (memory_cap={max_by_memory})\")\n",
    "            log_print(f\"✅ Dynamic parallelism: {self.max_parallelism}\")\n",
    "\n",
    "        english_translations = self.translation_service.get_translations(\"English\")\n",
    "        final_consolidated_use_cases = []\n",
    "        \n",
    "        # === OPERATION MODE ROUTING ===\n",
    "        if self.operation_mode == \"Re-generate SQL\":\n",
    "            self.logger.info(\"\uD83D\uDD27 RE-GENERATE SQL MODE: Regenerating failed SQL queries\")\n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"\uD83D\uDD27 RE-GENERATE SQL MODE\")\n",
    "            log_print(f\"{'='*80}\")\n",
    "            log_print(f\"ℹ️  This mode regenerates SQL and samples for flagged use cases in existing notebooks\")\n",
    "            \n",
    "            try:\n",
    "                self._run_queries_fixing_mode()\n",
    "                return\n",
    "            except Exception as e:\n",
    "                self.logger.critical(f\"Failed to run Re-generate SQL mode: {e}\")\n",
    "                log_print(f\"❌ Error in Re-generate SQL mode: {e}\", level=\"ERROR\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                return\n",
    "        \n",
    "        if self.operation_mode == \"Generate Sample Result\":\n",
    "            self.logger.info(\"\uD83D\uDCCA GENERATE SAMPLE RESULT MODE: Executing SQL and generating sample outputs\")\n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"\uD83D\uDCCA GENERATE SAMPLE RESULT MODE\")\n",
    "            log_print(f\"{'='*80}\")\n",
    "            log_print(f\"ℹ️  This mode executes SQL for use cases with generate_sample_result:Yes and generates sample outputs\")\n",
    "            \n",
    "            try:\n",
    "                self._run_generate_sample_result_mode()\n",
    "                return\n",
    "            except Exception as e:\n",
    "                self.logger.critical(f\"Failed to run Generate Sample Result mode: {e}\")\n",
    "                log_print(f\"❌ Error in Generate Sample Result mode: {e}\", level=\"ERROR\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                return\n",
    "        \n",
    "        if self.json_file_path:\n",
    "            self.logger.info(f\"\uD83D\uDE80 JSON MODE: Loading use cases from JSON file: {self.json_file_path}\")\n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"\uD83D\uDE80 JSON MODE ACTIVATED\")\n",
    "            log_print(f\"{'='*80}\")\n",
    "            log_print(f\"\uD83D\uDCC1 JSON File: {self.json_file_path}\")\n",
    "            log_print(f\"\uD83D\uDCCB Languages: {', '.join(self.output_languages)}\")\n",
    "            log_print(f\"\\n⚠️  SKIPPING:\", level=\"WARNING\")\n",
    "            log_print(f\"   ❌ Use Case Generation (using existing data from JSON)\")\n",
    "            log_print(f\"\\n✅ GENERATING:\")\n",
    "            log_print(f\"   \uD83D\uDCD3 Notebooks (always generated)\")\n",
    "            log_print(f\"   \uD83D\uDCC4 PDF Catalogs\" if \"PDF Catalog\" in self.generate_choices or \"Use Cases Catalog PDF\" in self.generate_choices else \"   (PDF generation not selected)\")\n",
    "            log_print(f\"   \uD83D\uDCCA Presentations\" if \"Presentation\" in self.generate_choices else \"   (Presentation generation not selected)\")\n",
    "            log_print(f\"{'='*80}\\n\")\n",
    "            \n",
    "            try:\n",
    "                # Load the JSON catalog\n",
    "                (final_consolidated_use_cases, summary_dict, english_grouped_data) = self._load_usecases_catalog_json(self.json_file_path)\n",
    "                \n",
    "                # 1. Generate Notebooks (always generated)\n",
    "                if final_consolidated_use_cases:\n",
    "                    self.logger.info(\"Starting notebook generation from JSON data...\")\n",
    "                    self.assemble_use_case_notebooks(final_consolidated_use_cases, english_translations, summary_dict)\n",
    "                else:\n",
    "                    self.logger.warning(\"No use cases found in JSON, skipping notebook creation.\")\n",
    "                \n",
    "                # 2. Generate documents for all languages\n",
    "                if \"PDF Catalog\" in self.generate_choices or \"Presentation\" in self.generate_choices or \"Use Cases Catalog PDF\" in self.generate_choices:\n",
    "                    self._generate_documents_for_all_languages(\n",
    "                        final_consolidated_use_cases,\n",
    "                        english_grouped_data=english_grouped_data,\n",
    "                        summary_dict=summary_dict\n",
    "                    )\n",
    "                \n",
    "                # Report table inclusion/exclusion statistics\n",
    "                if final_consolidated_use_cases:\n",
    "                    self._report_table_statistics(final_consolidated_use_cases)\n",
    "                \n",
    "                # Upload log file and show summary\n",
    "                self.logger.info(f\"✅ All artifacts for {self.business_name} generated successfully from JSON\")\n",
    "                self.logger.info(\"Uploading log file...\")\n",
    "                self._upload_log_file()\n",
    "                AIAgent.get_summary_report()\n",
    "                \n",
    "                # Final success message\n",
    "                log_print(f\"\\n{'='*80}\")\n",
    "                log_print(f\"✅ SUCCESS: All artifacts generated successfully from JSON\")\n",
    "                log_print(f\"{'='*80}\\n\")\n",
    "                return\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.critical(f\"Failed to process in docs-only mode: {e}\")\n",
    "                log_print(f\"❌ Error: Failed to process in docs-only mode: {e}\")\n",
    "                AIAgent.get_summary_report()\n",
    "                return\n",
    "        \n",
    "        # === NORMAL PATH: Generate use cases from metadata ===\n",
    "        \n",
    "        # === NEW: Call Business Context Worker first ===\n",
    "        self.logger.info(\"=\" * 80)\n",
    "        self.logger.info(\"\uD83D\uDE80 STEP 1: EXTRACTING BUSINESS CONTEXT, STRATEGIC GOALS, AND PRIORITIES\")\n",
    "        self.logger.info(\"=\" * 80)\n",
    "        \n",
    "        # Prepare user context string early - now uses business_domains instead of use_cases_focus\n",
    "        user_domains_str = ', '.join(self.user_business_domains) if self.user_business_domains else ''\n",
    "        \n",
    "        # Extract business context\n",
    "        llm_business_context = self._get_business_context_from_llm()\n",
    "        self.logger.info(\"✅ Business context extraction completed\")\n",
    "        \n",
    "        # Merge with user-provided domains (user takes precedence)\n",
    "        merged_business_context = self._merge_business_contexts(llm_business_context, user_domains_str)\n",
    "        \n",
    "        # Handle user-provided strategic goals (hard focus)\n",
    "        if self.user_strategic_goals:\n",
    "            self.logger.info(f\"✅ User provided {len(self.user_strategic_goals)} strategic goals - ONLY these will be used\")\n",
    "            merged_business_context[\"strategic_goals\"] = self.user_strategic_goals\n",
    "            self.logger.info(f\"   Strategic Goals: {', '.join(self.user_strategic_goals)}\")\n",
    "        else:\n",
    "            self.logger.info(\"ℹ️ No user strategic goals provided - will generate goals based on business context\")\n",
    "            llm_goals = merged_business_context.get(\"strategic_goals\", [])\n",
    "            if isinstance(llm_goals, str):\n",
    "                llm_goals = [g.strip() for g in llm_goals.split(\",\") if g.strip()]\n",
    "            merged_business_context[\"strategic_goals\"] = llm_goals\n",
    "        \n",
    "        # Handle user-provided business priorities\n",
    "        if self.user_business_priorities:\n",
    "            self.logger.info(f\"✅ User provided {len(self.user_business_priorities)} business priorities - these will drive ranking\")\n",
    "            merged_business_context[\"business_priorities\"] = self.user_business_priorities\n",
    "            self.logger.info(f\"   Business Priorities: {', '.join(self.user_business_priorities)}\")\n",
    "        \n",
    "        # Handle user-provided business domains\n",
    "        if self.user_business_domains:\n",
    "            self.logger.info(f\"✅ User provided {len(self.user_business_domains)} business domains - use cases will be aligned to these domains ONLY\")\n",
    "            merged_business_context[\"user_business_domains\"] = self.user_business_domains\n",
    "            self.logger.info(f\"   Business Domains: {', '.join(self.user_business_domains)}\")\n",
    "        else:\n",
    "            self.logger.info(\"ℹ️ No user business domains provided - domains will be inferred from data\")\n",
    "        \n",
    "        self.logger.info(\"✅ Business context extracted and merged.\")\n",
    "        self.logger.info(\"=\" * 80)\n",
    "        \n",
    "        # Store merged context for use in prompt generation\n",
    "        self.merged_business_context = merged_business_context\n",
    "        \n",
    "        # Extract business context fields for prompts\n",
    "        ctx_business_context = merged_business_context.get(\"business_context\", \"\")\n",
    "        ctx_strategic_goals = merged_business_context.get(\"strategic_goals\", [])\n",
    "        # Handle if strategic_goals is a string (comma separated) or list\n",
    "        if isinstance(ctx_strategic_goals, str):\n",
    "            ctx_strategic_goals = [s.strip() for s in ctx_strategic_goals.split(\",\") if s.strip()]\n",
    "        \n",
    "        ctx_business_priorities = merged_business_context.get(\"business_priorities\", [])\n",
    "        if isinstance(ctx_business_priorities, str):\n",
    "            ctx_business_priorities = [s.strip() for s in ctx_business_priorities.split(\",\") if s.strip()]\n",
    "        ctx_strategic_initiative = merged_business_context.get(\"strategic_initiative\", \"\")\n",
    "        ctx_value_chain = merged_business_context.get(\"value_chain\", \"\")\n",
    "        ctx_revenue_model = merged_business_context.get(\"revenue_model\", \"\")\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            if self.data_loader:\n",
    "                self.logger.debug(\"Data loader found. Starting batched table processing with MAX_CONTEXT_CHARS management...\")\n",
    "                \n",
    "                # Generate unstructured document list first (needs schema overview)\n",
    "                # Check if unstructured data is enabled\n",
    "                if self.use_unstructured_data:\n",
    "                    # We'll use a sampling approach or generate it once with limited schema\n",
    "                    self.logger.info(\"Generating unstructured document list and extracting business context...\")\n",
    "                    # Get a small sample for unstructured doc generation\n",
    "                    sample_columns = self.data_loader.getNextTables(self.scan_parallelism)\n",
    "                    if sample_columns:\n",
    "                        sample_columns = self._augment_columns_with_foreign_keys(sample_columns)\n",
    "                        sample_schema_markdown = self._format_schema_for_prompt(sample_columns)\n",
    "                        business_context_dict = self._generate_unstructured_docs(sample_schema_markdown)\n",
    "                        \n",
    "                        # Extract individual variables from the returned dict\n",
    "                        unstructured_docs_markdown = business_context_dict.get(\"unstructured_docs_markdown\", \"\")\n",
    "                        strategic_goals = business_context_dict.get(\"strategic_goals\", [])\n",
    "                        business_context = business_context_dict.get(\"business_context\", \"\")\n",
    "                        business_priorities = business_context_dict.get(\"business_priorities\", [])\n",
    "                        strategic_initiative = business_context_dict.get(\"strategic_initiative\", \"\")\n",
    "                        value_chain = business_context_dict.get(\"value_chain\", \"\")\n",
    "                        revenue_model = business_context_dict.get(\"revenue_model\", \"\")\n",
    "                        \n",
    "                        # Reset the data loader state to start from beginning\n",
    "                        self.data_loader.current_table_idx = 0\n",
    "                    else:\n",
    "                        unstructured_docs_markdown = \"\"\n",
    "                        strategic_goals = ctx_strategic_goals\n",
    "                        business_context = \"\"\n",
    "                        business_priorities = []\n",
    "                        strategic_initiative = \"\"\n",
    "                        value_chain = \"\"\n",
    "                        revenue_model = \"\"\n",
    "                        self.logger.warning(\"No tables found for unstructured doc generation\")\n",
    "                else:\n",
    "                    self.logger.info(\"Unstructured data generation is disabled. Skipping...\")\n",
    "                    unstructured_docs_markdown = \"\"\n",
    "                    strategic_goals = ctx_strategic_goals\n",
    "                    business_context = \"\"\n",
    "                    business_priorities = []\n",
    "                    strategic_initiative = \"\"\n",
    "                    value_chain = \"\"\n",
    "                    revenue_model = \"\"\n",
    "\n",
    "                # Now process tables in batches with model-specific context limits from TECHNICAL_CONTEXT\n",
    "                use_case_gen_context_limit = get_max_context_chars(\"English\", \"BASE_USE_CASE_GEN_PROMPT\")\n",
    "                safe_context_limit = get_safe_context_limit(\"English\", buffer_percent=0.9, prompt_name=\"BASE_USE_CASE_GEN_PROMPT\")\n",
    "                batch_num = 1\n",
    "                accumulated_columns = []\n",
    "                accumulated_schema_size = 0\n",
    "                batches_to_process = []  # List of (batch_num, column_details) tuples\n",
    "                \n",
    "                # Get the base prompt template size estimate\n",
    "                base_prompt_template = PROMPT_TEMPLATES.get(\"BASE_USE_CASE_GEN_PROMPT\", \"\")\n",
    "                base_template_size = len(base_prompt_template) + len(unstructured_docs_markdown)\n",
    "                base_prompt_size = base_template_size + 2000\n",
    "                self.logger.debug(f\"Base prompt template size: {base_template_size} chars, context limit: {use_case_gen_context_limit}\")\n",
    "                \n",
    "                # CRITICAL FIX: Keep pulling tables until we fill the model's context limit\n",
    "                self.logger.info(\"Collecting batches for parallel processing (MAXIMIZING context utilization)...\")\n",
    "                while True:\n",
    "                    # Keep pulling table batches until we hit the context limit\n",
    "                    while True:\n",
    "                        batch_columns = self.data_loader.getNextTables(self.scan_parallelism)\n",
    "                        \n",
    "                        if batch_columns is None:\n",
    "                            # No more tables available\n",
    "                            break\n",
    "\n",
    "                        batch_columns = self._augment_columns_with_foreign_keys(batch_columns)\n",
    "                        \n",
    "                        batch_schema_size = self._estimate_schema_markdown_size(batch_columns)\n",
    "                        estimated_prompt_size = base_prompt_size + accumulated_schema_size + batch_schema_size\n",
    "                        \n",
    "                        self.logger.debug(f\"Batch {batch_num}: Got {len(batch_columns)} columns. \"\n",
    "                                       f\"Total accumulated: {len(accumulated_columns) + len(batch_columns)} columns. \"\n",
    "                                       f\"Estimated prompt size: {estimated_prompt_size}/{safe_context_limit} chars\")\n",
    "                        \n",
    "                        if estimated_prompt_size > safe_context_limit:\n",
    "                            if not accumulated_columns:\n",
    "                                self.logger.warning(f\"Initial table pull exceeds context limit ({estimated_prompt_size} chars). Splitting into sub-batches.\")\n",
    "                                split_batches = self._split_columns_to_fit_context(batch_columns, base_prompt_size, safe_context_limit)\n",
    "                                for idx, split_columns in enumerate(split_batches, start=1):\n",
    "                                    table_count = len({(c[0], c[1], c[2]) for c in split_columns})\n",
    "                                    self.logger.warning(f\"   Sub-batch {idx}: {table_count} tables ({len(split_columns)} columns)\")\n",
    "                                    batches_to_process.append((batch_num, split_columns))\n",
    "                                    batch_num += 1\n",
    "                                accumulated_columns = []\n",
    "                                accumulated_schema_size = 0\n",
    "                                break\n",
    "                            else:\n",
    "                                # Would exceed limit - save current accumulator as a batch\n",
    "                                self.logger.info(f\"Context limit reached. Saving batch {batch_num} with {len(accumulated_columns)} columns ({base_prompt_size + accumulated_schema_size} chars)\")\n",
    "                                batches_to_process.append((batch_num, accumulated_columns))\n",
    "                                batch_num += 1\n",
    "                                \n",
    "                                # Start new accumulator with the current batch\n",
    "                                accumulated_columns = batch_columns\n",
    "                                accumulated_schema_size = batch_schema_size\n",
    "                                break\n",
    "                        else:\n",
    "                            # Fits - keep accumulating and PULL MORE TABLES\n",
    "                            accumulated_columns.extend(batch_columns)\n",
    "                            accumulated_schema_size += batch_schema_size\n",
    "                            # Continue inner loop to pull more tables\n",
    "                    \n",
    "                    # Check if we're done (no more tables)\n",
    "                    if batch_columns is None:\n",
    "                        # Add any remaining accumulated columns to batches\n",
    "                        if accumulated_columns:\n",
    "                            self.logger.info(f\"Adding final batch {batch_num} with {len(accumulated_columns)} columns\")\n",
    "                            batches_to_process.append((batch_num, accumulated_columns))\n",
    "                        break\n",
    "                \n",
    "                if not batches_to_process:\n",
    "                    self.logger.warning(\"No batches to process. Skipping all generation.\")\n",
    "                    return\n",
    "                \n",
    "                total_tables = len({(c, s, t) for _, cols in batches_to_process for (c, s, t, _, _, _) in cols})\n",
    "                tables_per_call = self._determine_tables_per_call(total_tables)\n",
    "                if tables_per_call > 0:\n",
    "                    adjusted_batches = []\n",
    "                    next_batch_num = 1\n",
    "                    for _, batch_columns in batches_to_process:\n",
    "                        grouped = self._split_by_table_limit(batch_columns, tables_per_call)\n",
    "                        for group in grouped:\n",
    "                            estimated_prompt_size = base_prompt_size + self._estimate_schema_markdown_size(group)\n",
    "                            if estimated_prompt_size > safe_context_limit:\n",
    "                                singles = self._split_by_table_limit(group, 1)\n",
    "                                for single in singles:\n",
    "                                    adjusted_batches.append((next_batch_num, single))\n",
    "                                    next_batch_num += 1\n",
    "                            else:\n",
    "                                adjusted_batches.append((next_batch_num, group))\n",
    "                                next_batch_num += 1\n",
    "                    batches_to_process = adjusted_batches\n",
    "                    self.logger.info(f\"Table-based batching: {tables_per_call} tables per call, {len(batches_to_process)} batches\")\n",
    "                \n",
    "                # === NEW: Filter business vs technical tables ===\n",
    "                self.logger.info(\"\uD83D\uDD0D Filtering tables into BUSINESS vs TECHNICAL categories...\")\n",
    "                log_print(f\"\\n{'='*80}\")\n",
    "                log_print(f\"\uD83D\uDD0D FILTERING TABLES: Business Data vs Technical/Metadata\")\n",
    "                log_print(f\"{'='*80}\\n\")\n",
    "                \n",
    "                # Collect all columns from all batches for filtering\n",
    "                all_batch_columns = []\n",
    "                for batch_num, batch_columns in batches_to_process:\n",
    "                    all_batch_columns.extend(batch_columns)\n",
    "                \n",
    "                if self.auto_parallelism:\n",
    "                    total_tables = len({(c, s, t) for (c, s, t, _, _, _) in all_batch_columns})\n",
    "                    total_schema_chars = self._estimate_schema_markdown_size(all_batch_columns)\n",
    "                    (recommended, tables_per_batch, est_batches, avg_table_chars, max_by_memory) = self._calculate_dynamic_parallelism(\n",
    "                        total_tables,\n",
    "                        total_schema_chars,\n",
    "                        safe_context_limit,\n",
    "                        base_prompt_size\n",
    "                    )\n",
    "                    self.max_parallelism = recommended\n",
    "                    self.logger.info(f\"Dynamic parallelism set to {self.max_parallelism} (tables={total_tables}, avg_table_chars={avg_table_chars}, tables_per_batch={tables_per_batch}, est_batches={est_batches}, memory_cap={max_by_memory})\")\n",
    "                    log_print(f\"✅ Dynamic parallelism: {self.max_parallelism} (tables={total_tables}, est_batches={est_batches})\")\n",
    "                \n",
    "                # Get industry from business context if available\n",
    "                industry = \"\"\n",
    "                if 'business_context' in locals() and business_context:\n",
    "                    # Extract industry info (simplified - you could enhance this)\n",
    "                    industry = business_context.split('\\n')[0] if business_context else \"\"\n",
    "                \n",
    "                # Filter tables\n",
    "                (business_details, technical_details, business_tables, technical_tables, business_scores, data_category_map, master_tables_set, transactional_tables_set, reference_tables_set) = self._filter_business_tables(\n",
    "                    all_batch_columns,\n",
    "                    business_context=business_context if 'business_context' in locals() else \"\",\n",
    "                    industry=industry,\n",
    "                    exclusion_strategy=self.technical_exclusion_strategy\n",
    "                )\n",
    "                \n",
    "                # === MEMORY OPTIMIZATION: Clear all_batch_columns after filtering ===\n",
    "                del all_batch_columns\n",
    "                gc.collect()\n",
    "                self.logger.debug(\"\uD83E\uDDF9 Cleared all_batch_columns from memory\")\n",
    "                \n",
    "                self._business_column_details_global = business_details\n",
    "                self.global_table_names = {f\"{c}.{s}.{t}\" for (c, s, t, _, _, _) in business_details}\n",
    "                \n",
    "                # Store business_scores for later use in truncation\n",
    "                self.business_scores = business_scores\n",
    "                self.data_category_map = data_category_map\n",
    "                \n",
    "                if not business_details:\n",
    "                    self.logger.warning(\"No business tables found after filtering. All tables were classified as technical/metadata.\")\n",
    "                    log_print(\"⚠️ No business tables found. All tables appear to be technical/metadata.\", level=\"WARNING\")\n",
    "                    return\n",
    "                \n",
    "                self.logger.info(f\"✅ Filtering complete: Proceeding with {len(business_tables)} business tables, \"\n",
    "                               f\"excluding {len(technical_tables)} technical/metadata tables, \"\n",
    "                               f\"excluding {len(reference_tables_set)} reference tables\")\n",
    "                \n",
    "                master_details = []\n",
    "                transactional_details = []\n",
    "                for detail in business_details:\n",
    "                    (catalog, schema, table, _, _, _) = detail\n",
    "                    fqtn = f\"{catalog}.{schema}.{table}\"\n",
    "                    if fqtn in transactional_tables_set:\n",
    "                        transactional_details.append(detail)\n",
    "                    else:\n",
    "                        master_details.append(detail)\n",
    "                \n",
    "                master_tables_count = len(master_tables_set)\n",
    "                tables_per_call = self._determine_tables_per_call(master_tables_count)\n",
    "                \n",
    "                adjusted_batches = []\n",
    "                next_batch_num = 1\n",
    "                \n",
    "                if master_details:\n",
    "                    grouped_master = self._split_by_table_limit(master_details, tables_per_call)\n",
    "                    for group in grouped_master:\n",
    "                        estimated_prompt_size = base_prompt_size + self._estimate_schema_markdown_size(group)\n",
    "                        if estimated_prompt_size > safe_context_limit:\n",
    "                            singles = self._split_by_table_limit(group, 1)\n",
    "                            for single in singles:\n",
    "                                adjusted_batches.append((next_batch_num, single))\n",
    "                                next_batch_num += 1\n",
    "                        else:\n",
    "                            adjusted_batches.append((next_batch_num, group))\n",
    "                            next_batch_num += 1\n",
    "                \n",
    "                if transactional_details:\n",
    "                    grouped_tx = self._split_by_table_limit(transactional_details, 1)\n",
    "                    for group in grouped_tx:\n",
    "                        adjusted_batches.append((next_batch_num, group))\n",
    "                        next_batch_num += 1\n",
    "                \n",
    "                augmented_batches = []\n",
    "                for batch_num, batch_columns in adjusted_batches:\n",
    "                    augmented = self._augment_columns_with_related_tables(batch_columns)\n",
    "                    augmented_batches.append((batch_num, augmented))\n",
    "                batches_to_process = augmented_batches\n",
    "                log_print(f\"✅ Business tables: {len(business_tables)}\")\n",
    "                log_print(f\"   \uD83D\uDCCA Master Data tables: {len(master_tables_set)}\")\n",
    "                log_print(f\"   \uD83D\uDCC8 Transactional tables: {len(transactional_tables_set)}\")\n",
    "                log_print(f\"\uD83D\uDFE1 Reference tables (excluded): {len(reference_tables_set)}\")\n",
    "                log_print(f\"❌ Technical tables (excluded): {len(technical_tables)}\")\n",
    "                log_print(f\"{'='*80}\\n\")\n",
    "\n",
    "                # Update honesty tracking\n",
    "                self.processing_honesty['total_tables_discovered'] = len(business_tables) + len(technical_tables) + len(reference_tables_set)\n",
    "                self.processing_honesty['total_tables_processed'] = len(business_tables)\n",
    "                self.processing_honesty['total_batches_created'] = len(batches_to_process)\n",
    "\n",
    "                # Process each batch once (deduplication handles redundancy)\n",
    "                # MEMORY OPTIMIZATION: Use file-based storage instead of keeping everything in memory\n",
    "                # ADAPTIVE PARALLELISM: Calculate based on batches, tables and columns\n",
    "                total_batch_columns = sum(len(cols) for _, cols in batches_to_process)\n",
    "                avg_prompt_chars = total_batch_columns * 100  # Estimate ~100 chars per column\n",
    "                \n",
    "                batch_parallelism, reason = calculate_adaptive_parallelism(\n",
    "                    \"use_case_generation\", self.max_parallelism,\n",
    "                    num_items=len(batches_to_process),\n",
    "                    total_columns=total_batch_columns,\n",
    "                    avg_prompt_chars=avg_prompt_chars,\n",
    "                    is_llm_operation=True, logger=self.logger\n",
    "                )\n",
    "                \n",
    "                log_print(f\"\\n{'='*80}\")\n",
    "                log_print(f\"\uD83D\uDD04 USE CASE GENERATION: SERIAL ENSEMBLE (2 PASSES)\")\n",
    "                log_print(f\"{'='*80}\")\n",
    "                log_print(f\"\uD83D\uDCCB PASS 1: Generate initial use cases from {len(batches_to_process)} batch(es)\")\n",
    "                log_print(f\"\uD83D\uDCCB PASS 2: Generate NEW use cases not in PASS 1 (with feedback)\")\n",
    "                log_print(\"\uD83D\uDCBE Using file-based intermediate storage to prevent memory explosion\")\n",
    "                log_print(f\"{'='*80}\\n\")\n",
    "                \n",
    "                self.storage_manager.initialize()\n",
    "                \n",
    "                # === PASS 1: Generate initial use cases (parallel within pass) ===\n",
    "                self.logger.info(\"\uD83D\uDD04 PASS 1: Generating initial use cases...\")\n",
    "                log_print(f\"\\n{'='*60}\")\n",
    "                log_print(f\"\uD83D\uDD04 PASS 1: Initial Use Case Generation\")\n",
    "                log_print(f\"{'='*60}\")\n",
    "                \n",
    "                with ThreadPoolExecutor(max_workers=batch_parallelism, thread_name_prefix=\"Pass1Batch\") as executor:\n",
    "                    future_to_batch = {}\n",
    "                    for batch_num, column_details in batches_to_process:\n",
    "                        unique_batch_id = f\"P1_{batch_num}\"\n",
    "                        future = executor.submit(\n",
    "                            self._process_batch_with_retry,\n",
    "                            column_details,\n",
    "                            unique_batch_id,\n",
    "                            unstructured_docs_markdown,\n",
    "                            strategic_goals,\n",
    "                            business_context if 'business_context' in locals() else \"\",\n",
    "                            business_priorities if 'business_priorities' in locals() else \"\",\n",
    "                            strategic_initiative if 'strategic_initiative' in locals() else \"\",\n",
    "                            value_chain if 'value_chain' in locals() else \"\",\n",
    "                            revenue_model if 'revenue_model' in locals() else \"\",\n",
    "                            3,  # max_attempts\n",
    "                            \"\"  # No feedback for PASS 1\n",
    "                        )\n",
    "                        future_to_batch[future] = unique_batch_id\n",
    "                        self.logger.info(f\"✓ [PASS 1] Submitted batch {batch_num}\")\n",
    "                    \n",
    "                    total_submissions = len(batches_to_process)\n",
    "                    batches_completed = 0\n",
    "                    total_timeout = (total_submissions * 900) // self.max_parallelism + 600\n",
    "                    \n",
    "                    try:\n",
    "                        for future in concurrent.futures.as_completed(future_to_batch, timeout=total_timeout):\n",
    "                            unique_batch_id = future_to_batch[future]\n",
    "                            try:\n",
    "                                use_cases = future.result(timeout=900)\n",
    "                                if use_cases:\n",
    "                                    self.storage_manager.save_batch(unique_batch_id, use_cases)\n",
    "                                    batches_completed += 1\n",
    "                                    self.logger.info(f\"✓ [PASS 1] Batch {unique_batch_id}: {len(use_cases)} use cases ({batches_completed}/{total_submissions})\")\n",
    "                                    log_print(f\"✓ [PASS 1] Batch complete ({batches_completed}/{total_submissions})\")\n",
    "                            except Exception as e:\n",
    "                                self.logger.error(f\"❌ [PASS 1] Batch {unique_batch_id} failed: {e}\")\n",
    "                    except concurrent.futures.TimeoutError:\n",
    "                        self.logger.error(f\"⚠️ [PASS 1] Timeout after {total_timeout}s. Proceeding with {batches_completed}/{total_submissions} completed.\")\n",
    "                \n",
    "                # === MEMORY OPTIMIZATION: Save PASS 1 results to disk immediately ===\n",
    "                # Count use cases and save IDs without loading all into memory\n",
    "                pass1_count = self.storage_manager.get_total_count()\n",
    "                self.logger.info(f\"✅ PASS 1 complete: Generated {pass1_count} use cases\")\n",
    "                log_print(f\"✅ PASS 1 complete: {pass1_count} use cases generated\")\n",
    "                \n",
    "                # Save PASS 1 IDs to disk for later comparison (memory-efficient)\n",
    "                pass1_ids = []\n",
    "                for batch in self.storage_manager.iter_batches():\n",
    "                    for uc in batch:\n",
    "                        pass1_ids.append(uc.get('No', ''))\n",
    "                self.storage_manager.save_pass1_ids(pass1_ids)\n",
    "                del pass1_ids  # Free memory immediately\n",
    "                \n",
    "                # Force garbage collection after PASS 1\n",
    "                gc.collect()\n",
    "                self.logger.debug(\"\uD83E\uDDF9 Memory cleanup after PASS 1\")\n",
    "                \n",
    "                # === PASS 2: Generate NEW use cases with feedback (TRANSACTIONAL TABLES ONLY) ===\n",
    "                # Create transactional-only batches for PASS 2\n",
    "                transactional_batches = []\n",
    "                for batch_num, column_details in batches_to_process:\n",
    "                    tx_columns = [col for col in column_details \n",
    "                                  if f\"{col[0]}.{col[1]}.{col[2]}\" in transactional_tables_set]\n",
    "                    if tx_columns:\n",
    "                        transactional_batches.append((batch_num, tx_columns))\n",
    "                \n",
    "                if pass1_count > 0 and transactional_batches:\n",
    "                    self.logger.info(\"\uD83D\uDD04 PASS 2: Generating NEW use cases from TRANSACTIONAL TABLES (with PASS 1 feedback)...\")\n",
    "                    log_print(f\"\\n{'='*60}\")\n",
    "                    log_print(f\"\uD83D\uDD04 PASS 2: Ensemble - TRANSACTIONAL TABLES ONLY\")\n",
    "                    log_print(f\"{'='*60}\")\n",
    "                    log_print(f\"\uD83D\uDCCB Feedback: {pass1_count} use cases from PASS 1\")\n",
    "                    log_print(f\"\uD83D\uDCCA Transactional batches: {len(transactional_batches)} (focusing on event/transaction data)\")\n",
    "                    log_print(f\"\uD83C\uDFAF Goal: Find NEW use cases from transactional data NOT covered in PASS 1\")\n",
    "                    \n",
    "                    # === MEMORY OPTIMIZATION: Build feedback iteratively and save to disk ===\n",
    "                    feedback_lines = [\"**\uD83D\uDE80 PASS 2: YOUR MISSION IS TO FIND EVEN MORE VALUE! \uD83D\uDE80**\\n\"]\n",
    "                    feedback_lines.append(\"Pass 1 generated some use cases, but there is MUCH MORE VALUE to extract from the transactional data!\")\n",
    "                    feedback_lines.append(\"Your job is to find DIFFERENT, COMPLEMENTARY use cases that Pass 1 missed.\\n\")\n",
    "                    feedback_lines.append(\"**Already generated (reference only - avoid exact duplicates, but EXPLORE VARIATIONS):**\")\n",
    "                    feedback_lines.append(\"| No | Name | Tables Involved |\")\n",
    "                    feedback_lines.append(\"|---|---|---|\")\n",
    "                    \n",
    "                    # Use memory-efficient iterator (doesn't load all at once)\n",
    "                    feedback_count = 0\n",
    "                    for idx, name, tables in self.storage_manager.iter_pass1_use_cases_for_feedback(limit=200):\n",
    "                        feedback_lines.append(f\"| {idx} | {name} | {tables} |\")\n",
    "                        feedback_count = idx\n",
    "                    \n",
    "                    if pass1_count > 200:\n",
    "                        feedback_lines.append(f\"\\n*... and {pass1_count - 200} more use cases (not shown)*\")\n",
    "                    \n",
    "                    feedback_lines.append(\"\\n**\uD83D\uDD25 PASS 2 MISSION: FIND HIGH-VALUE USE CASES THAT PASS 1 MISSED \uD83D\uDD25**\")\n",
    "                    feedback_lines.append(\"\")\n",
    "                    feedback_lines.append(\"**VALUE-FOCUSED EXPLORATION:**\")\n",
    "                    feedback_lines.append(\"Pass 1 covered some ground, but transactional data often hides MASSIVE business value.\")\n",
    "                    feedback_lines.append(\"Your job is to find HIGH-ROI use cases that Pass 1 missed - NOT to generate filler.\")\n",
    "                    feedback_lines.append(\"\")\n",
    "                    feedback_lines.append(\"**WHAT TO LOOK FOR (only if they deliver REAL business value):**\")\n",
    "                    feedback_lines.append(\"- \uD83D\uDCB0 **Revenue opportunities**: Patterns that could increase sales, reduce churn, optimize pricing\")\n",
    "                    feedback_lines.append(\"- \uD83D\uDEE1️ **Risk signals**: Fraud patterns, compliance risks, operational anomalies worth preventing\")\n",
    "                    feedback_lines.append(\"- \uD83D\uDCCA **Strategic insights**: Cross-table relationships that reveal hidden business drivers\")\n",
    "                    feedback_lines.append(\"- ⚡ **Efficiency gains**: Process bottlenecks, resource optimization opportunities\")\n",
    "                    feedback_lines.append(\"\")\n",
    "                    feedback_lines.append(\"**QUALITY RULES:**\")\n",
    "                    feedback_lines.append(\"- ❌ Avoid duplicates of Pass 1 use cases (check the table above)\")\n",
    "                    feedback_lines.append(\"- ❌ Do NOT generate low-value filler just to add more use cases\")\n",
    "                    feedback_lines.append(\"- ✅ Variations that add DISTINCT business value ARE encouraged\")\n",
    "                    feedback_lines.append(\"- ✅ Different business angles on same data = valuable if ROI is clear\")\n",
    "                    feedback_lines.append(\"- ✅ Cross-table joins often unlock the highest value - explore these\")\n",
    "                    feedback_lines.append(\"\")\n",
    "                    feedback_lines.append(\"**SELF-CHECK**: For each use case ask: 'Would a CFO fund this? Does it impact revenue or reduce costs?'\")\n",
    "                    \n",
    "                    # Save feedback to disk and clear from memory\n",
    "                    self.storage_manager.save_feedback_file(feedback_lines)\n",
    "                    previous_use_cases_feedback = \"\\n\".join(feedback_lines)\n",
    "                    del feedback_lines  # Free memory\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    with ThreadPoolExecutor(max_workers=batch_parallelism, thread_name_prefix=\"Pass2Batch\") as executor:\n",
    "                        future_to_batch = {}\n",
    "                        for batch_num, column_details in transactional_batches:\n",
    "                            unique_batch_id = f\"P2_{batch_num}\"\n",
    "                            future = executor.submit(\n",
    "                                self._process_batch_with_retry,\n",
    "                                column_details,\n",
    "                                unique_batch_id,\n",
    "                                unstructured_docs_markdown,\n",
    "                                strategic_goals,\n",
    "                                business_context if 'business_context' in locals() else \"\",\n",
    "                                business_priorities if 'business_priorities' in locals() else \"\",\n",
    "                                strategic_initiative if 'strategic_initiative' in locals() else \"\",\n",
    "                                value_chain if 'value_chain' in locals() else \"\",\n",
    "                                revenue_model if 'revenue_model' in locals() else \"\",\n",
    "                                3,  # max_attempts\n",
    "                                previous_use_cases_feedback  # Include feedback from PASS 1\n",
    "                            )\n",
    "                            future_to_batch[future] = unique_batch_id\n",
    "                            self.logger.info(f\"✓ [PASS 2] Submitted transactional batch {batch_num}\")\n",
    "                        \n",
    "                        batches_completed = 0\n",
    "                        pass2_submissions = len(transactional_batches)\n",
    "                        try:\n",
    "                            for future in concurrent.futures.as_completed(future_to_batch, timeout=total_timeout):\n",
    "                                unique_batch_id = future_to_batch[future]\n",
    "                                try:\n",
    "                                    use_cases = future.result(timeout=900)\n",
    "                                    if use_cases:\n",
    "                                        self.storage_manager.save_batch(unique_batch_id, use_cases)\n",
    "                                        batches_completed += 1\n",
    "                                        self.logger.info(f\"✓ [PASS 2] Batch {unique_batch_id}: {len(use_cases)} NEW use cases ({batches_completed}/{pass2_submissions})\")\n",
    "                                        log_print(f\"✓ [PASS 2] Batch complete ({batches_completed}/{pass2_submissions})\")\n",
    "                                except Exception as e:\n",
    "                                    self.logger.error(f\"❌ [PASS 2] Batch {unique_batch_id} failed: {e}\")\n",
    "                        except concurrent.futures.TimeoutError:\n",
    "                            self.logger.error(f\"⚠️ [PASS 2] Timeout. Proceeding with {batches_completed}/{pass2_submissions} completed.\")\n",
    "                    \n",
    "                    # === MEMORY OPTIMIZATION: Count PASS 2 results without loading all into memory ===\n",
    "                    total_after_pass2 = self.storage_manager.get_total_count()\n",
    "                    pass2_new_count = total_after_pass2 - pass1_count\n",
    "                    self.logger.info(f\"✅ PASS 2 complete: Generated {pass2_new_count} additional NEW use cases from transactional tables\")\n",
    "                    log_print(f\"✅ PASS 2 complete: {pass2_new_count} NEW use cases generated\")\n",
    "                    \n",
    "                    # Clean up PASS 2 variables to free memory\n",
    "                    del previous_use_cases_feedback\n",
    "                    del transactional_batches\n",
    "                    gc.collect()\n",
    "                    self.logger.debug(\"\uD83E\uDDF9 Memory cleanup after PASS 2\")\n",
    "                elif not transactional_batches:\n",
    "                    self.logger.info(\"⚠️ PASS 2 skipped: No transactional tables found for ensemble pass\")\n",
    "                \n",
    "                log_print(f\"\\n{'='*60}\")\n",
    "                log_print(f\"✅ SERIAL ENSEMBLE COMPLETE\")\n",
    "                log_print(f\"{'='*60}\")\n",
    "                \n",
    "                # Check if any batches were successfully processed\n",
    "                storage_stats = self.storage_manager.get_stats()\n",
    "                if storage_stats['num_batches'] == 0:\n",
    "                    self.logger.warning(\"No use cases were generated from any batch. Skipping all generation.\")\n",
    "                    self.storage_manager.cleanup()  # Cleanup if no use cases generated\n",
    "                    return\n",
    "                \n",
    "                self.logger.info(f\"\uD83D\uDCCA Batch processing complete. Storage stats: {storage_stats['num_batches']} batches, \"\n",
    "                               f\"{storage_stats['use_case_count']} use cases, {storage_stats['total_size_mb']:.2f} MB on disk\")\n",
    "                \n",
    "                # MEMORY OPTIMIZATION: Load use cases from disk for deduplication only when needed\n",
    "                self.logger.info(\"Loading use cases from disk for deduplication...\")\n",
    "                all_use_cases = self.storage_manager.load_all_use_cases()\n",
    "                \n",
    "                # Filter out use cases without valid tables (before deduplication)\n",
    "                # Keep use cases that have either:\n",
    "                # 1. Valid table references (non-empty and not just catalog/schema prefix)\n",
    "                # 2. Volume paths (for unstructured data use cases)\n",
    "                pre_filter_count = len(all_use_cases)\n",
    "                all_use_cases = [\n",
    "                    uc for uc in all_use_cases \n",
    "                    if (uc.get('Tables Involved', '').strip() and (\n",
    "                        uc.get('Tables Involved', '').startswith('/Volumes') or \n",
    "                        '.' in uc.get('Tables Involved', '')  # Has at least one dot (catalog.schema.table format)\n",
    "                    ))\n",
    "                ]\n",
    "                filtered_count = pre_filter_count - len(all_use_cases)\n",
    "                if filtered_count > 0:\n",
    "                    self.logger.warning(f\"⚠️ Filtered out {filtered_count} use cases without valid tables before deduplication\")\n",
    "\n",
    "                # Deduplicate per domain in parallel (skip global deduplication to maximize parallelization)\n",
    "                self.logger.debug(f\"Total use cases generated (pre-deduplication): {len(all_use_cases)}\")\n",
    "                self.logger.info(\"\uD83D\uDD04 Starting domain-level parallel deduplication (skipping global deduplication for max parallelization)...\")\n",
    "                unique_use_cases = all_use_cases\n",
    "                \n",
    "                # === RESTRUCTURED: Prepare all columns for SQL generation FIRST ===\n",
    "                all_columns_for_sql = []\n",
    "                for batch_num, batch_columns in batches_to_process:\n",
    "                    for detail in batch_columns:\n",
    "                        (catalog, schema, table, _, _, _) = detail\n",
    "                        fqtn = f\"{catalog}.{schema}.{table}\"\n",
    "                        if reference_tables_set and fqtn in reference_tables_set:\n",
    "                            continue\n",
    "                        all_columns_for_sql.append(detail)\n",
    "                \n",
    "                # === MEMORY OPTIMIZATION: Clear batches_to_process after extracting columns ===\n",
    "                del batches_to_process\n",
    "                gc.collect()\n",
    "                self.logger.debug(\"\uD83E\uDDF9 Cleared batches_to_process from memory\")\n",
    "                \n",
    "                # === NEW: Catch uncovered tables BEFORE clustering/scoring/SQL ===\n",
    "                self.logger.info(\"\uD83D\uDD0D Checking table coverage before clustering and scoring...\")\n",
    "                catchall_rounds = 3\n",
    "                for round_idx in range(catchall_rounds):\n",
    "                    pre_scoring_retry = self._retry_missing_table_coverage(\n",
    "                        unique_use_cases,\n",
    "                        all_columns_for_sql,\n",
    "                        unstructured_docs_markdown,\n",
    "                        strategic_goals,\n",
    "                        include_business_catchall=True\n",
    "                    )\n",
    "                    if not pre_scoring_retry:\n",
    "                        if round_idx == 0:\n",
    "                            self.logger.info(\"✅ All tables covered by initial use cases\")\n",
    "                        else:\n",
    "                            self.logger.info(f\"✅ All tables covered after catch-all pass {round_idx}\")\n",
    "                        break\n",
    "                    self.logger.info(f\"✅ Generated {len(pre_scoring_retry)} additional use cases for uncovered tables (pass {round_idx + 1})\")\n",
    "                    pre_scoring_retry = [\n",
    "                        uc for uc in pre_scoring_retry\n",
    "                        if uc.get('Tables Involved', '').strip() and not uc.get('Tables Involved', '').startswith('/Volumes')\n",
    "                    ]\n",
    "                    self.logger.info(f\"✅ After filtering, {len(pre_scoring_retry)} pre-scoring retry use cases have valid tables\")\n",
    "                    if not pre_scoring_retry:\n",
    "                        continue\n",
    "                    unique_use_cases.extend(pre_scoring_retry)\n",
    "                    self.logger.info(f\"\uD83D\uDCCA Total use cases after pre-scoring coverage pass {round_idx + 1}: {len(unique_use_cases)}\")\n",
    "                else:\n",
    "                    self.logger.info(\"⚠️ Catch-all reached maximum retries\")\n",
    "\n",
    "                # === PHASE 1: Cluster domains/subdomains FIRST (before scoring) ===\n",
    "                self.logger.info(f\"\uD83D\uDCCA Clustering {len(unique_use_cases)} deduplicated use cases into domains and subdomains...\")\n",
    "                clustered_use_cases = self._cluster_domains_and_subdomains(unique_use_cases, \"English\")\n",
    "                \n",
    "                # === PHASE 2: Score per domain in parallel FIRST (before deduplication) ===\n",
    "                # We need scores to make intelligent decisions during deduplication (keep highest ROI)\n",
    "                self.logger.info(f\"\uD83D\uDD04 PHASE 1: Scoring use cases per domain in parallel...\")\n",
    "                unique_use_cases_scored = self._score_per_domain_parallel(\n",
    "                    clustered_use_cases, # Use clustered cases here (not deduped yet)\n",
    "                    business_context=ctx_business_context,\n",
    "                    strategic_goals=ctx_strategic_goals,\n",
    "                    business_priorities=ctx_business_priorities,\n",
    "                    strategic_initiative=ctx_strategic_initiative,\n",
    "                    value_chain=ctx_value_chain,\n",
    "                    revenue_model=ctx_revenue_model\n",
    "                )\n",
    "                self.logger.info(\"✅ Phase 1 complete: All use cases scored\")\n",
    "                \n",
    "                # === PHASE 3: Intelligent Deduplication (Using Scores) ===\n",
    "                # Now that we have scores, deduplicate by keeping the highest ROI/Strategic Alignment\n",
    "                self.logger.info(\"\uD83D\uDD04 Starting INTELLIGENT domain-level deduplication (using scores)...\")\n",
    "                final_deduplicated_use_cases = self._deduplicate_use_cases_by_domain_parallel(unique_use_cases_scored)\n",
    "                \n",
    "                final_consolidated_use_cases = final_deduplicated_use_cases\n",
    "                \n",
    "                # Re-number use case IDs to match the domain-based notebook prefixes (C01, C02, etc.)\n",
    "                self.logger.debug(\"Re-numbering use case IDs to match domain-based notebook prefixes...\")\n",
    "                grouped_by_domain = self._group_use_cases_by_domain_flat(final_consolidated_use_cases)\n",
    "                \n",
    "                # Sort domains by impact (most impactful first)\n",
    "                domain_impact_scores = {domain: self._calculate_domain_impact_score(use_cases) \n",
    "                                       for domain, use_cases in grouped_by_domain.items()}\n",
    "                sorted_domain_names = sorted(grouped_by_domain.keys(), \n",
    "                                            key=lambda d: domain_impact_scores[d], \n",
    "                                            reverse=True)\n",
    "                \n",
    "                renumbered_use_cases = []\n",
    "                domain_source_counters = defaultdict(lambda: defaultdict(int))\n",
    "                for domain_idx, domain_name in enumerate(sorted_domain_names):\n",
    "                    domain_use_cases = grouped_by_domain[domain_name]\n",
    "                    domain_prefix = f\"N{domain_idx+1:02d}\"  # Two-digit domain prefix\n",
    "                    \n",
    "                    # Use cases are already sorted by priority from _group_use_cases_by_domain_flat\n",
    "                    for uc_idx, uc in enumerate(domain_use_cases, start=1):\n",
    "                        old_id = uc.get('No', '')\n",
    "                        source_flag = 'AI' if uc.get('_source') == 'AI' else 'ST'\n",
    "                        domain_source_counters[domain_name][source_flag] += 1\n",
    "                        seq_num = domain_source_counters[domain_name][source_flag]\n",
    "                        new_id = f\"{domain_prefix}-{source_flag}{seq_num:02d}\"\n",
    "                        uc['No'] = new_id\n",
    "                        \n",
    "                        # Update SQL comment if present\n",
    "                        if 'SQL' in uc and uc['SQL'] and old_id:\n",
    "                            uc['SQL'] = uc['SQL'].replace(f\"-- Use Case ID: {old_id}\", f\"-- Use Case ID: {new_id}\")\n",
    "                        \n",
    "                        renumbered_use_cases.append(uc)\n",
    "                \n",
    "                self.logger.debug(f\"Re-numbered {len(renumbered_use_cases)} use cases to match notebook prefixes.\")\n",
    "                \n",
    "                # === NEW: Filter out any use cases with \"Pending\" priority (safety check) ===\n",
    "                pending_use_cases = [uc for uc in renumbered_use_cases if uc.get('Priority') == 'Pending']\n",
    "                if pending_use_cases:\n",
    "                    self.logger.warning(f\"⚠️ Found {len(pending_use_cases)} use cases with 'Pending' priority - these will be filtered out\")\n",
    "                    for uc in pending_use_cases[:5]:  # Log first 5 for debugging\n",
    "                        self.logger.warning(f\"  - {uc.get('No', 'N/A')}: {uc.get('Name', 'N/A')}\")\n",
    "                    renumbered_use_cases = [uc for uc in renumbered_use_cases if uc.get('Priority') != 'Pending']\n",
    "                    self.logger.info(f\"✅ Filtered to {len(renumbered_use_cases)} scored use cases (removed {len(pending_use_cases)} pending)\")\n",
    "                \n",
    "                # === QUALITY FILTER: Volume-Based Filtering ===\n",
    "                # Priorities: Ultra High (6), Very High (5), High (4), Medium (3), Low (2), Very Low (1), Ultra Low (0)\n",
    "                # Rules:\n",
    "                # 1. If total > 200: Drop <= Medium (Keep High+)\n",
    "                # 2. If total > 100: Drop <= Low (Keep Medium+)\n",
    "                # 3. If total > 50: Drop <= Ultra Low (Keep Very Low+)\n",
    "                \n",
    "                total_count = len(renumbered_use_cases)\n",
    "                quality_priority_map = {\n",
    "                    \"Ultra High\": 6, \"Very High\": 5, \"High\": 4, \n",
    "                    \"Medium\": 3, \"Low\": 2, \"Very Low\": 1, \"Ultra Low\": 0, \"Pending\": -1\n",
    "                }\n",
    "                \n",
    "                min_priority_threshold = 0 # Default: Keep everything (except Pending)\n",
    "                filter_reason = \"Base (All)\"\n",
    "                \n",
    "                if total_count > 200:\n",
    "                    min_priority_threshold = 4 # Keep High (4) and above\n",
    "                    filter_reason = \"Volume > 200 (High+ only)\"\n",
    "                elif total_count > 100:\n",
    "                    min_priority_threshold = 3 # Keep Medium (3) and above\n",
    "                    filter_reason = \"Volume > 100 (Medium+ only)\"\n",
    "                elif total_count > 50:\n",
    "                    min_priority_threshold = 1 # Keep Very Low (1) and above (Drop Ultra Low 0)\n",
    "                    filter_reason = \"Volume > 50 (Very Low+ only)\"\n",
    "                \n",
    "                filtered_use_cases = [\n",
    "                    uc for uc in renumbered_use_cases \n",
    "                    if quality_priority_map.get(uc.get('Priority', 'Medium'), 3) >= min_priority_threshold\n",
    "                ]\n",
    "                \n",
    "                dropped_count = total_count - len(filtered_use_cases)\n",
    "                if dropped_count > 0:\n",
    "                    self.logger.info(f\"\uD83D\uDCC9 VOLUME FILTER: {filter_reason}. Dropped {dropped_count} low-priority use cases.\")\n",
    "                    log_print(f\"\\n\uD83D\uDCC9 VOLUME FILTER: {filter_reason}. Focusing on {len(filtered_use_cases)} higher-quality use cases (dropped {dropped_count}).\")\n",
    "                    \n",
    "                    # === RE-NORMALIZATION: Re-score relative to the new highest ===\n",
    "                    self.logger.info(\"\uD83D\uDD04 Re-normalizing scores for filtered set...\")\n",
    "                    final_consolidated_use_cases = self._normalize_priority_scores(filtered_use_cases)\n",
    "                else:\n",
    "                    self.logger.info(f\"✅ Volume filter applied ({filter_reason}), but no use cases were dropped.\")\n",
    "                    final_consolidated_use_cases = renumbered_use_cases\n",
    "                \n",
    "            else:\n",
    "                 self.logger.warning(\"No data loader. Skipping use case, PDF, and Presentation generation.\")\n",
    "                 return\n",
    "        except Exception as e:\n",
    "            self.logger.critical(f\"A critical error occurred during English generation: {e}\")\n",
    "            self.storage_manager.cleanup()  # Cleanup on error\n",
    "            AIAgent.get_summary_report()\n",
    "            return\n",
    "\n",
    "        english_grouped_data = self._group_use_cases_by_domain_flat(final_consolidated_use_cases) if final_consolidated_use_cases else {}\n",
    "        summary_dict = None\n",
    "\n",
    "        # ALWAYS Generate English Excel before SQL generation\n",
    "        if final_consolidated_use_cases:\n",
    "            self.logger.info(\"Generating English Excel before SQL generation...\")\n",
    "            lang_abbr_en = self._get_lang_abbr(\"English\")\n",
    "            try:\n",
    "                self._generate_use_case_excel(\"English\", lang_abbr_en, english_grouped_data)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to generate English Excel before SQL: {e}\")\n",
    "\n",
    "        \n",
    "        # === PHASE 2: DOMAIN-BY-DOMAIN SQL GENERATION & NOTEBOOK CREATION ===\n",
    "        # Generate summary_dict BEFORE domain-by-domain processing (needed for notebook creation)\n",
    "        if final_consolidated_use_cases and not self.json_file_path:\n",
    "            if summary_dict is None:\n",
    "                self.logger.info(\"Generating executive summaries for notebooks...\")\n",
    "                (summary_dict, _) = self._get_salesy_summary(english_grouped_data, self.business_name, \"English\", english_translations)\n",
    "        \n",
    "        # === START DOCUMENTATION GENERATION IN PARALLEL WITH SQL ===\n",
    "        # Launch PDF/PPTX generation in background thread while SQL proceeds\n",
    "        doc_generation_future = None\n",
    "        doc_generation_executor = None\n",
    "        remaining_langs = [lang for lang in self.output_languages if lang != \"English\"]\n",
    "        target_langs = [\"English\"] + remaining_langs if \"English\" in self.output_languages else remaining_langs\n",
    "        \n",
    "        if final_consolidated_use_cases and target_langs and (\"PDF Catalog\" in self.generate_choices or \"Presentation\" in self.generate_choices):\n",
    "            self.logger.info(\"\uD83D\uDE80 Starting PDF/PPTX documentation generation in parallel with SQL...\")\n",
    "            log_print(f\"\uD83D\uDCC4 Documentation generation starting in background (languages: {', '.join(target_langs)})\")\n",
    "            doc_generation_executor = concurrent.futures.ThreadPoolExecutor(max_workers=1, thread_name_prefix=\"DocGen\")\n",
    "            doc_generation_future = doc_generation_executor.submit(\n",
    "                self._generate_documents_for_all_languages,\n",
    "                final_consolidated_use_cases,\n",
    "                english_grouped_data,\n",
    "                summary_dict,\n",
    "                target_langs,\n",
    "                [\"English\"]  # skip_excel_langs - already generated\n",
    "            )\n",
    "        \n",
    "        self.logger.info(f\"\uD83D\uDD04 PHASE 2: Domain-by-domain SQL generation & notebook creation...\")\n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"\uD83D\uDD27 PHASE 2: DOMAIN-BY-DOMAIN SQL & NOTEBOOKS\")\n",
    "        log_print(f\"{'='*80}\")\n",
    "        log_print(f\"Total use cases: {len(final_consolidated_use_cases)}\")\n",
    "        log_print(f\"Parallel workers: {self.max_parallelism}\")\n",
    "        log_print(f\"Strategy: Generate SQL per domain → Create notebook → Move to next domain\")\n",
    "        log_print(f\"Order: Smallest domains first (for quick demo testing)\")\n",
    "        log_print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Domain-by-domain: Generate SQL and create notebook for each domain immediately\n",
    "        # Domains are processed in order of use case count (smallest first) for quick testing\n",
    "        if final_consolidated_use_cases and not self.json_file_path:\n",
    "            final_consolidated_use_cases = self._generate_sql_and_notebooks_by_domain(\n",
    "                final_consolidated_use_cases,\n",
    "                all_columns_for_sql,\n",
    "                unstructured_docs_markdown,\n",
    "                english_translations,\n",
    "                summary_dict\n",
    "            )\n",
    "        else:\n",
    "            # Fallback to parallel SQL generation if JSON mode (no notebooks needed)\n",
    "            final_consolidated_use_cases = self._generate_sql_parallel(\n",
    "                final_consolidated_use_cases,\n",
    "                all_columns_for_sql,\n",
    "                unstructured_docs_markdown\n",
    "            )\n",
    "        self.logger.info(\"✅ Phase 2 complete: All domains processed (SQL + Notebooks)\")\n",
    "        \n",
    "        # === WAIT FOR DOCUMENTATION GENERATION TO COMPLETE ===\n",
    "        if doc_generation_future:\n",
    "            try:\n",
    "                self.logger.info(\"⏳ Waiting for documentation generation to complete...\")\n",
    "                doc_generation_future.result(timeout=1800)  # 30 minute timeout\n",
    "                self.logger.info(\"✅ Documentation generation completed\")\n",
    "                log_print(\"✅ Documentation generation completed\")\n",
    "            except concurrent.futures.TimeoutError:\n",
    "                self.logger.warning(\"⚠️ Documentation generation timed out after 30 minutes\")\n",
    "                log_print(\"⚠️ Documentation generation timed out\", level=\"WARNING\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"❌ Documentation generation failed: {e}\")\n",
    "                log_print(f\"❌ Documentation generation failed: {str(e)[:100]}\", level=\"ERROR\")\n",
    "            finally:\n",
    "                if doc_generation_executor:\n",
    "                    doc_generation_executor.shutdown(wait=False)\n",
    "\n",
    "        # === POPULATE PRIMARY TABLE (Analytics Technique comes from LLM during use case generation) ===\n",
    "        if final_consolidated_use_cases:\n",
    "            for uc in final_consolidated_use_cases:\n",
    "                # Analytics Technique is now generated by LLM during use case creation\n",
    "                # Only set a default if missing (for legacy use cases)\n",
    "                if not uc.get('Analytics Technique') or uc.get('Analytics Technique') == 'N/A':\n",
    "                    uc['Analytics Technique'] = 'AI Analysis'  # Default fallback\n",
    "                \n",
    "                # Extract Primary Table from Tables Involved\n",
    "                tables_involved = uc.get('Tables Involved', '')\n",
    "                uc['Primary Table'] = self._extract_primary_table(tables_involved)\n",
    "            \n",
    "            self.logger.info(f\"✅ Populated Primary Table for {len(final_consolidated_use_cases)} use cases\")\n",
    "\n",
    "        # === SAVE JSON CATALOG (POST-SQL) ===\n",
    "        if final_consolidated_use_cases and not self.json_file_path:\n",
    "            self.logger.info(\"Saving JSON Catalog with generated SQL and columns...\")\n",
    "            summary_dict = self._save_usecases_catalog_json(final_consolidated_use_cases, english_translations, summary_dict)\n",
    "        \n",
    "        # Note: PDF/PPTX documentation generation was started in parallel earlier\n",
    "        # Only generate here if parallel generation was NOT started (no PDF/Presentation selected initially)\n",
    "        if final_consolidated_use_cases and not doc_generation_future:\n",
    "            remaining_langs = [lang for lang in self.output_languages if lang != \"English\"]\n",
    "            target_langs = [\"English\"] + remaining_langs if \"English\" in self.output_languages else remaining_langs\n",
    "            if target_langs and (\"PDF Catalog\" in self.generate_choices or \"Presentation\" in self.generate_choices):\n",
    "                if summary_dict is None:\n",
    "                    (summary_dict, _) = self._get_salesy_summary(english_grouped_data, self.business_name, \"English\", english_translations)\n",
    "                self.logger.info(\"Generating PDFs/Presentations and translations...\")\n",
    "                self._generate_documents_for_all_languages(\n",
    "                    final_consolidated_use_cases,\n",
    "                    english_grouped_data=english_grouped_data,\n",
    "                    summary_dict=summary_dict,\n",
    "                    languages=target_langs,\n",
    "                    skip_excel_langs=[\"English\"]\n",
    "                )\n",
    "        \n",
    "        # Report table inclusion/exclusion statistics\n",
    "        if final_consolidated_use_cases and self.data_loader:\n",
    "            self._report_table_statistics(final_consolidated_use_cases)\n",
    "        \n",
    "        # Cleanup intermediate storage\n",
    "        self.storage_manager.cleanup()\n",
    "        \n",
    "        # Upload log file and show summary BEFORE final success message\n",
    "        self.logger.info(f\"✅ All Use cases for {self.business_name} generated successfully\")\n",
    "        self.logger.info(\"Uploading log file...\")\n",
    "        self._upload_log_file()\n",
    "        \n",
    "        # Show processing honesty report\n",
    "        self._report_processing_honesty()\n",
    "        \n",
    "        # Show AI usage summary\n",
    "        AIAgent.get_summary_report()\n",
    "        \n",
    "        # Final success message with green checkmark - THIS MUST BE THE LAST OUTPUT\n",
    "        log_print(f\"✅ All Use cases for {self.business_name} generated successfully\")\n",
    "            \n",
    "\n",
    "    def _generate_unstructured_docs(self, combined_schema_markdown: str) -> dict:\n",
    "        \"\"\"\n",
    "        Generates unstructured doc list using fallback approach.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary containing:\n",
    "                - unstructured_docs_markdown: str\n",
    "                - strategic_goals: list\n",
    "                - business_context: str\n",
    "                - business_priorities: list\n",
    "                - strategic_initiative: str\n",
    "                - value_chain: str\n",
    "                - revenue_model: str\n",
    "        \"\"\"\n",
    "        if not combined_schema_markdown:\n",
    "            self.logger.warning(\"No schema markdown provided, cannot generate unstructured docs list.\")\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            self.logger.info(\"Generating fallback unstructured document list...\")\n",
    "            fallback_context = {\n",
    "                \"unstructured_docs_markdown\": self._generate_fallback_unstructured_docs(),\n",
    "                \"strategic_goals\": [],\n",
    "                \"business_context\": \"General business operations\",\n",
    "                \"business_priorities\": [],\n",
    "                \"strategic_initiative\": \"Data-driven transformation\",\n",
    "                \"value_chain\": \"Standard business operations\",\n",
    "                \"revenue_model\": \"Product and service sales\"\n",
    "            }\n",
    "            return fallback_context\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to generate unstructured document list: {e}. Proceeding with empty list.\")\n",
    "            fallback_context = {\n",
    "                \"unstructured_docs_markdown\": \"\",\n",
    "                \"strategic_goals\": [],\n",
    "                \"business_context\": \"General business operations\",\n",
    "                \"business_priorities\": [],\n",
    "                \"strategic_initiative\": \"Data-driven transformation\",\n",
    "                \"value_chain\": \"Standard business operations\",\n",
    "                \"revenue_model\": \"Product and service sales\"\n",
    "            }\n",
    "            return fallback_context\n",
    "    \n",
    "    def _generate_fallback_unstructured_docs(self) -> str:\n",
    "        \"\"\"Fallback: Generate a minimal set of generic documents.\"\"\"\n",
    "        return \"\"\"| \"Document Name\" | \"Description\" | \"Type\" | \"Extracted Entities\" | \"File Path\" |\n",
    "|---|---|---|---|---|\n",
    "| \"Business Invoices\" | \"PDF invoices from vendors\" | \"PDF\" | \"vendor_name, invoice_number, amount, date\" | \"/Volumes/finance/invoices/\" |\n",
    "| \"Customer Emails\" | \"Email correspondence with customers\" | \"DOCX\" | \"customer_name, subject, date, sentiment\" | \"/Volumes/communications/emails/\" |\n",
    "| \"Product Images\" | \"Product photography and diagrams\" | \"JPG\" | \"product_id, image_type\" | \"/Volumes/products/images/\" |\n",
    "\"\"\"\n",
    "\n",
    "    # === MODIFIED: _get_salesy_summary (Req 2) ===\n",
    "    def _get_salesy_summary(self, grouped_data: dict, business_name: str, language: str, translations: dict) -> tuple:\n",
    "        self.logger.debug(f\"Calling LLM for executive and domain summaries in {language}...\")\n",
    "        t = translations\n",
    "        summary_dict = {}\n",
    "        transliterated_name = business_name # Default\n",
    "        try:\n",
    "            domain_list_for_prompt = \"\\n\".join([f\"- {domain}\" for domain in grouped_data.keys()])\n",
    "            total_cases = sum(len(cases) for cases in grouped_data.values())\n",
    "            prompt_vars = {\n",
    "                \"business_name\": business_name, \"total_cases\": str(total_cases),\n",
    "                \"domain_list\": domain_list_for_prompt, \"output_language\": language\n",
    "            }\n",
    "            summary_csv_raw = self.ai_agent.run_worker(\n",
    "                step_name=f\"Executive_Summary_{language}\",\n",
    "                worker_prompt_path=\"SUMMARY_GEN_PROMPT\",\n",
    "                prompt_vars=prompt_vars, response_schema=None\n",
    "            )\n",
    "            self.logger.info(f\"LLM summaries (CSV) received for {language}.\")\n",
    "            \n",
    "            # === ROBUST CSV PARSING (Req 2) ===\n",
    "            # Support both quoted and unquoted headers from LLM\n",
    "            header_3_col_quoted = '\"Type\",\"Summary\",\"TransliteratedBusinessName\"'\n",
    "            header_3_col_unquoted = 'Type,Summary,TransliteratedBusinessName'\n",
    "            header_2_col_quoted = '\"Type\",\"Summary\"'\n",
    "            header_2_col_unquoted = 'Type,Summary'\n",
    "            \n",
    "            header_start_index = summary_csv_raw.find(header_3_col_quoted)\n",
    "            is_3_col = True\n",
    "            \n",
    "            if header_start_index == -1:\n",
    "                header_start_index = summary_csv_raw.find(header_3_col_unquoted)\n",
    "            \n",
    "            if header_start_index == -1:\n",
    "                self.logger.warning(f\"Could not find 3-column CSV header in LLM response for {language}. Attempting 2-col parse.\")\n",
    "                is_3_col = False\n",
    "                header_start_index = summary_csv_raw.find(header_2_col_quoted)\n",
    "                if header_start_index == -1:\n",
    "                    header_start_index = summary_csv_raw.find(header_2_col_unquoted)\n",
    "                if header_start_index == -1:\n",
    "                    self.logger.error(f\"Could not find 2-column or 3-column CSV header. Aborting summary parse. Response: {summary_csv_raw[:200]}\")\n",
    "                    raise ValueError(f\"Could not parse summary CSV for {language}\")\n",
    "\n",
    "            self.logger.info(f\"Found CSV header at index {header_start_index}. Parsing as {3 if is_3_col else 2}-column.\")\n",
    "            summary_csv_clean = summary_csv_raw[header_start_index:]\n",
    "            # ==================================\n",
    "\n",
    "            # Use centralized CSV parser\n",
    "            csv_rows = CSVParser.parse_csv_list(\n",
    "                summary_csv_clean,\n",
    "                logger=self.logger,\n",
    "                context=\"Domain summary\",\n",
    "                delimiter=',',\n",
    "                quotechar='\"',\n",
    "                quoting=csv.QUOTE_ALL,\n",
    "                skipinitialspace=True\n",
    "            )\n",
    "            if csv_rows:\n",
    "                header = csv_rows[0]  # First row is header\n",
    "                csv_reader = csv_rows[1:]  # Rest are data rows\n",
    "            else:\n",
    "                csv_reader = []\n",
    "            \n",
    "            if is_3_col:\n",
    "                for row in csv_reader:\n",
    "                    # Handle rows that may span multiple lines or have embedded content\n",
    "                    if len(row) >= 3:\n",
    "                        row_type = row[0].strip()\n",
    "                        summary_dict[row_type] = row[1].strip()\n",
    "                        if row_type == \"Executive\" and row[2].strip():\n",
    "                            transliterated_name = row[2].strip()\n",
    "                    elif len(row) > 0:\n",
    "                        # Partial row - try to accumulate\n",
    "                        self.logger.debug(f\"Partial 3-col row (len={len(row)}): {row[:1]}\")\n",
    "                        # Skip partial rows gracefully without warning\n",
    "            else: # 2-col fallback\n",
    "                for row in csv_reader:\n",
    "                    if len(row) >= 2:\n",
    "                        summary_dict[row[0].strip()] = row[1].strip()\n",
    "                    elif len(row) > 0:\n",
    "                        self.logger.debug(f\"Partial 2-col row (len={len(row)}): {row[:1]}\")\n",
    "            transliterated_name = business_name if not is_3_col else transliterated_name\n",
    "            \n",
    "            if \"Executive\" not in summary_dict:\n",
    "                self.logger.error(\"Failed to parse 'Executive' summary from LLM response.\")\n",
    "                raise ValueError(\"Missing 'Executive' summary\")\n",
    "                \n",
    "            self.logger.info(f\"Successfully parsed {len(summary_dict)} summaries for {language}. Transliterated name: {transliterated_name}\")\n",
    "            return summary_dict, transliterated_name\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"LLM summary generation failed for {language}: {e}. Using default text.\")\n",
    "            fallback_dict = {}\n",
    "            total_cases_fallback = sum(len(cases) for cases in grouped_data.values())\n",
    "            p1 = t['pdf_fallback_summary_p1'].format(total_cases=total_cases_fallback, business_name=business_name)\n",
    "            p2 = t['pdf_fallback_summary_p2']\n",
    "            fallback_dict[\"Executive\"] = f\"<p>{p1}</p><p>{p2}</p>\"\n",
    "            for domain in grouped_data.keys():\n",
    "                fallback_dict[domain] = \"<p>Summary generation failed. This domain's key responsibilities and opportunities have been identified for AI transformation.</p>\"\n",
    "            return fallback_dict, business_name # Return default name\n",
    "\n",
    "    def _merge_small_domains(self, use_cases: list, min_cases_per_domain: int = 4) -> list:\n",
    "        \"\"\"\n",
    "        Merges business domains that have fewer than min_cases_per_domain use cases.\n",
    "        Provides domain counts to LLM and asks for better merged domain names.\n",
    "        Default minimum is now 4 use cases per domain.\n",
    "        \"\"\"\n",
    "        if not use_cases:\n",
    "            return use_cases\n",
    "        \n",
    "        # Count use cases per domain\n",
    "        domain_counts = defaultdict(int)\n",
    "        for uc in use_cases:\n",
    "            domain = uc.get('Business Domain', 'Other')\n",
    "            domain_counts[domain] += 1\n",
    "        \n",
    "        # Identify small domains\n",
    "        small_domains = {domain: count for domain, count in domain_counts.items() if count < min_cases_per_domain}\n",
    "        \n",
    "        if not small_domains:\n",
    "            self.logger.info(f\"All domains have at least {min_cases_per_domain} use cases. No merging needed.\")\n",
    "            return use_cases\n",
    "        \n",
    "        self.logger.debug(f\"Found {len(small_domains)} domains with fewer than {min_cases_per_domain} use cases: {list(small_domains.keys())}\")\n",
    "        \n",
    "        # Build domain info for LLM including counts\n",
    "        domain_info_lines = []\n",
    "        for domain, count in sorted(domain_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            status = \"✓ OK\" if count >= min_cases_per_domain else f\"❌ TOO SMALL (needs merging)\"\n",
    "            domain_info_lines.append(f\"  - {domain}: {count} use cases {status}\")\n",
    "        \n",
    "        domain_info_str = \"\\n\".join(domain_info_lines)\n",
    "        \n",
    "        # Create merge prompt with domain counts\n",
    "        merge_prompt = f\"\"\"You are a business domain expert. Analyze this list of business domains and their use case counts:\n",
    "\n",
    "{domain_info_str}\n",
    "\n",
    "**CRITICAL REQUIREMENT**: Each domain MUST have at least {min_cases_per_domain} use cases or it will be merged into a larger, related domain.\n",
    "\n",
    "**Your Task**:\n",
    "1. Identify domains with fewer than {min_cases_per_domain} use cases (marked with ❌)\n",
    "2. For each small domain, determine which larger domain (marked with ✓) it should be merged into based on semantic similarity\n",
    "3. Come up with a BETTER, more comprehensive name for merged domains that reflects the combined scope\n",
    "4. If merging creates a new combined domain, ensure the name is professional and encompasses both areas\n",
    "\n",
    "**Output Format** (honesty-wrapped JSON):\n",
    "Your response MUST be wrapped: {{{{\"honesty_score\": XX, \"honesty_justification\": \"...\", \"data\": <your_mapping>}}}}\n",
    "\n",
    "Example of wrapped output:\n",
    "{{{{{{{{\n",
    "  \"honesty_score\": 95,\n",
    "  \"honesty_justification\": \"Merged domains based on semantic similarity with high confidence.\",\n",
    "  \"data\": {{{{{{{{\n",
    "    \"Customer Service\": \"Customer Experience & Support\",\n",
    "    \"Billing\": \"Finance & Revenue Management\"\n",
    "  }}}}}}}}\n",
    "}}}}}}}}\n",
    "\n",
    "**Rules**:\n",
    "1. Domains with < {min_cases_per_domain} use cases MUST be merged\n",
    "2. Only merge semantically related domains\n",
    "3. Prefer descriptive, professional domain names\n",
    "4. If creating a new combined name, make it comprehensive\n",
    "5. Domains with ≥ {min_cases_per_domain} can stay as-is UNLESS you have a significantly better name\n",
    "6. Domain naming rules STILL APPLY: every domain name must be EXACTLY ONE WORD (no spaces), unique core word, industry-specific\n",
    "\n",
    "Start your response with: {{{{\"honesty_score\":\n",
    "\"\"\" + HONESTY_CHECK_JSON\n",
    "        \n",
    "        try:\n",
    "            # Use run_worker with a direct prompt\n",
    "            merge_prompt_key = \"DOMAINS_MERGER_PROMPT\"\n",
    "            self.ai_agent.prompt_templates[merge_prompt_key] = merge_prompt\n",
    "            \n",
    "            response = self.ai_agent.run_worker(\n",
    "                step_name=f\"Merge_Small_Domains\",\n",
    "                worker_prompt_path=merge_prompt_key,\n",
    "                prompt_vars={},\n",
    "                response_schema=None\n",
    "            )\n",
    "            \n",
    "            # Clean up temporary prompt template\n",
    "            del self.ai_agent.prompt_templates[merge_prompt_key]\n",
    "            \n",
    "            merge_mapping = json.loads(clean_json_response(response))\n",
    "            \n",
    "            # Extract data from honesty-wrapped response if present\n",
    "            if isinstance(merge_mapping, dict) and 'data' in merge_mapping:\n",
    "                merge_mapping = merge_mapping['data']\n",
    "            \n",
    "            # Ensure merge_mapping is a dict with string keys/values\n",
    "            if not isinstance(merge_mapping, dict):\n",
    "                self.logger.warning(f\"Domain merge response is not a dict: {type(merge_mapping)}. Skipping merge.\")\n",
    "                return use_cases\n",
    "            \n",
    "            # Resolve transitive merges (e.g., A->B, B->C  =>  A->C, B->C)\n",
    "            # This handles cases where a domain is renamed, and another domain is merged into the OLD name\n",
    "            # or where multiple merges happen in a chain.\n",
    "            for _ in range(5):  # Max depth 5 to prevent infinite loops\n",
    "                updated = False\n",
    "                for key, val in merge_mapping.items():\n",
    "                    if val in merge_mapping and merge_mapping[val] != val:\n",
    "                         # Update target to the final destination\n",
    "                         merge_mapping[key] = merge_mapping[val]\n",
    "                         updated = True\n",
    "                if not updated:\n",
    "                    break\n",
    "            \n",
    "            # Apply merge mapping to use cases\n",
    "            merged_count = 0\n",
    "            for uc in use_cases:\n",
    "                old_domain = uc.get('Business Domain', 'Other')\n",
    "                if old_domain in merge_mapping:\n",
    "                    new_domain = merge_mapping[old_domain]\n",
    "                    if old_domain != new_domain:\n",
    "                        self.logger.info(f\"Merging '{old_domain}' ({domain_counts[old_domain]} cases) → '{new_domain}'\")\n",
    "                        uc['Business Domain'] = new_domain\n",
    "                        merged_count += 1\n",
    "            \n",
    "            self.logger.debug(f\"Domain merging complete. {merged_count} use cases reassigned.\")\n",
    "            \n",
    "            # Verify all domains now have at least min_cases_per_domain\n",
    "            final_counts = defaultdict(int)\n",
    "            for uc in use_cases:\n",
    "                final_counts[uc.get('Business Domain', 'Other')] += 1\n",
    "            \n",
    "            remaining_small = [d for d, count in final_counts.items() if count < min_cases_per_domain]\n",
    "            if remaining_small:\n",
    "                self.logger.warning(f\"WARNING: {len(remaining_small)} domains still have fewer than {min_cases_per_domain} use cases: {remaining_small}\")\n",
    "            else:\n",
    "                self.logger.debug(f\"SUCCESS: All domains now have at least {min_cases_per_domain} use cases.\")\n",
    "            \n",
    "            return use_cases\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to merge domains: {e}\")\n",
    "            return use_cases  # Return original if merge fails\n",
    "\n",
    "    def _cluster_domains_and_subdomains(self, use_cases: list, language: str) -> list:\n",
    "        \"\"\"\n",
    "        Cluster use cases into appropriate domains and subdomains using LLM TWO-STEP approach:\n",
    "        Step 1: Assign domains to all use cases\n",
    "        Step 2: For each domain, assign subdomains in parallel\n",
    "        \n",
    "        Args:\n",
    "            use_cases: List of use case dictionaries\n",
    "            language: Output language\n",
    "            \n",
    "        Returns:\n",
    "            List of use cases with updated domains and subdomains\n",
    "        \"\"\"\n",
    "        from collections import defaultdict\n",
    "        import io\n",
    "        import csv\n",
    "        from concurrent.futures import ThreadPoolExecutor\n",
    "        import concurrent.futures\n",
    "        \n",
    "        self.logger.info(f\"\uD83C\uDFAF Starting TWO-STEP domain/subdomain clustering for {len(use_cases)} use cases...\")\n",
    "        \n",
    "        if not use_cases:\n",
    "            return use_cases\n",
    "        \n",
    "        # === USER-PROVIDED BUSINESS DOMAINS ENFORCEMENT ===\n",
    "        # If user provided business domains, we MUST force all use cases to align to those domains only\n",
    "        if self.user_business_domains and len(self.user_business_domains) > 0:\n",
    "            self.logger.info(f\"\uD83D\uDEA8 USER-PROVIDED DOMAINS DETECTED: Forcing use cases to align ONLY to: {', '.join(self.user_business_domains)}\")\n",
    "            log_print(f\"\\n\uD83D\uDEA8 USER-PROVIDED DOMAINS: Aligning all use cases to: {', '.join(self.user_business_domains)}\")\n",
    "            \n",
    "            # Use LLM to intelligently assign use cases to user-provided domains\n",
    "            return self._assign_to_user_domains(use_cases, self.user_business_domains, language)\n",
    "        \n",
    "        # === NOTE: Removed legacy fallback - always use two-step approach ===\n",
    "        # The two-step approach (DOMAIN_FINDER_PROMPT + SUBDOMAIN_DETECTOR_PROMPT) \n",
    "        # provides better quality and more consistent results than the old single-step approach\n",
    "        if len(use_cases) > 250:\n",
    "            self.logger.info(\n",
    "                f\"\uD83D\uDCCA Large use case set detected ({len(use_cases)} use cases). \"\n",
    "                f\"Using two-step clustering with parallel subdomain detection for optimal quality.\"\n",
    "            )\n",
    "        \n",
    "        # === STEP 1: DOMAIN DETECTION ===\n",
    "        \n",
    "        try:\n",
    "            # Convert use cases to CSV for LLM (without Business Domain and Subdomain since those will be detected)\n",
    "            output = io.StringIO()\n",
    "            if use_cases:\n",
    "                fieldnames = ['No', 'Name', 'type', 'Analytics Technique', 'Statement', 'Solution', \n",
    "                             'Business Value', 'Beneficiary', 'Sponsor', \n",
    "                             'Tables Involved']\n",
    "                writer = csv.DictWriter(output, fieldnames=fieldnames, extrasaction='ignore')\n",
    "                writer.writeheader()\n",
    "                writer.writerows(use_cases)\n",
    "            use_cases_csv = output.getvalue()\n",
    "            \n",
    "            # Check context size for domain detection (uses model-specific limits from TECHNICAL_CONTEXT)\n",
    "            prompt_template = self.ai_agent.prompt_templates.get(\"DOMAIN_FINDER_PROMPT\", \"\")\n",
    "            estimated_size = len(prompt_template) + len(use_cases_csv) + 1000\n",
    "            MAX_CONTEXT_CHARS = get_max_context_chars(language, \"DOMAIN_FINDER_PROMPT\")\n",
    "            \n",
    "            if estimated_size > MAX_CONTEXT_CHARS:\n",
    "                # === BATCHED DOMAIN DETECTION: Process in smaller chunks ===\n",
    "                self.logger.warning(\n",
    "                    f\"Domain detection prompt size ({estimated_size:,} chars) exceeds MAX_CONTEXT_CHARS ({MAX_CONTEXT_CHARS:,}). \"\n",
    "                    f\"Using BATCHED domain detection to process {len(use_cases)} use cases in smaller chunks.\"\n",
    "                )\n",
    "                \n",
    "                # Calculate batch size based on available context space\n",
    "                prompt_overhead = len(prompt_template) + 5000  # Buffer for prompt template + response\n",
    "                available_chars = MAX_CONTEXT_CHARS - prompt_overhead\n",
    "                \n",
    "                # Estimate chars per use case from the CSV\n",
    "                chars_per_use_case = len(use_cases_csv) / len(use_cases) if use_cases else 500\n",
    "                batch_size = max(50, int(available_chars / chars_per_use_case * 0.7))  # 70% safety margin\n",
    "                \n",
    "                self.logger.info(f\"\uD83D\uDCE6 BATCHED DOMAIN DETECTION: Processing {len(use_cases)} use cases in batches of ~{batch_size}\")\n",
    "                \n",
    "                # Process use cases in batches\n",
    "                batched_domain_assignments = []\n",
    "                all_discovered_domains = set()\n",
    "                \n",
    "                for batch_idx in range(0, len(use_cases), batch_size):\n",
    "                    batch_use_cases = use_cases[batch_idx:batch_idx + batch_size]\n",
    "                    batch_num = (batch_idx // batch_size) + 1\n",
    "                    total_batches = (len(use_cases) + batch_size - 1) // batch_size\n",
    "                    \n",
    "                    self.logger.info(f\"\uD83D\uDCCD BATCH {batch_num}/{total_batches}: Processing {len(batch_use_cases)} use cases for domain detection...\")\n",
    "                    \n",
    "                    try:\n",
    "                        # Create CSV for this batch\n",
    "                        batch_output = io.StringIO()\n",
    "                        batch_writer = csv.DictWriter(batch_output, fieldnames=fieldnames, extrasaction='ignore')\n",
    "                        batch_writer.writeheader()\n",
    "                        batch_writer.writerows(batch_use_cases)\n",
    "                        batch_csv = batch_output.getvalue()\n",
    "                        \n",
    "                        # Include previously discovered domains as context for consistency\n",
    "                        domain_context = \"\"\n",
    "                        if all_discovered_domains:\n",
    "                            domain_context = f\"\\n\\n**PREVIOUSLY DISCOVERED DOMAINS (reuse these where appropriate):**\\n{', '.join(sorted(all_discovered_domains))}\\n\"\n",
    "                        \n",
    "                        batch_prompt_vars = {\n",
    "                            \"use_cases_csv\": batch_csv,\n",
    "                            \"output_language\": language,\n",
    "                            \"business_name\": self.business_name,\n",
    "                            \"industries\": \", \".join(self.industries) if hasattr(self, 'industries') and self.industries else \"General Business\",\n",
    "                            \"business_context\": getattr(self, 'business_context', \"General business operations\") + domain_context,\n",
    "                            \"previous_violations\": \"\"\n",
    "                        }\n",
    "                        \n",
    "                        # Call LLM for this batch\n",
    "                        batch_response = self.ai_agent.run_worker(\n",
    "                            step_name=f\"Detect_Domains_Batch{batch_num}_{language}\",\n",
    "                            worker_prompt_path=\"DOMAIN_FINDER_PROMPT\",\n",
    "                            prompt_vars=batch_prompt_vars,\n",
    "                            response_schema=None\n",
    "                        )\n",
    "                        \n",
    "                        if batch_response and batch_response.strip():\n",
    "                            batch_response_clean = clean_json_response(batch_response)\n",
    "                            batch_csv_rows = CSVParser.parse_csv_string(\n",
    "                                batch_response_clean,\n",
    "                                logger=self.logger,\n",
    "                                context=f\"Domain detection batch {batch_num}\"\n",
    "                            )\n",
    "                            \n",
    "                            # Apply domain assignments to batch use cases\n",
    "                            batch_domain_map = {}\n",
    "                            for row in batch_csv_rows:\n",
    "                                uc_id_raw = row.get('use_case_id', '') or ''\n",
    "                                domain_raw = row.get('domain', '') or ''\n",
    "                                uc_id = uc_id_raw.strip() if isinstance(uc_id_raw, str) else str(uc_id_raw).strip()\n",
    "                                domain = domain_raw.strip() if isinstance(domain_raw, str) else str(domain_raw).strip()\n",
    "                                if uc_id and domain:\n",
    "                                    batch_domain_map[uc_id] = domain\n",
    "                                    all_discovered_domains.add(domain)\n",
    "                            \n",
    "                            # Apply to batch use cases\n",
    "                            for uc in batch_use_cases:\n",
    "                                uc_copy = uc.copy()\n",
    "                                uc_id = uc_copy.get('No', '')\n",
    "                                if uc_id in batch_domain_map:\n",
    "                                    uc_copy['Business Domain'] = batch_domain_map[uc_id]\n",
    "                                else:\n",
    "                                    # Assign to most common domain in batch as fallback\n",
    "                                    if batch_domain_map:\n",
    "                                        from collections import Counter\n",
    "                                        most_common = Counter(batch_domain_map.values()).most_common(1)[0][0]\n",
    "                                        uc_copy['Business Domain'] = most_common\n",
    "                                    else:\n",
    "                                        uc_copy['Business Domain'] = 'Uncategorized'\n",
    "                                uc_copy['Subdomain'] = ''\n",
    "                                batched_domain_assignments.append(uc_copy)\n",
    "                            \n",
    "                            self.logger.info(f\"✅ BATCH {batch_num}/{total_batches}: Assigned domains to {len(batch_use_cases)} use cases. Discovered domains so far: {len(all_discovered_domains)}\")\n",
    "                        else:\n",
    "                            # Fallback for failed batch\n",
    "                            self.logger.warning(f\"⚠️ BATCH {batch_num}: Empty response, using fallback domains\")\n",
    "                            for uc in batch_use_cases:\n",
    "                                uc_copy = uc.copy()\n",
    "                                uc_copy['Business Domain'] = 'Uncategorized'\n",
    "                                uc_copy['Subdomain'] = ''\n",
    "                                batched_domain_assignments.append(uc_copy)\n",
    "                                \n",
    "                    except Exception as batch_err:\n",
    "                        self.logger.error(f\"❌ BATCH {batch_num} failed: {batch_err}. Using fallback domains for this batch.\")\n",
    "                        for uc in batch_use_cases:\n",
    "                            uc_copy = uc.copy()\n",
    "                            uc_copy['Business Domain'] = 'Uncategorized'\n",
    "                            uc_copy['Subdomain'] = ''\n",
    "                            batched_domain_assignments.append(uc_copy)\n",
    "                \n",
    "                self.logger.info(f\"\uD83D\uDCE6 BATCHED DOMAIN DETECTION COMPLETE: {len(batched_domain_assignments)} use cases assigned to {len(all_discovered_domains)} domains\")\n",
    "                \n",
    "                # Use the batched results and continue to domain merging (Step 1.5)\n",
    "                # The domain merging will consolidate similar domains across batches\n",
    "                domain_assignments = batched_domain_assignments\n",
    "                \n",
    "                # Skip the single-batch domain detection below, jump to Step 1.5\n",
    "                # === STEP 1.5: DOMAIN MERGING (MERGE SMALL/SIMILAR DOMAINS) ===\n",
    "                self.logger.info(\"\uD83D\uDCCD STEP 1.5: Merging small/similar domains from batched detection...\")\n",
    "                domain_assignments = self._merge_small_domains(domain_assignments, min_cases_per_domain=4)\n",
    "                \n",
    "                # === STEP 2: SUBDOMAIN DETECTION (PARALLEL FOR EACH DOMAIN) ===\n",
    "                self.logger.info(f\"\uD83D\uDCCD STEP 2: Detecting subdomains for each domain in parallel...\")\n",
    "                \n",
    "                # Group use cases by domain\n",
    "                domain_usecases_map = defaultdict(list)\n",
    "                for uc in domain_assignments:\n",
    "                    domain = uc.get('Business Domain', '').strip()\n",
    "                    if domain:\n",
    "                        domain_usecases_map[domain].append(uc)\n",
    "                \n",
    "                # ADAPTIVE PARALLELISM: Calculate based on domains and use cases\n",
    "                subdomain_parallelism, reason = calculate_adaptive_parallelism(\n",
    "                    \"subdomain_detection\", self.max_parallelism,\n",
    "                    num_items=len(domain_assignments),\n",
    "                    num_domains=len(domain_usecases_map),\n",
    "                    is_llm_operation=True, logger=self.logger\n",
    "                )\n",
    "                log_adaptive_parallelism_decision(\"subdomain_detection\", subdomain_parallelism, self.max_parallelism, reason)\n",
    "                self.logger.info(f\"Processing {len(domain_usecases_map)} domains for subdomain detection...\")\n",
    "                \n",
    "                # Process each domain in parallel\n",
    "                final_use_cases_with_subdomains = []\n",
    "                \n",
    "                with ThreadPoolExecutor(max_workers=subdomain_parallelism, \n",
    "                                       thread_name_prefix=\"SubdomainDetect\") as executor:\n",
    "                    future_to_domain = {}\n",
    "                    for domain_name, domain_use_cases in domain_usecases_map.items():\n",
    "                        future = executor.submit(\n",
    "                            self._detect_subdomains_for_domain,\n",
    "                            domain_name,\n",
    "                            domain_use_cases,\n",
    "                            language\n",
    "                        )\n",
    "                        future_to_domain[future] = domain_name\n",
    "                    \n",
    "                    for future in concurrent.futures.as_completed(future_to_domain):\n",
    "                        domain_name = future_to_domain[future]\n",
    "                        try:\n",
    "                            use_cases_with_subdomains = future.result()\n",
    "                            if use_cases_with_subdomains:\n",
    "                                final_use_cases_with_subdomains.extend(use_cases_with_subdomains)\n",
    "                            else:\n",
    "                                domain_use_cases = domain_usecases_map.get(domain_name, [])\n",
    "                                final_use_cases_with_subdomains.extend(domain_use_cases)\n",
    "                        except Exception as e:\n",
    "                            self.logger.error(f\"❌ Domain '{domain_name}': Subdomain detection failed: {e}\")\n",
    "                            domain_use_cases = domain_usecases_map.get(domain_name, [])\n",
    "                            final_use_cases_with_subdomains.extend(domain_use_cases)\n",
    "                \n",
    "                self.logger.info(f\"✅ BATCHED two-step clustering complete! {len(final_use_cases_with_subdomains)} use cases processed across {len(domain_usecases_map)} domains\")\n",
    "                return final_use_cases_with_subdomains\n",
    "            \n",
    "            # Warn if prompt is very large (might cause slowness)\n",
    "            if estimated_size > MAX_CONTEXT_CHARS * 0.7:\n",
    "                self.logger.warning(\n",
    "                    f\"⚠️ Domain detection prompt is very large ({estimated_size:,} chars, {len(use_cases)} use cases). \"\n",
    "                    f\"This may take 2-5 minutes to process. Please be patient...\"\n",
    "                )\n",
    "                log_print(f\"\\n⏳ Processing {len(use_cases)} use cases for domain detection...\")\n",
    "                log_print(f\"   Prompt size: {estimated_size:,} characters\")\n",
    "                log_print(f\"   This may take 2-5 minutes. Please wait...\\n\")\n",
    "            \n",
    "            # Call LLM for domain detection (respect global retry cap)\n",
    "            max_attempts = (getattr(self, \"max_retry_attempts\", 1) or 0) + 1\n",
    "            prompt_vars = {\n",
    "                \"use_cases_csv\": use_cases_csv,\n",
    "                \"output_language\": language,\n",
    "                \"business_name\": self.business_name,\n",
    "                \"industries\": \", \".join(self.industries) if hasattr(self, 'industries') and self.industries else \"General Business\",\n",
    "                \"business_context\": getattr(self, 'business_context', \"General business operations\"),\n",
    "                \"previous_violations\": \"\"  # Will be populated on retry attempts\n",
    "            }\n",
    "            \n",
    "            domain_assignments = None\n",
    "            for attempt in range(1, max_attempts + 1):\n",
    "                try:\n",
    "                    self.logger.info(f\"\uD83D\uDCCD STEP 1: Detecting domains for all {len(use_cases)} use cases ({estimated_size:,} chars) - {attempt}/{max_attempts}\")\n",
    "                    \n",
    "                    import time\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    response_raw = self.ai_agent.run_worker(\n",
    "                        step_name=f\"Detect_Domains_{language}_Attempt{attempt}\",\n",
    "                        worker_prompt_path=\"DOMAIN_FINDER_PROMPT\",\n",
    "                        prompt_vars=prompt_vars,\n",
    "                        response_schema=None\n",
    "                    )\n",
    "                    \n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    self.logger.info(f\"✅ LLM response received in {elapsed_time:.1f} seconds\")\n",
    "                    \n",
    "                    # Validate response is not empty\n",
    "                    if not response_raw or len(response_raw.strip()) == 0:\n",
    "                        raise ValueError(\"LLM returned empty response\")\n",
    "                    \n",
    "                    # Clean response (remove markdown fences if present)\n",
    "                    response_clean = clean_json_response(response_raw)\n",
    "                    \n",
    "                    # Validate cleaned response\n",
    "                    if not response_clean or len(response_clean.strip()) == 0:\n",
    "                        raise ValueError(\"Cleaned response is empty\")\n",
    "                    \n",
    "                    # Parse CSV using centralized utility\n",
    "                    try:\n",
    "                        csv_rows = CSVParser.parse_csv_string(\n",
    "                            response_clean,\n",
    "                            logger=self.logger,\n",
    "                            context=\"Domain detection\"\n",
    "                        )\n",
    "                        domain_assignment_map = {}\n",
    "                        row_count = 0\n",
    "                        \n",
    "                        for row in csv_rows:\n",
    "                            row_count += 1\n",
    "                            # Handle both possible column names - also handle None values\n",
    "                            uc_id_raw = row.get('use_case_id', '') or ''\n",
    "                            domain_raw = row.get('domain', '') or ''\n",
    "                            uc_id = uc_id_raw.strip() if isinstance(uc_id_raw, str) else str(uc_id_raw).strip()\n",
    "                            domain = domain_raw.strip() if isinstance(domain_raw, str) else str(domain_raw).strip()\n",
    "                            \n",
    "                            if uc_id and domain:\n",
    "                                domain_assignment_map[uc_id] = domain\n",
    "                        \n",
    "                        if row_count == 0:\n",
    "                            raise ValueError(\"CSV has no data rows\")\n",
    "                        \n",
    "                        if not domain_assignment_map:\n",
    "                            raise ValueError(\"No valid domain assignments found in CSV\")\n",
    "                            \n",
    "                    except Exception as csv_err:\n",
    "                        self.logger.error(f\"CSV parsing failed. Raw response (first 500 chars): {response_raw[:500]}\")\n",
    "                        self.logger.error(f\"Cleaned response (first 500 chars): {response_clean[:500]}\")\n",
    "                        raise ValueError(f\"Failed to parse CSV: {csv_err}\")\n",
    "                    \n",
    "                    # Apply domain assignments to use cases\n",
    "                    domain_assigned_use_cases = []\n",
    "                    for uc in use_cases:\n",
    "                        uc_copy = uc.copy()\n",
    "                        uc_id = uc_copy.get('No', '')\n",
    "                        if uc_id in domain_assignment_map:\n",
    "                            uc_copy['Business Domain'] = domain_assignment_map[uc_id]\n",
    "                            # Clear subdomain for now (will be assigned in step 2)\n",
    "                            uc_copy['Subdomain'] = ''\n",
    "                        domain_assigned_use_cases.append(uc_copy)\n",
    "                    \n",
    "                    # Validate domain assignments\n",
    "                    violations = []\n",
    "                    \n",
    "                    # Group by domain\n",
    "                    domain_usecases = defaultdict(list)\n",
    "                    \n",
    "                    for uc in domain_assigned_use_cases:\n",
    "                        domain = uc.get('Business Domain', '').strip()\n",
    "                        if domain:\n",
    "                            domain_usecases[domain].append(uc)\n",
    "                    \n",
    "                    # Separate HARD violations (blocking) from SOFT warnings (acceptable)\n",
    "                    hard_violations = []\n",
    "                    soft_warnings = []\n",
    "                    \n",
    "                    # Validate domain count (3-25) - HARD LIMIT on maximum\n",
    "                    total_domains = len(domain_usecases)\n",
    "                    if total_domains < 3:\n",
    "                        hard_violations.append(f\"Only {total_domains} domains, minimum required: 3\")\n",
    "                    if total_domains > 25:\n",
    "                        hard_violations.append(f\"\uD83D\uDEA8 CRITICAL: {total_domains} domains exceeds MAXIMUM 25 (HARD LIMIT) - THIS IS UNACCEPTABLE\")\n",
    "                    \n",
    "                    # Validate domain naming (must be 1 word) - HARD REQUIREMENT\n",
    "                    # Also check for shared words between domains\n",
    "                    domain_words = {}\n",
    "                    for domain in domain_usecases.keys():\n",
    "                        word_count = len(domain.split())\n",
    "                        if word_count != 1:\n",
    "                            hard_violations.append(f\"Domain '{domain}' has {word_count} word(s), must be exactly 1 word\")\n",
    "                        else:\n",
    "                            # Track words used in each domain\n",
    "                            domain_word = domain.lower().strip()\n",
    "                            if domain_word in domain_words:\n",
    "                                hard_violations.append(f\"Domains share word '{domain_word}': '{domain_words[domain_word]}' and '{domain}' - merge these domains\")\n",
    "                            else:\n",
    "                                domain_words[domain_word] = domain\n",
    "                    \n",
    "                    # Validate use cases per domain (4-80) - SOFT guideline for minimum\n",
    "                    total_use_cases = len(use_cases)\n",
    "                    small_domains = []  # Track domains with <4 use cases\n",
    "                    for domain, ucs in domain_usecases.items():\n",
    "                        count = len(ucs)\n",
    "                        if count < 4:\n",
    "                            # Only soft warning if total use cases is small\n",
    "                            if total_use_cases < 50:\n",
    "                                soft_warnings.append(f\"Domain '{domain}' has {count} use case(s) (acceptable for small dataset)\")\n",
    "                            else:\n",
    "                                hard_violations.append(f\"Domain '{domain}' has only {count} use case(s), minimum required: 4\")\n",
    "                                small_domains.append((domain, count))\n",
    "                        elif count > 80:\n",
    "                            hard_violations.append(f\"Domain '{domain}' has {count} use case(s), maximum allowed: 80\")\n",
    "                    \n",
    "                    violations = hard_violations  # Only hard violations cause retries\n",
    "                    \n",
    "                    # Log soft warnings (informational only, doesn't block)\n",
    "                    if soft_warnings:\n",
    "                        self.logger.info(f\"ℹ️ Soft warnings (acceptable for small datasets): {'; '.join(soft_warnings[:3])}\")\n",
    "                    \n",
    "                    # If no HARD violations, domain assignment is successful!\n",
    "                    if not violations:\n",
    "                        self.logger.info(f\"✅ Domain detection successful on attempt {attempt}! Created {total_domains} domains\")\n",
    "                        if soft_warnings:\n",
    "                            self.logger.info(f\"ℹ️ Note: {len(soft_warnings)} domains have <4 use cases (acceptable for dataset of {total_use_cases} total use cases)\")\n",
    "                        domain_assignments = domain_assigned_use_cases\n",
    "                        break  # Exit retry loop\n",
    "                    else:\n",
    "                        self.logger.warning(f\"⚠️ Domain detection attempt {attempt} has {len(violations)} HARD violations\")\n",
    "                        if attempt == max_attempts:\n",
    "                            self.logger.error(f\"❌ Max attempts reached with {len(violations)} HARD violations - This should not happen!\")\n",
    "                            self.logger.error(f\"HARD Violations: {'; '.join(violations[:5])}\")\n",
    "                            # Still use it but log as error\n",
    "                            domain_assignments = domain_assigned_use_cases\n",
    "                            break\n",
    "                        else:\n",
    "                            self.logger.info(f\"Retrying domain detection (attempt {attempt + 1}/{max_attempts})...\")\n",
    "                            \n",
    "                            # Prepare violation summary with actionable suggestions for next attempt\n",
    "                            violation_summary = \"\\n\\n**\uD83D\uDEA8 PREVIOUS ATTEMPT VIOLATIONS - YOU MUST FIX THESE \uD83D\uDEA8**:\\n\"\n",
    "                            violation_summary += \"\\n\".join([f\"- {v}\" for v in violations])\n",
    "                            \n",
    "                            # Add special guidance for small domains (only if it's a hard violation)\n",
    "                            if small_domains and total_use_cases >= 50:\n",
    "                                violation_summary += \"\\n\\n**\uD83D\uDCA1 SUGGESTION FOR SMALL DOMAINS**:\\n\"\n",
    "                                violation_summary += \"Domains with fewer than 4 use cases should be merged into larger related domains.\\n\"\n",
    "                                for small_domain, count in small_domains:\n",
    "                                    violation_summary += f\"  - '{small_domain}' ({count} use cases) → Merge with a related domain\\n\"\n",
    "                            \n",
    "                            # Emphasize the HARD LIMIT on maximum domains\n",
    "                            if total_domains > 25:\n",
    "                                violation_summary += f\"\\n\\n**\uD83D\uDEA8 CRITICAL: MAXIMUM 25 DOMAINS IS AN ABSOLUTE HARD LIMIT \uD83D\uDEA8**\\n\"\n",
    "                                violation_summary += f\"You created {total_domains} domains. You MUST reduce to 25 or fewer.\\n\"\n",
    "                                violation_summary += f\"Merge related domains together to stay within the limit.\\n\"\n",
    "                            \n",
    "                            # Update prompt_vars for next attempt\n",
    "                            prompt_vars['previous_violations'] = violation_summary\n",
    "                            continue\n",
    "                \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Domain detection attempt {attempt} failed: {e}\")\n",
    "                    if attempt == max_attempts:\n",
    "                        self.logger.error(\"Max attempts reached. Using DEFAULT domains as fallback...\")\n",
    "                        # FALLBACK: Assign default domains \"Domain 1\", \"Domain 2\", etc.\n",
    "                        domain_assignments = self._assign_default_domains(use_cases)\n",
    "                        self.logger.warning(f\"✅ Fallback complete: Assigned {len(set(uc.get('Business Domain', '') for uc in domain_assignments))} default domains to {len(domain_assignments)} use cases\")\n",
    "                        break\n",
    "            \n",
    "            # Check if domain detection was successful\n",
    "            if not domain_assignments:\n",
    "                self.logger.error(\"Domain detection failed. Using DEFAULT domains as fallback...\")\n",
    "                # FALLBACK: Assign default domains \"Domain 1\", \"Domain 2\", etc.\n",
    "                domain_assignments = self._assign_default_domains(use_cases)\n",
    "                self.logger.warning(f\"✅ Fallback complete: Assigned {len(set(uc.get('Business Domain', '') for uc in domain_assignments))} default domains to {len(domain_assignments)} use cases\")\n",
    "            \n",
    "            # === STEP 1.5: DOMAIN MERGING (MERGE SMALL DOMAINS) ===\n",
    "            self.logger.info(\"\uD83D\uDCCD STEP 1.5: Merging small domains (if any)...\")\n",
    "            domain_assignments = self._merge_small_domains(domain_assignments, min_cases_per_domain=4)\n",
    "            \n",
    "            # === STEP 2: SUBDOMAIN DETECTION (PARALLEL FOR EACH DOMAIN) ===\n",
    "            self.logger.info(f\"\uD83D\uDCCD STEP 2: Detecting subdomains for each domain in parallel...\")\n",
    "            \n",
    "            # Group use cases by domain\n",
    "            domain_usecases_map = defaultdict(list)\n",
    "            for uc in domain_assignments:\n",
    "                domain = uc.get('Business Domain', '').strip()\n",
    "                if domain:\n",
    "                    domain_usecases_map[domain].append(uc)\n",
    "            \n",
    "            # ADAPTIVE PARALLELISM: Calculate based on domains and use cases\n",
    "            subdomain_parallelism, reason = calculate_adaptive_parallelism(\n",
    "                \"subdomain_detection\", self.max_parallelism,\n",
    "                num_items=len(domain_assignments),\n",
    "                num_domains=len(domain_usecases_map),\n",
    "                is_llm_operation=True, logger=self.logger\n",
    "            )\n",
    "            log_adaptive_parallelism_decision(\"subdomain_detection\", subdomain_parallelism, self.max_parallelism, reason)\n",
    "            self.logger.info(f\"Processing {len(domain_usecases_map)} domains for subdomain detection...\")\n",
    "            \n",
    "            # Process each domain in parallel\n",
    "            final_use_cases_with_subdomains = []\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=subdomain_parallelism, \n",
    "                                   thread_name_prefix=\"SubdomainDetect\") as executor:\n",
    "                # Submit subdomain detection for each domain\n",
    "                future_to_domain = {}\n",
    "                for domain_name, domain_use_cases in domain_usecases_map.items():\n",
    "                    future = executor.submit(\n",
    "                        self._detect_subdomains_for_domain,\n",
    "                        domain_name,\n",
    "                        domain_use_cases,\n",
    "                        language\n",
    "                    )\n",
    "                    future_to_domain[future] = domain_name\n",
    "                    self.logger.debug(f\"✓ Submitted subdomain detection for domain '{domain_name}' ({len(domain_use_cases)} use cases)\")\n",
    "                \n",
    "                # Collect results as they complete\n",
    "                for future in concurrent.futures.as_completed(future_to_domain):\n",
    "                    domain_name = future_to_domain[future]\n",
    "                    try:\n",
    "                        use_cases_with_subdomains = future.result()\n",
    "                        if use_cases_with_subdomains:\n",
    "                            self.logger.info(f\"✅ Domain '{domain_name}': Subdomain detection complete ({len(use_cases_with_subdomains)} use cases)\")\n",
    "                            final_use_cases_with_subdomains.extend(use_cases_with_subdomains)\n",
    "                        else:\n",
    "                            # CRITICAL FIX: Assign default subdomains when detection returns empty\n",
    "                            self.logger.warning(f\"⚠️ Domain '{domain_name}': Subdomain detection returned no use cases - assigning default subdomains\")\n",
    "                            domain_use_cases = domain_usecases_map.get(domain_name, [])\n",
    "                            for uc in domain_use_cases:\n",
    "                                if not uc.get('Subdomain'):\n",
    "                                    uc['Subdomain'] = f\"General {domain_name}\"\n",
    "                            final_use_cases_with_subdomains.extend(domain_use_cases)\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"❌ Domain '{domain_name}': Subdomain detection failed: {e}\")\n",
    "                        # CRITICAL FIX: Assign default subdomains on exception\n",
    "                        domain_use_cases = domain_usecases_map.get(domain_name, [])\n",
    "                        self.logger.warning(f\"Using default subdomains for domain '{domain_name}'\")\n",
    "                        for uc in domain_use_cases:\n",
    "                            if not uc.get('Subdomain'):\n",
    "                                uc['Subdomain'] = f\"General {domain_name}\"\n",
    "                        final_use_cases_with_subdomains.extend(domain_use_cases)\n",
    "            \n",
    "            self.logger.info(f\"✅ Two-step clustering complete! {len(final_use_cases_with_subdomains)} use cases processed\")\n",
    "            return final_use_cases_with_subdomains\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Domain/subdomain clustering failed with error: {e}. Using DEFAULT domains/subdomains as fallback...\")\n",
    "            # FALLBACK: Assign default domains and subdomains on any error\n",
    "            fallback_use_cases = self._assign_default_domains(use_cases)\n",
    "            # Also assign default subdomains for each domain\n",
    "            domain_map = {}\n",
    "            for uc in fallback_use_cases:\n",
    "                domain = uc.get('Business Domain', 'Domain1')\n",
    "                if domain not in domain_map:\n",
    "                    domain_map[domain] = []\n",
    "                domain_map[domain].append(uc)\n",
    "            \n",
    "            final_use_cases = []\n",
    "            for domain_name, domain_ucs in domain_map.items():\n",
    "                subdomained_ucs = self._assign_default_subdomains(domain_ucs, domain_name)\n",
    "                final_use_cases.extend(subdomained_ucs)\n",
    "            \n",
    "            self.logger.warning(f\"✅ Complete fallback applied: {len(final_use_cases)} use cases with default domains and subdomains\")\n",
    "            return final_use_cases\n",
    "\n",
    "    def _assign_default_domains(self, use_cases: list) -> list:\n",
    "        \"\"\"\n",
    "        FALLBACK: Assign default domains when LLM domain detection fails.\n",
    "        Distributes use cases evenly across 5 default domains: Domain1, Domain2, Domain3, Domain4, Domain5.\n",
    "        \n",
    "        Args:\n",
    "            use_cases: List of use case dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            List of use cases with default 'Business Domain' assigned\n",
    "        \"\"\"\n",
    "        if not use_cases:\n",
    "            return use_cases\n",
    "        \n",
    "        # Create 5 default domains (single-word names as required by validation)\n",
    "        default_domains = [\"Domain1\", \"Domain2\", \"Domain3\", \"Domain4\", \"Domain5\"]\n",
    "        \n",
    "        # Distribute use cases evenly across domains\n",
    "        result = []\n",
    "        for i, uc in enumerate(use_cases):\n",
    "            uc_copy = uc.copy()\n",
    "            domain_idx = i % len(default_domains)\n",
    "            uc_copy['Business Domain'] = default_domains[domain_idx]\n",
    "            uc_copy['Subdomain'] = ''  # Will be assigned in subdomain step\n",
    "            result.append(uc_copy)\n",
    "        \n",
    "        self.logger.info(f\"\uD83D\uDCCB Default domain assignment: Distributed {len(result)} use cases across {len(default_domains)} domains\")\n",
    "        \n",
    "        # Log distribution\n",
    "        domain_counts = {}\n",
    "        for uc in result:\n",
    "            d = uc.get('Business Domain', '')\n",
    "            domain_counts[d] = domain_counts.get(d, 0) + 1\n",
    "        for domain, count in sorted(domain_counts.items()):\n",
    "            self.logger.debug(f\"   - {domain}: {count} use cases\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def _assign_default_subdomains(self, use_cases: list, domain_name: str) -> list:\n",
    "        \"\"\"\n",
    "        FALLBACK: Assign default subdomains when LLM subdomain detection fails.\n",
    "        Distributes use cases evenly across default subdomains: \"Sub Domain1\", \"Sub Domain2\", etc.\n",
    "        Creates enough subdomains to ensure minimum 2 use cases per subdomain.\n",
    "        \n",
    "        Args:\n",
    "            use_cases: List of use case dictionaries\n",
    "            domain_name: Name of the domain (for logging)\n",
    "            \n",
    "        Returns:\n",
    "            List of use cases with default 'Subdomain' assigned\n",
    "        \"\"\"\n",
    "        if not use_cases:\n",
    "            return use_cases\n",
    "        \n",
    "        # Calculate number of subdomains needed (2-5, ensuring min 2 use cases each)\n",
    "        num_use_cases = len(use_cases)\n",
    "        # At least 2 use cases per subdomain, so max subdomains = num_use_cases // 2\n",
    "        max_subdomains = max(2, min(5, num_use_cases // 2))\n",
    "        \n",
    "        # Create default subdomains (2-word names as required by validation)\n",
    "        default_subdomains = [f\"Sub Domain{i}\" for i in range(1, max_subdomains + 1)]\n",
    "        \n",
    "        # Distribute use cases evenly across subdomains\n",
    "        result = []\n",
    "        for i, uc in enumerate(use_cases):\n",
    "            uc_copy = uc.copy()\n",
    "            subdomain_idx = i % len(default_subdomains)\n",
    "            uc_copy['Subdomain'] = default_subdomains[subdomain_idx]\n",
    "            result.append(uc_copy)\n",
    "        \n",
    "        self.logger.info(f\"\uD83D\uDCCB [{domain_name}] Default subdomain assignment: Distributed {len(result)} use cases across {len(default_subdomains)} subdomains\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def _detect_subdomains_for_domain(self, domain_name: str, use_cases: list, language: str) -> list:\n",
    "        \"\"\"\n",
    "        Detect subdomains for a single domain's use cases using LLM.\n",
    "        This method is designed to be called in parallel for each domain.\n",
    "        \n",
    "        Args:\n",
    "            domain_name: Name of the domain\n",
    "            use_cases: List of use case dictionaries belonging to this domain\n",
    "            language: Output language\n",
    "            \n",
    "        Returns:\n",
    "            List of use cases with updated subdomains\n",
    "        \"\"\"\n",
    "        from collections import defaultdict\n",
    "        import io\n",
    "        import csv\n",
    "        \n",
    "        log_prefix = f\"[Domain: {domain_name}]\"\n",
    "        self.logger.debug(f\"{log_prefix} Starting subdomain detection for {len(use_cases)} use cases...\")\n",
    "        \n",
    "        if not use_cases:\n",
    "            return use_cases\n",
    "        \n",
    "        try:\n",
    "            # Convert use cases to CSV for LLM (Business Domain is set, Subdomain will be detected)\n",
    "            output = io.StringIO()\n",
    "            fieldnames = ['No', 'Name', 'type', 'Analytics Technique', 'Statement', 'Solution', \n",
    "                         'Business Value', 'Beneficiary', 'Sponsor', \n",
    "                         'Tables Involved']\n",
    "            writer = csv.DictWriter(output, fieldnames=fieldnames, extrasaction='ignore')\n",
    "            writer.writeheader()\n",
    "            writer.writerows(use_cases)\n",
    "            use_cases_csv = output.getvalue()\n",
    "            \n",
    "            # Check context size (uses model-specific limits from TECHNICAL_CONTEXT)\n",
    "            prompt_template = self.ai_agent.prompt_templates.get(\"SUBDOMAIN_DETECTOR_PROMPT\", \"\")\n",
    "            estimated_size = len(prompt_template) + len(use_cases_csv) + 1000\n",
    "            MAX_CONTEXT_CHARS = get_max_context_chars(language, \"SUBDOMAIN_DETECTOR_PROMPT\")\n",
    "            \n",
    "            if estimated_size > MAX_CONTEXT_CHARS:\n",
    "                self.logger.warning(\n",
    "                    f\"{log_prefix} Subdomain prompt size ({estimated_size:,} chars) exceeds MAX_CONTEXT_CHARS ({MAX_CONTEXT_CHARS:,}). \"\n",
    "                    f\"Splitting into smaller batches for subdomain detection.\"\n",
    "                )\n",
    "                # CONTEXT SPLITTING: Split use cases into smaller batches and process each\n",
    "                return self._detect_subdomains_with_context_splitting(\n",
    "                    domain_name, use_cases, language, prompt_template, MAX_CONTEXT_CHARS\n",
    "                )\n",
    "            \n",
    "            # Call LLM for subdomain detection (respect global retry cap)\n",
    "            max_attempts = (getattr(self, \"max_retry_attempts\", 1) or 0) + 1\n",
    "            prompt_vars = {\n",
    "                \"domain_name\": domain_name,\n",
    "                \"use_cases_csv\": use_cases_csv,\n",
    "                \"output_language\": language,\n",
    "                \"business_name\": self.business_name,\n",
    "                \"industries\": \", \".join(self.industries) if hasattr(self, 'industries') and self.industries else \"General Business\",\n",
    "                \"business_context\": getattr(self, 'business_context', \"General business operations\"),\n",
    "                \"previous_violations\": \"\"\n",
    "            }\n",
    "            \n",
    "            for attempt in range(1, max_attempts + 1):\n",
    "                try:\n",
    "                    self.logger.debug(f\"{log_prefix} Subdomain detection attempt {attempt}/{max_attempts}...\")\n",
    "                    \n",
    "                    response_raw = self.ai_agent.run_worker(\n",
    "                        step_name=f\"Detect_Subdomains_{domain_name}_{language}_Attempt{attempt}\",\n",
    "                        worker_prompt_path=\"SUBDOMAIN_DETECTOR_PROMPT\",\n",
    "                        prompt_vars=prompt_vars,\n",
    "                        response_schema=None,\n",
    "                        timeout_override=self.llm_timeout_seconds  # Explicit timeout\n",
    "                    )\n",
    "                    \n",
    "                    # Validate response\n",
    "                    if not response_raw or len(response_raw.strip()) == 0:\n",
    "                        raise ValueError(\"LLM returned empty response\")\n",
    "                    \n",
    "                    # Clean response (remove markdown fences if present)\n",
    "                    response_clean = clean_json_response(response_raw)\n",
    "                    if not response_clean or len(response_clean.strip()) == 0:\n",
    "                        raise ValueError(\"Cleaned response is empty\")\n",
    "                    \n",
    "                    # Parse CSV using centralized utility\n",
    "                    try:\n",
    "                        csv_rows = CSVParser.parse_csv_string(\n",
    "                            response_clean,\n",
    "                            logger=self.logger,\n",
    "                            context=\"Subdomain detection\"\n",
    "                        )\n",
    "                        subdomain_assignment_map = {}\n",
    "                        row_count = 0\n",
    "                        \n",
    "                        for row in csv_rows:\n",
    "                            row_count += 1\n",
    "                            # Handle both possible column names - also handle None values\n",
    "                            uc_id_raw = row.get('use_case_id', '') or ''\n",
    "                            subdomain_raw = row.get('subdomain', '') or ''\n",
    "                            uc_id = uc_id_raw.strip() if isinstance(uc_id_raw, str) else str(uc_id_raw).strip()\n",
    "                            subdomain = subdomain_raw.strip() if isinstance(subdomain_raw, str) else str(subdomain_raw).strip()\n",
    "                            \n",
    "                            if uc_id and subdomain:\n",
    "                                subdomain_assignment_map[uc_id] = subdomain\n",
    "                        \n",
    "                        if row_count == 0:\n",
    "                            raise ValueError(\"CSV has no data rows\")\n",
    "                        \n",
    "                        if not subdomain_assignment_map:\n",
    "                            raise ValueError(\"No valid subdomain assignments found in CSV\")\n",
    "                            \n",
    "                    except Exception as csv_err:\n",
    "                        self.logger.error(f\"{log_prefix} CSV parsing failed. Raw response (first 500 chars): {response_raw[:500]}\")\n",
    "                        self.logger.error(f\"{log_prefix} Cleaned response (first 500 chars): {response_clean[:500]}\")\n",
    "                        raise ValueError(f\"Failed to parse CSV: {csv_err}\")\n",
    "                    \n",
    "                    # Apply subdomain assignments\n",
    "                    subdomain_assigned_use_cases = []\n",
    "                    for uc in use_cases:\n",
    "                        uc_copy = uc.copy()\n",
    "                        uc_id = uc_copy.get('No', '')\n",
    "                        if uc_id in subdomain_assignment_map:\n",
    "                            uc_copy['Subdomain'] = subdomain_assignment_map[uc_id]\n",
    "                        subdomain_assigned_use_cases.append(uc_copy)\n",
    "                    \n",
    "                    # Validate subdomain assignments\n",
    "                    violations = []\n",
    "                    \n",
    "                    # Group by subdomain\n",
    "                    subdomain_usecases = defaultdict(list)\n",
    "                    for uc in subdomain_assigned_use_cases:\n",
    "                        subdomain = uc.get('Subdomain', '').strip()\n",
    "                        if subdomain:\n",
    "                            subdomain_usecases[subdomain].append(uc)\n",
    "                    \n",
    "                    # Validate subdomain count (2-10)\n",
    "                    total_subdomains = len(subdomain_usecases)\n",
    "                    if total_subdomains < 2:\n",
    "                        violations.append(f\"Only {total_subdomains} subdomain(s), minimum required: 2\")\n",
    "                    if total_subdomains > 10:\n",
    "                        violations.append(f\"Too many subdomains: {total_subdomains}, maximum allowed: 10\")\n",
    "                    \n",
    "                    # Validate subdomain naming (must be 2 words)\n",
    "                    for subdomain in subdomain_usecases.keys():\n",
    "                        word_count = len(subdomain.split())\n",
    "                        if word_count != 2:\n",
    "                            violations.append(f\"Subdomain '{subdomain}' has {word_count} word(s), must be exactly 2 words\")\n",
    "                    \n",
    "                    # Validate use cases per subdomain (minimum 2)\n",
    "                    for subdomain, ucs in subdomain_usecases.items():\n",
    "                        count = len(ucs)\n",
    "                        if count < 2:\n",
    "                            violations.append(f\"Subdomain '{subdomain}' has only {count} use case(s), minimum required: 2\")\n",
    "                    \n",
    "                    # If no violations, success!\n",
    "                    if not violations:\n",
    "                        self.logger.debug(f\"{log_prefix} ✅ Subdomain detection successful! Created {total_subdomains} subdomains\")\n",
    "                        return subdomain_assigned_use_cases\n",
    "                    else:\n",
    "                        self.logger.warning(f\"{log_prefix} ⚠️ Subdomain detection attempt {attempt} has {len(violations)} violations\")\n",
    "                        if attempt == max_attempts:\n",
    "                            self.logger.warning(f\"{log_prefix} Max attempts reached. Using subdomain assignments despite {len(violations)} violations\")\n",
    "                            self.logger.warning(f\"{log_prefix} Violations: {'; '.join(violations[:3])}\")\n",
    "                            return subdomain_assigned_use_cases\n",
    "                        else:\n",
    "                            # Prepare violation summary for retry\n",
    "                            violation_summary = \"\\n\\n**\uD83D\uDEA8 PREVIOUS ATTEMPT VIOLATIONS - YOU MUST FIX THESE \uD83D\uDEA8**:\\n\"\n",
    "                            violation_summary += \"\\n\".join([f\"- {v}\" for v in violations])\n",
    "                            prompt_vars['previous_violations'] = violation_summary\n",
    "                            continue\n",
    "                \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"{log_prefix} Subdomain detection attempt {attempt} failed: {e}\")\n",
    "                    if attempt == max_attempts:\n",
    "                        self.logger.error(f\"{log_prefix} Max attempts reached. Using DEFAULT subdomains as fallback...\")\n",
    "                        # FALLBACK: Assign default subdomains \"Sub Domain 1\", \"Sub Domain 2\", etc.\n",
    "                        fallback_use_cases = self._assign_default_subdomains(use_cases, domain_name)\n",
    "                        self.logger.warning(f\"{log_prefix} ✅ Fallback complete: Assigned default subdomains to {len(fallback_use_cases)} use cases\")\n",
    "                        return fallback_use_cases\n",
    "            \n",
    "            # If we reach here without returning, use fallback\n",
    "            self.logger.warning(f\"{log_prefix} No subdomain assignments made. Using DEFAULT subdomains as fallback...\")\n",
    "            return self._assign_default_subdomains(use_cases, domain_name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"{log_prefix} Subdomain detection failed with error: {e}. Using DEFAULT subdomains as fallback...\")\n",
    "            # FALLBACK: Assign default subdomains on any error\n",
    "            return self._assign_default_subdomains(use_cases, domain_name)\n",
    "\n",
    "    def _detect_subdomains_with_context_splitting(self, domain_name: str, use_cases: list, language: str, prompt_template: str, max_context_chars: int) -> list:\n",
    "        \"\"\"\n",
    "        Handle subdomain detection when context exceeds max size by splitting into batches.\n",
    "        Processes each batch separately and merges results.\n",
    "        \n",
    "        Args:\n",
    "            domain_name: Name of the domain\n",
    "            use_cases: List of use case dictionaries\n",
    "            language: Output language\n",
    "            prompt_template: The prompt template string\n",
    "            max_context_chars: Maximum context size in characters\n",
    "            \n",
    "        Returns:\n",
    "            List of use cases with assigned subdomains\n",
    "        \"\"\"\n",
    "        import io\n",
    "        import csv\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        log_prefix = f\"[Domain: {domain_name}][Context Split]\"\n",
    "        \n",
    "        # Calculate how many use cases can fit in one batch\n",
    "        # Estimate average use case size in CSV format\n",
    "        sample_output = io.StringIO()\n",
    "        sample_fieldnames = ['No', 'Name', 'type', 'Analytics Technique', 'Statement', 'Solution', \n",
    "                            'Business Value', 'Beneficiary', 'Sponsor', 'Tables Involved']\n",
    "        sample_writer = csv.DictWriter(sample_output, fieldnames=sample_fieldnames, extrasaction='ignore')\n",
    "        sample_writer.writeheader()\n",
    "        if use_cases:\n",
    "            sample_writer.writerow(use_cases[0])\n",
    "        avg_use_case_size = len(sample_output.getvalue()) // max(1, 1)  # Size of header + 1 row\n",
    "        \n",
    "        # Calculate available space for use cases (subtract prompt template and buffer)\n",
    "        available_chars = max_context_chars - len(prompt_template) - 2000  # 2000 char buffer\n",
    "        batch_size = max(2, available_chars // max(1, avg_use_case_size))  # At least 2 use cases per batch\n",
    "        \n",
    "        self.logger.info(f\"{log_prefix} Splitting {len(use_cases)} use cases into batches of ~{batch_size}\")\n",
    "        \n",
    "        # Split use cases into batches\n",
    "        batches = []\n",
    "        for i in range(0, len(use_cases), batch_size):\n",
    "            batches.append(use_cases[i:i + batch_size])\n",
    "        \n",
    "        self.logger.info(f\"{log_prefix} Created {len(batches)} batches for subdomain detection\")\n",
    "        \n",
    "        # Process each batch and collect subdomain assignments\n",
    "        all_subdomain_assignments = {}  # uc_id -> subdomain\n",
    "        \n",
    "        for batch_idx, batch in enumerate(batches, start=1):\n",
    "            self.logger.info(f\"{log_prefix} Processing batch {batch_idx}/{len(batches)} ({len(batch)} use cases)\")\n",
    "            \n",
    "            try:\n",
    "                # Convert batch to CSV\n",
    "                batch_output = io.StringIO()\n",
    "                batch_writer = csv.DictWriter(batch_output, fieldnames=sample_fieldnames, extrasaction='ignore')\n",
    "                batch_writer.writeheader()\n",
    "                batch_writer.writerows(batch)\n",
    "                batch_csv = batch_output.getvalue()\n",
    "                \n",
    "                # Call LLM for this batch\n",
    "                max_attempts = (getattr(self, \"max_retry_attempts\", 1) or 0) + 1\n",
    "                prompt_vars = {\n",
    "                    \"domain_name\": domain_name,\n",
    "                    \"use_cases_csv\": batch_csv,\n",
    "                    \"output_language\": language,\n",
    "                    \"business_name\": self.business_name,\n",
    "                    \"industries\": \", \".join(self.industries) if hasattr(self, 'industries') and self.industries else \"General Business\",\n",
    "                    \"business_context\": getattr(self, 'business_context', \"General business operations\"),\n",
    "                    \"previous_violations\": \"\"\n",
    "                }\n",
    "                \n",
    "                batch_success = False\n",
    "                for attempt in range(1, max_attempts + 1):\n",
    "                    try:\n",
    "                        response_raw = self.ai_agent.run_worker(\n",
    "                            step_name=f\"Detect_Subdomains_{domain_name}_Batch{batch_idx}_{language}_Attempt{attempt}\",\n",
    "                            worker_prompt_path=\"SUBDOMAIN_DETECTOR_PROMPT\",\n",
    "                            prompt_vars=prompt_vars,\n",
    "                            response_schema=None,\n",
    "                            timeout_override=self.llm_timeout_seconds\n",
    "                        )\n",
    "                        \n",
    "                        if not response_raw or len(response_raw.strip()) == 0:\n",
    "                            raise ValueError(\"LLM returned empty response\")\n",
    "                        \n",
    "                        response_clean = clean_json_response(response_raw)\n",
    "                        if not response_clean or len(response_clean.strip()) == 0:\n",
    "                            raise ValueError(\"Cleaned response is empty\")\n",
    "                        \n",
    "                        # Parse CSV response\n",
    "                        csv_rows = CSVParser.parse_csv_string(\n",
    "                            response_clean,\n",
    "                            logger=self.logger,\n",
    "                            context=f\"Subdomain detection batch {batch_idx}\"\n",
    "                        )\n",
    "                        \n",
    "                        # Extract subdomain assignments\n",
    "                        for row in csv_rows:\n",
    "                            uc_id_raw = row.get('use_case_id', '') or ''\n",
    "                            subdomain_raw = row.get('subdomain', '') or ''\n",
    "                            uc_id = str(uc_id_raw).strip()\n",
    "                            subdomain = str(subdomain_raw).strip()\n",
    "                            if uc_id and subdomain:\n",
    "                                all_subdomain_assignments[uc_id] = subdomain\n",
    "                        \n",
    "                        batch_success = True\n",
    "                        self.logger.info(f\"{log_prefix} Batch {batch_idx} completed successfully\")\n",
    "                        break\n",
    "                        \n",
    "                    except Exception as attempt_err:\n",
    "                        self.logger.warning(f\"{log_prefix} Batch {batch_idx} attempt {attempt} failed: {attempt_err}\")\n",
    "                        if attempt == max_attempts:\n",
    "                            self.logger.error(f\"{log_prefix} Batch {batch_idx} failed after {max_attempts} attempts\")\n",
    "                \n",
    "                if not batch_success:\n",
    "                    # Assign default subdomains for failed batch\n",
    "                    for uc in batch:\n",
    "                        uc_id = str(uc.get('No', '')).strip()\n",
    "                        if uc_id and uc_id not in all_subdomain_assignments:\n",
    "                            all_subdomain_assignments[uc_id] = f\"General {domain_name}\"\n",
    "                            \n",
    "            except Exception as batch_err:\n",
    "                self.logger.error(f\"{log_prefix} Batch {batch_idx} processing error: {batch_err}\")\n",
    "                for uc in batch:\n",
    "                    uc_id = str(uc.get('No', '')).strip()\n",
    "                    if uc_id and uc_id not in all_subdomain_assignments:\n",
    "                        all_subdomain_assignments[uc_id] = f\"General {domain_name}\"\n",
    "        \n",
    "        # Apply subdomain assignments to use cases\n",
    "        result = []\n",
    "        for uc in use_cases:\n",
    "            uc_copy = uc.copy()\n",
    "            uc_id = str(uc.get('No', '')).strip()\n",
    "            subdomain = all_subdomain_assignments.get(uc_id, f\"General {domain_name}\")\n",
    "            uc_copy['Subdomain'] = subdomain\n",
    "            result.append(uc_copy)\n",
    "        \n",
    "        # Log statistics\n",
    "        subdomain_counts = defaultdict(int)\n",
    "        for uc in result:\n",
    "            subdomain_counts[uc.get('Subdomain', 'Unknown')] += 1\n",
    "        \n",
    "        self.logger.info(f\"{log_prefix} Context splitting complete: {len(result)} use cases assigned to {len(subdomain_counts)} subdomains\")\n",
    "        for subdomain, count in sorted(subdomain_counts.items(), key=lambda x: -x[1]):\n",
    "            self.logger.debug(f\"{log_prefix}   - {subdomain}: {count} use cases\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def _report_table_statistics(self, use_cases: list):\n",
    "        \"\"\"\n",
    "        Reports statistics on table inclusion/exclusion based on generated use cases.\n",
    "        Compares tables used in use cases vs. total available tables.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get all available tables from the data loader\n",
    "            all_available_tables = set()\n",
    "            if self.data_loader and hasattr(self.data_loader, 'db_details_cache'):\n",
    "                for (catalog, schema, table, _, _, _) in self.data_loader.db_details_cache:\n",
    "                    fqtn = f\"`{catalog}`.`{schema}`.`{table}`\"\n",
    "                    all_available_tables.add(fqtn)\n",
    "            \n",
    "            total_available = len(all_available_tables)\n",
    "            \n",
    "            if total_available == 0:\n",
    "                self.logger.warning(\"No available tables found in data loader cache. Skipping table statistics report.\")\n",
    "                return\n",
    "            \n",
    "            # Extract all unique tables used in use cases\n",
    "            tables_used = set()\n",
    "            for uc in use_cases:\n",
    "                tables_involved = uc.get('Tables Involved', '')\n",
    "                if tables_involved:\n",
    "                    # Parse comma-separated table list\n",
    "                    for table in tables_involved.split(','):\n",
    "                        table = table.strip()\n",
    "                        if table:\n",
    "                            tables_used.add(table)\n",
    "            \n",
    "            total_included = len(tables_used)\n",
    "            total_excluded = total_available - total_included\n",
    "            \n",
    "            # Calculate percentages\n",
    "            pct_included = (total_included / total_available * 100) if total_available > 0 else 0\n",
    "            pct_excluded = (total_excluded / total_available * 100) if total_available > 0 else 0\n",
    "            \n",
    "            log_print(\"\\n\" + \"=\"*80)\n",
    "            log_print(\"TABLE UTILIZATION REPORT\")\n",
    "            log_print(\"=\"*80)\n",
    "            log_print(f\"Total Available Tables: {total_available}\")\n",
    "            log_print(f\"Tables Included in Use Cases: {total_included} ({pct_included:.1f}%)\")\n",
    "            log_print(f\"Tables Excluded (No Business Value): {total_excluded} ({pct_excluded:.1f}%)\")\n",
    "            log_print(\"=\"*80 + \"\\n\")\n",
    "            \n",
    "            # Log it as well\n",
    "            self.logger.info(f\"Table Statistics: {total_included}/{total_available} tables included ({pct_included:.1f}%), {total_excluded} excluded ({pct_excluded:.1f}%)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to generate table statistics report: {e}\")\n",
    "    \n",
    "    def _priority_sort_key(self, use_case: dict) -> tuple:\n",
    "        \"\"\"\n",
    "        Returns a tuple for sorting use cases by priority (descending).\n",
    "        Very High -> High -> Medium -> Low -> Very Low\n",
    "        \"\"\"\n",
    "        priority_map = {\n",
    "            \"Very High\": 0,\n",
    "            \"High\": 1,\n",
    "            \"Medium\": 2,\n",
    "            \"Low\": 3,\n",
    "            \"Very Low\": 4,\n",
    "            \"N/A\": 5\n",
    "        }\n",
    "        priority_label = use_case.get('Priority', 'N/A')\n",
    "        priority_order = priority_map.get(priority_label, 5)\n",
    "        # Secondary sort by use case number for stability\n",
    "        use_case_no = use_case.get('No', 'N-999-AI')\n",
    "        return (priority_order, use_case_no)\n",
    "    \n",
    "    def _natural_sort_key(self, use_case: dict) -> tuple:\n",
    "        \"\"\"\n",
    "        Natural sort key for use case IDs to ensure proper ordering.\n",
    "        \n",
    "        Handles IDs like: N01-AI01, N01-AI02, ..., N01-AI10, N01-AI11, N01-ST01, etc.\n",
    "        Standard string sort would incorrectly order: AI1, AI10, AI11, AI2, AI3...\n",
    "        Natural sort correctly orders: AI1, AI2, AI3, ..., AI10, AI11...\n",
    "        \n",
    "        Returns tuple: (domain_num, source_type, sequence_num, original_id)\n",
    "        Example: \"N01-AI05\" -> (1, 'AI', 5, 'N01-AI05')\n",
    "        \"\"\"\n",
    "        import re\n",
    "        use_case_id = use_case.get('No', 'N99-ZZ999')\n",
    "        \n",
    "        # Parse ID format: N##-XX## (e.g., N01-AI05, N02-ST10)\n",
    "        match = re.match(r'N(\\d+)-([A-Z]+)(\\d+)', use_case_id)\n",
    "        if match:\n",
    "            domain_num = int(match.group(1))\n",
    "            source_type = match.group(2)  # AI or ST\n",
    "            seq_num = int(match.group(3))\n",
    "            # Sort AI before ST (alphabetically)\n",
    "            return (domain_num, source_type, seq_num, use_case_id)\n",
    "        \n",
    "        # Fallback: Try to parse older formats like AI-XXX-U## or just extract numbers\n",
    "        # Pattern: AI-XXX-U## or ST-XXX-U##\n",
    "        match2 = re.match(r'(AI|ST)-([A-Z0-9_]+)-U(\\d+)', use_case_id)\n",
    "        if match2:\n",
    "            source_type = match2.group(1)\n",
    "            seq_num = int(match2.group(3))\n",
    "            return (99, source_type, seq_num, use_case_id)\n",
    "        \n",
    "        # Final fallback: extract any numbers from the ID for some ordering\n",
    "        numbers = re.findall(r'\\d+', use_case_id)\n",
    "        if numbers:\n",
    "            # Use first number as domain, last number as sequence\n",
    "            return (int(numbers[0]) if len(numbers) > 0 else 99, \n",
    "                    'ZZ', \n",
    "                    int(numbers[-1]) if len(numbers) > 0 else 999, \n",
    "                    use_case_id)\n",
    "        \n",
    "        # No numbers found - sort at the end\n",
    "        return (999, 'ZZ', 999, use_case_id)\n",
    "    \n",
    "    def _calculate_domain_impact_score(self, use_cases: list) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the average priority score for a domain to determine its impact.\n",
    "        Higher score = more impactful domain.\n",
    "        \"\"\"\n",
    "        if not use_cases:\n",
    "            return 0.0\n",
    "        # Ensure Priority Score is converted to float (it might be stored as string or int)\n",
    "        total_score = sum(float(uc.get('Priority Score', 5.0)) for uc in use_cases)\n",
    "        return total_score / len(use_cases)\n",
    "\n",
    "    def assemble_use_case_notebooks(self, all_use_cases: list, translations: dict, summary_dict: dict = None):\n",
    "        self.logger.debug(\"--- Starting Use Case Notebook Assembly (English) ---\")\n",
    "        if not all_use_cases:\n",
    "            self.logger.warning(\"No use cases were provided for notebook assembly.\")\n",
    "            return\n",
    "        \n",
    "        def get_prefix_for_group(use_cases_list):\n",
    "            if not use_cases_list: return \"N00\"\n",
    "            try:\n",
    "                first_id = use_cases_list[0]['No']\n",
    "                return first_id.split('-')[0]\n",
    "            except Exception: return \"N00\"\n",
    "\n",
    "        # === MODIFIED: Always use business domain grouping ===\n",
    "        self.logger.info(\"Assembling one notebook for each business domain...\")\n",
    "        grouped_by_domain = self._group_use_cases_by_domain_flat(all_use_cases)\n",
    "        \n",
    "        # CRITICAL FIX: Extract domain PREFIX from USE CASE IDs - ensures notebook names match use case IDs\n",
    "        # Sort by USE CASE COUNT (smallest first) for quick testing, but use PREFIX from IDs for notebook naming\n",
    "        domain_prefix_info = {}\n",
    "        for domain, use_cases in grouped_by_domain.items():\n",
    "            prefix = get_prefix_for_group(use_cases)\n",
    "            try:\n",
    "                prefix_num = int(prefix[1:]) if prefix.startswith('N') else 99\n",
    "            except ValueError:\n",
    "                prefix_num = 99\n",
    "            domain_prefix_info[domain] = (prefix_num, prefix)\n",
    "        \n",
    "        # Sort domains by USE CASE COUNT (smallest first) - enables quick testing of smaller domains\n",
    "        # NOTE: Notebook prefix comes from use case IDs (domain_prefix_info), NOT from this sort order\n",
    "        sorted_domain_names = sorted(grouped_by_domain.keys(), \n",
    "                                    key=lambda d: len(grouped_by_domain[d]))\n",
    "        \n",
    "        total_domains = len(sorted_domain_names)\n",
    "        self.logger.info(f\"\uD83D\uDCDA Generating {total_domains} notebooks for {len(all_use_cases)} use cases IN PARALLEL...\")\n",
    "        log_print(f\"\uD83D\uDCDA Generating {total_domains} notebooks in parallel (max {self.max_parallelism} concurrent)...\")\n",
    "        \n",
    "        # Process all domains in parallel using ThreadPoolExecutor\n",
    "        def create_notebook_for_domain(domain_data):\n",
    "            \"\"\"Helper function to create a notebook for a single domain.\"\"\"\n",
    "            i, domain_name = domain_data\n",
    "            domain_use_cases = grouped_by_domain[domain_name]\n",
    "            # Sort use cases by ID using natural sort (AI01, AI02, ..., AI10, AI11 - not AI1, AI10, AI11, AI2)\n",
    "            # Note: PDF/PPT/XLS are sorted by priority descending via _group_use_cases_by_domain_flat\n",
    "            sorted_cases = sorted(domain_use_cases, key=self._natural_sort_key)\n",
    "            # CRITICAL FIX: Use prefix from use case IDs, not from loop index\n",
    "            # This ensures N15-AI01 use cases go into N15-xxx.ipynb, not N06-xxx.ipynb\n",
    "            domain_prefix = domain_prefix_info.get(domain_name, (i, f\"N{i:02d}\"))[1]\n",
    "            notebook_name = f\"{domain_prefix}-{self._sanitize_name(domain_name)}\"\n",
    "            \n",
    "            self.logger.info(f\"\uD83D\uDCDD [{i}/{total_domains}] Assembling notebook '{notebook_name}' ({len(sorted_cases)} use cases)...\")\n",
    "            \n",
    "            # Get domain executive summary if available\n",
    "            domain_summary = None\n",
    "            if summary_dict:\n",
    "                domain_summary = summary_dict.get(domain_name, None)\n",
    "            \n",
    "            try:\n",
    "                self._assemble_notebook_for_db(\n",
    "                    db_name=domain_name, use_cases=sorted_cases, translations=translations, \n",
    "                    db_prefix=domain_prefix, filename_override=notebook_name, domain_summary=domain_summary\n",
    "                )\n",
    "                self.logger.info(f\"✅ [{i}/{total_domains}] Notebook '{notebook_name}' completed successfully\")\n",
    "                return (i, notebook_name, True)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"❌ [{i}/{total_domains}] Notebook '{notebook_name}' failed: {e}\")\n",
    "                return (i, notebook_name, False)\n",
    "        \n",
    "        # ADAPTIVE PARALLELISM: Calculate based on number of domains and use cases\n",
    "        total_use_cases = sum(len(grouped_by_domain[d]) for d in sorted_domain_names)\n",
    "        \n",
    "        notebook_parallelism, reason = calculate_adaptive_parallelism(\n",
    "            \"notebook_generation\", self.max_parallelism,\n",
    "            num_items=total_use_cases,\n",
    "            num_domains=total_domains,\n",
    "            is_llm_operation=False, logger=self.logger\n",
    "        )\n",
    "        log_adaptive_parallelism_decision(\"notebook_generation\", notebook_parallelism, self.max_parallelism, reason)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=notebook_parallelism, thread_name_prefix=\"NotebookGen\") as executor:\n",
    "            # Submit all notebook generation jobs\n",
    "            domain_data_list = [(i, domain_name) for i, domain_name in enumerate(sorted_domain_names, start=1)]\n",
    "            futures = [executor.submit(create_notebook_for_domain, domain_data) for domain_data in domain_data_list]\n",
    "            \n",
    "            # Collect results\n",
    "            completed_count = 0\n",
    "            failed_count = 0\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    i, notebook_name, success = future.result()\n",
    "                    if success:\n",
    "                        completed_count += 1\n",
    "                        log_print(f\"   ✅ Notebook {i}/{total_domains} completed: {notebook_name}\")\n",
    "                    else:\n",
    "                        failed_count += 1\n",
    "                        log_print(f\"   ❌ Notebook {i}/{total_domains} failed: {notebook_name}\")\n",
    "                except Exception as e:\n",
    "                    failed_count += 1\n",
    "                    self.logger.error(f\"Notebook generation future failed: {e}\")\n",
    "        \n",
    "        self.logger.info(f\"✅ Notebook generation complete: {completed_count} succeeded, {failed_count} failed\")\n",
    "        log_print(f\"✅ All {total_domains} notebooks processed: {completed_count} succeeded, {failed_count} failed\")\n",
    "\n",
    "    # === NEW: Helper method to determine if use case should show examples ===\n",
    "    def should_show_example_for_use_case(self, use_case_dict: dict, domain_index: int) -> bool:\n",
    "        \"\"\"\n",
    "        Determines if a use case should show example results based on the configured option.\n",
    "        Simplified to Yes/No: If \"Yes\", show ALL use cases. If \"No\", show none.\n",
    "        \n",
    "        Args:\n",
    "            use_case_dict: The use case dictionary (kept for compatibility)\n",
    "            domain_index: The index of the use case within its domain (kept for compatibility)\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if examples should be shown (option is \"Yes\"), False otherwise\n",
    "        \"\"\"\n",
    "        return str(getattr(self, \"show_query_results_option\", \"\")).strip().lower() == \"yes\" and bool(self.sql_warehouse_id)\n",
    "    \n",
    "    def _ensure_sql_results_cache_dir(self):\n",
    "        if getattr(self, \"sql_results_cache_dir\", None):\n",
    "            return self.sql_results_cache_dir\n",
    "        cache_dir = os.path.join(tempfile.gettempdir(), f\"inspire_sql_cache_{self.business_name.replace(' ', '_')}\")\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        self.sql_results_cache_dir = cache_dir\n",
    "        return cache_dir\n",
    "\n",
    "    def _prepare_example_result(self, columns, schema_columns, row_data, use_case_id):\n",
    "        original_columns = []\n",
    "        free_text_columns = []\n",
    "        llm_columns_with_nulls = []\n",
    "        llm_keywords = ['plan', 'strategy', 'narrative', 'rationale', 'recommendation',\n",
    "                       'tactics', 'steps', 'measures', 'summary', 'brief', 'assessment',\n",
    "                       'analysis', 'insights', 'guidance', 'approach', 'methodology']\n",
    "        column_values = {}\n",
    "        for i, col_name in enumerate(columns):\n",
    "            if i < len(row_data):\n",
    "                column_values[col_name] = row_data[i]\n",
    "            else:\n",
    "                column_values[col_name] = None\n",
    "        for col_schema in schema_columns:\n",
    "            col_name = getattr(col_schema, \"name\", None)\n",
    "            if isinstance(col_schema, dict):\n",
    "                col_name = col_schema.get(\"name\", col_name)\n",
    "                col_type_raw = col_schema.get(\"type_name\") or col_schema.get(\"type_text\")\n",
    "            else:\n",
    "                col_type_raw = getattr(col_schema, \"type_name\", None)\n",
    "            col_type = str(col_type_raw) if col_type_raw else 'STRING'\n",
    "            if not col_name:\n",
    "                continue\n",
    "            value = column_values.get(col_name)\n",
    "            is_string = 'STRING' in col_type.upper() or 'CHAR' in col_type.upper() or col_type.upper() == 'BINARY'\n",
    "            is_llm_column = any(keyword in col_name.lower() for keyword in llm_keywords)\n",
    "            if is_string:\n",
    "                str_value = str(value) if value is not None else None\n",
    "                if is_llm_column and (value is None or str_value in ['None', '', 'null', 'NULL']):\n",
    "                    llm_columns_with_nulls.append(col_name)\n",
    "                if str_value and len(str_value) > 100:\n",
    "                    free_text_columns.append(col_name)\n",
    "                elif is_llm_column and str_value:\n",
    "                    free_text_columns.append(col_name)\n",
    "                else:\n",
    "                    original_columns.append(col_name)\n",
    "            else:\n",
    "                original_columns.append(col_name)\n",
    "        if llm_columns_with_nulls:\n",
    "            return {\n",
    "                'status': 'requires_modification',\n",
    "                'data': [],\n",
    "                'message': f\"Query requires user modification - null values in LLM columns: {', '.join(llm_columns_with_nulls)}\"\n",
    "            }\n",
    "        if not free_text_columns:\n",
    "            return {\n",
    "                'status': 'no_free_text',\n",
    "                'data': [],\n",
    "                'message': 'No free text columns found in results'\n",
    "            }\n",
    "        selected_original = original_columns[:5]\n",
    "        selected_free_text = free_text_columns[:5]\n",
    "        selected_columns = selected_original + selected_free_text\n",
    "        transposed_data = []\n",
    "        for col in selected_columns:\n",
    "            value = column_values.get(col)\n",
    "            col_type = 'free_text' if col in free_text_columns else 'original'\n",
    "            if value is None:\n",
    "                formatted_value = 'N/A'\n",
    "            elif isinstance(value, (int, float)):\n",
    "                formatted_value = str(value)\n",
    "            else:\n",
    "                formatted_value = str(value)\n",
    "            transposed_data.append({\n",
    "                'column': col,\n",
    "                'value': formatted_value,\n",
    "                'type': col_type\n",
    "            })\n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'data': transposed_data,\n",
    "            'message': f'Successfully retrieved example with {len(selected_original)} original columns and {len(selected_free_text)} free text columns'\n",
    "        }\n",
    "    \n",
    "    # === NEW: Query Execution for Example Results ===\n",
    "    def execute_query_for_example(self, sql_query: str, use_case_id: str) -> dict:\n",
    "        \"\"\"\n",
    "        Executes a query with LIMIT 1 using SQL Warehouse, transposes columns to rows, and prepares example results.\n",
    "        \n",
    "        Args:\n",
    "            sql_query: The SQL query to execute (will be modified to LIMIT 1)\n",
    "            use_case_id: The use case identifier for logging\n",
    "            \n",
    "        Returns:\n",
    "            dict with 'status', 'data' (list of {column, value, type} dicts), and 'message'\n",
    "        \"\"\"\n",
    "        # Check if SQL Warehouse name is provided\n",
    "        if not self.sql_warehouse_name or not self.sql_warehouse_id:\n",
    "            self.logger.warning(f\"SQL Warehouse not resolved - skipping example results for use case {use_case_id}\")\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'data': [],\n",
    "                'message': 'SQL Warehouse not configured'\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            # Modify query to LIMIT 1\n",
    "            import re\n",
    "            if 'LIMIT' in sql_query.upper():\n",
    "                modified_query = re.sub(r'LIMIT\\s+\\d+', 'LIMIT 1', sql_query, flags=re.IGNORECASE)\n",
    "            else:\n",
    "                modified_query = sql_query.rstrip().rstrip(';') + ' LIMIT 1'\n",
    "            \n",
    "            self.logger.info(f\"Executing query for example results (use case {use_case_id}) on warehouse: {self.sql_warehouse_name}...\")\n",
    "            \n",
    "            from databricks.sdk.service import sql as sql_service\n",
    "            warehouse_id = self.sql_warehouse_id\n",
    "            \n",
    "            # Execute query using statement execution API\n",
    "            try:\n",
    "                statement = self.w_client.statement_execution.execute_statement(\n",
    "                    warehouse_id=warehouse_id,\n",
    "                    statement=modified_query,\n",
    "                    wait_timeout=\"50s\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Query execution failed for use case {use_case_id}: {str(e)[:100]}\")\n",
    "                return {\n",
    "                    'status': 'error',\n",
    "                    'data': [],\n",
    "                    'message': 'Query execution failed'\n",
    "                }\n",
    "            \n",
    "            # Check if query succeeded\n",
    "            if statement.status.state != sql_service.StatementState.SUCCEEDED:\n",
    "                # Extract ONLY the error message, not the full SQL query (for detailed log file)\n",
    "                if statement.status.error:\n",
    "                    full_error = statement.status.error.message\n",
    "                    # Split on \"== SQL ==\" to get only error part\n",
    "                    if \"== SQL ==\" in full_error:\n",
    "                        clean_error = full_error.split(\"== SQL ==\")[0].strip()\n",
    "                    else:\n",
    "                        # Take only first 500 chars if no SQL marker\n",
    "                        clean_error = full_error[:500]\n",
    "                    error_msg = clean_error\n",
    "                else:\n",
    "                    error_msg = f\"Query failed with state {statement.status.state}\"\n",
    "                # Log detailed error to file only (DEBUG level), not to console\n",
    "                self.logger.debug(f\"SQL execution error for use case {use_case_id}: {error_msg}\")\n",
    "                return {\n",
    "                    'status': 'error',\n",
    "                    'data': [],\n",
    "                    'message': error_msg\n",
    "                }\n",
    "            \n",
    "            # Check if we have results\n",
    "            if not statement.result or not statement.result.data_array or not statement.result.data_array:\n",
    "                return {\n",
    "                    'status': 'empty',\n",
    "                    'data': [],\n",
    "                    'message': 'Query returned no results'\n",
    "                }\n",
    "            \n",
    "            # Get column names from manifest\n",
    "            if not statement.manifest or not statement.manifest.schema or not statement.manifest.schema.columns:\n",
    "                self.logger.warning(f\"Query result has no schema for use case {use_case_id}\")\n",
    "                return {\n",
    "                    'status': 'error',\n",
    "                    'data': [],\n",
    "                    'message': 'Query result has no schema'\n",
    "                }\n",
    "            \n",
    "            # Get first row of data\n",
    "            columns = [col.name for col in statement.manifest.schema.columns]\n",
    "            if not statement.result.data_array or len(statement.result.data_array) == 0:\n",
    "                return {\n",
    "                    'status': 'empty',\n",
    "                    'data': [],\n",
    "                    'message': 'Query returned no results'\n",
    "                }\n",
    "            \n",
    "            row_data = statement.result.data_array[0]  # First row\n",
    "            \n",
    "            return self._prepare_example_result(columns, statement.manifest.schema.columns, row_data, use_case_id)\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            self.logger.warning(f\"Could not execute query for example results (use case {use_case_id}): {str(e)[:200]}\")\n",
    "            self.logger.debug(f\"Full traceback for {use_case_id}: {traceback.format_exc()}\")\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'data': [],\n",
    "                'message': f'Query execution error: {str(e)[:100]}'\n",
    "            }\n",
    "\n",
    "    def execute_sql_with_fixing(self, use_case: dict, directly_involved_schema: str = \"\") -> dict:\n",
    "        \"\"\"\n",
    "        Execute SQL for a use case with automatic error fixing and regeneration.\n",
    "        Strategy:\n",
    "        1. Execute original SQL\n",
    "        2. If fails, try to fix it once\n",
    "        3. If still fails, regenerate completely new SQL from scratch\n",
    "        4. Execute new SQL\n",
    "        5. If fails, try to fix it once\n",
    "        6. If still fails, proceed with comment for user to review\n",
    "        \n",
    "        Args:\n",
    "            use_case: Use case dictionary with SQL field\n",
    "            directly_involved_schema: Optional schema details for SQL fixing (can be empty)\n",
    "            \n",
    "        Returns:\n",
    "            dict with 'status', 'data', 'message', and 'sql' (potentially fixed/regenerated SQL)\n",
    "        \"\"\"\n",
    "        use_case_id = use_case.get('No', 'Unknown')\n",
    "        sql_query = use_case.get('SQL', '')\n",
    "        tables_involved_str = use_case.get('Tables Involved', '')\n",
    "        \n",
    "        if not sql_query:\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'data': [],\n",
    "                'message': 'No SQL provided',\n",
    "                'sql': sql_query\n",
    "            }\n",
    "        \n",
    "        # STEP 1: Try to execute original SQL\n",
    "        exec_result = self.execute_query_for_example(sql_query, use_case_id)\n",
    "        \n",
    "        # If successful, return immediately\n",
    "        if exec_result['status'] != 'error':\n",
    "            exec_result['sql'] = sql_query\n",
    "            return exec_result\n",
    "        \n",
    "        # STEP 2: Execution failed - try to fix it once\n",
    "        error_message = exec_result.get('message', 'Unknown error')\n",
    "        concise_error = error_message.split(\" - Error: \")[-1] if \" - Error: \" in error_message else error_message[:300]\n",
    "        \n",
    "        self.logger.warning(f\"Use Case {use_case_id} failed to execute - attempting to fix\")\n",
    "        \n",
    "        # Prepare prompt for SQL Syntax Reviewer\n",
    "        reviewer_prompt_vars = {\n",
    "            \"use_case_id\": use_case_id,\n",
    "            \"use_case_name\": use_case.get('Name', ''),\n",
    "            \"business_domain\": use_case.get('Business Domain', ''),\n",
    "            \"statement\": use_case.get('Statement', ''),\n",
    "            \"tables_involved\": tables_involved_str,\n",
    "            \"directly_involved_schema\": directly_involved_schema,\n",
    "            \"original_sql\": sql_query,\n",
    "            \"explain_error\": concise_error,\n",
    "            \"use_case_columns\": use_case.get('Involved Columns') or use_case.get('Columns Involved') or \"\"\n",
    "        }\n",
    "        \n",
    "        adaptive_timeout = self._calculate_adaptive_sql_timeout(use_case)\n",
    "        \n",
    "        try:\n",
    "            fixed_sql = self.ai_agent.run_worker(\n",
    "                step_name=f\"Fix_SQL_Execution_{use_case_id}_Attempt1\",\n",
    "                worker_prompt_path=\"USE_CASE_SQL_FIX_PROMPT\",\n",
    "                prompt_vars=reviewer_prompt_vars,\n",
    "                response_schema=None,\n",
    "                timeout_override=adaptive_timeout,\n",
    "                max_retries_override=self.max_retry_attempts\n",
    "            )\n",
    "            \n",
    "            # Test fixed SQL\n",
    "            exec_result_fixed = self.execute_query_for_example(fixed_sql, use_case_id)\n",
    "            \n",
    "            if exec_result_fixed['status'] != 'error':\n",
    "                self.logger.info(f\"Use case {use_case_id} SQL fixed successfully on first fix attempt\")\n",
    "                exec_result_fixed['sql'] = fixed_sql\n",
    "                use_case['SQL'] = fixed_sql\n",
    "                return exec_result_fixed\n",
    "            \n",
    "            # STEP 3: Fix failed - regenerate completely new SQL\n",
    "            self.logger.warning(f\"Use Case {use_case_id} fix failed - regenerating SQL from scratch\")\n",
    "            error_message_fixed = exec_result_fixed.get('message', 'Unknown error')\n",
    "            concise_error_fixed = error_message_fixed.split(\" - Error: \")[-1] if \" - Error: \" in error_message_fixed else error_message_fixed[:300]\n",
    "            \n",
    "        except Exception as fix_e:\n",
    "            self.logger.error(f\"Use case {use_case_id} SQL fix attempt failed: {str(fix_e)[:100]}\")\n",
    "            self.logger.warning(f\"Use Case {use_case_id} - regenerating SQL from scratch\")\n",
    "            concise_error_fixed = str(fix_e)[:300]\n",
    "        \n",
    "        # STEP 3: Regenerate SQL from scratch\n",
    "        try:\n",
    "            self.logger.info(f\"Regenerating SQL for Use Case {use_case_id}\")\n",
    "            \n",
    "            # Get enriched business context from merged_business_context\n",
    "            enriched_ctx = getattr(self, 'merged_business_context', {})\n",
    "            \n",
    "            # Use _generate_sql_for_use_case if available, otherwise use simpler approach\n",
    "            regeneration_prompt_vars = {\n",
    "                \"use_case_id\": use_case_id,\n",
    "                \"use_case_name\": use_case.get('Name', ''),\n",
    "                \"business_domain\": use_case.get('Business Domain', ''),\n",
    "                \"subdomain\": use_case.get('Subdomain', ''),\n",
    "                \"type\": use_case.get('type', ''),\n",
    "                \"statement\": use_case.get('Statement', ''),\n",
    "                \"solution\": use_case.get('Solution', ''),\n",
    "                \"tables_involved\": tables_involved_str,\n",
    "                \"directly_involved_schema\": directly_involved_schema,\n",
    "                \"use_case_columns\": use_case.get('Involved Columns') or use_case.get('Columns Involved') or \"\",\n",
    "                \"foreign_key_relationships\": \"None\",\n",
    "                \"unstructured_docs\": \"\",  # Not available during execution/validation, use empty string\n",
    "                \"previous_feedback\": \"\",  # Required by USE_CASE_SQL_GEN_PROMPT\n",
    "                \"ai_functions_summary\": generate_ai_functions_doc(\"summary\"),  # Required by USE_CASE_SQL_GEN_PROMPT\n",
    "                \"statistical_functions_detailed\": generate_statistical_functions_doc(\"detailed\"),\n",
    "                \"previous_sql\": sql_query,\n",
    "                \"previous_error\": concise_error_fixed,\n",
    "                \"business_name\": self.business_name,\n",
    "                \"sql_model_serving\": self.sql_model_serving,\n",
    "                # Enriched business context for persona enrichment in ai_query prompts\n",
    "                \"enriched_business_context\": enriched_ctx.get('business_context', 'General business operations'),\n",
    "                \"enriched_strategic_goals\": enriched_ctx.get('strategic_goals', 'Operational excellence and customer satisfaction') if isinstance(enriched_ctx.get('strategic_goals'), str) else ', '.join(enriched_ctx.get('strategic_goals', ['Operational excellence'])),\n",
    "                \"enriched_business_priorities\": enriched_ctx.get('business_priorities', 'Digital transformation and cost optimization') if isinstance(enriched_ctx.get('business_priorities'), str) else ', '.join(enriched_ctx.get('business_priorities', ['Digital transformation'])),\n",
    "                \"enriched_strategic_initiative\": enriched_ctx.get('strategic_initiative', 'Data-driven decision making'),\n",
    "                \"enriched_value_chain\": enriched_ctx.get('value_chain', 'Standard business operations'),\n",
    "                \"enriched_revenue_model\": enriched_ctx.get('revenue_model', 'Diverse revenue streams'),\n",
    "                \"interpreted_regeneration_context\": \"\"  # Required by USE_CASE_SQL_GEN_PROMPT\n",
    "            }\n",
    "            \n",
    "            regenerated_sql = self.ai_agent.run_worker(\n",
    "                step_name=f\"Regenerate_SQL_{use_case_id}\",\n",
    "                worker_prompt_path=\"USE_CASE_SQL_GEN_PROMPT\",\n",
    "                prompt_vars=regeneration_prompt_vars,\n",
    "                response_schema=None,\n",
    "                timeout_override=adaptive_timeout,\n",
    "                max_retries_override=self.max_retry_attempts\n",
    "            )\n",
    "            \n",
    "            # STEP 4: Execute regenerated SQL\n",
    "            exec_result_regen = self.execute_query_for_example(regenerated_sql, use_case_id)\n",
    "            \n",
    "            if exec_result_regen['status'] != 'error':\n",
    "                self.logger.info(f\"Use case {use_case_id} SQL regenerated and executed successfully\")\n",
    "                exec_result_regen['sql'] = regenerated_sql\n",
    "                use_case['SQL'] = regenerated_sql\n",
    "                return exec_result_regen\n",
    "            \n",
    "            # STEP 5: Regenerated SQL failed - try to fix it once\n",
    "            self.logger.warning(f\"Use Case {use_case_id} regenerated SQL failed - attempting final fix\")\n",
    "            error_message_regen = exec_result_regen.get('message', 'Unknown error')\n",
    "            concise_error_regen = error_message_regen.split(\" - Error: \")[-1] if \" - Error: \" in error_message_regen else error_message_regen[:300]\n",
    "            \n",
    "            reviewer_prompt_vars_final = {\n",
    "                \"use_case_id\": use_case_id,\n",
    "                \"use_case_name\": use_case.get('Name', ''),\n",
    "                \"business_domain\": use_case.get('Business Domain', ''),\n",
    "                \"statement\": use_case.get('Statement', ''),\n",
    "                \"tables_involved\": tables_involved_str,\n",
    "                \"directly_involved_schema\": directly_involved_schema,\n",
    "                \"original_sql\": regenerated_sql,\n",
    "                \"explain_error\": concise_error_regen,\n",
    "                \"use_case_columns\": use_case.get('Involved Columns') or use_case.get('Columns Involved') or \"\"\n",
    "            }\n",
    "            \n",
    "            final_fixed_sql = self.ai_agent.run_worker(\n",
    "                step_name=f\"Fix_SQL_Execution_{use_case_id}_FinalAttempt\",\n",
    "                worker_prompt_path=\"USE_CASE_SQL_FIX_PROMPT\",\n",
    "                prompt_vars=reviewer_prompt_vars_final,\n",
    "                response_schema=None,\n",
    "                timeout_override=adaptive_timeout,\n",
    "                max_retries_override=self.max_retry_attempts\n",
    "            )\n",
    "            \n",
    "            # Test final fixed SQL\n",
    "            exec_result_final = self.execute_query_for_example(final_fixed_sql, use_case_id)\n",
    "            \n",
    "            if exec_result_final['status'] != 'error':\n",
    "                self.logger.info(f\"Use case {use_case_id} SQL fixed successfully on final attempt\")\n",
    "                exec_result_final['sql'] = final_fixed_sql\n",
    "                use_case['SQL'] = final_fixed_sql\n",
    "                return exec_result_final\n",
    "            \n",
    "            # STEP 6: All attempts failed - return with user review comment\n",
    "            self.logger.error(f\"Use case {use_case_id} - All SQL validation attempts failed. User needs to review.\")\n",
    "            error_message_final = exec_result_final.get('message', 'Unknown error')\n",
    "            \n",
    "            # Add comment to SQL for user review\n",
    "            commented_sql = f\"-- ⚠️ USER REVIEW REQUIRED: SQL has syntax errors that could not be auto-fixed\\n-- Last error: {error_message_final[:200]}\\n\\n{final_fixed_sql}\"\n",
    "            \n",
    "            use_case['SQL'] = commented_sql\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'data': [],\n",
    "                'message': f'⚠️ USER REVIEW REQUIRED: SQL validation failed after multiple attempts. Error: {error_message_final[:200]}',\n",
    "                'sql': commented_sql\n",
    "            }\n",
    "            \n",
    "        except Exception as regen_e:\n",
    "            self.logger.error(f\"Use case {use_case_id} SQL regeneration failed: {str(regen_e)[:100]}\")\n",
    "            \n",
    "            # Add comment to original SQL for user review\n",
    "            commented_sql = f\"-- ⚠️ USER REVIEW REQUIRED: SQL has syntax errors and regeneration failed\\n-- Error: {str(regen_e)[:200]}\\n\\n{sql_query}\"\n",
    "            \n",
    "            use_case['SQL'] = commented_sql\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'data': [],\n",
    "                'message': f'⚠️ USER REVIEW REQUIRED: SQL validation and regeneration failed. Error: {str(regen_e)[:200]}',\n",
    "                'sql': commented_sql\n",
    "            }\n",
    "\n",
    "    def _validate_and_cache_sql_results_parallel(self, use_cases: list):\n",
    "        \"\"\"\n",
    "        Validate SQL queries and cache results to disk in parallel.\n",
    "        This avoids running queries sequentially during PDF generation.\n",
    "        \n",
    "        Args:\n",
    "            use_cases: List of use case dictionaries with SQL field\n",
    "        \"\"\"\n",
    "        if not self.sql_warehouse_name or not self.sql_warehouse_id:\n",
    "            self.logger.info(\"SQL Warehouse not configured or ID not resolved; skipping SQL validation and caching.\")\n",
    "            return\n",
    "        cache_dir = self._ensure_sql_results_cache_dir()\n",
    "        \n",
    "        self.logger.info(f\"\uD83D\uDCC1 SQL results will be cached to: {cache_dir}\")\n",
    "        status_map = {}\n",
    "        message_map = {}\n",
    "\n",
    "        use_cases_to_validate = []\n",
    "        uc_lookup = {}\n",
    "        reused_cached = 0\n",
    "        show_results_enabled = str(getattr(self, \"show_query_results_option\", \"\")).strip().lower() == \"yes\"\n",
    "        if show_results_enabled:\n",
    "            # Single pass to check cache and identify what needs validation\n",
    "            for idx, uc in enumerate(use_cases, start=1):\n",
    "                if uc.get('SQL') and self.should_show_example_for_use_case(uc, idx):\n",
    "                    use_case_id = uc.get('No', f'UC-{idx}')\n",
    "                    cache_file = os.path.join(cache_dir, f\"{use_case_id}.json\")\n",
    "                    cached_result = None\n",
    "                    \n",
    "                    if os.path.exists(cache_file):\n",
    "                        try:\n",
    "                            with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                                cached_result = json.load(f)\n",
    "                        except Exception as read_err:\n",
    "                            self.logger.debug(f\"Failed to read cached result for {use_case_id}: {str(read_err)[:100]}\")\n",
    "\n",
    "                    # Check if cache is valid and matches current SQL\n",
    "                    if cached_result and cached_result.get('sql', '').strip() == uc.get('SQL', '').strip() and cached_result.get('status') != 'error':\n",
    "                        status_map[use_case_id] = 'success'\n",
    "                        message_map[use_case_id] = cached_result.get('message', '')\n",
    "                        reused_cached += 1\n",
    "                        self.logger.debug(f\"Using cached result for {use_case_id}\")\n",
    "                    else:\n",
    "                        # Needs validation\n",
    "                        use_cases_to_validate.append((idx, uc))\n",
    "                        uc_lookup[use_case_id] = uc\n",
    "                        if not cached_result:\n",
    "                             self.logger.warning(f\"No cached result found for {use_case_id}; expected from validation run.\")\n",
    "        \n",
    "        if not use_cases_to_validate:\n",
    "            self.logger.info(f\"No new SQL validations required; reused {reused_cached} cached results.\")\n",
    "            return\n",
    "        \n",
    "        total_queries = len(use_cases_to_validate)\n",
    "        self.logger.info(f\"\uD83D\uDD04 Validating and caching {total_queries} SQL queries in parallel (max {self.max_parallelism} concurrent)... Reused {reused_cached} cached results.\")\n",
    "        log_print(f\"\uD83D\uDD04 Validating and caching {total_queries} SQL queries in parallel (max {self.max_parallelism} concurrent)...\")\n",
    "        log_print(f\"   ⏱️  Each query takes ~5-10 seconds. Estimated time: {(total_queries * 7 / self.max_parallelism / 60):.1f} minutes\")\n",
    "        \n",
    "        # Define worker function for parallel execution\n",
    "        def validate_and_cache_worker(idx_uc_tuple):\n",
    "            idx, uc = idx_uc_tuple\n",
    "            use_case_id = uc.get('No', f'UC-{idx}')\n",
    "            cache_file = os.path.join(cache_dir, f\"{use_case_id}.json\")\n",
    "            \n",
    "            try:\n",
    "                cached_result = None\n",
    "                if os.path.exists(cache_file):\n",
    "                    try:\n",
    "                        with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                            cached_result = json.load(f)\n",
    "                    except Exception as read_err:\n",
    "                        self.logger.debug(f\"Failed to read cached result for {use_case_id}: {str(read_err)[:100]}\")\n",
    "                if cached_result:\n",
    "                    cached_sql = cached_result.get('sql')\n",
    "                    if cached_sql and cached_sql.strip() == uc.get('SQL', '').strip() and cached_result.get('status') != 'error':\n",
    "                        status_value = 'success'\n",
    "                        status_map[use_case_id] = status_value\n",
    "                        message_map[use_case_id] = cached_result.get('message', '')\n",
    "                        self.logger.info(f\"Using cached SQL result for {use_case_id}; skipping re-execution.\")\n",
    "                        return (use_case_id, status_value, cached_result.get('message', ''))\n",
    "                # Execute SQL with automatic fixing\n",
    "                result = self.execute_sql_with_fixing(uc)\n",
    "                status_value = 'success' if result['status'] != 'error' else 'error'\n",
    "                status_map[use_case_id] = status_value\n",
    "                message_map[use_case_id] = result.get('message', '')\n",
    "                \n",
    "                # Cache result to disk\n",
    "                with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "                \n",
    "                return (use_case_id, status_value, result.get('message', ''))\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to validate/cache {use_case_id}: {str(e)[:100]}\")\n",
    "                # Cache error result\n",
    "                error_result = {\n",
    "                    'status': 'error',\n",
    "                    'data': [],\n",
    "                    'message': f'Validation failed: {str(e)[:100]}',\n",
    "                    'sql': uc.get('SQL', '')\n",
    "                }\n",
    "                with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(error_result, f, ensure_ascii=False, indent=2)\n",
    "                status_map[use_case_id] = 'error'\n",
    "                message_map[use_case_id] = str(e)\n",
    "                return (use_case_id, 'error', str(e))\n",
    "        \n",
    "        # ADAPTIVE PARALLELISM: Calculate based on number of queries to validate\n",
    "        validation_parallelism, reason = calculate_adaptive_parallelism(\n",
    "            \"sql_validation\", self.max_parallelism,\n",
    "            num_items=len(use_cases_to_validate),\n",
    "            is_llm_operation=False, logger=self.logger\n",
    "        )\n",
    "        log_adaptive_parallelism_decision(\"sql_validation\", validation_parallelism, self.max_parallelism, reason)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=validation_parallelism, thread_name_prefix=\"SQLValidator\") as executor:\n",
    "            futures = {executor.submit(validate_and_cache_worker, idx_uc): idx_uc for idx_uc in use_cases_to_validate}\n",
    "            \n",
    "            completed = 0\n",
    "            for future in as_completed(futures):\n",
    "                completed += 1\n",
    "                try:\n",
    "                    use_case_id, status, message = future.result(timeout=180)  # 3 min timeout per query\n",
    "                    \n",
    "                    # Progress logging every 10% or every 5 queries\n",
    "                    if completed % max(1, total_queries // 10) == 0 or completed % 5 == 0:\n",
    "                        self.logger.info(f\"\uD83D\uDCCA SQL validation progress: {completed}/{total_queries} ({100*completed/total_queries:.1f}%)\")\n",
    "                        log_print(f\"   ✓ {completed}/{total_queries} queries validated ({100*completed/total_queries:.1f}%)\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Query validation task failed: {str(e)[:100]}\")\n",
    "        \n",
    "        timeout_ids = [\n",
    "            uc_id for uc_id, msg in message_map.items()\n",
    "            if msg and ('timeout' in msg.lower() or 'time out' in msg.lower())\n",
    "        ]\n",
    "        \n",
    "        if timeout_ids:\n",
    "            retry_workers = max(1, self.max_parallelism // 2)\n",
    "            self.logger.warning(f\"⚠️  Retrying {len(timeout_ids)} timeout failures with reduced parallelism ({retry_workers})...\")\n",
    "            with ThreadPoolExecutor(max_workers=retry_workers, thread_name_prefix=\"SQLValidatorRetry\") as executor:\n",
    "                retry_futures = {\n",
    "                    executor.submit(validate_and_cache_worker, (idx, uc_lookup[uc_id])): uc_id\n",
    "                    for idx, uc in use_cases_to_validate\n",
    "                    for uc_id in [uc.get('No', f'UC-{idx}')]\n",
    "                    if uc_id in timeout_ids\n",
    "                }\n",
    "                for future in as_completed(retry_futures):\n",
    "                    try:\n",
    "                        future.result(timeout=180)\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Retry validation task failed: {str(e)[:100]}\")\n",
    "            \n",
    "            timeout_ids = [\n",
    "                uc_id for uc_id, msg in message_map.items()\n",
    "                if msg and ('timeout' in msg.lower() or 'time out' in msg.lower()) and status_map.get(uc_id) == 'error'\n",
    "            ]\n",
    "        \n",
    "        discard_ids = timeout_ids\n",
    "        if discard_ids:\n",
    "            self.validation_timeouts_discarded = discard_ids\n",
    "            self.logger.warning(f\"Discarding {len(discard_ids)} use cases due to repeated timeouts: {', '.join(discard_ids)}\")\n",
    "            use_cases[:] = [uc for uc in use_cases if uc.get('No') not in discard_ids]\n",
    "        \n",
    "        success_count = sum(1 for status in status_map.values() if status == 'success')\n",
    "        error_count = sum(1 for status in status_map.values() if status != 'success')\n",
    "        total_final = success_count + error_count\n",
    "        self.logger.info(f\"✅ SQL validation complete: {success_count} succeeded, {error_count} failed/errored (Total: {total_final})\")\n",
    "        log_print(f\"✅ SQL validation complete: {success_count} succeeded, {error_count} failed\")\n",
    "        log_print(f\"\uD83D\uDCC1 Results cached to: {cache_dir}\")\n",
    "        self.sql_validation_status_map = status_map\n",
    "        self.sql_validation_error_ids = [uc_id for uc_id, status in status_map.items() if status != 'success']\n",
    "        return {\n",
    "            \"success_count\": success_count,\n",
    "            \"error_count\": error_count,\n",
    "            \"status_map\": status_map,\n",
    "            \"message_map\": message_map\n",
    "        }\n",
    "\n",
    "    def _get_cached_sql_result(self, use_case_id: str) -> dict:\n",
    "        \"\"\"\n",
    "        Retrieve cached SQL result from disk.\n",
    "        \n",
    "        Args:\n",
    "            use_case_id: Use case ID\n",
    "            \n",
    "        Returns:\n",
    "            Cached result dict or error dict if not found\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'sql_results_cache_dir'):\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'data': [],\n",
    "                'message': 'Cache not initialized'\n",
    "            }\n",
    "        \n",
    "        cache_file = os.path.join(self.sql_results_cache_dir, f\"{use_case_id}.json\")\n",
    "        \n",
    "        if not os.path.exists(cache_file):\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'data': [],\n",
    "                'message': 'Cached result not found'\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to read cached result for {use_case_id}: {str(e)[:100]}\")\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'data': [],\n",
    "                'message': f'Failed to read cache: {str(e)[:100]}'\n",
    "            }\n",
    "\n",
    "    # === MODIFIED: PDF Generation (Req 1, 2, 3) ===\n",
    "    def generate_catalog_pdf(self, language: str, lang_abbr: str, translations: dict, summary_dict: dict, grouped_data: dict, transliterated_name: str):\n",
    "        self.logger.info(f\"--- Starting PDF Catalog Generation for {language} ---\")\n",
    "        \n",
    "        t = translations\n",
    "        is_rtl = (language == \"Arabic\")\n",
    "        \n",
    "        def _install_dependencies(logger_instance) -> bool:\n",
    "            try:\n",
    "                import weasyprint\n",
    "                logger_instance.info(\"PDF package (weasyprint) already installed.\")\n",
    "                return True\n",
    "            except ImportError:\n",
    "                logger_instance.info(\"Installing required PDF package (weasyprint)...\")\n",
    "                try: \n",
    "                    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"weasyprint\"])\n",
    "                    import weasyprint\n",
    "                    logger_instance.info(\"Successfully installed weasyprint.\")\n",
    "                    return True\n",
    "                except Exception as e: \n",
    "                    logger_instance.error(f\"Failed to install weasyprint: {e}\")\n",
    "                    print(\"ERROR: Failed to install 'weasyprint'. PDF generation cannot continue.\", file=sys.stderr)\n",
    "                    return False\n",
    "\n",
    "        def _build_html(grouped_data: dict, summary_dict: dict, business_name: str, translations: dict, is_rtl: bool) -> str:\n",
    "            self.logger.info(f\"Building HTML for PDF ({language})...\")\n",
    "            t = translations; now = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            direction = \"rtl\" if is_rtl else \"ltr\"; align = \"right\" if is_rtl else \"left\"\n",
    "            def e(text): return html.escape(str(text))\n",
    "\n",
    "            # === MODIFIED: CSS WITH FIXES (Request #13) ===\n",
    "            # 1. @import rules MUST be at the very top (before @font-face)\n",
    "            # 2. Removed unsupported properties: box-shadow, text-shadow\n",
    "            # 3. Font warnings will be suppressed in WeasyPrint config\n",
    "            css = f\"\"\"\n",
    "            /* @import rules MUST be first in CSS */\n",
    "            @import url('https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&display=swap');\n",
    "            @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Devanagari:wght@300;400;700&display=swap');\n",
    "            @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Arabic:wght@300;400;700&display=swap');\n",
    "            @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@300;400;700&display=swap');\n",
    "            @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@300;400;700&display=swap');\n",
    "            @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;700&display=swap');\n",
    "            @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Tamil:wght@300;400;700&display=swap');\n",
    "            @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+Thai:wght@300;400;700&display=swap');\n",
    "           \n",
    "            /* Font-face declarations with local fallbacks */\n",
    "            @font-face {{\n",
    "                font-family: 'NotoSansDevanagari';\n",
    "                src: local('Noto Sans Devanagari'), local('Lohit Devanagari'), local('Mangal');\n",
    "                font-weight: normal;\n",
    "                font-style: normal;\n",
    "            }}\n",
    "            @font-face {{\n",
    "                font-family: 'NotoSansArabic';\n",
    "                src: local('Noto Sans Arabic'), local('Arial Unicode MS'), local('DejaVu Sans');\n",
    "                font-weight: normal;\n",
    "                font-style: normal;\n",
    "            }}\n",
    "            @font-face {{\n",
    "                font-family: 'NotoSansCJK';\n",
    "                src: local('Noto Sans CJK'), local('Microsoft YaHei'), local('SimSun'), local('MS Gothic');\n",
    "                font-weight: normal;\n",
    "                font-style: normal;\n",
    "            }}\n",
    "            @font-face {{\n",
    "                font-family: 'NotoSansJP';\n",
    "                src: local('Noto Sans JP'), local('Yu Gothic'), local('MS Gothic'), local('Meiryo');\n",
    "                font-weight: normal;\n",
    "                font-style: normal;\n",
    "            }}\n",
    "            @font-face {{\n",
    "                font-family: 'NotoSansKR';\n",
    "                src: local('Noto Sans KR'), local('Malgun Gothic'), local('Gulim');\n",
    "                font-weight: normal;\n",
    "                font-style: normal;\n",
    "            }}\n",
    "            \n",
    "            @page {{\n",
    "                size: A4; margin: 2.5cm;\n",
    "                @bottom-left {{\n",
    "                    content: 'Databricks Inspire AI';\n",
    "                    font-family: 'Roboto', 'Noto Sans Devanagari', 'Noto Sans Arabic', 'Noto Sans SC', 'Noto Sans JP', 'Noto Sans KR', 'Noto Sans Tamil', sans-serif; font-size: 9pt; color: #555;\n",
    "                }}\n",
    "                @bottom-right {{\n",
    "                    content: '{now}';\n",
    "                    font-family: 'Roboto', 'Noto Sans Devanagari', 'Noto Sans Arabic', 'Noto Sans SC', 'Noto Sans JP', 'Noto Sans KR', 'Noto Sans Tamil', sans-serif; font-size: 9pt; color: #555;\n",
    "                }}\n",
    "            }}\n",
    "            html {{ counter-reset: page-counter; }}\n",
    "            body {{ \n",
    "                font-family: 'Roboto', 'Noto Sans Devanagari', 'Noto Sans Arabic', 'Noto Sans SC', 'Noto Sans JP', 'Noto Sans KR', 'Noto Sans Tamil', 'Noto Sans Thai', sans-serif; color: #333; line-height: 1.6; \n",
    "                direction: {direction}; text-align: {align}; \n",
    "            }}\n",
    "            h1, h2, h3 {{ font-weight: 700; color: #003366; margin-bottom: 0.5em; text-align: {align}; }}\n",
    "            \n",
    "            /* Page counter logic */\n",
    "            h1.page-title {{ font-size: 24pt; counter-increment: page-counter; }}\n",
    "            h2.page-title {{ font-size: 22pt; counter-increment: page-counter; }}\n",
    "            h2.domain-header {{ font-size: 18pt; counter-increment: none; }}\n",
    "            \n",
    "            h3 {{ color: #0B579E; font-size: 14pt; border-bottom: 2px solid #FF6900; padding-bottom: 5px; }}\n",
    "            p {{ margin-bottom: 1.2em; text-align: {align}; }}\n",
    "            table {{ width: 100%; border-collapse: collapse; margin-bottom: 2em; page-break-inside: auto; }}\n",
    "            th, td {{ border: 1px solid #ddd; padding: 10px; text-align: {align}; word-wrap: break-word; }}\n",
    "            th {{ background-color: #003366; color: white; font-weight: 700; }}\n",
    "            tr:nth-child(even) {{ background-color: #f9f9f9; }}\n",
    "            tr {{ page-break-inside: avoid; }}\n",
    "            \n",
    "            /* Enhanced cover page with gradient */\n",
    "            .cover-page {{ \n",
    "                display: flex; flex-direction: column; justify-content: space-between; align-items: center; \n",
    "                height: 24cm;\n",
    "                background: linear-gradient(135deg, #001a33 0%, #003366 50%, #004d99 100%);\n",
    "                color: white; text-align: center; page-break-after: always; \n",
    "                counter-increment: none;\n",
    "                position: relative;\n",
    "                overflow: visible;\n",
    "                padding: 2cm 0;\n",
    "            }}\n",
    "            /* Geometric decorative elements */\n",
    "            .cover-page::before {{\n",
    "                content: '';\n",
    "                position: absolute;\n",
    "                width: 450px;\n",
    "                height: 450px;\n",
    "                background: radial-gradient(circle, rgba(255, 105, 0, 0.15) 0%, rgba(0, 188, 212, 0.1) 100%);\n",
    "                border-radius: 50%;\n",
    "                top: -200px;\n",
    "                left: -200px;\n",
    "                z-index: 0;\n",
    "            }}\n",
    "            .cover-page::after {{\n",
    "                content: '';\n",
    "                position: absolute;\n",
    "                width: 350px;\n",
    "                height: 350px;\n",
    "                background: radial-gradient(circle, rgba(156, 39, 176, 0.12) 0%, rgba(76, 175, 80, 0.08) 100%);\n",
    "                border-radius: 50%;\n",
    "                bottom: -150px;\n",
    "                right: -150px;\n",
    "                z-index: 0;\n",
    "            }}\n",
    "            .cover-box-1 {{ text-align: center; z-index: 1; position: relative; }}\n",
    "            .cover-box-2 {{ \n",
    "                text-align: center; \n",
    "                margin-top: 2em; \n",
    "                max-width: 85%; \n",
    "                margin-left: auto; \n",
    "                margin-right: auto; \n",
    "                z-index: 1; \n",
    "                position: relative;\n",
    "                padding: 2.5em;\n",
    "                background: linear-gradient(135deg, rgba(255, 255, 255, 0.08) 0%, rgba(255, 105, 0, 0.05) 100%);\n",
    "                border-radius: 20px;\n",
    "                border: 2px solid rgba(255, 105, 0, 0.4);\n",
    "                /* REMOVED: box-shadow (unsupported by WeasyPrint) */\n",
    "            }}\n",
    "            .cover-page h1 {{ \n",
    "                font-size: 3.2em; \n",
    "                color: white; \n",
    "                margin: 0; \n",
    "                text-align: center; \n",
    "                counter-increment: none;\n",
    "                /* REMOVED: text-shadow (unsupported by WeasyPrint) */\n",
    "                letter-spacing: 2px;\n",
    "            }}\n",
    "            .cover-page h2 {{ \n",
    "                font-size: 2.8em; \n",
    "                color: #FF9944; \n",
    "                font-weight: 400; \n",
    "                margin: 0.5em 0; \n",
    "                text-align: center; \n",
    "                counter-increment: none;\n",
    "                /* REMOVED: text-shadow (unsupported by WeasyPrint) */\n",
    "            }}\n",
    "            .cover-page p {{ \n",
    "                font-size: 1.3em; \n",
    "                font-weight: 300; \n",
    "                margin-top: 1.5em; \n",
    "                text-align: center; \n",
    "                color: #e8e8e8;\n",
    "                letter-spacing: 1px;\n",
    "            }}\n",
    "            \n",
    "            .page-break {{ page-break-after: always; }}\n",
    "            .exec-summary {{ page-break-after: always; position: relative; }}\n",
    "            .exec-summary p {{ font-size: 1.1em; }}\n",
    "            /* Add decorative triangle to executive summary */\n",
    "            .exec-summary::before {{\n",
    "                content: '';\n",
    "                position: absolute;\n",
    "                top: 0;\n",
    "                right: 0;\n",
    "                width: 0;\n",
    "                height: 0;\n",
    "                border-style: solid;\n",
    "                border-width: 0 80px 80px 0;\n",
    "                border-color: transparent #00BCD4 transparent transparent;\n",
    "                opacity: 0.15;\n",
    "            }}\n",
    "            \n",
    "            .toc-page {{ page-break-after: always; position: relative; }}\n",
    "            .toc-table a {{ text-decoration: none; color: #003366; font-weight: bold; }}\n",
    "            .toc-table .page-ref {{ float: right; color: #555; }}\n",
    "            .toc-table .page-ref::before {{ content: target-counter(attr(href), page-counter); }}\n",
    "            /* Add colorful accent to TOC */\n",
    "            .toc-page::after {{\n",
    "                content: '';\n",
    "                position: absolute;\n",
    "                bottom: 20px;\n",
    "                left: 0;\n",
    "                width: 6px;\n",
    "                height: 200px;\n",
    "                background: linear-gradient(180deg, #FF6900 0%, #00BCD4 50%, #9C27B0 100%);\n",
    "                border-radius: 3px;\n",
    "            }}\n",
    "\n",
    "            /* Enhanced domain summary pages */\n",
    "            .domain-summary-page {{\n",
    "                page-break-before: always; page-break-after: always;\n",
    "                background: linear-gradient(135deg, #fafafa 0%, #f5f5f5 100%); \n",
    "                border: 2px solid #e0e0e0;\n",
    "                padding: 2cm; border-radius: 12px;\n",
    "                position: relative;\n",
    "                overflow: hidden;\n",
    "            }}\n",
    "            /* Colorful corner decorations for domain summaries */\n",
    "            .domain-summary-page::before {{\n",
    "                content: '';\n",
    "                position: absolute;\n",
    "                top: -30px;\n",
    "                right: -30px;\n",
    "                width: 120px;\n",
    "                height: 120px;\n",
    "                background: radial-gradient(circle, rgba(0, 188, 212, 0.2) 0%, transparent 70%);\n",
    "                border-radius: 50%;\n",
    "            }}\n",
    "            .domain-summary-page::after {{\n",
    "                content: '';\n",
    "                position: absolute;\n",
    "                bottom: -40px;\n",
    "                left: -40px;\n",
    "                width: 150px;\n",
    "                height: 150px;\n",
    "                background: radial-gradient(circle, rgba(156, 39, 176, 0.15) 0%, transparent 70%);\n",
    "                border-radius: 50%;\n",
    "            }}\n",
    "            .domain-summary-page h2 {{\n",
    "                border-bottom: 4px solid transparent;\n",
    "                border-image: linear-gradient(90deg, #FF6900 0%, #00BCD4 50%, #9C27B0 100%) 1;\n",
    "                padding-bottom: 12px;\n",
    "                position: relative;\n",
    "                z-index: 1;\n",
    "            }}\n",
    "            .domain-summary-page p {{\n",
    "                font-size: 12pt; line-height: 1.8;\n",
    "                position: relative;\n",
    "                z-index: 1;\n",
    "            }}\n",
    "            .domain-header {{ \n",
    "                page-break-before: avoid;\n",
    "                border-bottom: 3px solid #003366; padding-bottom: 10px;\n",
    "                background: linear-gradient(90deg, rgba(0, 51, 102, 0.05) 0%, transparent 100%);\n",
    "                padding-left: 10px;\n",
    "            }}\n",
    "            .domain-count {{ font-size: 1.2em; color: #555; font-weight: 400; text-align: {align}; margin-top: -0.5em; margin-bottom: 1.5em; }}\n",
    "            /* Enhanced use case blocks */\n",
    "            .use-case-block {{\n",
    "                page-break-inside: avoid; margin-bottom: 1.2em;\n",
    "                background: linear-gradient(135deg, #ffffff 0%, #fefefe 100%);\n",
    "                border: 1px solid #e8e8e8; padding: 14px; border-radius: 8px;\n",
    "                {'border-right: 4px solid transparent;' if is_rtl else 'border-left: 4px solid transparent;'}\n",
    "                {'border-image: linear-gradient(180deg, #FF6900 0%, #00BCD4 100%) 1;' if is_rtl else 'border-image: linear-gradient(180deg, #FF6900 0%, #00BCD4 100%) 1;'}\n",
    "                /* REMOVED: box-shadow (unsupported by WeasyPrint) */\n",
    "                position: relative;\n",
    "            }}\n",
    "            .use-case-block.page-break-after {{\n",
    "                page-break-after: always;\n",
    "            }}\n",
    "            /* Add small decorative element to use case blocks */\n",
    "            .use-case-block::before {{\n",
    "                content: '';\n",
    "                position: absolute;\n",
    "                top: 8px;\n",
    "                {'left: 8px;' if is_rtl else 'right: 8px;'}\n",
    "                width: 6px;\n",
    "                height: 6px;\n",
    "                background: #00BCD4;\n",
    "                border-radius: 50%;\n",
    "                opacity: 0.6;\n",
    "            }}\n",
    "            .use-case-block h3 {{ margin-bottom: 0.5em; font-size: 13pt; }}\n",
    "            .use-case-block p {{ margin-bottom: 0.5em; font-size: 11pt; line-height: 1.4; }}\n",
    "            .disclaimer {{ font-size: 0.9em; color: #555; border-top: 1px solid #ddd; padding-top: 1em; margin-top: 2em; }}\n",
    "            \"\"\"\n",
    "            html_parts = [f\"<html><head><meta charset='UTF-8'><style>{css}</style></head><body>\"]\n",
    "            \n",
    "            # Req 1: Use transliterated_name\n",
    "            h1_text = e(t[\"pdf_title\"]); h2_text = e(business_name); p_text = now\n",
    "            # For Arabic, we want to keep the text centered regardless of RTL\n",
    "            h2_style = ''  # Always center, no dir attribute needed\n",
    "            p_style = 'dir=\"ltr\"' if is_rtl else ''\n",
    "            \n",
    "            # Modified cover page structure: single centered box with title, date, and business name\n",
    "            html_parts.append('<div class=\"cover-page\">')\n",
    "            html_parts.append(f'<div class=\"cover-box-1\">')\n",
    "            html_parts.append(f'<h1>{h1_text}</h1>')\n",
    "            html_parts.append(f'<p {p_style}>{p_text}</p>')\n",
    "            # Business name now in the same box, centered after title and date\n",
    "            html_parts.append(f'<h2 {h2_style} style=\"margin-top: 2em;\">{h2_text}</h2>')\n",
    "            html_parts.append('</div>')\n",
    "            html_parts.append('</div>')\n",
    "            \n",
    "            summary_text = summary_dict.get('Executive', f'<p>{e(t[\"executive_summary_not_available\"])}</p>')\n",
    "            # Req 6: Use translated disclaimer text directly\n",
    "            disclaimer_text = t[\"disclaimer\"]\n",
    "            html_parts.append(f'<div class=\"exec-summary\"><h1 class=\"page-title\">{e(t[\"pdf_exec_summary\"])}</h1>{summary_text}<div class=\"disclaimer\"><strong>{e(t[\"pdf_disclaimer_title\"])}:</strong> {e(disclaimer_text)}</div></div>')\n",
    "            \n",
    "            # Req 3 & 5: TOC\n",
    "            html_parts.append(f'<div class=\"toc-page\">')\n",
    "            html_parts.append(f'<h1 class=\"page-title\">{e(t[\"pdf_toc_title\"])}</h1>')\n",
    "            html_parts.append(f\"<table class='toc-table'><tr><th>{e(t['domain'])}</th><th>{e(t['total'])}</th></tr>\")\n",
    "            toc_rows = []; domain_id_map = {}\n",
    "            for i, (domain, domain_use_cases) in enumerate(grouped_data.items()):\n",
    "                domain_slug = f\"domain-{i}\"\n",
    "                domain_id_map[domain] = domain_slug\n",
    "                toc_rows.append(f\"<tr><td><a href='#{domain_slug}'>{e(domain)}</a></td><td>{len(domain_use_cases)}</td></tr>\")\n",
    "            html_parts.extend(toc_rows)\n",
    "            html_parts.append(\"</table>\")\n",
    "            html_parts.append('</div>')\n",
    "            \n",
    "            for domain, domain_use_cases in grouped_data.items():\n",
    "                domain_slug = domain_id_map[domain]\n",
    "                domain_summary_html = summary_dict.get(domain, f\"<p>{e(t['domain_summary_not_available'])}</p>\")\n",
    "                \n",
    "                # Req 2 & 3: Domain Summary Page\n",
    "                html_parts.append(f'<div class=\"domain-summary-page\">')\n",
    "                html_parts.append(f'<h2 class=\"page-title\" id=\"{domain_slug}\">{e(domain)}</h2>') \n",
    "                html_parts.append(domain_summary_html) # Req 2\n",
    "                html_parts.append(f'</div>')\n",
    "                \n",
    "                html_parts.append(f'<h2 class=\"domain-header\">{e(domain)} - {e(t[\"pdf_detailed_view\"])}</h2>')\n",
    "                html_parts.append(f'<p class=\"domain-count\">{len(domain_use_cases)} {e(t[\"pptx_domain_suffix\"])}</p>')\n",
    "                \n",
    "                # Sort use cases: successful SQL results first, then by Priority Score descending\n",
    "                def get_use_case_sort_key(uc):\n",
    "                    use_case_id = uc.get('No', 'Unknown')\n",
    "                    example_result = self._get_cached_sql_result(use_case_id)\n",
    "                    has_success = 1 if example_result.get('status') == 'success' else 0\n",
    "                    priority_score = float(uc.get('Priority Score', 0)) if isinstance(uc.get('Priority Score'), (int, float, str)) else 0\n",
    "                    try:\n",
    "                        priority_score = float(priority_score)\n",
    "                    except (ValueError, TypeError):\n",
    "                        priority_score = 0\n",
    "                    return (-has_success, -priority_score)  # Negative for descending order\n",
    "                \n",
    "                sorted_domain_use_cases = sorted(domain_use_cases, key=get_use_case_sort_key)\n",
    "                \n",
    "                # Helper function to translate field values\n",
    "                def translate_pdf_value(value):\n",
    "                    \"\"\"Translate Type and Priority values for PDF\"\"\"\n",
    "                    if not value or value == 'N/A':\n",
    "                        return value\n",
    "                    \n",
    "                    value_key_map = {\n",
    "                        'Problem': 'value_type_problem', 'Risk': 'value_type_risk',\n",
    "                        'Opportunity': 'value_type_opportunity', 'Improvement': 'value_type_improvement',\n",
    "                        'Ultra High': 'value_priority_ultra_high', 'Very High': 'value_priority_very_high',\n",
    "                        'High': 'value_priority_high', 'Medium': 'value_priority_medium',\n",
    "                        'Low': 'value_priority_low', 'Very Low': 'value_priority_very_low',\n",
    "                        'Ultra Low': 'value_priority_ultra_low'\n",
    "                    }\n",
    "                    translation_key = value_key_map.get(value)\n",
    "                    return t.get(translation_key, value) if translation_key else value\n",
    "                \n",
    "                def translate_strategic_value(value):\n",
    "                    \"\"\"Translate Strategic Goals and Business Priority alignment values\"\"\"\n",
    "                    if not value or value == 'N/A':\n",
    "                        return value\n",
    "                    \n",
    "                    strategic_key_map = {\n",
    "                        'General Improvement': 'value_general_improvement',\n",
    "                        'Reduce Cost': 'value_reduce_cost',\n",
    "                        'Increase Revenue': 'value_increase_revenue',\n",
    "                        'Boost Productivity': 'value_boost_productivity',\n",
    "                        'Mitigate Risk': 'value_mitigate_risk',\n",
    "                        'Protect Revenue': 'value_protect_revenue',\n",
    "                        'Align to Regulations': 'value_align_to_regulations',\n",
    "                        'Improve Customer Experience': 'value_improve_customer_experience',\n",
    "                        'Enable Data-Driven Decisions': 'value_enable_data_driven_decisions',\n",
    "                        'Optimize Operations': 'value_optimize_operations',\n",
    "                        'Empower Talent': 'value_empower_talent',\n",
    "                        'Enhance Experience': 'value_enhance_experience',\n",
    "                        'Drive Innovation': 'value_drive_innovation',\n",
    "                        'Achieve ESG': 'value_achieve_esg',\n",
    "                        'Execute Strategy': 'value_execute_strategy',\n",
    "                    }\n",
    "                    \n",
    "                    # Handle comma-separated values\n",
    "                    if ',' in value:\n",
    "                        parts = [p.strip() for p in value.split(',')]\n",
    "                        translated_parts = []\n",
    "                        for part in parts:\n",
    "                            key = strategic_key_map.get(part)\n",
    "                            translated_parts.append(t.get(key, part) if key else part)\n",
    "                        return ', '.join(translated_parts)\n",
    "                    \n",
    "                    translation_key = strategic_key_map.get(value)\n",
    "                    return t.get(translation_key, value) if translation_key else value\n",
    "                \n",
    "                def translate_analytics_technique(value):\n",
    "                    \"\"\"Translate Analytics Technique values with inline fallback translations\"\"\"\n",
    "                    if not value or value == 'N/A':\n",
    "                        return value\n",
    "                    \n",
    "                    analytics_key_map = {\n",
    "                        'Forecasting': 'value_forecasting',\n",
    "                        'Classification': 'value_classification',\n",
    "                        'Anomaly Detection': 'value_anomaly_detection',\n",
    "                        'Cohort Analysis': 'value_cohort_analysis',\n",
    "                        'Segmentation': 'value_segmentation',\n",
    "                        'Sentiment Analysis': 'value_sentiment_analysis',\n",
    "                        'Trend Analysis': 'value_trend_analysis',\n",
    "                        'Prescriptive Analytics': 'value_prescriptive_analytics',\n",
    "                        'Root Cause Analysis': 'value_root_cause_analysis',\n",
    "                        'Optimization': 'value_optimization',\n",
    "                        'Recommendation': 'value_recommendation',\n",
    "                        'Time Series Analysis': 'value_time_series_analysis',\n",
    "                        'Predictive Analytics': 'value_predictive_analytics',\n",
    "                        'Descriptive Analytics': 'value_descriptive_analytics',\n",
    "                    }\n",
    "                    \n",
    "                    analytics_fallbacks = {\n",
    "                        'Chinese (Mandarin)': {'Forecasting': '预测', 'Classification': '分类', 'Anomaly Detection': '异常检测', 'Cohort Analysis': '队列分析', 'Segmentation': '细分', 'Sentiment Analysis': '情感分析', 'Trend Analysis': '趋势分析', 'Prescriptive Analytics': '规范性分析', 'Root Cause Analysis': '根因分析', 'Optimization': '优化', 'Recommendation': '推荐', 'Time Series Analysis': '时间序列分析', 'Predictive Analytics': '预测分析', 'Descriptive Analytics': '描述性分析'},\n",
    "                        'Arabic': {'Forecasting': 'التنبؤ', 'Classification': 'التصنيف', 'Anomaly Detection': 'كشف الشذوذ', 'Cohort Analysis': 'تحليل الأتراب', 'Segmentation': 'التجزئة', 'Sentiment Analysis': 'تحليل المشاعر', 'Trend Analysis': 'تحليل الاتجاهات', 'Prescriptive Analytics': 'التحليلات الوصفية', 'Root Cause Analysis': 'تحليل السبب الجذري', 'Optimization': 'التحسين', 'Recommendation': 'التوصية', 'Time Series Analysis': 'تحليل السلاسل الزمنية', 'Predictive Analytics': 'التحليلات التنبؤية', 'Descriptive Analytics': 'التحليلات الوصفية'},\n",
    "                        'Spanish': {'Forecasting': 'Pronóstico', 'Classification': 'Clasificación', 'Anomaly Detection': 'Detección de Anomalías', 'Cohort Analysis': 'Análisis de Cohortes', 'Segmentation': 'Segmentación', 'Sentiment Analysis': 'Análisis de Sentimiento', 'Trend Analysis': 'Análisis de Tendencias', 'Prescriptive Analytics': 'Analítica Prescriptiva', 'Root Cause Analysis': 'Análisis de Causa Raíz', 'Optimization': 'Optimización', 'Recommendation': 'Recomendación', 'Time Series Analysis': 'Análisis de Series Temporales', 'Predictive Analytics': 'Analítica Predictiva', 'Descriptive Analytics': 'Analítica Descriptiva'},\n",
    "                        'French': {'Forecasting': 'Prévision', 'Classification': 'Classification', 'Anomaly Detection': 'Détection d\\'Anomalies', 'Cohort Analysis': 'Analyse de Cohortes', 'Segmentation': 'Segmentation', 'Sentiment Analysis': 'Analyse de Sentiments', 'Trend Analysis': 'Analyse des Tendances', 'Prescriptive Analytics': 'Analytique Prescriptive', 'Root Cause Analysis': 'Analyse des Causes Profondes', 'Optimization': 'Optimisation', 'Recommendation': 'Recommandation', 'Time Series Analysis': 'Analyse de Séries Temporelles', 'Predictive Analytics': 'Analytique Prédictive', 'Descriptive Analytics': 'Analytique Descriptive'},\n",
    "                        'German': {'Forecasting': 'Vorhersage', 'Classification': 'Klassifikation', 'Anomaly Detection': 'Anomalieerkennung', 'Cohort Analysis': 'Kohortenanalyse', 'Segmentation': 'Segmentierung', 'Sentiment Analysis': 'Stimmungsanalyse', 'Trend Analysis': 'Trendanalyse', 'Prescriptive Analytics': 'Präskriptive Analytik', 'Root Cause Analysis': 'Ursachenanalyse', 'Optimization': 'Optimierung', 'Recommendation': 'Empfehlung', 'Time Series Analysis': 'Zeitreihenanalyse', 'Predictive Analytics': 'Prädiktive Analytik', 'Descriptive Analytics': 'Deskriptive Analytik'},\n",
    "                        'Portuguese': {'Forecasting': 'Previsão', 'Classification': 'Classificação', 'Anomaly Detection': 'Detecção de Anomalias', 'Cohort Analysis': 'Análise de Coorte', 'Segmentation': 'Segmentação', 'Sentiment Analysis': 'Análise de Sentimento', 'Trend Analysis': 'Análise de Tendências', 'Prescriptive Analytics': 'Análise Prescritiva', 'Root Cause Analysis': 'Análise de Causa Raiz', 'Optimization': 'Otimização', 'Recommendation': 'Recomendação', 'Time Series Analysis': 'Análise de Séries Temporais', 'Predictive Analytics': 'Análise Preditiva', 'Descriptive Analytics': 'Análise Descritiva'},\n",
    "                        'Italian': {'Forecasting': 'Previsione', 'Classification': 'Classificazione', 'Anomaly Detection': 'Rilevamento Anomalie', 'Cohort Analysis': 'Analisi di Coorte', 'Segmentation': 'Segmentazione', 'Sentiment Analysis': 'Analisi del Sentimento', 'Trend Analysis': 'Analisi delle Tendenze', 'Prescriptive Analytics': 'Analisi Prescrittiva', 'Root Cause Analysis': 'Analisi delle Cause Profonde', 'Optimization': 'Ottimizzazione', 'Recommendation': 'Raccomandazione', 'Time Series Analysis': 'Analisi delle Serie Temporali', 'Predictive Analytics': 'Analisi Predittiva', 'Descriptive Analytics': 'Analisi Descrittiva'},\n",
    "                        'Japanese': {'Forecasting': '予測', 'Classification': '分類', 'Anomaly Detection': '異常検出', 'Cohort Analysis': 'コホート分析', 'Segmentation': 'セグメンテーション', 'Sentiment Analysis': '感情分析', 'Trend Analysis': 'トレンド分析', 'Prescriptive Analytics': '処方的分析', 'Root Cause Analysis': '根本原因分析', 'Optimization': '最適化', 'Recommendation': 'レコメンデーション', 'Time Series Analysis': '時系列分析', 'Predictive Analytics': '予測分析', 'Descriptive Analytics': '記述分析'},\n",
    "                        'Korean': {'Forecasting': '예측', 'Classification': '분류', 'Anomaly Detection': '이상 탐지', 'Cohort Analysis': '코호트 분석', 'Segmentation': '세분화', 'Sentiment Analysis': '감정 분석', 'Trend Analysis': '추세 분석', 'Prescriptive Analytics': '처방적 분석', 'Root Cause Analysis': '근본 원인 분석', 'Optimization': '최적화', 'Recommendation': '추천', 'Time Series Analysis': '시계열 분석', 'Predictive Analytics': '예측 분석', 'Descriptive Analytics': '기술 분석'},\n",
    "                        'Hindi': {'Forecasting': 'पूर्वानुमान', 'Classification': 'वर्गीकरण', 'Anomaly Detection': 'विसंगति पता लगाना', 'Cohort Analysis': 'समूह विश्लेषण', 'Segmentation': 'विभाजन', 'Sentiment Analysis': 'भावना विश्लेषण', 'Trend Analysis': 'रुझान विश्लेषण', 'Prescriptive Analytics': 'निर्देशात्मक विश्लेषण', 'Root Cause Analysis': 'मूल कारण विश्लेषण', 'Optimization': 'अनुकूलन', 'Recommendation': 'सिफारिश', 'Time Series Analysis': 'समय श्रृंखला विश्लेषण', 'Predictive Analytics': 'भविष्य कथन विश्लेषण', 'Descriptive Analytics': 'वर्णनात्मक विश्लेषण'},\n",
    "                        'Russian': {'Forecasting': 'Прогнозирование', 'Classification': 'Классификация', 'Anomaly Detection': 'Обнаружение Аномалий', 'Cohort Analysis': 'Когортный Анализ', 'Segmentation': 'Сегментация', 'Sentiment Analysis': 'Анализ Настроений', 'Trend Analysis': 'Анализ Трендов', 'Prescriptive Analytics': 'Предписывающая Аналитика', 'Root Cause Analysis': 'Анализ Первопричин', 'Optimization': 'Оптимизация', 'Recommendation': 'Рекомендация', 'Time Series Analysis': 'Анализ Временных Рядов', 'Predictive Analytics': 'Предиктивная Аналитика', 'Descriptive Analytics': 'Описательная Аналитика'},\n",
    "                    }\n",
    "                    \n",
    "                    translation_key = analytics_key_map.get(value)\n",
    "                    translated = t.get(translation_key, None) if translation_key else None\n",
    "                    if translated and translated != value:\n",
    "                        return translated\n",
    "                    if language in analytics_fallbacks and value in analytics_fallbacks[language]:\n",
    "                        return analytics_fallbacks[language][value]\n",
    "                    return value\n",
    "                \n",
    "                for idx, uc in enumerate(sorted_domain_use_cases, start=1):\n",
    "                    # Add page-break-after class to every 2nd use case (2, 4, 6, 8, etc.)\n",
    "                    page_break_class = ' page-break-after' if idx % 2 == 0 else ''\n",
    "                    html_parts.append(f'<div class=\"use-case-block{page_break_class}\">')\n",
    "                    html_parts.append(f\"<h3>{e(uc['No'])}: {e(uc['Name'])}</h3>\")\n",
    "                    # Add header line with Subdomain, Type, Analytics Technique, and Priority (with translations)\n",
    "                    subdomain_val = e(uc.get('Subdomain', 'N/A'))\n",
    "                    type_val = e(translate_pdf_value(uc.get('type', 'N/A')))\n",
    "                    analytics_technique_val = e(translate_analytics_technique(uc.get('Analytics Technique', 'N/A')))\n",
    "                    priority_val = e(translate_pdf_value(uc.get('Priority', 'N/A')))\n",
    "                    html_parts.append(f\"<p style='font-weight: bold; color: #0066cc;'>{e(t['subdomain'])}: {subdomain_val} | {e(t['type'])}: {type_val}, {e(t.get('analytics_technique', 'Analytics Technique'))}: {analytics_technique_val}, {e(t['priority'])}: {priority_val}</p>\")\n",
    "                    html_parts.append(f\"<p><strong>{e(t['statement'])}:</strong> {e(uc.get('Statement', 'N/A'))}</p>\")\n",
    "                    html_parts.append(f\"<p><strong>{e(t['solution'])}:</strong> {e(uc.get('Solution', 'N/A'))}</p>\")\n",
    "                    html_parts.append(f\"<p><strong>{e(t['business_value'])}:</strong> {e(uc.get('Business Value', 'N/A'))}</p>\")\n",
    "                    html_parts.append(f\"<p><strong>{e(t['beneficiary'])}:</strong> {e(uc.get('Beneficiary', 'N/A'))}</p>\")\n",
    "                    html_parts.append(f\"<p><strong>{e(t['sponsor'])}:</strong> {e(uc.get('Sponsor', 'N/A'))}</p>\")\n",
    "                    html_parts.append(f\"<p><strong>{e(t.get('business_priority_alignment', 'Business Priority Alignment'))}:</strong> {e(translate_strategic_value(uc.get('Business Priority Alignment', 'General Improvement')))}</p>\")\n",
    "                    html_parts.append(f\"<p><strong>{e(t.get('strategic_goals_alignment', 'Strategic Goals Alignment'))}:</strong> {e(translate_strategic_value(uc.get('Strategic Goals Alignment', 'General Improvement')))}</p>\")\n",
    "                    \n",
    "                    html_parts.append('</div>')\n",
    "            html_parts.append(\"</body></html>\")\n",
    "            return \"\".join(html_parts)\n",
    "\n",
    "        def _save_pdf(html_content: str, workspace_path: str, logger_instance):\n",
    "            import weasyprint\n",
    "            import logging\n",
    "            try: from weasyprint.fonts import FontConfiguration\n",
    "            except ImportError: FontConfiguration = None \n",
    "            local_pdf_path = None\n",
    "            try:\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file: local_pdf_path = tmp_file.name\n",
    "                logger_instance.info(f\"Generating PDF at local temp path: {local_pdf_path}\")\n",
    "                font_config = FontConfiguration() if FontConfiguration else None\n",
    "                \n",
    "                # Suppress font-face warnings from WeasyPrint\n",
    "                weasyprint_logger = logging.getLogger('weasyprint')\n",
    "                original_level = weasyprint_logger.level\n",
    "                weasyprint_logger.setLevel(logging.ERROR)  # Only show errors, suppress warnings\n",
    "                \n",
    "                try:\n",
    "                    weasyprint.HTML(string=html_content).write_pdf(local_pdf_path, font_config=font_config)\n",
    "                finally:\n",
    "                    weasyprint_logger.setLevel(original_level)  # Restore original level\n",
    "                with open(local_pdf_path, \"rb\") as f: pdf_data = f.read()\n",
    "                if not pdf_data: raise ValueError(\"Generated PDF file is empty.\")\n",
    "                logger_instance.info(f\"Uploading PDF to workspace path: {workspace_path}\")\n",
    "                pdf_data_b64 = base64.b64encode(pdf_data).decode()\n",
    "                self.w_client.workspace.import_(path=workspace_path, content=pdf_data_b64, format=workspace.ImportFormat.AUTO, overwrite=True)\n",
    "                abs_path = self.w_client.workspace.get_status(workspace_path).path\n",
    "                logger_instance.info(f\"Success! PDF Catalog uploaded to: {abs_path}\")\n",
    "                log_print(f\"Success! PDF Catalog ({language}) generated: {abs_path}\")\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                logger_instance.critical(f\"Failed to generate and save PDF for {language}: {e}\")\n",
    "                logger_instance.critical(f\"Full traceback: {traceback.format_exc()}\")\n",
    "            finally:\n",
    "                if local_pdf_path and os.path.exists(local_pdf_path): os.remove(local_pdf_path)\n",
    "\n",
    "        # --- Main execution logic for generate_catalog_pdf ---\n",
    "        try:\n",
    "            if not _install_dependencies(self.logger):\n",
    "                self.logger.error(\"Skipping PDF generation due to missing weasyprint dependency.\")\n",
    "                return\n",
    "                \n",
    "            if not grouped_data:\n",
    "                self.logger.warning(f\"No use cases provided to generate_catalog_pdf for {language}. Skipping.\")\n",
    "                return\n",
    "            final_html = _build_html(grouped_data, summary_dict, transliterated_name, t, is_rtl)\n",
    "            pdf_workspace_path = os.path.join(self.docs_output_dir, f\"{self.business_name}-dbx_inspire_{lang_abbr}.pdf\")\n",
    "            _save_pdf(final_html, pdf_workspace_path, self.logger)\n",
    "        except Exception as e:\n",
    "            self.logger.critical(f\"An error occurred during PDF generation for {language}: {e}\")\n",
    "\n",
    "    # === MODIFIED: PPTX Generation (Req 1, 2, 4, 5, 6) ===\n",
    "    def generate_presentation_pptx(self, language: str, lang_abbr: str, translations: dict, summary_dict: dict, grouped_data: dict, transliterated_name: str):\n",
    "        self.logger.info(f\"--- Starting PPTX Presentation Generation for {language} ---\")\n",
    "        \n",
    "        t = translations\n",
    "        is_rtl = (language == \"Arabic\")\n",
    "\n",
    "        def _install_pptx_dependencies(logger_instance) -> bool:\n",
    "            try:\n",
    "                import pptx\n",
    "                self.logger.info(\"PPTX package (python-pptx) already installed.\")\n",
    "                return True\n",
    "            except ImportError:\n",
    "                logger_instance.info(\"Installing required PPTX package (python-pptx)...\")\n",
    "                try: \n",
    "                    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"python-pptx\"])\n",
    "                    import pptx\n",
    "                    self.logger.info(\"Successfully installed python-pptx.\")\n",
    "                    return True\n",
    "                except Exception as e: \n",
    "                    logger_instance.error(f\"Failed to install python-pptx: {e}\")\n",
    "                    print(\"ERROR: Failed to install 'python-pptx'. Presentation generation cannot continue.\", file=sys.stderr)\n",
    "                    return False\n",
    "\n",
    "        # === MODIFIED: _build_presentation (Req 1, 3, 4, 5) ===\n",
    "        def _build_presentation(grouped_data: dict, summary_dict: dict, business_name: str, translations: dict, workspace_path: str, logger_instance, is_rtl: bool):\n",
    "            try:\n",
    "                from pptx import Presentation\n",
    "                from pptx.util import Inches, Pt, Cm\n",
    "                from pptx.dml.color import RGBColor\n",
    "                from pptx.enum.text import PP_ALIGN, MSO_ANCHOR\n",
    "                from pptx.enum.shapes import MSO_SHAPE\n",
    "            except ImportError as e:\n",
    "                logger_instance.error(f\"FATAL: python-pptx import failed inside _build_presentation: {e}. Aborting PPTX generation.\")\n",
    "                return\n",
    "            \n",
    "            DATABRICKS_BLUE = RGBColor(0, 51, 102); DATABRICKS_ORANGE = RGBColor(255, 105, 0); TEXT_COLOR = RGBColor(0x33, 0x33, 0x33)\n",
    "            LIGHT_GRAY = RGBColor(0xFA, 0xFA, 0xFA); WHITE_COLOR = RGBColor(0xFF, 0xFF, 0xFF); FOOTER_COLOR = RGBColor(0x88, 0x88, 0x88)\n",
    "            # Modern color palette - vibrant and futuristic\n",
    "            TEAL_ACCENT = RGBColor(0, 188, 212); PURPLE_ACCENT = RGBColor(156, 39, 176); GREEN_ACCENT = RGBColor(76, 175, 80)\n",
    "            \n",
    "            prs = Presentation(); prs.slide_width = Cm(33.867); prs.slide_height = Cm(19.05)\n",
    "            # === PPTX FIX (Req 1): Cast all float calculations to int() ===\n",
    "            CONTENT_WIDTH_CM = Cm(30.5)\n",
    "            LEFT_MARGIN_CM = (prs.slide_width - CONTENT_WIDTH_CM) / 2\n",
    "            \n",
    "            align = PP_ALIGN.RIGHT if is_rtl else PP_ALIGN.LEFT\n",
    "            title_align = PP_ALIGN.CENTER\n",
    "\n",
    "            SLIDE_LAYOUT_TITLE = prs.slide_layouts[0]; SLIDE_LAYOUT_TITLE_AND_CONTENT = prs.slide_layouts[1]; SLIDE_LAYOUT_BLANK = prs.slide_layouts[6]\n",
    "            def set_font_color(run, color=TEXT_COLOR): run.font.color.rgb = color\n",
    "\n",
    "            now = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            footer_text = f\"Databricks Inspire AI  |  {now}\"\n",
    "            def add_footer(slide):\n",
    "                try:\n",
    "                    left = int(LEFT_MARGIN_CM); width = int(CONTENT_WIDTH_CM); top = int(Cm(18.2)); height = int(Cm(0.8))\n",
    "                    txBox = slide.shapes.add_textbox(left, top, width, height)\n",
    "                    p = txBox.text_frame.paragraphs[0]; p.text = footer_text\n",
    "                    p.font.size = Pt(10); p.font.color.rgb = FOOTER_COLOR\n",
    "                    p.alignment = PP_ALIGN.CENTER if is_rtl else PP_ALIGN.RIGHT\n",
    "                except Exception as e:\n",
    "                    logger_instance.warning(f\"Failed to add footer to slide: {e}\")\n",
    "\n",
    "            # Enhanced title slide with decorative shapes\n",
    "            logger_instance.info(\"Building Title Slide...\") \n",
    "            slide = prs.slides.add_slide(SLIDE_LAYOUT_TITLE); slide.background.fill.solid(); slide.background.fill.fore_color.rgb = DATABRICKS_BLUE\n",
    "            \n",
    "            # Add decorative circular shapes for visual interest\n",
    "            circle1 = slide.shapes.add_shape(MSO_SHAPE.OVAL, int(Cm(28)), int(Cm(1)), int(Cm(4)), int(Cm(4)))\n",
    "            circle1.fill.solid(); circle1.fill.fore_color.rgb = TEAL_ACCENT; circle1.line.fill.background()\n",
    "            circle1.fill.transparency = 0.3\n",
    "            \n",
    "            circle2 = slide.shapes.add_shape(MSO_SHAPE.OVAL, int(Cm(1)), int(Cm(14)), int(Cm(5)), int(Cm(5)))\n",
    "            circle2.fill.solid(); circle2.fill.fore_color.rgb = PURPLE_ACCENT; circle2.line.fill.background()\n",
    "            circle2.fill.transparency = 0.2\n",
    "            \n",
    "            # Additional decorative elements for futuristic feel\n",
    "            circle3 = slide.shapes.add_shape(MSO_SHAPE.OVAL, int(Cm(30)), int(Cm(16)), int(Cm(3)), int(Cm(3)))\n",
    "            circle3.fill.solid(); circle3.fill.fore_color.rgb = GREEN_ACCENT; circle3.line.fill.background()\n",
    "            circle3.fill.transparency = 0.4\n",
    "            \n",
    "            # Add small accent circles\n",
    "            accent_circle1 = slide.shapes.add_shape(MSO_SHAPE.OVAL, int(Cm(3)), int(Cm(3)), int(Cm(1.5)), int(Cm(1.5)))\n",
    "            accent_circle1.fill.solid(); accent_circle1.fill.fore_color.rgb = DATABRICKS_ORANGE; accent_circle1.line.fill.background()\n",
    "            accent_circle1.fill.transparency = 0.5\n",
    "            \n",
    "            accent_circle2 = slide.shapes.add_shape(MSO_SHAPE.OVAL, int(Cm(29)), int(Cm(8)), int(Cm(2)), int(Cm(2)))\n",
    "            accent_circle2.fill.solid(); accent_circle2.fill.fore_color.rgb = TEAL_ACCENT; accent_circle2.line.fill.background()\n",
    "            accent_circle2.fill.transparency = 0.6\n",
    "            \n",
    "            txBox_top = slide.shapes.add_textbox(int(LEFT_MARGIN_CM), int(Cm(2.0)), int(CONTENT_WIDTH_CM), int(Cm(5.0)))\n",
    "            tf_top = txBox_top.text_frame; tf_top.vertical_anchor = MSO_ANCHOR.MIDDLE\n",
    "            p_top1 = tf_top.paragraphs[0]; p_top1.text = t['pptx_main_title']\n",
    "            p_top1.font.color.rgb = WHITE_COLOR; p_top1.font.size = Pt(44); p_top1.alignment = title_align # Req 3: Font size reduced\n",
    "            p_top2 = tf_top.add_paragraph(); p_top2.text = now\n",
    "            p_top2.font.color.rgb = RGBColor(0xCC, 0xCC, 0xCC); p_top2.font.size = Pt(20); p_top2.alignment = title_align\n",
    "            \n",
    "            txBox_bottom = slide.shapes.add_textbox(int(LEFT_MARGIN_CM), int(Cm(10.0)), int(CONTENT_WIDTH_CM), int(Cm(5.0)))\n",
    "            tf_bottom = txBox_bottom.text_frame; tf_bottom.vertical_anchor = MSO_ANCHOR.MIDDLE\n",
    "            p_bottom = tf_bottom.paragraphs[0]; p_bottom.text = f\"{t['pptx_for']} {business_name}\" # Req 1: Use transliterated name\n",
    "            p_bottom.font.color.rgb = DATABRICKS_ORANGE; p_bottom.font.size = Pt(32); p_bottom.alignment = title_align\n",
    "            \n",
    "            try: slide.shapes.title.element.getparent().remove(slide.shapes.title.element)\n",
    "            except: pass\n",
    "            try: slide.placeholders[1].element.getparent().remove(slide.placeholders[1].element)\n",
    "            except: pass\n",
    "            add_footer(slide)\n",
    "\n",
    "            # Enhanced Executive Summary with colorful accents\n",
    "            logger_instance.info(\"Building Executive Summary Slide...\")\n",
    "            slide = prs.slides.add_slide(SLIDE_LAYOUT_TITLE_AND_CONTENT); slide.background.fill.solid(); slide.background.fill.fore_color.rgb = LIGHT_GRAY\n",
    "            \n",
    "            # Add gradient accent bar (simulated with two rectangles)\n",
    "            accent_bar1 = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, 0, 0, int(Cm(0.8)), int(prs.slide_height))\n",
    "            accent_bar1.fill.solid(); accent_bar1.fill.fore_color.rgb = DATABRICKS_ORANGE; accent_bar1.line.fill.background()\n",
    "            \n",
    "            accent_bar2 = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, int(Cm(0.8)), 0, int(Cm(0.7)), int(prs.slide_height))\n",
    "            accent_bar2.fill.solid(); accent_bar2.fill.fore_color.rgb = TEAL_ACCENT; accent_bar2.line.fill.background()\n",
    "            accent_bar2.fill.transparency = 0.5\n",
    "            \n",
    "            title = slide.shapes.title; title.left, title.width = int(LEFT_MARGIN_CM), int(CONTENT_WIDTH_CM)\n",
    "            title.top = int(Cm(1.0)); title.height = int(Cm(2.5))\n",
    "            title.text = t['pdf_exec_summary']\n",
    "            p = title.text_frame.paragraphs[0]; p.font.color.rgb = DATABRICKS_BLUE; p.font.size = Pt(36); p.alignment = align\n",
    "            \n",
    "            content_placeholder = slide.placeholders[1]; content_placeholder.left, content_placeholder.width = int(LEFT_MARGIN_CM), int(Cm(30.5))\n",
    "            content_placeholder.top = int(Cm(1.0) + Cm(2.5))\n",
    "            content_placeholder.height = int(Cm(14.1))\n",
    "            content_frame = content_placeholder.text_frame; content_frame.clear(); content_frame.word_wrap = True\n",
    "            \n",
    "            summary_text = summary_dict.get('Executive', t['summary_not_available'])\n",
    "            summary_text = re.sub(r'</p>|<p>', ' ', summary_text); summary_text = re.sub(r'<[^>]+>', '', summary_text).strip()\n",
    "            \n",
    "            # Split into sentences and create bullet points for each statement\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', summary_text)\n",
    "            for sentence in sentences:\n",
    "                sentence = sentence.strip()\n",
    "                if not sentence: continue\n",
    "                p = content_frame.add_paragraph(); p.text = sentence; p.font.size = Pt(18)\n",
    "                p.level = 0; p.alignment = align; p.space_after = Pt(8)\n",
    "                set_font_color(p.runs[0], TEXT_COLOR)\n",
    "            \n",
    "            p = content_frame.add_paragraph(); p.space_before = Pt(24); p.alignment = align\n",
    "            disclaimer_text = t[\"disclaimer\"] # Req 6: Use translated disclaimer\n",
    "            run_label = p.add_run(); run_label.text = f\"{t['pptx_disclaimer_title']}: \"; run_label.font.bold = True; run_label.font.size = Pt(14)\n",
    "            set_font_color(run_label, DATABRICKS_BLUE)\n",
    "            run_value = p.add_run(); run_value.text = disclaimer_text; run_value.font.size = Pt(14); set_font_color(run_value, TEXT_COLOR)\n",
    "            add_footer(slide)\n",
    "\n",
    "            # === MODIFIED: PPTX TOC (Req 4, 5) ===\n",
    "            logger_instance.info(\"Building Table of Contents Slide(s)...\")\n",
    "            rows = list(grouped_data.items())\n",
    "            max_rows_per_slide = 10\n",
    "            num_slides = (len(rows) + max_rows_per_slide - 1) // max_rows_per_slide\n",
    "            for i in range(num_slides):\n",
    "                slide = prs.slides.add_slide(SLIDE_LAYOUT_TITLE_AND_CONTENT)\n",
    "                slide.background.fill.solid(); slide.background.fill.fore_color.rgb = LIGHT_GRAY\n",
    "                \n",
    "                # Colorful multi-bar accent\n",
    "                accent_bar1 = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, 0, 0, int(Cm(0.5)), int(prs.slide_height))\n",
    "                accent_bar1.fill.solid(); accent_bar1.fill.fore_color.rgb = DATABRICKS_ORANGE; accent_bar1.line.fill.background()\n",
    "                \n",
    "                accent_bar2 = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, int(Cm(0.5)), 0, int(Cm(0.5)), int(prs.slide_height))\n",
    "                accent_bar2.fill.solid(); accent_bar2.fill.fore_color.rgb = TEAL_ACCENT; accent_bar2.line.fill.background()\n",
    "                \n",
    "                accent_bar3 = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, int(Cm(1.0)), 0, int(Cm(0.5)), int(prs.slide_height))\n",
    "                accent_bar3.fill.solid(); accent_bar3.fill.fore_color.rgb = PURPLE_ACCENT; accent_bar3.line.fill.background()\n",
    "                \n",
    "                title = slide.shapes.title; title.left, title.width = int(LEFT_MARGIN_CM), int(CONTENT_WIDTH_CM)\n",
    "                title.top = int(Cm(1.0)); title.height = int(Cm(2.5))\n",
    "                title.text = t['pdf_toc_title']\n",
    "                if num_slides > 1: title.text += f\" ({i+1}/{num_slides})\"\n",
    "                p = title.text_frame.paragraphs[0]; p.font.color.rgb = DATABRICKS_BLUE; p.font.size = Pt(36); p.alignment = align\n",
    "                \n",
    "                chunk = rows[i*max_rows_per_slide : (i+1)*max_rows_per_slide]\n",
    "                num_table_rows = len(chunk) + 1; num_table_cols = 2\n",
    "                table_left = int(LEFT_MARGIN_CM); table_top = int(Cm(4.0)); table_width = int(CONTENT_WIDTH_CM); table_height = int(Cm(14.0))\n",
    "                table_shape = slide.shapes.add_table(num_table_rows, num_table_cols, table_left, table_top, table_width, table_height)\n",
    "                table = table_shape.table\n",
    "                table.columns[0].width = int(Cm(22.0)); table.columns[1].width = int(Cm(8.5))\n",
    "                table.horz_banding = False; table.first_row = True\n",
    "                \n",
    "                table.cell(0, 0).text = t['domain']; table.cell(0, 1).text = t['total']\n",
    "                \n",
    "                for c_idx in range(num_table_cols):\n",
    "                    cell = table.cell(0, c_idx); cell.fill.solid(); cell.fill.fore_color.rgb = DATABRICKS_BLUE\n",
    "                    para = cell.text_frame.paragraphs[0]; para.font.color.rgb = WHITE_COLOR; para.font.bold = True; para.font.size = Pt(18); para.alignment = align\n",
    "                    cell.vertical_anchor = MSO_ANCHOR.MIDDLE\n",
    "                for r_idx, (domain, domain_use_cases) in enumerate(chunk):\n",
    "                    table.cell(r_idx + 1, 0).text = domain\n",
    "                    table.cell(r_idx + 1, 1).text = str(len(domain_use_cases))\n",
    "                    for c_idx in range(num_table_cols):\n",
    "                        cell = table.cell(r_idx + 1, c_idx); para = cell.text_frame.paragraphs[0]\n",
    "                        para.font.color.rgb = TEXT_COLOR; para.font.size = Pt(16); para.alignment = align\n",
    "                        cell.vertical_anchor = MSO_ANCHOR.MIDDLE\n",
    "                try: slide.placeholders[1].element.getparent().remove(slide.placeholders[1].element)\n",
    "                except: pass\n",
    "                add_footer(slide)\n",
    "            \n",
    "            for domain, domain_use_cases in grouped_data.items():\n",
    "                slide = prs.slides.add_slide(SLIDE_LAYOUT_BLANK); slide.background.fill.solid(); slide.background.fill.fore_color.rgb = DATABRICKS_BLUE\n",
    "                txBox = slide.shapes.add_textbox(int(LEFT_MARGIN_CM), int(Cm(1.0)), int(CONTENT_WIDTH_CM), int(Cm(17.05)))\n",
    "                tf = txBox.text_frame; tf.vertical_anchor = MSO_ANCHOR.MIDDLE\n",
    "                p = tf.paragraphs[0]; p.text = f\"{domain}\\n{len(domain_use_cases)} {t['pptx_domain_suffix']}\"; p.alignment = align\n",
    "                p.font.color.rgb = DATABRICKS_ORANGE; p.font.size = Pt(44); p.font.bold = True\n",
    "                if len(tf.paragraphs) > 1:\n",
    "                    p2 = tf.paragraphs[1]; p2.font.color.rgb = WHITE_COLOR; p2.font.size = Pt(32); p2.font.bold = False; p2.alignment = align\n",
    "                add_footer(slide)\n",
    "\n",
    "                # --- Domain Summary Slide (MODIFIED: Req 1, 2) ---\n",
    "                slide = prs.slides.add_slide(SLIDE_LAYOUT_TITLE_AND_CONTENT); slide.background.fill.solid(); slide.background.fill.fore_color.rgb = LIGHT_GRAY\n",
    "                accent_bar = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, 0, 0, int(Cm(1.5)), int(prs.slide_height))\n",
    "                accent_bar.fill.solid(); accent_bar.fill.fore_color.rgb = DATABRICKS_ORANGE; accent_bar.line.fill.background()\n",
    "                \n",
    "                title = slide.shapes.title; title.left, title.width = int(LEFT_MARGIN_CM), int(CONTENT_WIDTH_CM)\n",
    "                title.top = int(Cm(1.0)); title.height = int(Cm(2.5))\n",
    "                title.text = domain # Req 1: Remove \": Summary\"\n",
    "                \n",
    "                p = title.text_frame.paragraphs[0]; p.font.color.rgb = DATABRICKS_BLUE; p.font.size = Pt(36); p.alignment = align\n",
    "                \n",
    "                content_placeholder = slide.placeholders[1]; content_placeholder.left, content_placeholder.width = int(LEFT_MARGIN_CM), int(Cm(30.5))\n",
    "                content_placeholder.top = int(Cm(1.0) + Cm(2.5)); content_placeholder.height = int(Cm(14.1))\n",
    "                content_frame = content_placeholder.text_frame; content_frame.clear(); content_frame.word_wrap = True\n",
    "                \n",
    "                domain_summary_text = summary_dict.get(domain, t['domain_summary_not_available'])\n",
    "                domain_summary_text = re.sub(r'</p>|<p>', ' ', domain_summary_text); domain_summary_text = re.sub(r'<[^>]+>', '', domain_summary_text).strip()\n",
    "                \n",
    "                # Split into sentences and create bullet points for each statement\n",
    "                sentences = re.split(r'(?<=[.!?])\\s+', domain_summary_text)\n",
    "                for sentence in sentences:\n",
    "                    sentence = sentence.strip()\n",
    "                    if not sentence: continue\n",
    "                    p = content_frame.add_paragraph(); p.text = sentence; p.font.size = Pt(18)\n",
    "                    p.level = 0; p.alignment = align; p.space_after = Pt(8)\n",
    "                    set_font_color(p.runs[0], TEXT_COLOR)\n",
    "                add_footer(slide)\n",
    "                \n",
    "                # Helper function to translate field values for PPTX\n",
    "                def translate_pptx_value(value):\n",
    "                    \"\"\"Translate Type and Priority values for PPTX\"\"\"\n",
    "                    if not value or value == 'N/A':\n",
    "                        return value\n",
    "                    \n",
    "                    value_key_map = {\n",
    "                        'Problem': 'value_type_problem', 'Risk': 'value_type_risk',\n",
    "                        'Opportunity': 'value_type_opportunity', 'Improvement': 'value_type_improvement',\n",
    "                        'Ultra High': 'value_priority_ultra_high', 'Very High': 'value_priority_very_high',\n",
    "                        'High': 'value_priority_high', 'Medium': 'value_priority_medium',\n",
    "                        'Low': 'value_priority_low', 'Very Low': 'value_priority_very_low',\n",
    "                        'Ultra Low': 'value_priority_ultra_low'\n",
    "                    }\n",
    "                    translation_key = value_key_map.get(value)\n",
    "                    return t.get(translation_key, value) if translation_key else value\n",
    "                \n",
    "                def translate_strategic_pptx_value(value):\n",
    "                    \"\"\"Translate Strategic Goals and Business Priority alignment values\"\"\"\n",
    "                    if not value or value == 'N/A':\n",
    "                        return value\n",
    "                    \n",
    "                    strategic_key_map = {\n",
    "                        'General Improvement': 'value_general_improvement',\n",
    "                        'Reduce Cost': 'value_reduce_cost',\n",
    "                        'Increase Revenue': 'value_increase_revenue',\n",
    "                        'Boost Productivity': 'value_boost_productivity',\n",
    "                        'Mitigate Risk': 'value_mitigate_risk',\n",
    "                        'Protect Revenue': 'value_protect_revenue',\n",
    "                        'Align to Regulations': 'value_align_to_regulations',\n",
    "                        'Improve Customer Experience': 'value_improve_customer_experience',\n",
    "                        'Enable Data-Driven Decisions': 'value_enable_data_driven_decisions',\n",
    "                        'Optimize Operations': 'value_optimize_operations',\n",
    "                        'Empower Talent': 'value_empower_talent',\n",
    "                        'Enhance Experience': 'value_enhance_experience',\n",
    "                        'Drive Innovation': 'value_drive_innovation',\n",
    "                        'Achieve ESG': 'value_achieve_esg',\n",
    "                        'Execute Strategy': 'value_execute_strategy',\n",
    "                    }\n",
    "                    \n",
    "                    # Handle comma-separated values\n",
    "                    if ',' in str(value):\n",
    "                        parts = [p.strip() for p in str(value).split(',')]\n",
    "                        translated_parts = []\n",
    "                        for part in parts:\n",
    "                            key = strategic_key_map.get(part)\n",
    "                            translated_parts.append(t.get(key, part) if key else part)\n",
    "                        return ', '.join(translated_parts)\n",
    "                    \n",
    "                    translation_key = strategic_key_map.get(value)\n",
    "                    return t.get(translation_key, value) if translation_key else value\n",
    "                \n",
    "                def translate_analytics_pptx_value(value):\n",
    "                    \"\"\"Translate Analytics Technique values for PPTX with inline fallback translations\"\"\"\n",
    "                    if not value or value == 'N/A':\n",
    "                        return value\n",
    "                    \n",
    "                    analytics_key_map = {\n",
    "                        'Forecasting': 'value_forecasting',\n",
    "                        'Classification': 'value_classification',\n",
    "                        'Anomaly Detection': 'value_anomaly_detection',\n",
    "                        'Cohort Analysis': 'value_cohort_analysis',\n",
    "                        'Segmentation': 'value_segmentation',\n",
    "                        'Sentiment Analysis': 'value_sentiment_analysis',\n",
    "                        'Trend Analysis': 'value_trend_analysis',\n",
    "                        'Prescriptive Analytics': 'value_prescriptive_analytics',\n",
    "                        'Root Cause Analysis': 'value_root_cause_analysis',\n",
    "                        'Optimization': 'value_optimization',\n",
    "                        'Recommendation': 'value_recommendation',\n",
    "                        'Time Series Analysis': 'value_time_series_analysis',\n",
    "                        'Predictive Analytics': 'value_predictive_analytics',\n",
    "                        'Descriptive Analytics': 'value_descriptive_analytics',\n",
    "                    }\n",
    "                    \n",
    "                    analytics_fallbacks = {\n",
    "                        'Chinese (Mandarin)': {'Forecasting': '预测', 'Classification': '分类', 'Anomaly Detection': '异常检测', 'Cohort Analysis': '队列分析', 'Segmentation': '细分', 'Sentiment Analysis': '情感分析', 'Trend Analysis': '趋势分析', 'Prescriptive Analytics': '规范性分析', 'Root Cause Analysis': '根因分析', 'Optimization': '优化', 'Recommendation': '推荐', 'Time Series Analysis': '时间序列分析', 'Predictive Analytics': '预测分析', 'Descriptive Analytics': '描述性分析'},\n",
    "                        'Arabic': {'Forecasting': 'التنبؤ', 'Classification': 'التصنيف', 'Anomaly Detection': 'كشف الشذوذ', 'Cohort Analysis': 'تحليل الأتراب', 'Segmentation': 'التجزئة', 'Sentiment Analysis': 'تحليل المشاعر', 'Trend Analysis': 'تحليل الاتجاهات', 'Prescriptive Analytics': 'التحليلات الوصفية', 'Root Cause Analysis': 'تحليل السبب الجذري', 'Optimization': 'التحسين', 'Recommendation': 'التوصية', 'Time Series Analysis': 'تحليل السلاسل الزمنية', 'Predictive Analytics': 'التحليلات التنبؤية', 'Descriptive Analytics': 'التحليلات الوصفية'},\n",
    "                        'Spanish': {'Forecasting': 'Pronóstico', 'Classification': 'Clasificación', 'Anomaly Detection': 'Detección de Anomalías', 'Cohort Analysis': 'Análisis de Cohortes', 'Segmentation': 'Segmentación', 'Sentiment Analysis': 'Análisis de Sentimiento', 'Trend Analysis': 'Análisis de Tendencias', 'Prescriptive Analytics': 'Analítica Prescriptiva', 'Root Cause Analysis': 'Análisis de Causa Raíz', 'Optimization': 'Optimización', 'Recommendation': 'Recomendación', 'Time Series Analysis': 'Análisis de Series Temporales', 'Predictive Analytics': 'Analítica Predictiva', 'Descriptive Analytics': 'Analítica Descriptiva'},\n",
    "                        'French': {'Forecasting': 'Prévision', 'Classification': 'Classification', 'Anomaly Detection': 'Détection d\\'Anomalies', 'Cohort Analysis': 'Analyse de Cohortes', 'Segmentation': 'Segmentation', 'Sentiment Analysis': 'Analyse de Sentiments', 'Trend Analysis': 'Analyse des Tendances', 'Prescriptive Analytics': 'Analytique Prescriptive', 'Root Cause Analysis': 'Analyse des Causes Profondes', 'Optimization': 'Optimisation', 'Recommendation': 'Recommandation', 'Time Series Analysis': 'Analyse de Séries Temporelles', 'Predictive Analytics': 'Analytique Prédictive', 'Descriptive Analytics': 'Analytique Descriptive'},\n",
    "                        'German': {'Forecasting': 'Vorhersage', 'Classification': 'Klassifikation', 'Anomaly Detection': 'Anomalieerkennung', 'Cohort Analysis': 'Kohortenanalyse', 'Segmentation': 'Segmentierung', 'Sentiment Analysis': 'Stimmungsanalyse', 'Trend Analysis': 'Trendanalyse', 'Prescriptive Analytics': 'Präskriptive Analytik', 'Root Cause Analysis': 'Ursachenanalyse', 'Optimization': 'Optimierung', 'Recommendation': 'Empfehlung', 'Time Series Analysis': 'Zeitreihenanalyse', 'Predictive Analytics': 'Prädiktive Analytik', 'Descriptive Analytics': 'Deskriptive Analytik'},\n",
    "                        'Portuguese': {'Forecasting': 'Previsão', 'Classification': 'Classificação', 'Anomaly Detection': 'Detecção de Anomalias', 'Cohort Analysis': 'Análise de Coorte', 'Segmentation': 'Segmentação', 'Sentiment Analysis': 'Análise de Sentimento', 'Trend Analysis': 'Análise de Tendências', 'Prescriptive Analytics': 'Análise Prescritiva', 'Root Cause Analysis': 'Análise de Causa Raiz', 'Optimization': 'Otimização', 'Recommendation': 'Recomendação', 'Time Series Analysis': 'Análise de Séries Temporais', 'Predictive Analytics': 'Análise Preditiva', 'Descriptive Analytics': 'Análise Descritiva'},\n",
    "                        'Italian': {'Forecasting': 'Previsione', 'Classification': 'Classificazione', 'Anomaly Detection': 'Rilevamento Anomalie', 'Cohort Analysis': 'Analisi di Coorte', 'Segmentation': 'Segmentazione', 'Sentiment Analysis': 'Analisi del Sentimento', 'Trend Analysis': 'Analisi delle Tendenze', 'Prescriptive Analytics': 'Analisi Prescrittiva', 'Root Cause Analysis': 'Analisi delle Cause Profonde', 'Optimization': 'Ottimizzazione', 'Recommendation': 'Raccomandazione', 'Time Series Analysis': 'Analisi delle Serie Temporali', 'Predictive Analytics': 'Analisi Predittiva', 'Descriptive Analytics': 'Analisi Descrittiva'},\n",
    "                        'Japanese': {'Forecasting': '予測', 'Classification': '分類', 'Anomaly Detection': '異常検出', 'Cohort Analysis': 'コホート分析', 'Segmentation': 'セグメンテーション', 'Sentiment Analysis': '感情分析', 'Trend Analysis': 'トレンド分析', 'Prescriptive Analytics': '処方的分析', 'Root Cause Analysis': '根本原因分析', 'Optimization': '最適化', 'Recommendation': 'レコメンデーション', 'Time Series Analysis': '時系列分析', 'Predictive Analytics': '予測分析', 'Descriptive Analytics': '記述分析'},\n",
    "                        'Korean': {'Forecasting': '예측', 'Classification': '분류', 'Anomaly Detection': '이상 탐지', 'Cohort Analysis': '코호트 분석', 'Segmentation': '세분화', 'Sentiment Analysis': '감정 분석', 'Trend Analysis': '추세 분석', 'Prescriptive Analytics': '처방적 분석', 'Root Cause Analysis': '근본 원인 분석', 'Optimization': '최적화', 'Recommendation': '추천', 'Time Series Analysis': '시계열 분석', 'Predictive Analytics': '예측 분석', 'Descriptive Analytics': '기술 분석'},\n",
    "                        'Hindi': {'Forecasting': 'पूर्वानुमान', 'Classification': 'वर्गीकरण', 'Anomaly Detection': 'विसंगति पता लगाना', 'Cohort Analysis': 'समूह विश्लेषण', 'Segmentation': 'विभाजन', 'Sentiment Analysis': 'भावना विश्लेषण', 'Trend Analysis': 'रुझान विश्लेषण', 'Prescriptive Analytics': 'निर्देशात्मक विश्लेषण', 'Root Cause Analysis': 'मूल कारण विश्लेषण', 'Optimization': 'अनुकूलन', 'Recommendation': 'सिफारिश', 'Time Series Analysis': 'समय श्रृंखला विश्लेषण', 'Predictive Analytics': 'भविष्य कथन विश्लेषण', 'Descriptive Analytics': 'वर्णनात्मक विश्लेषण'},\n",
    "                        'Russian': {'Forecasting': 'Прогнозирование', 'Classification': 'Классификация', 'Anomaly Detection': 'Обнаружение Аномалий', 'Cohort Analysis': 'Когортный Анализ', 'Segmentation': 'Сегментация', 'Sentiment Analysis': 'Анализ Настроений', 'Trend Analysis': 'Анализ Трендов', 'Prescriptive Analytics': 'Предписывающая Аналитика', 'Root Cause Analysis': 'Анализ Первопричин', 'Optimization': 'Оптимизация', 'Recommendation': 'Рекомендация', 'Time Series Analysis': 'Анализ Временных Рядов', 'Predictive Analytics': 'Предиктивная Аналитика', 'Descriptive Analytics': 'Описательная Аналитика'},\n",
    "                    }\n",
    "                    \n",
    "                    translation_key = analytics_key_map.get(value)\n",
    "                    translated = t.get(translation_key, None) if translation_key else None\n",
    "                    if translated and translated != value:\n",
    "                        return translated\n",
    "                    if language in analytics_fallbacks and value in analytics_fallbacks[language]:\n",
    "                        return analytics_fallbacks[language][value]\n",
    "                    return value\n",
    "                \n",
    "                for uc in domain_use_cases:\n",
    "                    slide = prs.slides.add_slide(SLIDE_LAYOUT_TITLE_AND_CONTENT); slide.background.fill.solid(); slide.background.fill.fore_color.rgb = WHITE_COLOR\n",
    "                    accent_bar = slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, 0, 0, int(Cm(1.5)), int(prs.slide_height))\n",
    "                    accent_bar.fill.solid(); accent_bar.fill.fore_color.rgb = DATABRICKS_ORANGE; accent_bar.line.fill.background()\n",
    "                    \n",
    "                    title = slide.shapes.title; title.left, title.width = int(LEFT_MARGIN_CM), int(CONTENT_WIDTH_CM)\n",
    "                    title.top = int(Cm(1.0)); title.height = int(Cm(2.5))\n",
    "                    title.text = f\"{uc['No']}: {uc['Name']}\"\n",
    "                    p = title.text_frame.paragraphs[0]; p.font.color.rgb = DATABRICKS_BLUE; p.font.size = Pt(32); p.alignment = align\n",
    "                    \n",
    "                    content_placeholder = slide.placeholders[1]; content_placeholder.left, content_placeholder.width = int(LEFT_MARGIN_CM), int(Cm(30.5))\n",
    "                    content_placeholder.top = int(Cm(4.1)); content_placeholder.height = int(Cm(14.1))\n",
    "                    content_frame = content_placeholder.text_frame; content_frame.clear(); content_frame.word_wrap = True\n",
    "                    \n",
    "                    # Add header line with Subdomain, Type, Analytics Technique, and Priority (with translations)\n",
    "                    # Use the first paragraph instead of adding a new one to avoid empty line at top\n",
    "                    header_p = content_frame.paragraphs[0]; header_p.level = 0; header_p.alignment = align\n",
    "                    subdomain_val = uc.get('Subdomain', 'N/A')\n",
    "                    type_val = translate_pptx_value(uc.get('type', 'N/A'))\n",
    "                    analytics_technique_val = translate_analytics_pptx_value(uc.get('Analytics Technique', 'N/A'))\n",
    "                    priority_val = translate_pptx_value(uc.get('Priority', 'N/A'))\n",
    "                    header_text = f\"{t['subdomain']}: {subdomain_val} | {t['type']}: {type_val}, {t.get('analytics_technique', 'Analytics Technique')}: {analytics_technique_val}, {t['priority']}: {priority_val}\"\n",
    "                    header_run = header_p.add_run(); header_run.text = header_text; header_run.font.bold = True; header_run.font.size = Pt(22)\n",
    "                    set_font_color(header_run, DATABRICKS_ORANGE)\n",
    "                    header_p.space_after = Pt(16)\n",
    "                    \n",
    "                    def add_detail_line(frame, label_key, value, align, is_first=False):\n",
    "                        p = frame.add_paragraph(); p.level = 0; p.alignment = align\n",
    "                        if not is_first: p.space_before = Pt(12)\n",
    "                        run_label = p.add_run(); run_label.text = f\"{t[label_key]}: \"; run_label.font.bold = True; run_label.font.size = Pt(20)\n",
    "                        set_font_color(run_label, DATABRICKS_BLUE)\n",
    "                        run_value = p.add_run(); run_value.text = value; run_value.font.size = Pt(20)\n",
    "                        set_font_color(run_value, TEXT_COLOR)\n",
    "                    # Type is already shown in header line above, no need to repeat it\n",
    "                    add_detail_line(content_frame, 'statement', uc.get('Statement', 'N/A'), align, is_first=True)\n",
    "                    add_detail_line(content_frame, 'solution', uc.get('Solution', 'N/A'), align)\n",
    "                    add_detail_line(content_frame, 'business_value', uc.get('Business Value', 'N/A'), align)\n",
    "                    add_detail_line(content_frame, 'beneficiary', uc.get('Beneficiary', 'N/A'), align)\n",
    "                    add_detail_line(content_frame, 'sponsor', uc.get('Sponsor', 'N/A'), align)\n",
    "                    # Add Business Priority Alignment\n",
    "                    priority_alignment_label = t.get('business_priority_alignment', 'Business Priority Alignment')\n",
    "                    p = content_frame.add_paragraph(); p.level = 0; p.alignment = align; p.space_before = Pt(12)\n",
    "                    run_label = p.add_run(); run_label.text = f\"{priority_alignment_label}: \"; run_label.font.bold = True; run_label.font.size = Pt(20)\n",
    "                    set_font_color(run_label, DATABRICKS_BLUE)\n",
    "                    run_value = p.add_run(); run_value.text = translate_strategic_pptx_value(uc.get('Business Priority Alignment', 'General Improvement')); run_value.font.size = Pt(20)\n",
    "                    set_font_color(run_value, TEXT_COLOR)\n",
    "                    # Add Strategic Goals Alignment\n",
    "                    strategic_goals_label = t.get('strategic_goals_alignment', 'Strategic Goals Alignment')\n",
    "                    p = content_frame.add_paragraph(); p.level = 0; p.alignment = align; p.space_before = Pt(8)\n",
    "                    run_label = p.add_run(); run_label.text = f\"{strategic_goals_label}: \"; run_label.font.bold = True; run_label.font.size = Pt(20)\n",
    "                    set_font_color(run_label, DATABRICKS_BLUE)\n",
    "                    run_value = p.add_run(); run_value.text = translate_strategic_pptx_value(uc.get('Strategic Goals Alignment', 'General Improvement')); run_value.font.size = Pt(20)\n",
    "                    set_font_color(run_value, TEXT_COLOR)\n",
    "                    # Analytics Technique is already shown in the header line above, no need to repeat it\n",
    "                    add_footer(slide)\n",
    "            \n",
    "            local_pptx_path = None\n",
    "            try:\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".pptx\") as tmp_file: local_pptx_path = tmp_file.name\n",
    "                prs.save(local_pptx_path)\n",
    "                logger_instance.info(f\"Presentation saved locally to {local_pptx_path}\")\n",
    "                _save_pptx(local_pptx_path, workspace_path, logger_instance)\n",
    "            except Exception as e: logger_instance.error(f\"Failed to save or upload PPTX: {e}\")\n",
    "            finally:\n",
    "                if local_pptx_path and os.path.exists(local_pptx_path): os.remove(local_pptx_path)\n",
    "\n",
    "        def _save_pptx(local_pptx_path: str, workspace_path: str, logger_instance):\n",
    "            try:\n",
    "                with open(local_pptx_path, \"rb\") as f: pptx_data = f.read()\n",
    "                if not pptx_data: raise ValueError(\"Generated PPTX file is empty.\")\n",
    "                logger_instance.info(f\"Uploading PPTX to workspace path: {workspace_path}\")\n",
    "                pptx_data_b64 = base64.b64encode(pptx_data).decode()\n",
    "                self.w_client.workspace.import_(path=workspace_path, content=pptx_data_b64, format=workspace.ImportFormat.AUTO, overwrite=True)\n",
    "                abs_path = self.w_client.workspace.get_status(workspace_path).path\n",
    "                logger_instance.info(f\"Success! Presentation uploaded to: {abs_path}\")\n",
    "                log_print(f\"Success! Presentation ({language}) generated: {abs_path}\")\n",
    "            except Exception as e: logger_instance.critical(f\"Failed to save and upload PPTX: {e}\")\n",
    "\n",
    "        # --- Main execution logic for generate_presentation_pptx ---\n",
    "        try:\n",
    "            if not _install_pptx_dependencies(self.logger):\n",
    "                self.logger.error(\"Skipping PPTX generation due to missing python-pptx dependency.\")\n",
    "                return\n",
    "\n",
    "            if not grouped_data:\n",
    "                self.logger.warning(f\"No use cases provided to generate_presentation_pptx for {language}. Skipping.\")\n",
    "                return\n",
    "            pptx_workspace_path = os.path.join(self.docs_output_dir, f\"{self.business_name}-dbx_inspire_{lang_abbr}.pptx\")\n",
    "            _build_presentation(grouped_data, summary_dict, transliterated_name, t, pptx_workspace_path, self.logger, is_rtl)\n",
    "        except Exception as e:\n",
    "            self.logger.critical(f\"An error occurred during PPTX generation for {language}: {e}\")\n",
    "    \n",
    "    def _install_excel_dependencies(self, logger_instance) -> bool:\n",
    "        \"\"\"Installs xlsxwriter if not present.\"\"\"\n",
    "        try:\n",
    "            import xlsxwriter\n",
    "            logger_instance.info(\"Excel package (xlsxwriter) already installed.\")\n",
    "            return True\n",
    "        except ImportError:\n",
    "            logger_instance.info(f\"Installing required Excel package: xlsxwriter...\")\n",
    "            try:\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"xlsxwriter\"])\n",
    "                import xlsxwriter\n",
    "                logger_instance.info(\"Successfully installed xlsxwriter.\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                logger_instance.error(f\"Failed to install xlsxwriter: {e}\")\n",
    "                print(\"ERROR: Failed to install 'xlsxwriter'. Excel generation cannot continue.\", file=sys.stderr)\n",
    "                return False\n",
    "\n",
    "    def _save_excel(self, local_excel_path: str, workspace_path: str, logger_instance, language: str):\n",
    "        \"\"\"Uploads a locally generated Excel file to the Databricks workspace.\"\"\"\n",
    "        try:\n",
    "            with open(local_excel_path, \"rb\") as f: excel_data = f.read()\n",
    "            if not excel_data: raise ValueError(\"Generated Excel file is empty.\")\n",
    "            logger_instance.info(f\"Uploading Excel to workspace path: {workspace_path}\")\n",
    "            excel_data_b64 = base64.b64encode(excel_data).decode()\n",
    "            self.w_client.workspace.import_(\n",
    "                path=workspace_path, content=excel_data_b64,\n",
    "                format=workspace.ImportFormat.AUTO, overwrite=True\n",
    "            )\n",
    "            abs_path = self.w_client.workspace.get_status(workspace_path).path\n",
    "            logger_instance.info(f\"Success! Excel Catalog uploaded to: {abs_path}\")\n",
    "            log_print(f\"Success! Excel Catalog ({language}) generated: {abs_path}\")\n",
    "        except Exception as e:\n",
    "            logger_instance.critical(f\"Failed to save and upload Excel: {e}\")\n",
    "\n",
    "    def _generate_use_case_excel(self, language: str, lang_abbr: str, grouped_data: dict):\n",
    "        warnings.filterwarnings('ignore', module='xlsxwriter')\n",
    "        # Only generate Excel for English\n",
    "        if language != \"English\":\n",
    "            self.logger.info(f\"Skipping Excel generation for {language} (only English Excel is generated).\")\n",
    "            return\n",
    "        \n",
    "        self.logger.info(f\"--- Starting Excel Catalog Generation with XlsxWriter for {language} ---\")\n",
    "        \n",
    "        local_excel_path = None\n",
    "        try:\n",
    "            if not self._install_excel_dependencies(self.logger):\n",
    "                self.logger.error(\"Skipping Excel generation due to missing xlsxwriter dependency.\")\n",
    "                return\n",
    "\n",
    "            import xlsxwriter\n",
    "            \n",
    "            # Prepare data\n",
    "            data_rows = []\n",
    "            \n",
    "            def safe_str(value, default='N/A'):\n",
    "                \"\"\"Safely convert value to string, handling None/empty.\"\"\"\n",
    "                if value is None or (isinstance(value, str) and not value.strip()):\n",
    "                    return default\n",
    "                return str(value)\n",
    "            \n",
    "            for domain, use_cases in grouped_data.items():\n",
    "                for uc in use_cases:\n",
    "                    data_rows.append([\n",
    "                        safe_str(uc.get('No'), 'N/A'),                                 # 0 - ID (A)\n",
    "                        safe_str(uc.get('Business Domain'), 'N/A'),                    # 1 - Business Domain (B)\n",
    "                        safe_str(uc.get('Subdomain'), 'N/A'),                          # 2 - Subdomain (C)\n",
    "                        safe_str(uc.get('Name'), 'N/A'),                               # 3 - Use Case (D)\n",
    "                        safe_str(uc.get('type'), 'N/A'),                               # 4 - Type (E)\n",
    "                        safe_str(uc.get('Analytics Technique'), 'N/A'),                # 5 - Analytics Technique (F)\n",
    "                        safe_str(uc.get('Business Priority Alignment'), 'General Improvement'),  # 6 - Business Priority Alignment (G)\n",
    "                        safe_str(uc.get('Strategic Goals Alignment'), 'General Improvement'),    # 7 - Strategic Goals Alignment (H)\n",
    "                        safe_str(uc.get('Priority'), 'N/A'),                           # 8 - Priority (I)\n",
    "                        safe_str(uc.get('Statement'), 'N/A'),                          # 9 - Statement (J)\n",
    "                        safe_str(uc.get('Solution'), 'N/A'),                           # 10 - Solution (K)\n",
    "                        safe_str(uc.get('Business Value'), 'N/A'),                     # 11 - Business Value (L)\n",
    "                        safe_str(uc.get('Beneficiary'), 'N/A'),                        # 12 - Beneficiary (M)\n",
    "                        safe_str(uc.get('Sponsor'), 'N/A'),                            # 13 - Sponsor (N)\n",
    "                        safe_str(uc.get('Tables Involved'), 'N/A'),                    # 14 - Tables Involved (O)\n",
    "                        uc.get('Strategic Alignment', 0),                              # 15 - Strategic Alignment (P)\n",
    "                        uc.get('Return on Investment', 0),                             # 16 - ROI (Q)\n",
    "                        uc.get('Reusability', 0),                                      # 17 - Reusability (R)\n",
    "                        uc.get('Time to Value', 0),                                    # 18 - Time to Value (S)\n",
    "                        uc.get('Data Availability', 0),                                # 19 - Data Availability (T)\n",
    "                        uc.get('Data Accessibility', 0),                               # 20 - Data Accessibility (U)\n",
    "                        uc.get('Architecture Fitness', 0),                             # 21 - Architecture Fitness (V)\n",
    "                        uc.get('Team Skills', 0),                                      # 22 - Team Skills (W)\n",
    "                        uc.get('Domain Knowledge', 0),                                 # 23 - Domain Knowledge (X)\n",
    "                        uc.get('People Allocation', 0),                                # 24 - People Allocation (Y)\n",
    "                        uc.get('Budget Allocation', 0),                                # 25 - Budget Allocation (Z)\n",
    "                        uc.get('Time to Production', 0),                               # 26 - Time to Production (AA)\n",
    "                        uc.get('Value', 0),                                            # 27 - Value Score (AB)\n",
    "                        uc.get('Feasibility', 0),                                      # 28 - Feasibility Score (AC)\n",
    "                        uc.get('Priority Score', 0),                                   # 29 - Priority Score (AD)\n",
    "                        safe_str(uc.get('Justification'), 'N/A')                       # 30 - Justification (AE)\n",
    "                    ])\n",
    "            \n",
    "            # Sort by Priority Score descending\n",
    "            priority_score_idx = 29  # Priority Score is now column AD (index 29)\n",
    "            data_rows.sort(key=lambda row: float(row[priority_score_idx]) if isinstance(row[priority_score_idx], (int, float)) else 0, reverse=True)\n",
    "            \n",
    "            if not data_rows:\n",
    "                self.logger.warning(f\"No data to write to Excel for {language}. Skipping.\")\n",
    "                return\n",
    "            \n",
    "            # Create Excel file\n",
    "            excel_file_name = f\"{self.business_name}-dbx_inspire.xlsx\"\n",
    "            with tempfile.NamedTemporaryFile(delete=False, suffix=\".xlsx\") as tmp_file:\n",
    "                local_excel_path = tmp_file.name\n",
    "            \n",
    "            self.logger.info(f\"Creating Excel file at {local_excel_path}\")\n",
    "            workbook = xlsxwriter.Workbook(local_excel_path, {'strings_to_numbers': False})\n",
    "            worksheet = workbook.add_worksheet('Use Cases')\n",
    "            \n",
    "            # Modern Business Color Palette\n",
    "            PRIMARY = '#2C3E50'      # Deep Slate\n",
    "            SECONDARY = '#E74C3C'    # Vibrant Coral  \n",
    "            ACCENT = '#3498DB'       # Bright Blue\n",
    "            BACKGROUND = '#ECF0F1'   # Soft Grey\n",
    "            TEXT = '#2C3E50'         # Dark Grey\n",
    "            \n",
    "            # Define cell formats\n",
    "            header_format = workbook.add_format({\n",
    "                'bold': True,\n",
    "                'font_color': 'white',\n",
    "                'bg_color': PRIMARY,\n",
    "                'border': 1,\n",
    "                'align': 'center',\n",
    "                'valign': 'vcenter',\n",
    "                'text_wrap': True,\n",
    "                'font_size': 11\n",
    "            })\n",
    "            \n",
    "            cell_format = workbook.add_format({\n",
    "                'border': 1,\n",
    "                'align': 'left',\n",
    "                'valign': 'top',\n",
    "                'text_wrap': True,\n",
    "                'font_size': 10\n",
    "            })\n",
    "            \n",
    "            numeric_format = workbook.add_format({\n",
    "                'border': 1,\n",
    "                'align': 'center',\n",
    "                'valign': 'vcenter',\n",
    "                'num_format': '0.00',\n",
    "                'font_size': 10\n",
    "            })\n",
    "            \n",
    "            # Column headers with both alignment columns\n",
    "            # A: ID, B: Business Domain, C: Subdomain, D: Use Case, E: Type, F: Analytics Technique\n",
    "            # G: Business Priority Alignment, H: Strategic Goals Alignment, I: Priority\n",
    "            # J: Statement, K: Solution, L: Business Value, M: Beneficiary, N: Sponsor, O: Tables Involved\n",
    "            # P: Strategic Alignment, Q: ROI, R: Reusability, S: Time to Value\n",
    "            # T: Data Availability, U: Data Accessibility, V: Architecture Fitness, W: Team Skills\n",
    "            # X: Domain Knowledge, Y: People Allocation, Z: Budget Allocation, AA: Time to Production\n",
    "            # AB: Value Score, AC: Feasibility Score, AD: Priority Score, AE: Justification\n",
    "            headers = [\n",
    "                \"ID\", \"Business Domain\", \"Subdomain\", \"Use Case\", \"Type\", \"Analytics Technique\",\n",
    "                \"Business Priority Alignment\", \"Strategic Goals Alignment\", \"Priority\",\n",
    "                \"Statement\", \"Solution\", \"Business Value\", \"Beneficiary\", \"Sponsor\", \"Tables Involved\",\n",
    "                \"Strategic Alignment\", \"ROI\", \"Reusability\", \"Time to Value\",\n",
    "                \"Data Availability\", \"Data Accessibility\", \"Architecture Fitness\", \"Team Skills\", \n",
    "                \"Domain Knowledge\", \"People Allocation\", \"Budget Allocation\", \"Time to Production\",\n",
    "                \"Value Score\",\n",
    "                \"Feasibility Score\",\n",
    "                \"Priority Score\",\n",
    "                \"Justification\"\n",
    "            ]\n",
    "            \n",
    "            # Write headers\n",
    "            for col_num, header in enumerate(headers):\n",
    "                worksheet.write(0, col_num, header, header_format)\n",
    "            \n",
    "            # Write data rows\n",
    "            for row_num, row_data in enumerate(data_rows, start=1):\n",
    "                numeric_start = 15  # Strategic Alignment score (column P, index 15)\n",
    "                numeric_end = 29    # Priority Score (column AD, index 29)\n",
    "                for col_num, cell_data in enumerate(row_data):\n",
    "                    # Use numeric format for score columns\n",
    "                    if col_num >= numeric_start and col_num <= numeric_end:\n",
    "                        try:\n",
    "                            numeric_value = float(cell_data) if cell_data not in ['N/A', '', None] else 0\n",
    "                            worksheet.write_number(row_num, col_num, numeric_value, numeric_format)\n",
    "                        except (ValueError, TypeError):\n",
    "                            worksheet.write(row_num, col_num, cell_data, cell_format)\n",
    "                    else:\n",
    "                        worksheet.write(row_num, col_num, cell_data, cell_format)\n",
    "            \n",
    "            # Auto-fit column widths to content\n",
    "            for col_num in range(len(headers)):\n",
    "                # Calculate max width for this column\n",
    "                max_width = len(str(headers[col_num]))\n",
    "                for row_data in data_rows:\n",
    "                    if col_num < len(row_data):\n",
    "                        cell_value = str(row_data[col_num])\n",
    "                        max_width = max(max_width, len(cell_value))\n",
    "                # Set width to fit entire text\n",
    "                column_width = max_width + 2\n",
    "                worksheet.set_column(col_num, col_num, column_width)\n",
    "            \n",
    "            # Freeze top row\n",
    "            worksheet.freeze_panes(1, 0)\n",
    "            \n",
    "            # Convert data range to native Excel Table\n",
    "            last_row = len(data_rows)\n",
    "            last_col = len(headers) - 1\n",
    "            worksheet.add_table(0, 0, last_row, last_col, {\n",
    "                'name': 'UseCaseTable',\n",
    "                'style': 'Table Style Medium 9',\n",
    "                'columns': [{'header': h} for h in headers]\n",
    "            })\n",
    "            \n",
    "            # Add conditional formatting - Data Bars for all scoring columns\n",
    "            # Column indices based on headers array (with Analytics Technique at F, Primary Table at O):\n",
    "            # 15: Strategic Alignment (P), 16: ROI (Q), 17: Reusability (R), 18: Time to Value (S)\n",
    "            # 19: Data Availability (T), 20: Data Accessibility (U), 21: Architecture Fitness (V)\n",
    "            # 22: Team Skills (W), 23: Domain Knowledge (X), 24: People Allocation (Y)\n",
    "            # 25: Budget Allocation (Z), 26: Time to Production (AA), 27: Value Score (AB)\n",
    "            # 28: Feasibility Score (AC), 29: Priority Score (AD)\n",
    "            scoring_columns = [\n",
    "                (15, '#4472C4'),  # Strategic Alignment (P)\n",
    "                (16, '#ED7D31'),  # ROI (Q)\n",
    "                (17, '#A5A5A5'),  # Reusability (R)\n",
    "                (18, '#FFC000'),  # Time to Value (S)\n",
    "                (19, '#5B9BD5'),  # Data Availability (T)\n",
    "                (20, '#70AD47'),  # Data Accessibility (U)\n",
    "                (21, '#264478'),  # Architecture Fitness (V)\n",
    "                (22, '#9E480E'),  # Team Skills (W)\n",
    "                (23, '#636363'),  # Domain Knowledge (X)\n",
    "                (24, '#997300'),  # People Allocation (Y)\n",
    "                (25, '#255E91'),  # Budget Allocation (Z)\n",
    "                (26, '#43682B'),  # Time to Production (AA)\n",
    "                (27, ACCENT),     # Value Score (AB)\n",
    "                (28, SECONDARY),  # Feasibility Score (AC)\n",
    "            ]\n",
    "            \n",
    "            for col_idx, bar_color in scoring_columns:\n",
    "                worksheet.conditional_format(1, col_idx, last_row, col_idx, {\n",
    "                    'type': 'data_bar',\n",
    "                    'bar_color': bar_color,\n",
    "                    'bar_only': False\n",
    "                })\n",
    "            \n",
    "            # Add conditional formatting - Color Scale for Priority Score (column 29 = Column AD)\n",
    "            # Headers: ..., 27: Value Score (AB), 28: Feasibility Score (AC), 29: Priority Score (AD), 30: Justification (AE)\n",
    "            worksheet.conditional_format(1, 29, last_row, 29, {\n",
    "                'type': '3_color_scale',\n",
    "                'min_color': '#F8696B',   # Red for low priority\n",
    "                'mid_color': '#FFEB84',   # Yellow for medium priority\n",
    "                'max_color': '#63BE7B'    # Green for high priority\n",
    "            })\n",
    "            \n",
    "            # Graph sheet removed per user request - no longer needed\n",
    "            \n",
    "            workbook.close()\n",
    "            self.logger.info(f\"Excel file created successfully with native tables\")\n",
    "            \n",
    "            workspace_excel_path = os.path.join(self.docs_output_dir, excel_file_name)\n",
    "            self._save_excel(local_excel_path, workspace_excel_path, self.logger, language)\n",
    "        except Exception as e:\n",
    "            self.logger.critical(f\"An error occurred during Excel generation for {language}: {e}\")\n",
    "            raise\n",
    "        finally:\n",
    "            if local_excel_path and os.path.exists(local_excel_path):\n",
    "                os.remove(local_excel_path)\n",
    "\n",
    "    def _generate_markdown_catalog(self, language: str, lang_abbr: str, grouped_data: dict, summary_dict: dict, transliterated_name: str):\n",
    "        \"\"\"\n",
    "        Generates a Markdown catalog file containing all use case information.\n",
    "        This is ALWAYS generated as a fallback when PDF generation may fail.\n",
    "        \"\"\"\n",
    "        if language != \"English\":\n",
    "            self.logger.info(f\"Skipping Markdown generation for {language} (only English Markdown is generated).\")\n",
    "            return\n",
    "        \n",
    "        self.logger.info(f\"--- Starting Markdown Catalog Generation for {language} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Build markdown content\n",
    "            md_content = []\n",
    "            \n",
    "            # Header\n",
    "            md_content.append(f\"# {self.business_name} - Databricks Inspire AI Use Cases Catalog\\n\")\n",
    "            md_content.append(f\"**Generated:** {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            md_content.append(f\"**Business Name:** {transliterated_name or self.business_name}\\n\")\n",
    "            \n",
    "            # Summary section if available\n",
    "            if summary_dict:\n",
    "                md_content.append(\"\\n## Executive Summary\\n\")\n",
    "                if summary_dict.get('executive_summary'):\n",
    "                    md_content.append(f\"{summary_dict.get('executive_summary')}\\n\")\n",
    "                if summary_dict.get('key_findings'):\n",
    "                    md_content.append(f\"\\n**Key Findings:** {summary_dict.get('key_findings')}\\n\")\n",
    "                if summary_dict.get('recommendations'):\n",
    "                    md_content.append(f\"\\n**Recommendations:** {summary_dict.get('recommendations')}\\n\")\n",
    "            \n",
    "            # Stats\n",
    "            total_use_cases = sum(len(ucs) for ucs in grouped_data.values())\n",
    "            md_content.append(f\"\\n## Overview\\n\")\n",
    "            md_content.append(f\"- **Total Use Cases:** {total_use_cases}\\n\")\n",
    "            md_content.append(f\"- **Business Domains:** {len(grouped_data)}\\n\")\n",
    "            \n",
    "            # Use cases by domain\n",
    "            md_content.append(\"\\n## Use Cases by Business Domain\\n\")\n",
    "            \n",
    "            for domain_name, use_cases in grouped_data.items():\n",
    "                md_content.append(f\"\\n### {domain_name}\\n\")\n",
    "                md_content.append(f\"*{len(use_cases)} use cases*\\n\")\n",
    "                \n",
    "                for uc in use_cases:\n",
    "                    uc_id = uc.get('No', 'N/A')\n",
    "                    uc_name = uc.get('Name', 'Unnamed')\n",
    "                    uc_priority = uc.get('Priority', 'Medium')\n",
    "                    uc_type = uc.get('type', 'N/A')\n",
    "                    uc_statement = uc.get('Statement', 'N/A')\n",
    "                    uc_solution = uc.get('Solution', 'N/A')\n",
    "                    uc_value = uc.get('Business Value', 'N/A')\n",
    "                    uc_beneficiary = uc.get('Beneficiary', 'N/A')\n",
    "                    uc_sponsor = uc.get('Sponsor', 'N/A')\n",
    "                    uc_tables = uc.get('Primary Table', 'N/A')\n",
    "                    uc_priority_score = uc.get('Priority Score', 0)\n",
    "                    uc_justification = uc.get('Justification', 'N/A')\n",
    "                    uc_subdomain = uc.get('Subdomain', 'N/A')\n",
    "                    uc_analytics = uc.get('Analytics Technique', 'N/A')\n",
    "                    uc_bp_alignment = uc.get('Business Priority Alignment', 'N/A')\n",
    "                    uc_sg_alignment = uc.get('Strategic Goals Alignment', 'N/A')\n",
    "                    \n",
    "                    md_content.append(f\"\\n#### {uc_id}: {uc_name}\\n\")\n",
    "                    md_content.append(f\"- **Subdomain:** {uc_subdomain}\\n\")\n",
    "                    md_content.append(f\"- **Type:** {uc_type}\\n\")\n",
    "                    md_content.append(f\"- **Priority:** {uc_priority} (Score: {uc_priority_score})\\n\")\n",
    "                    md_content.append(f\"- **Analytics Technique:** {uc_analytics}\\n\")\n",
    "                    md_content.append(f\"- **Business Priority Alignment:** {uc_bp_alignment}\\n\")\n",
    "                    md_content.append(f\"- **Strategic Goals Alignment:** {uc_sg_alignment}\\n\")\n",
    "                    md_content.append(f\"\\n**Problem Statement:**\\n{uc_statement}\\n\")\n",
    "                    md_content.append(f\"\\n**Solution:**\\n{uc_solution}\\n\")\n",
    "                    md_content.append(f\"\\n**Business Value:**\\n{uc_value}\\n\")\n",
    "                    md_content.append(f\"\\n**Beneficiary:** {uc_beneficiary}\\n\")\n",
    "                    md_content.append(f\"**Sponsor:** {uc_sponsor}\\n\")\n",
    "                    md_content.append(f\"**Primary Table:** {uc_tables}\\n\")\n",
    "                    if uc_justification and uc_justification != 'N/A':\n",
    "                        md_content.append(f\"\\n**Justification:**\\n{uc_justification}\\n\")\n",
    "                    md_content.append(\"\\n---\\n\")\n",
    "            \n",
    "            # Save to workspace\n",
    "            md_file_name = f\"{self.business_name}-dbx_inspire.md\"\n",
    "            workspace_md_path = os.path.join(self.docs_output_dir, md_file_name)\n",
    "            md_text = ''.join(md_content)\n",
    "            \n",
    "            # Upload to workspace\n",
    "            md_data_b64 = base64.b64encode(md_text.encode('utf-8')).decode()\n",
    "            self.w_client.workspace.import_(\n",
    "                path=workspace_md_path, content=md_data_b64,\n",
    "                format=workspace.ImportFormat.AUTO, overwrite=True\n",
    "            )\n",
    "            abs_path = self.w_client.workspace.get_status(workspace_md_path).path\n",
    "            self.logger.info(f\"Success! Markdown Catalog uploaded to: {abs_path}\")\n",
    "            log_print(f\"Success! Markdown Catalog ({language}) generated: {abs_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to generate Markdown catalog for {language}: {e}\")\n",
    "            import traceback\n",
    "            self.logger.error(f\"Full traceback: {traceback.format_exc()}\")\n",
    "\n",
    "    def _generate_csv_catalog(self, language: str, lang_abbr: str, grouped_data: dict):\n",
    "        \"\"\"\n",
    "        Generates a CSV catalog file containing all use case information.\n",
    "        This is ALWAYS generated as a fallback when Excel generation may fail.\n",
    "        \"\"\"\n",
    "        if language != \"English\":\n",
    "            self.logger.info(f\"Skipping CSV generation for {language} (only English CSV is generated).\")\n",
    "            return\n",
    "        \n",
    "        self.logger.info(f\"--- Starting CSV Catalog Generation for {language} ---\")\n",
    "        \n",
    "        try:\n",
    "            # Prepare data rows\n",
    "            data_rows = []\n",
    "            headers = [\n",
    "                \"ID\", \"Business Domain\", \"Subdomain\", \"Use Case\", \"Type\", \"Analytics Technique\",\n",
    "                \"Business Priority Alignment\", \"Strategic Goals Alignment\", \"Priority\",\n",
    "                \"Statement\", \"Solution\", \"Business Value\", \"Beneficiary\", \"Sponsor\", \"Primary Table\",\n",
    "                \"Strategic Alignment\", \"ROI\", \"Reusability\", \"Time to Value\",\n",
    "                \"Data Availability\", \"Data Accessibility\", \"Architecture Fitness\", \"Team Skills\", \n",
    "                \"Domain Knowledge\", \"People Allocation\", \"Budget Allocation\", \"Time to Production\",\n",
    "                \"Value Score\", \"Feasibility Score\", \"Priority Score\", \"Justification\"\n",
    "            ]\n",
    "            \n",
    "            for domain, use_cases in grouped_data.items():\n",
    "                for uc in use_cases:\n",
    "                    data_rows.append([\n",
    "                        uc.get('No', 'N/A'),\n",
    "                        uc.get('Business Domain', 'N/A'),\n",
    "                        uc.get('Subdomain', 'N/A'),\n",
    "                        uc.get('Name', 'N/A'),\n",
    "                        uc.get('type', 'N/A'),\n",
    "                        uc.get('Analytics Technique', 'N/A'),\n",
    "                        uc.get('Business Priority Alignment', 'General Improvement'),\n",
    "                        uc.get('Strategic Goals Alignment', 'General Improvement'),\n",
    "                        uc.get('Priority', 'N/A'),\n",
    "                        uc.get('Statement', 'N/A'),\n",
    "                        uc.get('Solution', 'N/A'),\n",
    "                        uc.get('Business Value', 'N/A'),\n",
    "                        uc.get('Beneficiary', 'N/A'),\n",
    "                        uc.get('Sponsor', 'N/A'),\n",
    "                        uc.get('Primary Table', 'N/A'),\n",
    "                        uc.get('Strategic Alignment', 0),\n",
    "                        uc.get('Return on Investment', 0),\n",
    "                        uc.get('Reusability', 0),\n",
    "                        uc.get('Time to Value', 0),\n",
    "                        uc.get('Data Availability', 0),\n",
    "                        uc.get('Data Accessibility', 0),\n",
    "                        uc.get('Architecture Fitness', 0),\n",
    "                        uc.get('Team Skills', 0),\n",
    "                        uc.get('Domain Knowledge', 0),\n",
    "                        uc.get('People Allocation', 0),\n",
    "                        uc.get('Budget Allocation', 0),\n",
    "                        uc.get('Time to Production', 0),\n",
    "                        uc.get('Value', 0),\n",
    "                        uc.get('Feasibility', 0),\n",
    "                        uc.get('Priority Score', 0),\n",
    "                        uc.get('Justification', 'N/A')\n",
    "                    ])\n",
    "            \n",
    "            # Sort by Priority Score descending\n",
    "            data_rows.sort(key=lambda row: float(row[29]) if isinstance(row[29], (int, float)) else 0, reverse=True)\n",
    "            \n",
    "            if not data_rows:\n",
    "                self.logger.warning(f\"No data to write to CSV for {language}. Skipping.\")\n",
    "                return\n",
    "            \n",
    "            # Build CSV content\n",
    "            output = io.StringIO()\n",
    "            writer = csv.writer(output, quoting=csv.QUOTE_ALL)\n",
    "            writer.writerow(headers)\n",
    "            for row in data_rows:\n",
    "                writer.writerow(row)\n",
    "            csv_content = output.getvalue()\n",
    "            \n",
    "            # Save to workspace\n",
    "            csv_file_name = f\"{self.business_name}-dbx_inspire.csv\"\n",
    "            workspace_csv_path = os.path.join(self.docs_output_dir, csv_file_name)\n",
    "            \n",
    "            # Upload to workspace\n",
    "            csv_data_b64 = base64.b64encode(csv_content.encode('utf-8')).decode()\n",
    "            self.w_client.workspace.import_(\n",
    "                path=workspace_csv_path, content=csv_data_b64,\n",
    "                format=workspace.ImportFormat.AUTO, overwrite=True\n",
    "            )\n",
    "            abs_path = self.w_client.workspace.get_status(workspace_csv_path).path\n",
    "            self.logger.info(f\"Success! CSV Catalog uploaded to: {abs_path}\")\n",
    "            log_print(f\"Success! CSV Catalog ({language}) generated: {abs_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to generate CSV catalog for {language}: {e}\")\n",
    "            import traceback\n",
    "            self.logger.error(f\"Full traceback: {traceback.format_exc()}\")\n",
    "\n",
    "    def _validate_use_case_tables(self, parsed_rows: list, full_schema_details: list, log_prefix: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Validate that all tables referenced in 'Tables Involved' field actually exist in the schema.\n",
    "        This catches LLM hallucinations where it invents table names.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (is_valid: bool, hallucinated_use_cases: list, valid_use_cases: list)\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Build set of available tables from schema\n",
    "        available_tables = set()\n",
    "        for detail in full_schema_details:\n",
    "            (catalog, schema, table, _, _, _) = detail\n",
    "            available_tables.add(f\"{catalog}.{schema}.{table}\")\n",
    "            available_tables.add(f\"`{catalog}`.`{schema}`.`{table}`\")\n",
    "            available_tables.add(f\"{catalog}.{schema}.{table}\".lower())\n",
    "        global_tables = getattr(self, \"global_table_names\", set())\n",
    "        for tbl in global_tables:\n",
    "            available_tables.add(tbl)\n",
    "            available_tables.add(tbl.lower())\n",
    "        \n",
    "        hallucinated_use_cases = []\n",
    "        valid_use_cases = []\n",
    "        \n",
    "        for row in parsed_rows:\n",
    "            tables_involved_str = row.get('Tables Involved', '').strip()\n",
    "            use_case_id = row.get('No', 'Unknown')\n",
    "            use_case_name = row.get('Name', 'Unknown')\n",
    "            \n",
    "            # Skip volume paths (for ai_parse_document use cases)\n",
    "            if tables_involved_str.startswith('/Volumes'):\n",
    "                valid_use_cases.append(row)\n",
    "                continue\n",
    "            \n",
    "            # Skip empty tables involved (will be caught by other validation)\n",
    "            if not tables_involved_str:\n",
    "                hallucinated_use_cases.append(row)\n",
    "                row['hallucination_reason'] = \"No tables specified\"\n",
    "                continue\n",
    "            \n",
    "            # Extract table names from comma-separated list\n",
    "            # Use simple approach: strip backticks, split by comma, parse each table\n",
    "            table_matches = []\n",
    "            for table_str in tables_involved_str.split(','):\n",
    "                table_str = table_str.strip()\n",
    "                if not table_str:\n",
    "                    continue\n",
    "                cat, sch, tbl = parse_three_level_name(table_str)\n",
    "                if cat and sch and tbl:\n",
    "                    table_matches.append((cat, sch, tbl))\n",
    "            \n",
    "            if not table_matches:\n",
    "                hallucinated_use_cases.append(row)\n",
    "                row['hallucination_reason'] = f\"Invalid table format: {tables_involved_str}\"\n",
    "                self.logger.warning(f\"{log_prefix} Use case {use_case_id}: Invalid table format: {tables_involved_str}\")\n",
    "                continue\n",
    "            \n",
    "            # Check if all tables exist in schema\n",
    "            all_tables_found = True\n",
    "            missing_tables = []\n",
    "            for match in table_matches:\n",
    "                catalog, schema, table = match\n",
    "                # Try multiple formats (normalized names for comparison)\n",
    "                table_formats = [\n",
    "                    f\"{catalog}.{schema}.{table}\",\n",
    "                    build_fqn(catalog, schema, table),\n",
    "                    f\"{catalog}.{schema}.{table}\".lower()\n",
    "                ]\n",
    "                \n",
    "                found = any(fmt in available_tables for fmt in table_formats)\n",
    "                if not found:\n",
    "                    all_tables_found = False\n",
    "                    missing_tables.append(f\"{catalog}.{schema}.{table}\")\n",
    "            \n",
    "            if not all_tables_found:\n",
    "                hallucinated_use_cases.append(row)\n",
    "                row['hallucination_reason'] = f\"Tables not found in schema: {', '.join(missing_tables)}\"\n",
    "                self.logger.warning(f\"{log_prefix} Use case {use_case_id}: Hallucinated tables: {', '.join(missing_tables)}\")\n",
    "            else:\n",
    "                reference_tables = {k.lower() for k, v in getattr(self, \"data_category_map\", {}).items() if v == \"REFERENCE\"}\n",
    "                if reference_tables:\n",
    "                    non_reference_found = False\n",
    "                    for match in table_matches:\n",
    "                        catalog, schema, table = match\n",
    "                        fqtn = f\"{catalog}.{schema}.{table}\".lower()\n",
    "                        if fqtn not in reference_tables:\n",
    "                            non_reference_found = True\n",
    "                            break\n",
    "                    if not non_reference_found:\n",
    "                        hallucinated_use_cases.append(row)\n",
    "                        row['hallucination_reason'] = \"Reference-only use case\"\n",
    "                        self.logger.warning(f\"{log_prefix} Use case {use_case_id}: Reference-only tables in use case\")\n",
    "                        continue\n",
    "                valid_use_cases.append(row)\n",
    "        \n",
    "        is_valid = len(hallucinated_use_cases) == 0\n",
    "        \n",
    "        if not is_valid:\n",
    "            self.logger.warning(f\"{log_prefix} ⚠️ Found {len(hallucinated_use_cases)} use cases with hallucinated/missing tables out of {len(parsed_rows)} total\")\n",
    "            self.logger.warning(f\"{log_prefix}    Valid use cases: {len(valid_use_cases)}\")\n",
    "            self.logger.warning(f\"{log_prefix}    Hallucinated use cases: {len(hallucinated_use_cases)}\")\n",
    "        else:\n",
    "            self.logger.info(f\"{log_prefix} ✓ Table validation passed: All {len(valid_use_cases)} use cases reference existing tables\")\n",
    "        \n",
    "        return (is_valid, hallucinated_use_cases, valid_use_cases)\n",
    "    \n",
    "    def _validate_subdomain_rules(self, parsed_rows: list, log_prefix: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Validate subdomain rules (silent - no individual violation logging):\n",
    "        1. Each domain must have at least 2 subdomains\n",
    "        2. Each subdomain must have at least 3 use cases\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (is_valid: bool, violations: list, corrected_rows: list)\n",
    "        \"\"\"\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        # Group by domain and subdomain\n",
    "        domain_subdomains = defaultdict(set)\n",
    "        subdomain_usecases = defaultdict(list)\n",
    "        \n",
    "        for row in parsed_rows:\n",
    "            domain = row.get('Business Domain', '').strip()\n",
    "            subdomain = row.get('Subdomain', '').strip()\n",
    "            \n",
    "            if domain and subdomain:\n",
    "                domain_subdomains[domain].add(subdomain)\n",
    "                subdomain_usecases[f\"{domain}::{subdomain}\"].append(row)\n",
    "        \n",
    "        violations = []\n",
    "        \n",
    "        # Check Rule 1: Each domain must have at least 2 subdomains\n",
    "        for domain, subdomains in domain_subdomains.items():\n",
    "            if len(subdomains) < 2:\n",
    "                violations.append(f\"Domain '{domain}' has only {len(subdomains)} subdomain(s). Minimum required: 2\")\n",
    "        \n",
    "        # Check Rule 2: Each subdomain must have at least 3 use cases\n",
    "        for key, use_cases in subdomain_usecases.items():\n",
    "            domain, subdomain = key.split('::', 1)\n",
    "            if len(use_cases) < 3:\n",
    "                violations.append(f\"Subdomain '{subdomain}' in domain '{domain}' has only {len(use_cases)} use case(s). Minimum required: 3\")\n",
    "        \n",
    "        # Don't log individual violations - they'll be fixed in consolidation\n",
    "        if violations:\n",
    "            self.logger.debug(f\"{log_prefix} Found {len(violations)} subdomain violations (will be fixed in domain consolidation)\")\n",
    "            return (False, violations, parsed_rows)\n",
    "        \n",
    "        self.logger.info(f\"{log_prefix} ✓ Subdomain validation passed: All domains have ≥2 subdomains, all subdomains have ≥2 use cases\")\n",
    "        return (True, [], parsed_rows)\n",
    "    \n",
    "    def _parse_llm_csv_response(self, llm_response: str, log_prefix: str) -> list:\n",
    "        self.logger.info(f\"{log_prefix} Starting robust 11-column CSV parsing (SQL and scoring metrics will be assigned separately)...\")\n",
    "        parsed_rows = []\n",
    "        \n",
    "        try:\n",
    "            # Clean response - remove markdown fences if present\n",
    "            csv_clean = llm_response.strip()\n",
    "            if csv_clean.startswith('```'):\n",
    "                csv_clean = re.sub(r'^```[a-z]*\\n', '', csv_clean)\n",
    "                csv_clean = re.sub(r'\\n```$', '', csv_clean)\n",
    "            \n",
    "            # Find header line (11 columns - Business Domain, Subdomain, SQL, and scoring columns will be calculated in code)\n",
    "            # Support both quoted and unquoted headers from LLM, with case-insensitive matching\n",
    "            # Analytics Technique is now generated by LLM as column 4\n",
    "            # Column 5 MUST be \"Statement\" (not \"Opportunity\" or any other name)\n",
    "            header_pattern_quoted = r'\"No\",\"Name\",\"[Tt]ype\",\"Analytics Technique\",\"Statement\",\"Solution\",\"Business Value\",\"Beneficiary\",\"Sponsor\",\"Tables Involved\",\"Technical Design\"'\n",
    "            header_pattern_unquoted = r'No,Name,[Tt]ype,Analytics Technique,Statement,Solution,Business Value,Beneficiary,Sponsor,Tables Involved,Technical Design'\n",
    "            header_match = re.search(header_pattern_quoted, csv_clean, re.IGNORECASE)\n",
    "            if not header_match:\n",
    "                header_match = re.search(header_pattern_unquoted, csv_clean, re.IGNORECASE)\n",
    "            \n",
    "            # Fallback: If LLM incorrectly used \"Opportunity\" instead of \"Statement\", fix it\n",
    "            if not header_match:\n",
    "                # Check if LLM used wrong column name\n",
    "                if '\"Opportunity\"' in csv_clean or ',Opportunity,' in csv_clean:\n",
    "                    self.logger.warning(f\"{log_prefix} LLM incorrectly used 'Opportunity' instead of 'Statement' - auto-correcting...\")\n",
    "                    csv_clean = csv_clean.replace('\"Opportunity\"', '\"Statement\"')\n",
    "                    csv_clean = csv_clean.replace(',Opportunity,', ',Statement,')\n",
    "                    # Try matching again after correction\n",
    "                    header_match = re.search(header_pattern_quoted, csv_clean, re.IGNORECASE)\n",
    "                    if not header_match:\n",
    "                        header_match = re.search(header_pattern_unquoted, csv_clean, re.IGNORECASE)\n",
    "            \n",
    "            # Final fallback: Try simpler pattern if exact match still fails\n",
    "            if not header_match:\n",
    "                simpler_pattern = r'(?:\"No\"|No)\\s*,\\s*(?:\"Name\"|Name)\\s*,\\s*(?:\"[Tt]ype\"|[Tt]ype)'\n",
    "                header_match = re.search(simpler_pattern, csv_clean, re.IGNORECASE)\n",
    "                if header_match:\n",
    "                    self.logger.warning(f\"{log_prefix} Using fallback CSV header detection (found simplified pattern)\")\n",
    "            \n",
    "            if not header_match:\n",
    "                # Log first 500 chars of response to help debug\n",
    "                preview = csv_clean[:500] if csv_clean else \"(empty)\"\n",
    "                self.logger.error(f\"{log_prefix} Could not find CSV header in LLM response. Response preview: {preview}\")\n",
    "                return []\n",
    "            \n",
    "            # Extract CSV starting from header\n",
    "            csv_data = csv_clean[header_match.start():]\n",
    "            \n",
    "            # Use centralized CSV parser for robust parsing\n",
    "            csv_rows = CSVParser.parse_csv_string(\n",
    "                csv_data,\n",
    "                logger=self.logger,\n",
    "                context=log_prefix,\n",
    "                quoting=csv.QUOTE_ALL\n",
    "            )\n",
    "            \n",
    "            for row_dict in csv_rows:\n",
    "                try:\n",
    "                    # Defensive helper to safely get and strip values\n",
    "                    def safe_get(d, key):\n",
    "                        value = d.get(key)\n",
    "                        if value is None:\n",
    "                            return ''\n",
    "                        if isinstance(value, str):\n",
    "                            return value.strip()\n",
    "                        return str(value).strip()\n",
    "                    \n",
    "                    # Extract and validate use case number (with null safety)\n",
    "                    use_case_no = safe_get(row_dict, 'No')\n",
    "                    valid_no = bool(use_case_no)  # Accept any non-empty ID\n",
    "                    if not valid_no:\n",
    "                        self.logger.warning(f\"{log_prefix} Skipping row with invalid No field: {use_case_no}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract Analytics Technique from LLM response (with fallback)\n",
    "                    analytics_technique = safe_get(row_dict, 'Analytics Technique')\n",
    "                    if not analytics_technique or analytics_technique == 'N/A':\n",
    "                        analytics_technique = 'AI Analysis'  # Default fallback\n",
    "                    \n",
    "                    # Helper to safely parse float scores\n",
    "                    def safe_float(d, key, default=3.0):\n",
    "                        try:\n",
    "                            value = safe_get(d, key)\n",
    "                            if not value:\n",
    "                                return default\n",
    "                            return float(value)\n",
    "                        except (ValueError, TypeError):\n",
    "                            self.logger.warning(f\"{log_prefix} Invalid float value for {key}: {value}, using default {default}\")\n",
    "                            return default\n",
    "                    \n",
    "                    # Scoring will be added by LLM scoring step after deduplication\n",
    "                    # Initialize with placeholder values that will be replaced\n",
    "                    strategic_alignment = 0.0\n",
    "                    return_on_investment = 0.0\n",
    "                    reusability = 0.0\n",
    "                    time_to_value = 0.0\n",
    "                    data_availability = 0.0\n",
    "                    data_accessibility = 0.0\n",
    "                    architecture_fitness = 0.0\n",
    "                    team_skills = 0.0\n",
    "                    domain_knowledge = 0.0\n",
    "                    people_allocation = 0.0\n",
    "                    budget_allocation = 0.0\n",
    "                    time_to_production = 0.0\n",
    "                    value_score = 0.0\n",
    "                    feasibility_score = 0.0\n",
    "                    priority_score = 0.0\n",
    "                    priority_label = \"Pending\"\n",
    "                    \n",
    "                    # Build row dictionary with all fields (SQL, Business Domain, and Subdomain will be added later)\n",
    "                    # Using safe_get to handle None values and type conversions\n",
    "                    # Column name MUST be \"Statement\" (auto-corrected above if LLM used wrong name)\n",
    "                    statement_value = safe_get(row_dict, 'Statement')\n",
    "                    \n",
    "                    row = {\n",
    "                        \"No\": use_case_no,\n",
    "                        \"Name\": safe_get(row_dict, 'Name'),\n",
    "                        \"Business Domain\": \"\",  # Will be set during domain clustering\n",
    "                        \"Subdomain\": \"\",  # Will be set during subdomain clustering\n",
    "                        \"type\": safe_get(row_dict, 'type'),\n",
    "                        \"Analytics Technique\": analytics_technique,  # From LLM response\n",
    "                        \"Statement\": statement_value,\n",
    "                        \"Solution\": safe_get(row_dict, 'Solution'),\n",
    "                        \"Business Value\": safe_get(row_dict, 'Business Value'),\n",
    "                        \"Beneficiary\": safe_get(row_dict, 'Beneficiary'),\n",
    "                        \"Sponsor\": safe_get(row_dict, 'Sponsor'),\n",
    "                        \"Tables Involved\": safe_get(row_dict, 'Tables Involved'),\n",
    "                        \"Technical Design\": safe_get(row_dict, 'Technical Design'),\n",
    "                        \"SQL\": \"\",  # Will be generated in parallel later\n",
    "                        # Scoring columns (only for Excel)\n",
    "                        \"Strategic Alignment\": strategic_alignment,\n",
    "                        \"Return on Investment\": return_on_investment,\n",
    "                        \"Reusability\": reusability,\n",
    "                        \"Time to Value\": time_to_value,\n",
    "                        \"Data Availability\": data_availability,\n",
    "                        \"Data Accessibility\": data_accessibility,\n",
    "                        \"Architecture Fitness\": architecture_fitness,\n",
    "                        \"Team Skills\": team_skills,\n",
    "                        \"Domain Knowledge\": domain_knowledge,\n",
    "                        \"People Allocation\": people_allocation,\n",
    "                        \"Budget Allocation\": budget_allocation,\n",
    "                        \"Time to Production\": time_to_production,\n",
    "                        # Calculated fields\n",
    "                        \"Value\": round(value_score, 2),\n",
    "                        \"Feasibility\": round(feasibility_score, 2),\n",
    "                        \"Priority Score\": round(priority_score, 2),\n",
    "                        \"Priority\": priority_label\n",
    "                    }\n",
    "                    \n",
    "                    # Validate row has minimum required fields\n",
    "                    if not row['Name'] or not statement_value:\n",
    "                        self.logger.warning(f\"{log_prefix} Skipping row #{use_case_no}: Missing required fields (Name or Statement)\")\n",
    "                        continue\n",
    "                    \n",
    "                    self.logger.debug(f\"{log_prefix} Parsed Scenario #{row['No']}: {row['Name']} [Analytics Technique: {analytics_technique}]\")\n",
    "                    \n",
    "                    parsed_rows.append(row)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # Log error with sanitized row data (limit length to avoid huge logs)\n",
    "                    try:\n",
    "                        row_summary = {k: str(v)[:100] for k, v in row_dict.items()} if isinstance(row_dict, dict) else str(row_dict)[:200]\n",
    "                        self.logger.error(f\"{log_prefix} Error processing CSV row: {e}. Row summary: {row_summary}\")\n",
    "                    except:\n",
    "                        self.logger.error(f\"{log_prefix} Error processing CSV row: {e}. Could not serialize row data.\")\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"{log_prefix} Failed to parse LLM CSV response: {e}\")\n",
    "            # Show snippet for debugging\n",
    "            snippet = llm_response[:500] if llm_response else \"Empty response\"\n",
    "            self.logger.error(f\"{log_prefix} Response snippet: {snippet}\")\n",
    "            return []\n",
    "        \n",
    "        # NOTE: Post-processing for naming conventions removed since AI Function field no longer exists\n",
    "        # The LLM will now innovate and choose functions during SQL generation\n",
    "        \n",
    "        self.logger.info(f\"{log_prefix} Robust parsing complete. Found {len(parsed_rows)} rows.\")\n",
    "        return parsed_rows\n",
    "\n",
    "\n",
    "    def _retry_missing_table_coverage(self, use_cases: list, all_columns: list, unstructured_docs_markdown: str, strategic_goals: list = None, include_business_catchall: bool = False) -> list:\n",
    "        \"\"\"\n",
    "        Retry use case generation for tables that have no use cases.\n",
    "        Each table can be retried up to 2 times maximum.\n",
    "        \n",
    "        Args:\n",
    "            use_cases: List of existing use case dictionaries\n",
    "            all_columns: List of all column details (catalog, schema, table, column, type, comment)\n",
    "            unstructured_docs_markdown: Markdown for unstructured documents\n",
    "            strategic_goals: List of strategic goals\n",
    "            include_business_catchall: If True, also include BUSINESS tables that were never involved in any use cases (catch-all mode)\n",
    "            \n",
    "        Returns:\n",
    "            List of newly generated use cases for missing tables\n",
    "        \"\"\"\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        # Extract all tables from column details\n",
    "        all_tables = set()\n",
    "        table_columns = defaultdict(list)\n",
    "        for col_tuple in all_columns:\n",
    "            catalog, schema, table, column, col_type, comment = col_tuple\n",
    "            fq_table = f\"{catalog}.{schema}.{table}\"\n",
    "            all_tables.add(fq_table)\n",
    "            table_columns[fq_table].append(col_tuple)\n",
    "        \n",
    "        # Extract tables that have use cases (INCLUDING those with empty tables field)\n",
    "        tables_with_use_cases = set()\n",
    "        for uc in use_cases:\n",
    "            tables_str = uc.get('Tables Involved', '')\n",
    "            if tables_str and not tables_str.startswith('/Volumes'):\n",
    "                for table in tables_str.split(','):\n",
    "                    table = table.strip().strip('`')\n",
    "                    if table:\n",
    "                        tables_with_use_cases.add(table)\n",
    "        \n",
    "        # Find tables without use cases\n",
    "        missing_tables = all_tables - tables_with_use_cases\n",
    "        \n",
    "        # === CATCH-ALL: Include BUSINESS tables that were never involved in any use cases ===\n",
    "        if include_business_catchall and hasattr(self, 'business_scores'):\n",
    "            self.logger.info(\"\uD83D\uDD0D CATCH-ALL MODE: Checking for BUSINESS tables that were never involved in use cases...\")\n",
    "            \n",
    "            # Get all BUSINESS tables that were classified\n",
    "            all_business_tables = {fqtn for fqtn, score in self.business_scores.items() if score > 0}\n",
    "            \n",
    "            # Find BUSINESS tables that were never involved in ANY use case (even those with empty tables)\n",
    "            unused_business_tables = all_business_tables - tables_with_use_cases\n",
    "            \n",
    "            # Filter to only include tables that are in all_columns (have column details available)\n",
    "            unused_business_tables = unused_business_tables.intersection(all_tables)\n",
    "            \n",
    "            if unused_business_tables:\n",
    "                self.logger.warning(f\"⚠️ Found {len(unused_business_tables)} BUSINESS tables that were never involved in any use cases\")\n",
    "                \n",
    "                # Add them to missing_tables for retry\n",
    "                missing_tables = missing_tables.union(unused_business_tables)\n",
    "                \n",
    "                # Show sample\n",
    "                unused_sample = sorted(list(unused_business_tables))[:10]\n",
    "                self.logger.info(f\"\uD83D\uDCCB Sample unused BUSINESS tables: {', '.join(unused_sample)}{'...' if len(unused_business_tables) > 10 else ''}\")\n",
    "            else:\n",
    "                self.logger.info(\"✅ All BUSINESS tables have been involved in use cases\")\n",
    "        \n",
    "        if not missing_tables:\n",
    "            self.logger.info(\"✅ All tables have at least one use case - no retry needed\")\n",
    "            return []\n",
    "        \n",
    "        coverage_percentage = ((len(all_tables) - len(missing_tables)) / len(all_tables)) * 100 if all_tables else 0\n",
    "        self.logger.warning(f\"⚠️ Found {len(missing_tables)} tables without use cases (out of {len(all_tables)} total tables - {coverage_percentage:.1f}% coverage)\")\n",
    "        \n",
    "        # Show sample of missing tables\n",
    "        missing_sample = sorted(list(missing_tables))[:10]\n",
    "        self.logger.info(f\"\uD83D\uDCCB Sample missing tables: {', '.join(missing_sample)}{'...' if len(missing_tables) > 10 else ''}\")\n",
    "        \n",
    "        # Provide actionable insights\n",
    "        if len(missing_tables) > len(all_tables) * 0.5:\n",
    "            self.logger.warning(f\"⚠️ More than 50% of tables lack use cases. Consider:\")\n",
    "            self.logger.warning(f\"   - Checking if LLM is generating use cases for all tables\")\n",
    "            self.logger.warning(f\"   - Verifying table names match between schema and use case generation\")\n",
    "            self.logger.warning(f\"   - Reviewing business vs technical table filtering\")\n",
    "        \n",
    "        # Track retry attempts per table (max 2 attempts)\n",
    "        if not hasattr(self, '_table_retry_counts'):\n",
    "            self._table_retry_counts = defaultdict(int)\n",
    "        \n",
    "        # Filter tables that haven't exceeded retry limit\n",
    "        tables_to_retry = []\n",
    "        for table in missing_tables:\n",
    "            if self._table_retry_counts[table] < 2:\n",
    "                tables_to_retry.append(table)\n",
    "                self._table_retry_counts[table] += 1\n",
    "            else:\n",
    "                self.logger.warning(f\"⚠️ Table {table} has been retried 2 times already - skipping\")\n",
    "        \n",
    "        if not tables_to_retry:\n",
    "            self.logger.info(\"No tables eligible for retry (all have reached 2 attempts)\")\n",
    "            return []\n",
    "        \n",
    "        self.logger.info(f\"\uD83D\uDD04 Retrying use case generation for {len(tables_to_retry)} tables...\")\n",
    "        \n",
    "        # Group tables into batches (max 50 tables per batch to avoid context overflow)\n",
    "        max_tables_per_batch = 50\n",
    "        retry_batches = []\n",
    "        for i in range(0, len(tables_to_retry), max_tables_per_batch):\n",
    "            batch_tables = tables_to_retry[i:i+max_tables_per_batch]\n",
    "            batch_columns = []\n",
    "            for table in batch_tables:\n",
    "                batch_columns.extend(table_columns[table])\n",
    "            retry_batches.append((batch_tables, batch_columns))\n",
    "        \n",
    "        self.logger.info(f\"\uD83D\uDCE6 Created {len(retry_batches)} retry batch(es) for {len(tables_to_retry)} tables\")\n",
    "        \n",
    "        # Process retry batches IN PARALLEL using centralized ParallelExecutor\n",
    "        all_retry_use_cases = []\n",
    "        \n",
    "        # ADAPTIVE PARALLELISM: Calculate based on retry batches and columns\n",
    "        total_retry_columns = sum(len(cols) for _, cols in retry_batches)\n",
    "        \n",
    "        retry_parallelism, reason = calculate_adaptive_parallelism(\n",
    "            \"use_case_generation\", self.max_parallelism,\n",
    "            num_items=len(retry_batches),\n",
    "            total_columns=total_retry_columns,\n",
    "            avg_prompt_chars=total_retry_columns * 100,\n",
    "            is_llm_operation=True, logger=self.logger\n",
    "        )\n",
    "        log_adaptive_parallelism_decision(\"use_case_generation\", retry_parallelism, self.max_parallelism, reason)\n",
    "        \n",
    "        self.logger.info(f\"\uD83D\uDD04 Processing {len(retry_batches)} retry batch(es) in parallel...\")\n",
    "        \n",
    "        # Prepare tasks for parallel execution\n",
    "        tasks = []\n",
    "        for batch_idx, (batch_tables, batch_columns) in enumerate(retry_batches, 1):\n",
    "            task = (\n",
    "                self._process_batch_with_retry,\n",
    "                (batch_columns, f\"RETRY_{batch_idx}\", unstructured_docs_markdown, strategic_goals, 2)\n",
    "            )\n",
    "            tasks.append(task)\n",
    "            self.logger.info(f\"✓ Prepared retry batch {batch_idx}/{len(retry_batches)} ({len(batch_tables)} tables)\")\n",
    "        \n",
    "        # Execute in parallel with centralized utility\n",
    "        results = ParallelExecutor.execute_parallel(\n",
    "            tasks=tasks,\n",
    "            max_workers=retry_parallelism,\n",
    "            task_name=\"Retry Batch\",\n",
    "            logger=self.logger,\n",
    "            thread_name_prefix=\"RetryBatch\",\n",
    "            return_exceptions=True\n",
    "        )\n",
    "        \n",
    "        # Collect successful results\n",
    "        for batch_idx, result in enumerate(results, 1):\n",
    "            if isinstance(result, Exception):\n",
    "                self.logger.error(f\"❌ Retry batch {batch_idx} failed: {result}\")\n",
    "                continue\n",
    "            if result:\n",
    "                self.logger.info(f\"✅ Retry batch {batch_idx}: Generated {len(result)} use cases\")\n",
    "                all_retry_use_cases.extend(result)\n",
    "            else:\n",
    "                self.logger.warning(f\"⚠️ Retry batch {batch_idx}: No use cases generated\")\n",
    "        \n",
    "        if all_retry_use_cases:\n",
    "            self.logger.info(f\"✅ Retry complete: Generated {len(all_retry_use_cases)} additional use cases\")\n",
    "        else:\n",
    "            self.logger.warning(\"⚠️ Retry complete: No additional use cases generated\")\n",
    "        \n",
    "        return all_retry_use_cases\n",
    "\n",
    "    def _collect_pending_results(self, current_results: list) -> list:\n",
    "        \"\"\"\n",
    "        Collect results from current batch plus any pending sub-batch results.\n",
    "        \n",
    "        Args:\n",
    "            current_results: Results from current batch\n",
    "            \n",
    "        Returns:\n",
    "            Combined list of current + pending results\n",
    "        \"\"\"\n",
    "        if hasattr(self, '_pending_sub_batch_results') and self._pending_sub_batch_results:\n",
    "            all_results = current_results + self._pending_sub_batch_results\n",
    "            # Clear pending results after collecting\n",
    "            self._pending_sub_batch_results = []\n",
    "            return all_results\n",
    "        return current_results\n",
    "    \n",
    "    def _process_batch_with_retry(self, column_details: list, batch_num, unstructured_docs_markdown: str, strategic_goals: list = None, business_context: str = \"\", business_priorities: str = \"\", strategic_initiative: str = \"\", value_chain: str = \"\", revenue_model: str = \"\", max_attempts: int = 3, previous_use_cases_feedback: str = \"\") -> list:\n",
    "        \"\"\"\n",
    "        Process a batch of column details to generate use cases with retry logic.\n",
    "        Automatically splits context if input is too long for the model.\n",
    "        \n",
    "        STRATEGY: When tables don't fit in context:\n",
    "        1. Split tables across multiple sub-batches (NEVER drop business tables)\n",
    "        2. Process ALL sub-batches recursively\n",
    "        3. Track which columns are kept from each table (saved to disk, not memory)\n",
    "        4. Column tracking is loaded from disk during SQL generation\n",
    "        \n",
    "        Args:\n",
    "            column_details: List of column tuples (catalog, schema, table, column, type, comment)\n",
    "            batch_num: Batch number for logging and prefixing (can be int or str)\n",
    "            unstructured_docs_markdown: Unstructured documents markdown\n",
    "            strategic_goals: List of strategic goals for the business (used for Strategic Alignment scoring)\n",
    "            max_attempts: Maximum number of attempts (default 3)\n",
    "            \n",
    "        Returns:\n",
    "            List of use case dictionaries (includes results from sub-batches)\n",
    "        \"\"\"\n",
    "        log_prefix = f\"[Batch {batch_num}]\"\n",
    "        \n",
    "        # === NEW: Register columns and tables for Bitmap ID generation ===\n",
    "        with self.registry_lock:\n",
    "            for col_tuple in column_details:\n",
    "                # col_tuple: (catalog, schema, table, column, type, comment)\n",
    "                fqn = f\"{col_tuple[0]}.{col_tuple[1]}.{col_tuple[2]}.{col_tuple[3]}\"\n",
    "                table_fqn = f\"{col_tuple[0]}.{col_tuple[1]}.{col_tuple[2]}\"\n",
    "                \n",
    "                # Register table if not already registered\n",
    "                if table_fqn not in self.table_id_map:\n",
    "                    table_id = str(self.next_table_id)\n",
    "                    self.next_table_id += 1\n",
    "                    self.table_id_map[table_fqn] = table_id\n",
    "                    self.id_table_map[table_id] = table_fqn\n",
    "                \n",
    "                if fqn not in self.column_id_map:\n",
    "                    col_id = str(self.next_column_id)\n",
    "                    self.next_column_id += 1\n",
    "                    self.column_id_map[fqn] = col_id\n",
    "                    \n",
    "                    # Create description (Type + Comment)\n",
    "                    desc = f\"{col_tuple[4]}\"\n",
    "                    if col_tuple[5]:\n",
    "                        desc += f\" - {col_tuple[5]}\"\n",
    "                    \n",
    "                    self.id_column_map[col_id] = {\n",
    "                        \"fqn\": fqn,\n",
    "                        \"description\": desc\n",
    "                    }\n",
    "        \n",
    "        current_column_details = column_details\n",
    "\n",
    "        prompt_template = self.ai_agent.prompt_templates.get(\"BASE_USE_CASE_GEN_PROMPT\", \"\")\n",
    "        safe_limit = get_safe_context_limit(language=\"English\", buffer_percent=0.9, prompt_name=\"BASE_USE_CASE_GEN_PROMPT\")\n",
    "        if strategic_goals and len(strategic_goals) > 0:\n",
    "            strategic_goals_text = \"\\n\".join([f\"- {goal}\" for goal in strategic_goals[:10]])\n",
    "        else:\n",
    "            strategic_goals_text = \"- Maximize operational efficiency\\n- Improve customer satisfaction\\n- Reduce operational costs\\n- Drive revenue growth\\n- Ensure compliance and risk management\"\n",
    "        if business_priorities and len(business_priorities) > 0:\n",
    "            business_priorities_text = \"\\n\".join([f\"- {priority}\" for priority in business_priorities[:10]])\n",
    "        else:\n",
    "            business_priorities_text = \"- None\"\n",
    "        if self.user_strategic_goals:\n",
    "            goals_text = \"\\n\".join([f\"- {goal}\" for goal in self.user_strategic_goals])\n",
    "            additional_context_section = f\"\"\"**STRATEGIC GOALS (HIGHEST PRIORITY)**:\n",
    "\n",
    "The user provided Strategic Goals that MUST be followed during generation.\n",
    "\n",
    "**STRATEGIC GOALS:**\n",
    "{goals_text}\n",
    "\n",
    "**REQUIREMENTS**:\n",
    "- Generate ONLY use cases that align with these Strategic Goals.\n",
    "- Generate EVERY possible use case that aligns with these Strategic Goals. Do not omit any valid use case.\n",
    "- Do NOT cap the number of use cases; completeness is mandatory.\n",
    "- Use semantic understanding of the goals; do NOT apply rigid keyword rules.\n",
    "- Do NOT generate use cases outside these goals.\"\"\"\n",
    "        else:\n",
    "            additional_context_section = \"*(No Strategic Goals provided by user - proceed with standard business analysis)*\"\n",
    "        if self.user_business_domains:\n",
    "            domains_list = \", \".join([f'\"{domain}\"' for domain in self.user_business_domains])\n",
    "            focus_areas_instruction = f\"\"\"  - **\uD83D\uDEA8 CRITICAL - USER-SPECIFIED BUSINESS DOMAINS \uD83D\uDEA8**: You MUST assign use cases ONLY to the following business domains: {domains_list}. \n",
    "   * These are the ONLY valid Business Domain values - DO NOT invent new domains.\n",
    "   * ALL use cases MUST be categorized into one of these exact domains.\n",
    "   * DO NOT create any domain that is not in this list.\n",
    "   * DO NOT modify, abbreviate, or expand these domain names - use them EXACTLY as provided.\n",
    "   * The Business Domain field MUST exactly match one of these domains.\"\"\"\n",
    "        else:\n",
    "            focus_areas_instruction = \"\"\n",
    "        ai_functions_summary = generate_ai_functions_doc(\"summary\")\n",
    "        ai_functions_detailed = generate_ai_functions_doc(\"detailed\")\n",
    "        statistical_functions_detailed = generate_statistical_functions_doc(\"detailed\")\n",
    "        base_prompt_size = len(prompt_template) + len(unstructured_docs_markdown) + len(business_context) + len(business_priorities_text) + len(strategic_initiative) + len(value_chain) + len(revenue_model) + len(strategic_goals_text) + len(additional_context_section) + len(focus_areas_instruction) + len(ai_functions_summary) + len(ai_functions_detailed) + len(statistical_functions_detailed) + len(previous_use_cases_feedback) + 1000\n",
    "        \n",
    "        self.logger.info(f\"{log_prefix} Starting batch processing with {len(column_details)} columns from {len(set([c[2] for c in column_details]))} tables\")\n",
    "        tables_in_call = sorted({f\"{c[0]}.{c[1]}.{c[2]}\" for c in column_details})\n",
    "        tables_in_call_str = \", \".join(tables_in_call)\n",
    "        self.logger.info(f\"{log_prefix} Tables in call ({len(tables_in_call)}): {tables_in_call_str}\")\n",
    "        log_print(f\"{log_prefix} Tables in call ({len(tables_in_call)}): {tables_in_call_str}\")\n",
    "        \n",
    "        for attempt in range(1, max_attempts + 1):\n",
    "            try:\n",
    "                if attempt > 1:\n",
    "                    self.logger.info(f\"{log_prefix} Retry attempt {attempt}/{max_attempts}...\")\n",
    "\n",
    "                estimated_schema_size = self._estimate_schema_markdown_size(current_column_details)\n",
    "                estimated_prompt_size = base_prompt_size + estimated_schema_size\n",
    "                if estimated_prompt_size > safe_limit:\n",
    "                    raise InputTooLongError(\n",
    "                        f\"Proactive split: Input length {estimated_prompt_size:,} characters exceeds \"\n",
    "                        f\"safe limit of {safe_limit:,} (with 10% buffer)\"\n",
    "                    )\n",
    "\n",
    "                self.logger.debug(f\"{log_prefix} Formatting schema for prompt...\")\n",
    "                schema_markdown = self._format_schema_for_prompt(current_column_details)\n",
    "                if not schema_markdown:\n",
    "                    self.logger.warning(f\"{log_prefix} Produced no schema markdown. Skipping.\")\n",
    "                    return []\n",
    "                \n",
    "                fk_relationships_text = \"None\"\n",
    "                try:\n",
    "                    if self.data_loader and getattr(self.data_loader, \"foreign_key_graph\", None):\n",
    "                        batch_tables = {(c[0], c[1], c[2]) for c in current_column_details}\n",
    "                        fk_relations = self.data_loader.get_foreign_key_relations(batch_tables)\n",
    "                        if fk_relations:\n",
    "                            rel_lines = []\n",
    "                            for rel in fk_relations:\n",
    "                                src = f\"{rel[0]}.{rel[1]}.{rel[2]}.{rel[3]}\"\n",
    "                                ref_catalog = rel[4] or rel[0]\n",
    "                                ref_schema = rel[5] or rel[1]\n",
    "                                tgt = f\"{ref_catalog}.{ref_schema}.{rel[6]}.{rel[7]}\"\n",
    "                                rel_lines.append(f\"{src} -> {tgt}\")\n",
    "                            if rel_lines:\n",
    "                                fk_relationships_text = \"\\n\".join(sorted(set(rel_lines)))\n",
    "                except Exception as fk_err:\n",
    "                    self.logger.debug(f\"{log_prefix} Failed to gather FK relationships: {str(fk_err)[:100]}\")\n",
    "                \n",
    "                prompt_vars = {\n",
    "                    \"schema_markdown\": schema_markdown,\n",
    "                    \"foreign_key_relationships\": fk_relationships_text,\n",
    "                    \"unstructured_documents_markdown\": unstructured_docs_markdown,\n",
    "                    \"business_context\": business_context,\n",
    "                    \"business_priorities\": business_priorities_text,\n",
    "                    \"strategic_initiative\": strategic_initiative,\n",
    "                    \"value_chain\": value_chain,\n",
    "                    \"revenue_model\": revenue_model,\n",
    "                    \"strategic_goals\": strategic_goals_text,\n",
    "                    \"additional_context_section\": additional_context_section,\n",
    "                    \"ai_functions_summary\": ai_functions_summary,\n",
    "                    \"ai_functions_detailed\": ai_functions_detailed,\n",
    "                    \"statistical_functions_detailed\": statistical_functions_detailed,\n",
    "                    \"focus_areas_instruction\": focus_areas_instruction,\n",
    "                    \"previous_use_cases_feedback\": previous_use_cases_feedback\n",
    "                }\n",
    "                \n",
    "                # PROACTIVE CHECK: Estimate prompt size and split BEFORE attempting LLM call\n",
    "                # This saves time by not waiting for LLM failures\n",
    "                try:\n",
    "                    estimated_prompt_size = base_prompt_size + len(schema_markdown) + len(fk_relationships_text)\n",
    "                    \n",
    "                    if estimated_prompt_size > safe_limit:\n",
    "                        # Prompt exceeds safe limit - proactively split WITHOUT attempting LLM call\n",
    "                        self.logger.warning(\n",
    "                            f\"{log_prefix} PROACTIVE SPLIT: Prompt size ({estimated_prompt_size:,} chars) exceeds \"\n",
    "                            f\"safe limit ({safe_limit:,} chars with 10% buffer). Splitting batch without attempting LLM call.\"\n",
    "                        )\n",
    "                        \n",
    "                        # Raise InputTooLongError to trigger the split logic below\n",
    "                        raise InputTooLongError(\n",
    "                            f\"Proactive split: Input length {estimated_prompt_size:,} characters exceeds \"\n",
    "                            f\"safe limit of {safe_limit:,} (with 10% buffer)\"\n",
    "                        )\n",
    "                    else:\n",
    "                        # Safe to proceed - log if we're approaching the limit (>80% of safe limit)\n",
    "                        if estimated_prompt_size > (safe_limit * 0.8):\n",
    "                            self.logger.info(\n",
    "                                f\"{log_prefix} Prompt size: {estimated_prompt_size:,} chars \"\n",
    "                                f\"({(estimated_prompt_size/safe_limit)*100:.1f}% of safe limit)\"\n",
    "                            )\n",
    "                \n",
    "                except InputTooLongError:\n",
    "                    # Re-raise to trigger split logic\n",
    "                    raise\n",
    "                except Exception as e:\n",
    "                    # Any other error in estimation - log and continue to actual LLM call\n",
    "                    self.logger.debug(f\"{log_prefix} Proactive size check failed: {e}\")\n",
    "                \n",
    "                # === PARALLEL EXECUTION: Send batch to BOTH AI and STATS prompts ===\n",
    "                self.logger.info(f\"⏳ {log_prefix} Sending batch to BOTH AI-focused and STATS-focused prompts in parallel...\")\n",
    "                \n",
    "                from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "                \n",
    "                def call_prompt(prompt_name, step_suffix):\n",
    "                    \"\"\"Helper function to call a specific prompt.\"\"\"\n",
    "                    self.logger.info(f\"⏳ {log_prefix} [{prompt_name}] Waiting for LLM response (may take 3-5 min)...\")\n",
    "                    response = self.ai_agent.run_worker(\n",
    "                        step_name=f\"Batch_{batch_num}_{step_suffix}\", \n",
    "                        worker_prompt_path=prompt_name,\n",
    "                        prompt_vars=prompt_vars,\n",
    "                        response_schema=None\n",
    "                    )\n",
    "                    self.logger.info(f\"✅ {log_prefix} [{prompt_name}] Received LLM response\")\n",
    "                    return prompt_name, response\n",
    "                \n",
    "                # Execute both prompts in parallel\n",
    "                with ThreadPoolExecutor(max_workers=2, thread_name_prefix=\"PromptCall\") as executor:\n",
    "                    futures = {\n",
    "                        executor.submit(call_prompt, \"AI_USE_CASE_GEN_PROMPT\", \"AI\"): \"AI\",\n",
    "                        executor.submit(call_prompt, \"STATS_USE_CASE_GEN_PROMPT\", \"STATS\"): \"STATS\"\n",
    "                    }\n",
    "                    \n",
    "                    ai_response_raw = None\n",
    "                    stats_response_raw = None\n",
    "                    \n",
    "                    for future in as_completed(futures):\n",
    "                        try:\n",
    "                            prompt_name, response = future.result()\n",
    "                            if prompt_name == \"AI_USE_CASE_GEN_PROMPT\":\n",
    "                                ai_response_raw = response\n",
    "                            elif prompt_name == \"STATS_USE_CASE_GEN_PROMPT\":\n",
    "                                stats_response_raw = response\n",
    "                        except Exception as e:\n",
    "                            prompt_type = futures[future]\n",
    "                            self.logger.error(f\"❌ {log_prefix} [{prompt_type}] Prompt call failed: {e}\")\n",
    "                            raise\n",
    "                \n",
    "                # Parse both responses\n",
    "                self.logger.info(f\"✅ {log_prefix} Received both responses, parsing CSVs...\")\n",
    "                \n",
    "                # Use clean_csv_response (NOT clean_json_response) to avoid extracting JSON from CSV\n",
    "                ai_response_clean = clean_csv_response(ai_response_raw) if ai_response_raw else \"\"\n",
    "                stats_response_clean = clean_csv_response(stats_response_raw) if stats_response_raw else \"\"\n",
    "                \n",
    "                ai_parsed_rows = self._parse_llm_csv_response(ai_response_clean, f\"{log_prefix}[AI]\") if ai_response_clean else []\n",
    "                stats_parsed_rows = self._parse_llm_csv_response(stats_response_clean, f\"{log_prefix}[STATS]\") if stats_response_clean else []\n",
    "                \n",
    "                # Mark source for each use case (AI vs STATS)\n",
    "                for row in ai_parsed_rows:\n",
    "                    row['_source'] = 'AI'\n",
    "                for row in stats_parsed_rows:\n",
    "                    row['_source'] = 'STATS'\n",
    "                \n",
    "                # Merge results from both prompts\n",
    "                parsed_rows = ai_parsed_rows + stats_parsed_rows\n",
    "                self.logger.info(f\"✅ {log_prefix} Merged results: {len(ai_parsed_rows)} AI use cases + {len(stats_parsed_rows)} STATS use cases = {len(parsed_rows)} total\")\n",
    "                \n",
    "                if not parsed_rows:\n",
    "                    raise Exception(\"LLM returned no use cases\")\n",
    "                \n",
    "                # CRITICAL: Validate that tables referenced in use cases actually exist in schema\n",
    "                # This catches LLM hallucinations where it invents non-existent table names\n",
    "                validation_schema = getattr(self, \"_business_column_details_global\", current_column_details)\n",
    "                is_tables_valid, hallucinated_use_cases, valid_use_cases = self._validate_use_case_tables(\n",
    "                    parsed_rows, validation_schema, log_prefix\n",
    "                )\n",
    "                \n",
    "                if not is_tables_valid:\n",
    "                    hallucinated_count = len(hallucinated_use_cases)\n",
    "                    hallucination_rate = hallucinated_count / len(parsed_rows) * 100\n",
    "                    valid_count = len(valid_use_cases)\n",
    "                    \n",
    "                    if valid_count == 0:\n",
    "                        self.logger.warning(f\"{log_prefix} ⚠️ Table hallucination detected: {hallucinated_count}/{len(parsed_rows)} use cases ({hallucination_rate:.1f}%)\")\n",
    "                        for i, uc in enumerate(hallucinated_use_cases[:3]):\n",
    "                            self.logger.warning(f\"{log_prefix}    Example {i+1}: {uc.get('No')}: {uc.get('Name')} - {uc.get('hallucination_reason')}\")\n",
    "                        if attempt < max_attempts:\n",
    "                            self.logger.warning(f\"{log_prefix}    Retrying batch (attempt {attempt + 1}/{max_attempts}) because no valid use cases were returned\")\n",
    "                            continue\n",
    "                        self.logger.error(f\"{log_prefix} ❌ No valid use cases after {max_attempts} attempts due to hallucinated tables\")\n",
    "                        return self._collect_pending_results([])\n",
    "                    \n",
    "                    self.logger.warning(f\"{log_prefix} ⚠️ Table hallucination detected: {hallucinated_count}/{len(parsed_rows)} use cases ({hallucination_rate:.1f}%). Dropping hallucinated use cases and continuing with {valid_count} valid use cases.\")\n",
    "                    for i, uc in enumerate(hallucinated_use_cases[:3]):\n",
    "                        self.logger.warning(f\"{log_prefix}    Example {i+1}: {uc.get('No')}: {uc.get('Name')} - {uc.get('hallucination_reason')}\")\n",
    "                    parsed_rows = valid_use_cases\n",
    "                \n",
    "                # Re-number use cases with batch prefix AND apply SQL validation\n",
    "                # Handle both int and string batch_num (for retry batches)\n",
    "                if isinstance(batch_num, int):\n",
    "                    batch_prefix = f\"{batch_num:02d}\"  # Changed from B{batch_num:03d} to just 2-digit number\n",
    "                else:\n",
    "                    batch_prefix = str(batch_num)  # Already formatted (e.g., \"RETRY_1\")\n",
    "                \n",
    "                for row in parsed_rows:\n",
    "                    try:\n",
    "                        original_id = row['No']\n",
    "                        use_case_num = original_id.split('-')[-1]\n",
    "                        # Use F for AI-sourced, S for STATS-sourced\n",
    "                        source_prefix = 'F' if row.get('_source') == 'AI' else 'S'\n",
    "                        new_id = f\"AI-{source_prefix}{batch_prefix}-U{use_case_num}\"\n",
    "                        row['No'] = new_id\n",
    "                        row['batch'] = batch_num\n",
    "                        if 'SQL' in row and row['SQL']:\n",
    "                            # Update use case ID in SQL comment\n",
    "                            if original_id in row['SQL']:\n",
    "                                row['SQL'] = row['SQL'].replace(f\"-- Use Case ID: {original_id}\", f\"-- Use Case ID: {new_id}\")\n",
    "                    except Exception as e:\n",
    "                        self.logger.warning(f\"{log_prefix} Failed to re-number row: {e}\")\n",
    "                        row['batch'] = batch_num\n",
    "                \n",
    "                self.logger.debug(f\"{log_prefix} Successfully processed {len(parsed_rows)} use cases on attempt {attempt}\")\n",
    "                \n",
    "                # Print top 5 use cases from AI and top 5 from STATS (total 10)\n",
    "                ai_cases = [uc for uc in parsed_rows if uc.get('_source') == 'AI']\n",
    "                stats_cases = [uc for uc in parsed_rows if uc.get('_source') == 'STATS']\n",
    "                \n",
    "                log_print(f\"\\n{'='*80}\")\n",
    "                log_print(f\"\uD83D\uDCCA TOP USE CASES FROM {log_prefix} (for early quality review):\")\n",
    "                log_print(f\"{'='*80}\\n\")\n",
    "                \n",
    "                if ai_cases:\n",
    "                    log_print(f\"\uD83E\uDD16 Top {min(5, len(ai_cases))} AI-focused use cases:\")\n",
    "                    for use_case in ai_cases[:5]:\n",
    "                        log_print(f\"   {use_case.get('No', 'N/A')}: {use_case.get('Name', 'N/A')}\")\n",
    "                    print()\n",
    "                \n",
    "                if stats_cases:\n",
    "                    log_print(f\"\uD83D\uDCCA Top {min(5, len(stats_cases))} STATS-focused use cases:\")\n",
    "                    for use_case in stats_cases[:5]:\n",
    "                        log_print(f\"   {use_case.get('No', 'N/A')}: {use_case.get('Name', 'N/A')}\")\n",
    "                    print()\n",
    "                \n",
    "                log_print(f\"{'='*80}\\n\")\n",
    "                \n",
    "                # Validate subdomain rules (silent check - accept response regardless)\n",
    "                # The domain fixer will fix any issues later\n",
    "                is_valid, violations, corrected_rows = self._validate_subdomain_rules(parsed_rows, log_prefix)\n",
    "                # Always accept the response - domain fixer will handle issues\n",
    "                # Collect any pending sub-batch results\n",
    "                return self._collect_pending_results(parsed_rows)\n",
    "                \n",
    "            except InputTooLongError as e:\n",
    "                # Handle \"input too long\" by intelligently splitting the batch\n",
    "                # NEVER DROP BUSINESS TABLES - keep splitting until they fit\n",
    "                \n",
    "                # Group columns by table\n",
    "                table_to_columns = {}\n",
    "                for col in current_column_details:\n",
    "                    table_key = (col[0], col[1], col[2])  # (catalog, schema, table)\n",
    "                    if table_key not in table_to_columns:\n",
    "                        table_to_columns[table_key] = []\n",
    "                    table_to_columns[table_key].append(col)\n",
    "                \n",
    "                num_tables = len(table_to_columns)\n",
    "                \n",
    "                # Get business scores to check if tables are marked as business\n",
    "                business_scores = getattr(self, 'business_scores', {})\n",
    "                \n",
    "                def is_business_table(table_key):\n",
    "                    \"\"\"Check if a table is marked as business (score > 0).\"\"\"\n",
    "                    fqtn = f\"{table_key[0]}.{table_key[1]}.{table_key[2]}\"\n",
    "                    return business_scores.get(fqtn, 0) > 0\n",
    "                \n",
    "                if num_tables > 1:\n",
    "                    tables_list = list(table_to_columns.keys())\n",
    "                    reference_tables_set = {k for k, v in getattr(self, \"data_category_map\", {}).items() if v == \"REFERENCE\"}\n",
    "                    filtered_tables = []\n",
    "                    for table_key in tables_list:\n",
    "                        fqtn = f\"{table_key[0]}.{table_key[1]}.{table_key[2]}\"\n",
    "                        if reference_tables_set and fqtn in reference_tables_set:\n",
    "                            continue\n",
    "                        filtered_tables.append(table_key)\n",
    "                    if not filtered_tables:\n",
    "                        self.logger.warning(f\"{log_prefix} Input too long ({str(e)}). Only reference tables present; skipping use case generation.\")\n",
    "                        return self._collect_pending_results([])\n",
    "                    self.logger.warning(f\"{log_prefix} Input too long ({str(e)}). Falling back to single-table calls for {num_tables} tables.\")\n",
    "                    self.processing_honesty['total_batch_splits'] += 1\n",
    "                    split_type = \"Proactive\" if \"Proactive split\" in str(e) else \"Reactive\"\n",
    "                    split_info = {\n",
    "                        'batch': batch_num,\n",
    "                        'original_tables': num_tables,\n",
    "                        'split_into': num_tables,\n",
    "                        'sub_batch_1_tables': 1,\n",
    "                        'sub_batch_2_tables': 1,\n",
    "                        'reason': 'Input too long for LLM',\n",
    "                        'split_type': split_type\n",
    "                    }\n",
    "                    self.processing_honesty['batch_split_history'].append(split_info)\n",
    "                    current_column_details = table_to_columns[filtered_tables[0]]\n",
    "                    current_column_details = self._augment_columns_with_related_tables(current_column_details)\n",
    "                    if not hasattr(self, '_pending_sub_batch_results'):\n",
    "                        self._pending_sub_batch_results = []\n",
    "                    for idx, table_key in enumerate(filtered_tables[1:], start=2):\n",
    "                        single_cols = table_to_columns[table_key]\n",
    "                        single_cols = self._augment_columns_with_related_tables(single_cols)\n",
    "                        single_batch_id = f\"{batch_num}_T{idx}\"\n",
    "                        single_use_cases = self._process_batch_with_retry(\n",
    "                            single_cols,\n",
    "                            single_batch_id,\n",
    "                            unstructured_docs_markdown,\n",
    "                            strategic_goals,\n",
    "                            max_attempts\n",
    "                        )\n",
    "                        if single_use_cases:\n",
    "                            self._pending_sub_batch_results.extend(single_use_cases)\n",
    "                            self.logger.info(f\"{log_prefix} Single-table sub-batch {single_batch_id} generated {len(single_use_cases)} use cases\")\n",
    "                    continue\n",
    "                    \n",
    "                elif num_tables == 1:\n",
    "                    # Single table is too big: try dropping columns\n",
    "                    table_key = list(table_to_columns.keys())[0]\n",
    "                    table_columns = table_to_columns[table_key]\n",
    "                    table_is_business = is_business_table(table_key)\n",
    "                    fqtn = f\"{table_key[0]}.{table_key[1]}.{table_key[2]}\"\n",
    "                    \n",
    "                    if len(table_columns) > 500:\n",
    "                        keep_count = 500\n",
    "                    else:\n",
    "                        keep_count = len(table_columns) - 100\n",
    "                    if keep_count < 5:\n",
    "                        if table_is_business:\n",
    "                            keep_count = 5\n",
    "                        else:\n",
    "                            self.logger.error(f\"{log_prefix} Input too long even with minimal columns ({len(table_columns)} columns from non-business table {table_key[2]}). Dropping this table.\")\n",
    "                            return self._collect_pending_results([])\n",
    "                    current_column_details = table_columns[:keep_count]\n",
    "                    \n",
    "                    kept_columns = [col[3] for col in current_column_details]  # col[3] is column name\n",
    "                    self.storage_manager.save_column_tracking(fqtn, kept_columns)\n",
    "                    \n",
    "                    dropped_count = len(table_columns) - keep_count\n",
    "                    drop_info = {\n",
    "                        'table': fqtn,\n",
    "                        'original_columns': len(table_columns),\n",
    "                        'kept_columns': keep_count,\n",
    "                        'dropped_columns': dropped_count,\n",
    "                        'drop_percentage': (dropped_count / len(table_columns)) * 100,\n",
    "                        'is_business': table_is_business\n",
    "                    }\n",
    "                    if fqtn not in [t['table'] for t in self.processing_honesty['tables_with_columns_dropped']]:\n",
    "                        self.processing_honesty['tables_with_columns_dropped'].append(drop_info)\n",
    "                    else:\n",
    "                        # Update existing entry with new drop info\n",
    "                        for idx, existing in enumerate(self.processing_honesty['tables_with_columns_dropped']):\n",
    "                            if existing['table'] == fqtn:\n",
    "                                self.processing_honesty['tables_with_columns_dropped'][idx] = drop_info\n",
    "                                break\n",
    "                    \n",
    "                    table_type = \"BUSINESS\" if table_is_business else \"non-business\"\n",
    "                    self.logger.warning(f\"{log_prefix} Input too long ({str(e)}). Single {table_type} table {table_key[2]} is too large. Dropping columns from {len(table_columns)} to {keep_count} columns and retrying...\")\n",
    "                    self.logger.info(f\"{log_prefix} Saved column tracking for {fqtn}: {len(kept_columns)} columns ({', '.join(kept_columns[:5])}{'...' if len(kept_columns) > 5 else ''})\")\n",
    "                    \n",
    "                    continue\n",
    "                    \n",
    "                else:\n",
    "                    # No tables? This shouldn't happen\n",
    "                    self.logger.error(f\"{log_prefix} Input too long but no tables found. Cannot process.\")\n",
    "                    return self._collect_pending_results([])\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt < max_attempts:\n",
    "                    self.logger.warning(f\"{log_prefix} Attempt {attempt} failed: {e}. Retrying...\")\n",
    "                else:\n",
    "                    self.logger.error(f\"{log_prefix} All {max_attempts} attempts failed: {e}\")\n",
    "                    return self._collect_pending_results([])\n",
    "        \n",
    "        # If we exhaust all attempts without success, still return any pending results\n",
    "        return self._collect_pending_results([])\n",
    "\n",
    "    def _assemble_notebook_for_db(self, db_name: str, use_cases: list, translations: dict, db_prefix: str, filename_override: str = None, domain_summary: str = None):\n",
    "        self.logger.debug(f\"--- Assembling notebook for: {db_name} (English) ---\")\n",
    "        if not use_cases:\n",
    "            self.logger.warning(f\"No use cases provided for {db_name}. Skipping notebook creation.\")\n",
    "            return\n",
    "        t = translations\n",
    "        grouped_by_domain = defaultdict(list)\n",
    "        for uc in use_cases: grouped_by_domain[uc.get('Business Domain') or 'Other'].append(uc)\n",
    "        \n",
    "        # === Add top title cell ===\n",
    "        final_cells = []\n",
    "        title_cell_source = [\n",
    "            f\"# {t['pdf_title']}\\n\\n\",\n",
    "            f\"## For {self.business_name}: {db_name}\\n\\n\"\n",
    "        ]\n",
    "        title_cell = {\n",
    "            \"cell_type\": \"markdown\", \n",
    "            \"metadata\": {\"application/vnd.databricks.v1+cell\": {\"nuid\": str(uuid.uuid4())}}, \n",
    "            \"source\": title_cell_source\n",
    "        }\n",
    "        final_cells.append(title_cell)\n",
    "        \n",
    "        # === Add disclaimer cell ===\n",
    "        generation_timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        disclaimer_cell_source = [\n",
    "            f\"*Generated by Databricks Inspire AI on {generation_timestamp}*\\n\\n\",\n",
    "            \"**Disclaimer:** All SQL queries are examples and must be validated for syntax and safety by a qualified engineer before being used in any production environment. Databricks is not liable for any issues arising from the use of this code.\\n\\n\",\n",
    "            \"---\\n\"\n",
    "        ]\n",
    "        disclaimer_cell = {\n",
    "            \"cell_type\": \"markdown\",\n",
    "            \"metadata\": {\"application/vnd.databricks.v1+cell\": {\"nuid\": str(uuid.uuid4())}},\n",
    "            \"source\": disclaimer_cell_source\n",
    "        }\n",
    "        final_cells.append(disclaimer_cell)\n",
    "        \n",
    "        # === Add domain executive summary if available ===\n",
    "        if domain_summary:\n",
    "            # Clean HTML tags from summary for better notebook display\n",
    "            import re\n",
    "            clean_summary = re.sub(r'<[^>]+>', '', domain_summary)\n",
    "            # Format as proper sentences (split on periods, ensure proper spacing)\n",
    "            sentences = [s.strip() + '.' for s in clean_summary.split('.') if s.strip()]\n",
    "            formatted_summary = '\\n\\n'.join(sentences)\n",
    "            \n",
    "            summary_cell_source = [\n",
    "                f\"{formatted_summary}\\n\\n\",\n",
    "                \"---\\n\\n\"\n",
    "            ]\n",
    "            summary_cell = {\n",
    "                \"cell_type\": \"markdown\", \n",
    "                \"metadata\": {\"application/vnd.databricks.v1+cell\": {\"nuid\": str(uuid.uuid4())}}, \n",
    "                \"source\": summary_cell_source\n",
    "            }\n",
    "            final_cells.append(summary_cell)\n",
    "            self.logger.info(f\"Added domain overview cell for '{db_name}'\")\n",
    "        \n",
    "        # Use Cases Summaries section - split by subdomain\n",
    "        \n",
    "        # Generate summary tables with Priority column, grouped by subdomain\n",
    "        first_section = True\n",
    "        for domain, domain_use_cases in sorted(grouped_by_domain.items()):\n",
    "            self.logger.debug(f\"Assembling domain summary tables: '{domain}' with {len(domain_use_cases)} use cases.\")\n",
    "            \n",
    "            # Group use cases by subdomain within this domain\n",
    "            subdomain_groups = defaultdict(list)\n",
    "            for uc in domain_use_cases:\n",
    "                subdomain = uc.get('Subdomain', 'General')\n",
    "                subdomain_groups[subdomain].append(uc)\n",
    "            \n",
    "            # Create a table for each subdomain\n",
    "            for subdomain, subdomain_use_cases in sorted(subdomain_groups.items()):\n",
    "                self.logger.debug(f\"  - Subdomain '{subdomain}': {len(subdomain_use_cases)} use cases\")\n",
    "                \n",
    "                if first_section:\n",
    "                    # First table includes the main header\n",
    "                    header_source = [\n",
    "                        f\"## {t['summaries']}\\n\\n\",\n",
    "                        f\"### {subdomain}\\n\\n\",\n",
    "                        f\"| {t['sum_id']} | {t['sum_name']} | {t['priority']} | {t['sum_value']} |\\n\",\n",
    "                        \"|---|---|---|---|\\n\"\n",
    "                    ]\n",
    "                    first_section = False\n",
    "                else:\n",
    "                    # Subsequent tables just have subdomain header\n",
    "                    header_source = [\n",
    "                        f\"\\n### {subdomain}\\n\\n\",\n",
    "                        f\"| {t['sum_id']} | {t['sum_name']} | {t['priority']} | {t['sum_value']} |\\n\",\n",
    "                        \"|---|---|---|---|\\n\"\n",
    "                    ]\n",
    "                \n",
    "                subdomain_header_cell = {\n",
    "                    \"cell_type\": \"markdown\",\n",
    "                    \"metadata\": {\"application/vnd.databricks.v1+cell\": {\"nuid\": str(uuid.uuid4())}},\n",
    "                    \"source\": header_source\n",
    "                }\n",
    "                # Sort use cases within subdomain using natural sort (AI01, AI02, ..., AI10)\n",
    "                sorted_subdomain_use_cases = sorted(subdomain_use_cases, key=self._natural_sort_key)\n",
    "                toc_entries = [f\"| {uc['No']} | {uc['Name']} | {uc.get('Priority', 'N/A')} | {uc['Business Value']} |\\n\" for uc in sorted_subdomain_use_cases]\n",
    "                subdomain_header_cell[\"source\"].extend(toc_entries)\n",
    "                final_cells.append(subdomain_header_cell)\n",
    "        \n",
    "        # Req 6: Use translated disclaimer\n",
    "        disclaimer_text = t[\"disclaimer\"]\n",
    "        disclaimer_html = f'<div style=\"background-color:#FFF3CD; color:#664D03; border: 1px solid #FFECB5; padding:10px; border-radius:5px; margin-top:10px;\"><b>Disclaimer:</b> {disclaimer_text}</div>'\n",
    "        final_cells.extend([\n",
    "            {\"cell_type\": \"markdown\", \"metadata\": {\"application/vnd.databricks.v1+cell\": {\"nuid\": str(uuid.uuid4())}}, \"source\": [disclaimer_html]},\n",
    "            {\"cell_type\": \"markdown\", \"metadata\": {\"application/vnd.databricks.v1+cell\": {\"nuid\": str(uuid.uuid4())}}, \"source\": [f\"<hr>\\n\\n# {t['detailed_scenarios']}\\n\"]}\n",
    "        ])\n",
    "        \n",
    "        # Sort use cases by use case ID using natural sort (AI01, AI02, ..., AI10 - not AI1, AI10, AI2)\n",
    "        use_cases_sorted = sorted(use_cases, key=self._natural_sort_key)\n",
    "        self.logger.debug(f\"Sorted {len(use_cases_sorted)} use cases by ID (natural order)\")\n",
    "        \n",
    "        # Validate and repair SQL before adding to notebook\n",
    "        excluded_count = 0\n",
    "        for use_case in use_cases_sorted:\n",
    "            use_case_id = use_case.get('No', 'UNKNOWN')\n",
    "            sql_content = use_case.get('SQL', '').strip()\n",
    "            \n",
    "            # CRITICAL: Check if SQL field is empty, None, or just a priority value\n",
    "            if not sql_content or len(sql_content) < 20:\n",
    "                self.logger.error(f\"EXCLUDING use case {use_case_id}: SQL field is empty or too short (len={len(sql_content)})\")\n",
    "                excluded_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Check if SQL field contains only priority values (shouldn't happen)\n",
    "            if sql_content.upper() in ['LOW', 'MEDIUM', 'HIGH']:\n",
    "                self.logger.error(f\"EXCLUDING use case {use_case_id}: SQL field contains only priority value '{sql_content}'\")\n",
    "                excluded_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Final check: Ensure SQL is not empty or trivial\n",
    "            if not sql_content or len(sql_content.strip()) < 20:\n",
    "                self.logger.error(f\"EXCLUDING use case {use_case_id}: SQL is empty or too short\")\n",
    "                excluded_count += 1\n",
    "                continue\n",
    "            \n",
    "            numbered_title = f\"{use_case['No']}: {use_case['Name']}\"\n",
    "            \n",
    "            # Helper function to translate field values\n",
    "            def translate_value(field_name, value):\n",
    "                \"\"\"Translate Type and Priority values\"\"\"\n",
    "                if not value or value == 'N/A':\n",
    "                    return value\n",
    "                \n",
    "                # Map English values to translation keys\n",
    "                value_key_map = {\n",
    "                    # Type values\n",
    "                    'Problem': 'value_type_problem',\n",
    "                    'Risk': 'value_type_risk',\n",
    "                    'Opportunity': 'value_type_opportunity',\n",
    "                    'Improvement': 'value_type_improvement',\n",
    "                    # Priority values\n",
    "                    'Very High': 'value_priority_very_high',\n",
    "                    'High': 'value_priority_high',\n",
    "                    'Low': 'value_priority_low',\n",
    "                    'Very Low': 'value_priority_very_low'\n",
    "                }\n",
    "                \n",
    "                # Check if Medium needs special handling based on field\n",
    "                if value == 'Medium':\n",
    "                    if field_name in ['type', 'Type']:\n",
    "                        return t.get('value_type_medium', value)\n",
    "                    elif field_name in ['priority', 'Priority', 'aspect_priority']:\n",
    "                        return t.get('value_priority_medium', value)\n",
    "                \n",
    "                # Get translation or return original value\n",
    "                translation_key = value_key_map.get(value)\n",
    "                return t.get(translation_key, value) if translation_key else value\n",
    "            \n",
    "            def safe_notebook_str(val):\n",
    "                \"\"\"Handle None/empty values for notebook display.\"\"\"\n",
    "                if val is None or (isinstance(val, str) and not val.strip()):\n",
    "                    return 'N/A'\n",
    "                return str(val)\n",
    "            \n",
    "            combined_source = [\n",
    "                f\"### {numbered_title}\\n\\n\",\n",
    "                f\"| {t['aspect']} | {t['description']} |\\n\", \"|---|---|\\n\",\n",
    "                f\"| **{t['subdomain']}** | {safe_notebook_str(use_case.get('Subdomain'))} |\\n\",\n",
    "                f\"| **{t['type']}** | {translate_value('type', use_case.get('type', 'N/A'))} |\\n\",\n",
    "                f\"| **{t.get('analytics_technique', 'Analytics Technique')}** | {safe_notebook_str(use_case.get('Analytics Technique'))} |\\n\",\n",
    "                f\"| **{t['priority']}** | {translate_value('priority', use_case.get('Priority', 'N/A'))} |\\n\",\n",
    "                f\"| **{t.get('primary_table', 'Primary Table')}** | {safe_notebook_str(use_case.get('Primary Table'))} |\\n\",\n",
    "                f\"| **{t['statement']}** | {safe_notebook_str(use_case.get('Statement'))} |\\n\",\n",
    "                f\"| **{t['solution']}** | {safe_notebook_str(use_case.get('Solution'))} |\\n\",\n",
    "                f\"| **{t['aspect_value']}** | {safe_notebook_str(use_case.get('Business Value'))} |\\n\",\n",
    "                f\"| **{t['aspect_tables']}** | {safe_notebook_str(use_case.get('Tables Involved'))} |\\n\"\n",
    "            ]\n",
    "            details_cell = {\"cell_type\": \"markdown\", \"metadata\": {\"application/vnd.databricks.v1+cell\": {\"nuid\": str(uuid.uuid4())}}, \"source\": combined_source}\n",
    "            # Use SQL directly without any formatting/correction, but add Inspire header at the top\n",
    "            use_case_id = use_case.get('No', 'UNKNOWN')\n",
    "            use_case_name = use_case.get('Name', '')\n",
    "            # generate_sample_result:No initially, user sets to Yes to generate sample output\n",
    "            # regenerate_sql:No initially, user sets to Yes to regenerate SQL\n",
    "            inspire_header = f\"--Use Case: {use_case_id} - {use_case_name}\\n--generate_sample_result:No\\n--regenerate_sql:No\\n\"\n",
    "            inspire_instructions_block = \"/**Regeneration Instruction Start\\n\\nRegeneration Instruction End**/\\n\\n\"\n",
    "            sql_lines = use_case['SQL'].split('\\n')\n",
    "            # Strip LLM-generated header lines to avoid duplication (our header already has use case info)\n",
    "            sql_lines_clean = []\n",
    "            skip_header = True\n",
    "            for line in sql_lines:\n",
    "                line_stripped = line.strip().lower()\n",
    "                if skip_header and (line_stripped.startswith('-- use case') or line_stripped.startswith('--use case')):\n",
    "                    continue\n",
    "                if skip_header and line_stripped.startswith('--') and not line_stripped.startswith('-- step') and not line_stripped.startswith('--step'):\n",
    "                    # Skip generic comment lines at the start (descriptions)\n",
    "                    if len(line_stripped) > 2 and not any(kw in line_stripped for kw in ['with', 'select', 'cte', 'step']):\n",
    "                        continue\n",
    "                skip_header = False\n",
    "                sql_lines_clean.append(line)\n",
    "            sql_with_header = [inspire_header, inspire_instructions_block] + [line + '\\n' for line in sql_lines_clean]\n",
    "            code_cell = {\"cell_type\": \"code\", \"execution_count\": 0, \"outputs\": [], \"metadata\": {\"application/vnd.databricks.v1+cell\": {\"nuid\": str(uuid.uuid4())}}, \"source\": sql_with_header}\n",
    "            final_cells.extend([details_cell, code_cell])\n",
    "        \n",
    "        if excluded_count > 0:\n",
    "            self.logger.warning(f\"NOTEBOOK {db_name}: Excluded {excluded_count} use case(s) with invalid/missing SQL\")\n",
    "        \n",
    "        if not filename_override:\n",
    "            self.logger.warning(f\"No filename_override provided for notebook assembly '{db_name}'. Defaulting.\")\n",
    "            notebook_name_sanitized = f\"{db_prefix}_{self._sanitize_name(db_name)}\"\n",
    "        else:\n",
    "            notebook_name_sanitized = filename_override\n",
    "\n",
    "        gen_dir = self.notebook_output_dir\n",
    "        \n",
    "        # Databricks SQL notebook metadata\n",
    "        notebook_metadata = {\n",
    "            \"application/vnd.databricks.v1+notebook\": {\n",
    "                \"computePreferences\": None,\n",
    "                \"dashboards\": [],\n",
    "                \"environmentMetadata\": {\n",
    "                    \"base_environment\": \"\",\n",
    "                    \"environment_version\": \"4\"\n",
    "                },\n",
    "                \"inputWidgetPreferences\": None,\n",
    "                \"language\": \"sql\",\n",
    "                \"notebookMetadata\": {\n",
    "                    \"pythonIndentUnit\": 2\n",
    "                },\n",
    "                \"notebookName\": notebook_name_sanitized,\n",
    "                \"widgets\": {}\n",
    "            },\n",
    "            \"language_info\": {\n",
    "                \"name\": \"sql\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        final_notebook_obj = { \"notebook_content\": { \"cells\": final_cells, \"metadata\": notebook_metadata, \"nbformat\": 4, \"nbformat_minor\": 0 } }\n",
    "        \n",
    "        # Retry logic for workspace import with exponential backoff (respect global retry setting)\n",
    "        max_retries = (getattr(self, \"max_retry_attempts\", 1) or 0) + 1\n",
    "        retry_delay = 2  # seconds\n",
    "        \n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                self.logger.info(f\"Importing notebook '{notebook_name_sanitized}.ipynb' (attempt {attempt}/{max_retries})...\")\n",
    "                notebook_full_path = os.path.join(gen_dir, f\"{notebook_name_sanitized}.ipynb\")\n",
    "                notebook_content_str = json.dumps(final_notebook_obj[\"notebook_content\"], indent=2)\n",
    "                \n",
    "                # Import with timeout handling\n",
    "                import time\n",
    "                start_time = time.time()\n",
    "                self.w_client.workspace.import_(\n",
    "                    path=notebook_full_path, overwrite=True, format=workspace.ImportFormat.JUPYTER,\n",
    "                    content=base64.b64encode(notebook_content_str.encode('utf-8')).decode(),\n",
    "                )\n",
    "                elapsed_time = time.time() - start_time\n",
    "                \n",
    "                self.logger.info(f\"✓ Notebook '{notebook_name_sanitized}.ipynb' imported successfully in {elapsed_time:.1f}s\")\n",
    "                abs_path = self.w_client.workspace.get_status(notebook_full_path).path\n",
    "                self.logger.info(f\"Notebook is located at: {abs_path}\")\n",
    "                log_print(f\"   ✓ Notebook saved to: {abs_path}\")\n",
    "                break  # Success - exit retry loop\n",
    "                \n",
    "            except Exception as err:\n",
    "                self.logger.warning(f\"Attempt {attempt}/{max_retries} failed: {err}\")\n",
    "                \n",
    "                if attempt < max_retries:\n",
    "                    # Exponential backoff\n",
    "                    wait_time = retry_delay * (2 ** (attempt - 1))\n",
    "                    self.logger.info(f\"Retrying in {wait_time} seconds...\")\n",
    "                    import time\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    # Final attempt failed\n",
    "                    self.logger.error(f\"❌ Failed to import notebook '{notebook_name_sanitized}' after {max_retries} attempts\")\n",
    "                    self.logger.error(f\"Error details: {err}\")\n",
    "                    log_print(f\"   ❌ ERROR: Failed to create notebook '{notebook_name_sanitized}': {err}\", file=sys.stderr)\n",
    "                    # Re-raise to allow caller to handle\n",
    "                    raise\n",
    "\n",
    "    def _filter_business_tables(self, db_details: list, business_context: str = \"\", industry: str = \"\", exclusion_strategy: str = \"Medium\") -> tuple:\n",
    "        \"\"\"\n",
    "        Filters tables into business-relevant vs technical/metadata tables using LLM.\n",
    "        RESPECTS MAX_CONTEXT_CHARS by batching tables if needed.\n",
    "        Implements RECURSIVE BATCHING: If a batch is too large, splits it and tries again recursively.\n",
    "        \n",
    "        Args:\n",
    "            db_details: List of (catalog, schema, table, column, type, comment) tuples\n",
    "            business_context: Business context description\n",
    "            industry: Industry classification\n",
    "            exclusion_strategy: Technical exclusion strategy (\"None\", \"Aggressive\", \"Medium\", \"Low\")\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (business_tables_details, technical_tables_details, business_table_names, technical_table_names,\n",
    "            business_scores_dict, data_category_map, master_tables_set, transactional_tables_set, reference_tables_set)\n",
    "        \"\"\"\n",
    "        if not db_details:\n",
    "            return ([], [], set(), set(), {})\n",
    "        \n",
    "        include_technical = exclusion_strategy == \"None\"\n",
    "        \n",
    "        # Get additional context from instance (user-provided instructions)\n",
    "        additional_context = getattr(self, 'additional_context', '') or ''\n",
    "        \n",
    "        # Call the main recursive processing logic\n",
    "        return self._filter_business_tables_with_batching(\n",
    "            db_details=db_details,\n",
    "            business_context=business_context,\n",
    "            industry=industry,\n",
    "            exclusion_strategy=exclusion_strategy,\n",
    "            additional_context=additional_context,\n",
    "            include_technical=include_technical\n",
    "        )\n",
    "    \n",
    "    def _process_filter_batch_recursive(self, batch_tables: list, batch_idx: int, total_batches: int, \n",
    "                                       business_name: str, industry: str, business_context: str,\n",
    "                                       exclusion_strategy: str, strategy_rule_text: str,\n",
    "                                       additional_context_section: str = \"\",\n",
    "                                       depth: int = 0, max_depth: int = 10) -> dict:\n",
    "        \"\"\"\n",
    "        Recursively process a batch of tables for filtering with automatic sub-batching if context limit exceeded.\n",
    "        \n",
    "        Args:\n",
    "            batch_tables: List of table names to classify\n",
    "            batch_idx: Batch index for logging\n",
    "            total_batches: Total number of batches\n",
    "            business_name: Business name\n",
    "            industry: Industry classification\n",
    "            business_context: Business context description\n",
    "            exclusion_strategy: Exclusion strategy\n",
    "            strategy_rule_text: Strategy-specific rules text\n",
    "            additional_context_section: User-provided filtering instructions (formatted)\n",
    "            depth: Current recursion depth\n",
    "            max_depth: Maximum recursion depth to prevent infinite loops\n",
    "            \n",
    "        Returns:\n",
    "            Dict mapping table_name -> (classification, business_score, data_category)\n",
    "        \"\"\"\n",
    "        if depth > max_depth:\n",
    "            self.logger.error(f\"[Batch {batch_idx}] Max recursion depth ({max_depth}) reached. Defaulting {len(batch_tables)} tables to BUSINESS.\")\n",
    "            return {table.replace('`', ''): ('BUSINESS', 50, 'MASTER') for table in batch_tables}\n",
    "        \n",
    "        depth_prefix = \"  \" * depth  # Indent based on depth for readability\n",
    "        # Fix batch naming: use \"Split\" suffix instead of excessive \"_SUB\" suffixes\n",
    "        if depth > 0:\n",
    "            log_prefix = f\"[Batch {batch_idx}-Split{depth}]\"\n",
    "        else:\n",
    "            log_prefix = f\"[Batch {batch_idx}]\"\n",
    "        \n",
    "        self.logger.info(f\"{depth_prefix}{log_prefix} Processing {len(batch_tables)} tables (depth={depth})...\")\n",
    "        \n",
    "        # Create markdown table list for this batch\n",
    "        tables_markdown = \"\\n\".join([f\"- {table}\" for table in batch_tables])\n",
    "        \n",
    "        # Verify this batch's prompt size\n",
    "        prompt_vars = {\n",
    "            \"business_name\": business_name,\n",
    "            \"industry\": industry,\n",
    "            \"business_context\": business_context,\n",
    "            \"exclusion_strategy\": exclusion_strategy,\n",
    "            \"strategy_rules\": strategy_rule_text,\n",
    "            \"additional_context_section\": additional_context_section,\n",
    "            \"tables_markdown\": tables_markdown\n",
    "        }\n",
    "        \n",
    "        # Format the prompt to check actual size (using model-specific limits from TECHNICAL_CONTEXT)\n",
    "        filter_context_limit = get_max_context_chars(\"English\", \"FILTER_BUSINESS_TABLES_PROMPT\")\n",
    "        test_prompt = self.ai_agent._load_and_format_prompt(\"FILTER_BUSINESS_TABLES_PROMPT\", prompt_vars)\n",
    "        actual_prompt_size = len(test_prompt)\n",
    "        \n",
    "        # Check if batch is too large\n",
    "        if actual_prompt_size > filter_context_limit:\n",
    "            # Need to split this batch\n",
    "            if len(batch_tables) <= 1:\n",
    "                # Cannot split further - single table is too large (very rare case)\n",
    "                self.logger.error(\n",
    "                    f\"{depth_prefix}{log_prefix} Single table name too long to process ({actual_prompt_size:,} chars). \"\n",
    "                    f\"Defaulting to BUSINESS.\"\n",
    "                )\n",
    "                return {batch_tables[0].replace('`', ''): ('BUSINESS', 50, 'MASTER')}\n",
    "            \n",
    "            # Split into 2 sub-batches\n",
    "            mid_point = len(batch_tables) // 2\n",
    "            first_half = batch_tables[:mid_point]\n",
    "            second_half = batch_tables[mid_point:]\n",
    "            \n",
    "            self.logger.warning(\n",
    "                f\"{depth_prefix}{log_prefix} Batch too large ({actual_prompt_size:,} chars > {filter_context_limit:,} limit). \"\n",
    "                f\"Splitting {len(batch_tables)} tables into 2 sub-batches: {len(first_half)} + {len(second_half)} tables\"\n",
    "            )\n",
    "            \n",
    "            # Process both halves recursively\n",
    "            results = {}\n",
    "            \n",
    "            # Process first half\n",
    "            first_results = self._process_filter_batch_recursive(\n",
    "                batch_tables=first_half,\n",
    "                batch_idx=f\"{batch_idx}a\",\n",
    "                total_batches=total_batches,\n",
    "                business_name=business_name,\n",
    "                industry=industry,\n",
    "                business_context=business_context,\n",
    "                exclusion_strategy=exclusion_strategy,\n",
    "                strategy_rule_text=strategy_rule_text,\n",
    "                additional_context_section=additional_context_section,\n",
    "                depth=depth + 1,\n",
    "                max_depth=max_depth\n",
    "            )\n",
    "            results.update(first_results)\n",
    "            \n",
    "            # Process second half\n",
    "            second_results = self._process_filter_batch_recursive(\n",
    "                batch_tables=second_half,\n",
    "                batch_idx=f\"{batch_idx}b\",\n",
    "                total_batches=total_batches,\n",
    "                business_name=business_name,\n",
    "                industry=industry,\n",
    "                business_context=business_context,\n",
    "                exclusion_strategy=exclusion_strategy,\n",
    "                strategy_rule_text=strategy_rule_text,\n",
    "                additional_context_section=additional_context_section,\n",
    "                depth=depth + 1,\n",
    "                max_depth=max_depth\n",
    "            )\n",
    "            results.update(second_results)\n",
    "            \n",
    "            self.logger.info(f\"{depth_prefix}{log_prefix} Sub-batches complete. Total: {len(results)} tables classified\")\n",
    "            return results\n",
    "        \n",
    "        # Batch size is OK - process it with retry logic\n",
    "        self.logger.debug(f\"{depth_prefix}{log_prefix} Batch size OK ({actual_prompt_size:,} chars)\")\n",
    "        \n",
    "        try:\n",
    "            # Retry on CSV parsing errors (respect global retry setting)\n",
    "            max_retries = (getattr(self, \"max_retry_attempts\", 1) or 0) + 1\n",
    "            for attempt in range(1, max_retries + 1):\n",
    "                try:\n",
    "                    # Call LLM to classify tables in this batch\n",
    "                    attempt_suffix = f\" (attempt {attempt}/{max_retries})\" if attempt > 1 else \"\"\n",
    "                    self.logger.info(f\"{depth_prefix}⏳ {log_prefix} Waiting for LLM response{attempt_suffix} (filtering {len(batch_tables)} tables into BUSINESS vs TECHNICAL)...\")\n",
    "                    response_raw = self.ai_agent.run_worker(\n",
    "                        step_name=f\"Filter_Business_Tables_Batch_{batch_idx}_Depth{depth}_Attempt{attempt}\",\n",
    "                        worker_prompt_path=\"FILTER_BUSINESS_TABLES_PROMPT\",\n",
    "                        prompt_vars=prompt_vars,\n",
    "                        response_schema=None\n",
    "                    )\n",
    "                    self.logger.info(f\"{depth_prefix}✅ {log_prefix} Received LLM response, parsing classifications...\")\n",
    "                    \n",
    "                    # Parse CSV response using centralized utility\n",
    "                    response_clean = clean_json_response(response_raw)\n",
    "                    \n",
    "                    csv_rows = CSVParser.parse_csv_string(\n",
    "                        response_clean,\n",
    "                        logger=self.logger,\n",
    "                        context=log_prefix\n",
    "                    )\n",
    "                    \n",
    "                    results = {}\n",
    "                    row_count = 0\n",
    "                    for row in csv_rows:\n",
    "                        row_count += 1\n",
    "                        # Safely get values with fallback to empty string if None\n",
    "                        table_name = (row.get('Table Name') or '').strip().replace('`', '')\n",
    "                        classification = (row.get('Classification') or '').strip().upper()\n",
    "                        data_category = (row.get('Data Category') or '').strip().upper()\n",
    "                        business_score_str = (row.get('Business Score') or '50').strip()\n",
    "                        \n",
    "                        # Skip invalid rows (empty table name or very short names that are likely parsing errors)\n",
    "                        if not table_name or len(table_name) < 3:\n",
    "                            self.logger.warning(f\"{depth_prefix}Skipping invalid row with table name '{table_name}' (too short or empty)\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Parse business score\n",
    "                        try:\n",
    "                            business_score = int(business_score_str)\n",
    "                            business_score = max(0, min(100, business_score))  # Clamp to 0-100\n",
    "                        except (ValueError, TypeError):\n",
    "                            business_score = 50  # Default to medium score if parsing fails\n",
    "                            self.logger.warning(f\"{depth_prefix}Table {table_name}: Invalid business score '{business_score_str}', using default 50\")\n",
    "                        \n",
    "                        if classification == 'BUSINESS':\n",
    "                            if data_category not in ('MASTER', 'REFERENCE', 'TRANSACTIONAL'):\n",
    "                                self.logger.warning(f\"{depth_prefix}Table {table_name}: LLM returned invalid data_category '{data_category}', defaulting to MASTER\")\n",
    "                                data_category = 'MASTER'\n",
    "                            results[table_name] = ('BUSINESS', business_score, data_category)\n",
    "                        elif classification == 'TECHNICAL':\n",
    "                            results[table_name] = ('TECHNICAL', 0, 'TECHNICAL')\n",
    "                        else:\n",
    "                            self.logger.warning(f\"{depth_prefix}Table {table_name} has unclear classification '{classification}', defaulting to BUSINESS/MASTER\")\n",
    "                            if data_category not in ('MASTER', 'REFERENCE', 'TRANSACTIONAL'):\n",
    "                                data_category = 'MASTER'\n",
    "                            results[table_name] = ('BUSINESS', business_score, data_category)\n",
    "                    \n",
    "                    # Validate we got results\n",
    "                    if not results or row_count == 0:\n",
    "                        raise ValueError(f\"CSV parsing returned no results (row_count={row_count})\")\n",
    "                    \n",
    "                    self.logger.info(f\"{depth_prefix}✅ {log_prefix} Complete: {len(results)} tables classified\")\n",
    "                    return results\n",
    "                    \n",
    "                except (ValueError, AttributeError, KeyError) as parse_error:\n",
    "                    # CSV parsing errors - retry\n",
    "                    if attempt < max_retries:\n",
    "                        self.logger.warning(f\"{depth_prefix}{log_prefix} CSV parsing error on attempt {attempt}/{max_retries}: {parse_error}. Retrying...\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        self.logger.error(f\"{depth_prefix}{log_prefix} Failed to parse CSV after {max_retries} attempts: {parse_error}. Defaulting batch tables to BUSINESS.\")\n",
    "                        return {table.replace('`', ''): ('BUSINESS', 50, 'MASTER') for table in batch_tables}\n",
    "            \n",
    "        except InputTooLongError as e:\n",
    "            # Even though we pre-checked, the LLM still rejected it. Split more aggressively.\n",
    "            if len(batch_tables) <= 1:\n",
    "                self.logger.error(\n",
    "                    f\"{depth_prefix}{log_prefix} LLM rejected even after pre-check. Single table too large. \"\n",
    "                    f\"Defaulting to BUSINESS. Error: {str(e)[:200]}\"\n",
    "                )\n",
    "                return {batch_tables[0].replace('`', ''): ('BUSINESS', 50, 'MASTER')}\n",
    "            \n",
    "            # Split into 2 sub-batches\n",
    "            mid_point = len(batch_tables) // 2\n",
    "            first_half = batch_tables[:mid_point]\n",
    "            second_half = batch_tables[mid_point:]\n",
    "            \n",
    "            self.logger.warning(\n",
    "                f\"{depth_prefix}{log_prefix} LLM rejected batch (InputTooLongError). \"\n",
    "                f\"Splitting {len(batch_tables)} tables into 2 sub-batches: {len(first_half)} + {len(second_half)} tables\"\n",
    "            )\n",
    "            \n",
    "            # Process both halves recursively\n",
    "            results = {}\n",
    "            first_results = self._process_filter_batch_recursive(\n",
    "                batch_tables=first_half,\n",
    "                batch_idx=f\"{batch_idx}a\",\n",
    "                total_batches=total_batches,\n",
    "                business_name=business_name,\n",
    "                industry=industry,\n",
    "                business_context=business_context,\n",
    "                exclusion_strategy=exclusion_strategy,\n",
    "                strategy_rule_text=strategy_rule_text,\n",
    "                additional_context_section=additional_context_section,\n",
    "                depth=depth + 1,\n",
    "                max_depth=max_depth\n",
    "            )\n",
    "            results.update(first_results)\n",
    "            \n",
    "            second_results = self._process_filter_batch_recursive(\n",
    "                batch_tables=second_half,\n",
    "                batch_idx=f\"{batch_idx}b\",\n",
    "                total_batches=total_batches,\n",
    "                business_name=business_name,\n",
    "                industry=industry,\n",
    "                business_context=business_context,\n",
    "                exclusion_strategy=exclusion_strategy,\n",
    "                strategy_rule_text=strategy_rule_text,\n",
    "                additional_context_section=additional_context_section,\n",
    "                depth=depth + 1,\n",
    "                max_depth=max_depth\n",
    "            )\n",
    "            results.update(second_results)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as batch_error:\n",
    "            self.logger.error(f\"{depth_prefix}{log_prefix} Failed to process batch: {batch_error}. Defaulting batch tables to BUSINESS.\")\n",
    "            # Default all tables in this batch to business with medium score\n",
    "            return {table.replace('`', ''): ('BUSINESS', 50, 'MASTER') for table in batch_tables}\n",
    "    \n",
    "    def _filter_business_tables_with_batching(self, db_details: list, business_context: str, \n",
    "                                             industry: str, exclusion_strategy: str,\n",
    "                                             additional_context: str = \"\", include_technical: bool = False) -> tuple:\n",
    "        \"\"\"\n",
    "        Main filtering logic with batching support and recursive sub-batching.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # === LOG USER ADDITIONAL CONTEXT INTERPRETATION ===\n",
    "            additional_context_section = \"\"\n",
    "            if additional_context:\n",
    "                self.logger.info(\"=\" * 80)\n",
    "                self.logger.info(\"\uD83D\uDD0D INTERPRETING USER ADDITIONAL CONTEXT FOR TABLE FILTERING\")\n",
    "                self.logger.info(\"=\" * 80)\n",
    "                self.logger.info(f\"\uD83D\uDCCB User Instructions: {additional_context[:500]}{'...' if len(additional_context) > 500 else ''}\")\n",
    "                \n",
    "                # Log interpretation of common patterns\n",
    "                context_lower = additional_context.lower()\n",
    "                \n",
    "                # Check for database/catalog exclusions\n",
    "                if 'ignore' in context_lower or 'exclude' in context_lower or 'skip' in context_lower:\n",
    "                    self.logger.info(\"\uD83C\uDFAF DETECTED: User wants to EXCLUDE/IGNORE certain data\")\n",
    "                \n",
    "                # Check for business entity references\n",
    "                business_entities = ['customer', 'subscriber', 'client', 'user', 'member', 'patient', 'employee', \n",
    "                                   'vendor', 'supplier', 'partner', 'order', 'transaction', 'product', 'inventory']\n",
    "                detected_entities = [e for e in business_entities if e in context_lower]\n",
    "                if detected_entities:\n",
    "                    self.logger.info(f\"\uD83C\uDFAF DETECTED BUSINESS ENTITIES: {', '.join(detected_entities)}\")\n",
    "                    self.logger.info(\"   → LLM will apply SEMANTIC understanding (e.g., 'subscriber' = 'customer')\")\n",
    "                \n",
    "                # Check for database/catalog references\n",
    "                if 'database' in context_lower or 'catalog' in context_lower or 'schema' in context_lower:\n",
    "                    self.logger.info(\"\uD83C\uDFAF DETECTED: Database/Catalog/Schema level filtering instructions\")\n",
    "                \n",
    "                self.logger.info(\"=\" * 80)\n",
    "                \n",
    "                # Build the additional context section for the prompt\n",
    "                additional_context_section = f\"\"\"\n",
    "**\uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8 HIGHEST PRIORITY: USER-PROVIDED FILTERING INSTRUCTIONS \uD83D\uDEA8\uD83D\uDEA8\uD83D\uDEA8**\n",
    "\n",
    "**⛔ YOU MUST FOLLOW THESE USER INSTRUCTIONS - THEY OVERRIDE ALL OTHER RULES ⛔**\n",
    "\n",
    "{additional_context}\n",
    "\n",
    "**CRITICAL INTERPRETATION RULES**:\n",
    "1. **Database/Catalog Exclusions**: If user says \"ignore database X\" or \"exclude catalog Y\", ALL tables from that database/catalog MUST be classified as TECHNICAL (filtered out)\n",
    "2. **Business Entity Semantics**: Apply BUSINESS UNDERSTANDING to user instructions:\n",
    "   - If user says \"ignore customers\", also ignore tables named: subscribers, clients, users, members, accounts, patrons, buyers\n",
    "   - If user says \"ignore orders\", also ignore tables named: transactions, purchases, bookings, reservations, sales\n",
    "   - If user says \"ignore products\", also ignore tables named: items, inventory, merchandise, goods, SKUs, catalog\n",
    "   - **USE YOUR BUSINESS DOMAIN KNOWLEDGE** to identify semantically equivalent entities\n",
    "3. **Explicit Overrides**: User instructions ALWAYS override the default classification rules\n",
    "4. **When in doubt**: If a table MIGHT relate to something user wants to ignore, classify it as TECHNICAL\n",
    "\n",
    "**EXAMPLE INTERPRETATIONS**:\n",
    "- User: \"ignore everything from database legacy_crm\" → ALL tables with `legacy_crm` in their path → TECHNICAL\n",
    "- User: \"ignore all customer data\" → Tables: customers, subscribers, clients, users, members, accounts → ALL TECHNICAL\n",
    "- User: \"focus only on finance\" → Tables NOT related to finance/accounting → TECHNICAL\n",
    "- User: \"exclude marketing tables\" → Tables: campaigns, leads, prospects, marketing_*, ads_* → ALL TECHNICAL\n",
    "\n",
    "**⚠️ FAILURE TO FOLLOW USER INSTRUCTIONS = ENTIRE OUTPUT REJECTED ⚠️**\n",
    "\"\"\"\n",
    "            else:\n",
    "                additional_context_section = \"*(No additional user filtering instructions provided)*\"\n",
    "            \n",
    "            # Define strategy-specific rules\n",
    "            strategy_rules = {\n",
    "                \"Aggressive\": \"\"\"**AGGRESSIVE FILTERING**:\n",
    "- Apply STRICT interpretation of technical patterns\n",
    "- Err on the side of EXCLUDING borderline tables\n",
    "- Any table with >50% technical-looking columns → TECHNICAL\n",
    "- Aggressively filter logs, metrics, configurations, snapshots, audits\n",
    "- **Exception**: Only include if clearly core to business operations AND business name indicates relevance\"\"\",\n",
    "                \n",
    "                \"Medium\": \"\"\"**MEDIUM FILTERING** (BALANCED APPROACH):\n",
    "- Apply MODERATE interpretation of technical patterns\n",
    "- Balance between business value and technical overhead\n",
    "- Tables with >70% technical columns → TECHNICAL\n",
    "- Filter obvious technical tables but preserve borderline cases\n",
    "- When moderately uncertain, prefer BUSINESS classification\"\"\",\n",
    "                \n",
    "                \"Low\": \"\"\"**LOW FILTERING** (PERMISSIVE APPROACH):\n",
    "- Apply LENIENT interpretation of technical patterns\n",
    "- Err on the side of INCLUDING borderline tables\n",
    "- Only filter tables that are >90% pure IT infrastructure\n",
    "- Preserve any table with even minor business relevance\n",
    "- When uncertain, default to BUSINESS classification\n",
    "- Include operational logs/audits that may contain business insights\"\"\"\n",
    "            }\n",
    "            \n",
    "            strategy_rule_text = strategy_rules.get(exclusion_strategy, strategy_rules[\"Medium\"])\n",
    "            \n",
    "            # Extract unique tables\n",
    "            unique_tables = set()\n",
    "            for (catalog, schema, table, column_name, data_type, comment) in db_details:\n",
    "                fqtn = f\"`{catalog}`.`{schema}`.`{table}`\"\n",
    "                unique_tables.add(fqtn)\n",
    "            \n",
    "            sorted_tables = sorted(unique_tables)\n",
    "            total_tables = len(sorted_tables)\n",
    "            \n",
    "            self.logger.info(f\"Filtering {total_tables} tables into business vs technical categories with '{exclusion_strategy}' strategy...\")\n",
    "            \n",
    "            # Get the prompt template to estimate base size (using model-specific limits from TECHNICAL_CONTEXT)\n",
    "            filter_tables_context_limit = get_max_context_chars(\"English\", \"FILTER_BUSINESS_TABLES_PROMPT\")\n",
    "            prompt_template = self.ai_agent.prompt_templates.get(\"FILTER_BUSINESS_TABLES_PROMPT\", \"\")\n",
    "            base_prompt_size = len(prompt_template) + len(self.business_name) + len(industry or \"General Business\") + len(business_context or \"General business operations\") + len(strategy_rule_text)\n",
    "            \n",
    "            # Reserve 20% of model's context limit for safety margin\n",
    "            available_chars = int(filter_tables_context_limit * 0.8) - base_prompt_size\n",
    "            \n",
    "            # Estimate chars per table (table name + markdown formatting) ~60 chars average\n",
    "            estimated_chars_per_table = 60\n",
    "            max_tables_per_batch = max(100, available_chars // estimated_chars_per_table)  # At least 100 tables per batch\n",
    "            \n",
    "            self.logger.info(f\"Base prompt size: {base_prompt_size} chars, Available for tables: {available_chars} chars\")\n",
    "            self.logger.info(f\"Estimated max tables per batch: {max_tables_per_batch}\")\n",
    "            \n",
    "            # Determine if batching is needed\n",
    "            if total_tables <= max_tables_per_batch:\n",
    "                # Can process all tables in one batch\n",
    "                self.logger.info(\"All tables fit in one batch. Processing...\")\n",
    "                batches = [sorted_tables]\n",
    "            else:\n",
    "                # Need to batch\n",
    "                num_batches = (total_tables + max_tables_per_batch - 1) // max_tables_per_batch\n",
    "                self.logger.info(f\"⚠️ Tables exceed single batch capacity. Splitting into {num_batches} batches...\")\n",
    "                batches = [sorted_tables[i:i+max_tables_per_batch] for i in range(0, total_tables, max_tables_per_batch)]\n",
    "            \n",
    "            # Process each batch with recursive splitting if needed (IN PARALLEL)\n",
    "            business_tables_set = set()\n",
    "            technical_tables_set = set()\n",
    "            master_tables_set = set()\n",
    "            transactional_tables_set = set()\n",
    "            reference_tables_set = set()\n",
    "            data_category_map = {}\n",
    "            business_scores = {}\n",
    "            \n",
    "            # ADAPTIVE PARALLELISM: Calculate based on batches and tables\n",
    "            classification_parallelism, reason = calculate_adaptive_parallelism(\n",
    "                \"domain_clustering\", self.max_parallelism,\n",
    "                num_items=total_tables,\n",
    "                num_domains=len(batches),\n",
    "                is_llm_operation=True, logger=self.logger\n",
    "            )\n",
    "            log_adaptive_parallelism_decision(\"domain_clustering\", classification_parallelism, self.max_parallelism, reason)\n",
    "            \n",
    "            self.logger.info(f\"Processing {len(batches)} classification batch(es) in parallel...\")\n",
    "            \n",
    "            # Prepare tasks for parallel execution\n",
    "            tasks = []\n",
    "            for batch_idx, batch_tables in enumerate(batches, 1):\n",
    "                task = (\n",
    "                    self._process_filter_batch_recursive,\n",
    "                    (batch_tables, batch_idx, len(batches), self.business_name,\n",
    "                     industry or \"General Business\", business_context or \"General business operations\",\n",
    "                     exclusion_strategy, strategy_rule_text, additional_context_section, 0, 10)\n",
    "                )\n",
    "                tasks.append(task)\n",
    "            \n",
    "            # Execute in parallel with centralized utility\n",
    "            results = ParallelExecutor.execute_parallel(\n",
    "                tasks=tasks,\n",
    "                max_workers=classification_parallelism,\n",
    "                task_name=\"Classification Batch\",\n",
    "                logger=self.logger,\n",
    "                thread_name_prefix=\"FilterBatch\",\n",
    "                return_exceptions=True\n",
    "            )\n",
    "            \n",
    "            # Merge results from all batches\n",
    "            for batch_idx, batch_results in enumerate(results, 1):\n",
    "                if isinstance(batch_results, Exception):\n",
    "                    self.logger.error(f\"❌ Classification batch {batch_idx} failed: {batch_results}\")\n",
    "                    continue\n",
    "                \n",
    "                # Merge results\n",
    "                for table_name, (classification, business_score, data_category) in batch_results.items():\n",
    "                    if classification == 'BUSINESS':\n",
    "                        if data_category == 'REFERENCE':\n",
    "                            reference_tables_set.add(table_name)\n",
    "                            data_category_map[table_name] = 'REFERENCE'\n",
    "                        elif data_category == 'TRANSACTIONAL':\n",
    "                            transactional_tables_set.add(table_name)\n",
    "                            business_tables_set.add(table_name)\n",
    "                            data_category_map[table_name] = 'TRANSACTIONAL'\n",
    "                        else:\n",
    "                            master_tables_set.add(table_name)\n",
    "                            business_tables_set.add(table_name)\n",
    "                            data_category_map[table_name] = 'MASTER'\n",
    "                        business_scores[table_name] = business_score\n",
    "                    elif classification == 'TECHNICAL':\n",
    "                        technical_tables_set.add(table_name)\n",
    "                        data_category_map[table_name] = 'TECHNICAL'\n",
    "                        business_scores[table_name] = 0\n",
    "                        if include_technical:\n",
    "                            master_tables_set.add(table_name)\n",
    "                            business_tables_set.add(table_name)\n",
    "                \n",
    "                self.logger.info(f\"✅ Classification batch {batch_idx}/{len(batches)} completed\")\n",
    "            \n",
    "            # === RETRY UNCLASSIFIED TABLES (UP TO 2 RETRIES) ===\n",
    "            # Find tables that were not classified\n",
    "            all_unique_tables = set()\n",
    "            for detail in db_details:\n",
    "                (catalog, schema, table, _, _, _) = detail\n",
    "                fqtn = f\"{catalog}.{schema}.{table}\"\n",
    "                all_unique_tables.add(fqtn)\n",
    "            \n",
    "            unclassified_tables = all_unique_tables - business_tables_set - technical_tables_set - reference_tables_set\n",
    "            \n",
    "            # Retry up to 2 times for unclassified tables\n",
    "            for retry_attempt in range(1, 3):  # 2 retry attempts\n",
    "                if not unclassified_tables:\n",
    "                    break\n",
    "                    \n",
    "                self.logger.warning(f\"\uD83D\uDD04 RETRY {retry_attempt}/2: Found {len(unclassified_tables)} unclassified tables. Retrying classification...\")\n",
    "                \n",
    "                # Prepare retry batch with table names (with backticks for compatibility)\n",
    "                retry_batch_tables = [f\"`{t.replace('.', '`.`')}`\" for t in unclassified_tables]\n",
    "                \n",
    "                try:\n",
    "                    # Retry classification for unclassified tables\n",
    "                    retry_results = self._process_filter_batch_recursive(\n",
    "                        batch_tables=retry_batch_tables,\n",
    "                        batch_idx=f\"RETRY_{retry_attempt}\",\n",
    "                        total_batches=1,\n",
    "                        business_name=self.business_name,\n",
    "                        industry=industry or \"General Business\",\n",
    "                        business_context=business_context or \"General business operations\",\n",
    "                        exclusion_strategy=exclusion_strategy,\n",
    "                        strategy_rule_text=strategy_rule_text,\n",
    "                        additional_context_section=additional_context_section,\n",
    "                        max_depth=10\n",
    "                    )\n",
    "                    \n",
    "                    # Merge retry results\n",
    "                    for table_name, (classification, business_score, data_category) in retry_results.items():\n",
    "                        if classification == 'BUSINESS':\n",
    "                            if data_category == 'REFERENCE':\n",
    "                                reference_tables_set.add(table_name)\n",
    "                                data_category_map[table_name] = 'REFERENCE'\n",
    "                            elif data_category == 'TRANSACTIONAL':\n",
    "                                transactional_tables_set.add(table_name)\n",
    "                                business_tables_set.add(table_name)\n",
    "                                data_category_map[table_name] = 'TRANSACTIONAL'\n",
    "                            else:\n",
    "                                master_tables_set.add(table_name)\n",
    "                                business_tables_set.add(table_name)\n",
    "                                data_category_map[table_name] = 'MASTER'\n",
    "                            business_scores[table_name] = business_score\n",
    "                            self.logger.info(f\"✅ Retry {retry_attempt} successful: {table_name} classified as BUSINESS\")\n",
    "                        elif classification == 'TECHNICAL':\n",
    "                            technical_tables_set.add(table_name)\n",
    "                            data_category_map[table_name] = 'TECHNICAL'\n",
    "                            business_scores[table_name] = 0\n",
    "                            if include_technical:\n",
    "                                master_tables_set.add(table_name)\n",
    "                                business_tables_set.add(table_name)\n",
    "                            self.logger.info(f\"✅ Retry {retry_attempt} successful: {table_name} classified as TECHNICAL\")\n",
    "                    \n",
    "                    # Update unclassified list for next retry\n",
    "                    unclassified_tables = all_unique_tables - business_tables_set - technical_tables_set - reference_tables_set\n",
    "                    \n",
    "                except Exception as retry_error:\n",
    "                    self.logger.error(f\"❌ Retry {retry_attempt} classification failed: {retry_error}\")\n",
    "            \n",
    "            # After 2 retries, default remaining unclassified tables to BUSINESS + MASTER\n",
    "            if unclassified_tables:\n",
    "                self.logger.warning(f\"⚠️ {len(unclassified_tables)} tables still unclassified after 2 retries. Defaulting to BUSINESS + MASTER.\")\n",
    "                for table_fqtn in unclassified_tables:\n",
    "                    business_tables_set.add(table_fqtn)\n",
    "                    master_tables_set.add(table_fqtn)\n",
    "                    business_scores[table_fqtn] = 50  # Default medium score\n",
    "                    data_category_map[table_fqtn] = 'MASTER'\n",
    "                    self.logger.info(f\"\uD83D\uDCCB Defaulted to BUSINESS/MASTER: {table_fqtn}\")\n",
    "            \n",
    "            # Split db_details into business and technical\n",
    "            business_details = []\n",
    "            technical_details = []\n",
    "            unclassified_tables_logged = set()  # Track tables already logged to avoid duplicates\n",
    "            \n",
    "            for detail in db_details:\n",
    "                (catalog, schema, table, column_name, data_type, comment) = detail\n",
    "                fqtn = f\"{catalog}.{schema}.{table}\"\n",
    "                \n",
    "                if fqtn in reference_tables_set:\n",
    "                    continue\n",
    "                elif fqtn in business_tables_set:\n",
    "                    if fqtn not in reference_tables_set:\n",
    "                        business_details.append(detail)\n",
    "                elif fqtn in technical_tables_set:\n",
    "                    technical_details.append(detail)\n",
    "                else:\n",
    "                    # Default: if not classified after retry, assume business (safer to include)\n",
    "                    if fqtn not in unclassified_tables_logged:\n",
    "                        self.logger.warning(f\"Table {fqtn} not classified by LLM after retry, defaulting to BUSINESS\")\n",
    "                        unclassified_tables_logged.add(fqtn)\n",
    "                    business_details.append(detail)\n",
    "                    business_scores[fqtn] = 50  # Default medium score\n",
    "                    master_tables_set.add(fqtn)\n",
    "                    data_category_map[fqtn] = 'MASTER'\n",
    "                    business_tables_set.add(fqtn)\n",
    "            \n",
    "            self.logger.info(f\"✅ Filtering complete: {len(business_tables_set)} business tables, {len(technical_tables_set)} technical tables, {len(reference_tables_set)} reference tables\")\n",
    "            # Only log technical tables (top 10), not business tables\n",
    "            if technical_tables_set:\n",
    "                technical_list = sorted(list(technical_tables_set))[:10]\n",
    "                more_indicator = f\" (showing 10 of {len(technical_tables_set)})\" if len(technical_tables_set) > 10 else \"\"\n",
    "                self.logger.info(f\"Technical tables excluded{more_indicator}: {', '.join(technical_list)}\")\n",
    "            else:\n",
    "                self.logger.debug(\"No technical tables to exclude.\")\n",
    "            \n",
    "            # Log sample business scores\n",
    "            if business_scores:\n",
    "                sample_scores = sorted(business_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "                self.logger.info(f\"Top business tables by score: {', '.join([f'{t}({s})' for t, s in sample_scores])}\")\n",
    "            \n",
    "            return (business_details, technical_details, business_tables_set, technical_tables_set, business_scores, data_category_map, master_tables_set, transactional_tables_set, reference_tables_set)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to filter business vs technical tables: {e}. Proceeding with all tables.\")\n",
    "            # On error, return all tables as business tables with default scores\n",
    "            all_tables = set()\n",
    "            default_scores = {}\n",
    "            for (catalog, schema, table, _, _, _) in db_details:\n",
    "                fqtn = f\"{catalog}.{schema}.{table}\"\n",
    "                all_tables.add(fqtn)\n",
    "                default_scores[fqtn] = 50  # Default medium score\n",
    "            return (db_details, [], all_tables, set(), default_scores, {}, all_tables, set(), set())\n",
    "\n",
    "    def _estimate_schema_markdown_size(self, db_details: list) -> int:\n",
    "        if not db_details:\n",
    "            return 0\n",
    "        header_len = len(\"| column | type | column_description |\\n| --- | --- | --- |\")\n",
    "        total = 0\n",
    "        seen = set()\n",
    "        for (catalog, schema, table, column_name, data_type, comment) in db_details:\n",
    "            fqtn = f\"`{catalog}`.`{schema}`.`{table}`\"\n",
    "            if fqtn not in seen:\n",
    "                seen.add(fqtn)\n",
    "                total += len(\"### \") + len(fqtn) + 1\n",
    "                total += header_len + 1\n",
    "            col = column_name or \"\"\n",
    "            dtype = data_type or \"unknown\"\n",
    "            desc = comment or \"\"\n",
    "            total += len(col) + len(dtype) + len(desc) + 11\n",
    "        total += len(seen)\n",
    "        return total\n",
    "\n",
    "    def _split_columns_to_fit_context(self, column_details: list, base_prompt_size: int, context_limit: int) -> list:\n",
    "        if not column_details:\n",
    "            return []\n",
    "        table_columns = {}\n",
    "        table_order = []\n",
    "        for col in column_details:\n",
    "            key = (col[0], col[1], col[2])\n",
    "            if key not in table_columns:\n",
    "                table_columns[key] = []\n",
    "                table_order.append(key)\n",
    "            table_columns[key].append(col)\n",
    "        table_sizes = {}\n",
    "        for key in table_order:\n",
    "            table_sizes[key] = self._estimate_schema_markdown_size(table_columns[key])\n",
    "        batches = []\n",
    "        current_tables = []\n",
    "        current_size = base_prompt_size\n",
    "        for key in table_order:\n",
    "            table_size = table_sizes[key]\n",
    "            if current_tables and (current_size + table_size) > context_limit:\n",
    "                batches.append(current_tables)\n",
    "                current_tables = []\n",
    "                current_size = base_prompt_size\n",
    "            current_tables.append(key)\n",
    "            current_size += table_size\n",
    "        if current_tables:\n",
    "            batches.append(current_tables)\n",
    "        column_batches = []\n",
    "        for table_keys in batches:\n",
    "            batch_cols = []\n",
    "            for key in table_keys:\n",
    "                batch_cols.extend(table_columns[key])\n",
    "            column_batches.append(batch_cols)\n",
    "        return column_batches\n",
    "\n",
    "    def _determine_tables_per_call(self, total_tables: int) -> int:\n",
    "        if total_tables < 50:\n",
    "            return 1\n",
    "        if total_tables < 100:\n",
    "            return 2\n",
    "        if total_tables < 200:\n",
    "            return 3\n",
    "        if total_tables < 400:\n",
    "            return 4\n",
    "        return 5\n",
    "\n",
    "    def _split_by_table_limit(self, column_details: list, max_tables: int) -> list:\n",
    "        if not column_details or max_tables <= 0:\n",
    "            return []\n",
    "        table_columns = {}\n",
    "        table_order = []\n",
    "        for col in column_details:\n",
    "            key = (col[0], col[1], col[2])\n",
    "            if key not in table_columns:\n",
    "                table_columns[key] = []\n",
    "                table_order.append(key)\n",
    "            table_columns[key].append(col)\n",
    "        batches = []\n",
    "        current_tables = []\n",
    "        for key in table_order:\n",
    "            current_tables.append(key)\n",
    "            if len(current_tables) >= max_tables:\n",
    "                batch_cols = []\n",
    "                for table_key in current_tables:\n",
    "                    batch_cols.extend(table_columns[table_key])\n",
    "                batches.append(batch_cols)\n",
    "                current_tables = []\n",
    "        if current_tables:\n",
    "            batch_cols = []\n",
    "            for table_key in current_tables:\n",
    "                batch_cols.extend(table_columns[table_key])\n",
    "            batches.append(batch_cols)\n",
    "        return batches\n",
    "\n",
    "    # === MODIFIED: _format_schema_for_prompt ===\n",
    "    def _format_schema_for_prompt(self, db_details: list, load_column_tracking: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Format schema for prompt, respecting column tracking from context splitting.\n",
    "        \n",
    "        If columns were dropped due to context limits, only include the tracked columns.\n",
    "        Column tracking is loaded from disk on-demand, not kept in memory.\n",
    "        \n",
    "        Args:\n",
    "            db_details: List of column tuples (catalog, schema, table, column, type, comment)\n",
    "            load_column_tracking: If True, load column tracking from disk for SQL generation\n",
    "        \n",
    "        Returns:\n",
    "            Formatted schema markdown\n",
    "        \"\"\"\n",
    "        if not db_details: return \"\"\n",
    "        tables = defaultdict(list)\n",
    "        \n",
    "        # Load column tracking from disk only when needed (SQL generation)\n",
    "        column_tracking_cache = {}\n",
    "        if load_column_tracking:\n",
    "            # Get unique tables from db_details\n",
    "            unique_tables = set()\n",
    "            for (catalog, schema, table, _, _, _) in db_details:\n",
    "                fqtn_plain = f\"{catalog}.{schema}.{table}\"\n",
    "                unique_tables.add(fqtn_plain)\n",
    "            \n",
    "            # Load column tracking for tables that have it\n",
    "            for fqtn in unique_tables:\n",
    "                if self.storage_manager.has_column_tracking(fqtn):\n",
    "                    tracked_cols = self.storage_manager.load_column_tracking(fqtn)\n",
    "                    if tracked_cols:\n",
    "                        column_tracking_cache[fqtn] = tracked_cols\n",
    "        \n",
    "        for (catalog, schema, table, column_name, data_type, comment) in db_details:\n",
    "            fqtn = f\"`{catalog}`.`{schema}`.`{table}`\"\n",
    "            fqtn_plain = f\"{catalog}.{schema}.{table}\"\n",
    "            \n",
    "            # Check if this table has column tracking (meaning columns were dropped)\n",
    "            if fqtn_plain in column_tracking_cache:\n",
    "                tracked_columns = column_tracking_cache[fqtn_plain]\n",
    "                # Only include columns that were tracked (kept during splitting)\n",
    "                if column_name in tracked_columns:\n",
    "                    tables[fqtn].append((column_name, data_type or 'unknown', comment or ''))\n",
    "            else:\n",
    "                # No tracking for this table - include all columns\n",
    "                tables[fqtn].append((column_name, data_type or 'unknown', comment or ''))\n",
    "        \n",
    "        markdown_parts = []\n",
    "        for fqtn, columns in tables.items():\n",
    "            if not columns:\n",
    "                continue  # Skip tables with no columns (shouldn't happen but safety check)\n",
    "            \n",
    "            table_header = f\"### {fqtn}\"\n",
    "            header = \"| column | type | column_description |\\n| --- | --- | --- |\"\n",
    "            rows = \"\\n\".join([f\"| {col} | {dtype} | {desc} |\" for col, dtype, desc in columns])\n",
    "            \n",
    "            # Add a note if columns were dropped\n",
    "            fqtn_plain = fqtn.replace('`', '')\n",
    "            if fqtn_plain in column_tracking_cache:\n",
    "                note = f\"<!-- NOTE: Table has {len(columns)} columns (subset used for context fitting) -->\"\n",
    "                markdown_parts.append(f\"{table_header}\\n{note}\\n{header}\\n{rows}\\n\")\n",
    "            else:\n",
    "                markdown_parts.append(f\"{table_header}\\n{header}\\n{rows}\\n\")\n",
    "        \n",
    "        return \"\\n\".join(markdown_parts)\n",
    "\n",
    "    def _augment_columns_with_foreign_keys(self, column_details: list) -> list:\n",
    "        if not column_details or not self.data_loader or not getattr(self.data_loader, \"foreign_key_graph\", None):\n",
    "            return column_details\n",
    "        tables_present = {(c, s, t) for (c, s, t, _, _, _) in column_details}\n",
    "        additional = []\n",
    "        for base_table in list(tables_present):\n",
    "            relations = self.data_loader.foreign_key_graph.get(base_table, [])\n",
    "            for rel in relations:\n",
    "                ref_catalog = rel[4] or base_table[0]\n",
    "                ref_schema = rel[5] or base_table[1]\n",
    "                ref_table = rel[6]\n",
    "                ref_tuple = (ref_catalog, ref_schema, ref_table)\n",
    "                if ref_tuple in tables_present:\n",
    "                    continue\n",
    "                try:\n",
    "                    ref_cols = self.data_loader._get_table_details(\n",
    "                        ref_catalog,\n",
    "                        ref_schema,\n",
    "                        ref_table,\n",
    "                        apply_sampling=self.data_loader.enable_column_sampling\n",
    "                    )\n",
    "                    if ref_cols:\n",
    "                        additional.extend(ref_cols)\n",
    "                        tables_present.add(ref_tuple)\n",
    "                        self.logger.debug(f\"Auto-added foreign key table {ref_tuple} to batch context\")\n",
    "                except Exception as e:\n",
    "                    self.logger.debug(f\"Could not load foreign key table {ref_tuple}: {str(e)[:80]}\")\n",
    "        if additional:\n",
    "            return column_details + additional\n",
    "        return column_details\n",
    "\n",
    "    def _get_reverse_foreign_key_graph(self):\n",
    "        if not self.data_loader or not getattr(self.data_loader, \"foreign_key_graph\", None):\n",
    "            return {}\n",
    "        graph_size = len(self.data_loader.foreign_key_graph)\n",
    "        cached_size = getattr(self, \"_reverse_foreign_key_graph_size\", -1)\n",
    "        if hasattr(self, \"_reverse_foreign_key_graph\") and cached_size == graph_size:\n",
    "            return self._reverse_foreign_key_graph\n",
    "        reverse_graph = defaultdict(list)\n",
    "        for src_key, rels in self.data_loader.foreign_key_graph.items():\n",
    "            for rel in rels:\n",
    "                ref_catalog = rel[4] or src_key[0]\n",
    "                ref_schema = rel[5] or src_key[1]\n",
    "                ref_table = rel[6]\n",
    "                ref_key = (ref_catalog, ref_schema, ref_table)\n",
    "                reverse_graph[ref_key].append(rel)\n",
    "        self._reverse_foreign_key_graph = reverse_graph\n",
    "        self._reverse_foreign_key_graph_size = graph_size\n",
    "        return reverse_graph\n",
    "\n",
    "    def _augment_columns_with_reference_tables(self, column_details: list, reference_tables_set: set) -> list:\n",
    "        if not column_details or not self.data_loader or not getattr(self.data_loader, \"foreign_key_graph\", None):\n",
    "            return column_details\n",
    "        if not reference_tables_set:\n",
    "            return column_details\n",
    "        tables_present = {(c, s, t) for (c, s, t, _, _, _) in column_details}\n",
    "        additional = []\n",
    "        reverse_graph = self._get_reverse_foreign_key_graph()\n",
    "        for base_table in list(tables_present):\n",
    "            relations = self.data_loader.foreign_key_graph.get(base_table, [])\n",
    "            for rel in relations:\n",
    "                ref_catalog = rel[4] or base_table[0]\n",
    "                ref_schema = rel[5] or base_table[1]\n",
    "                ref_table = rel[6]\n",
    "                ref_fqtn = f\"{ref_catalog}.{ref_schema}.{ref_table}\"\n",
    "                if ref_fqtn not in reference_tables_set:\n",
    "                    continue\n",
    "                ref_tuple = (ref_catalog, ref_schema, ref_table)\n",
    "                if ref_tuple in tables_present:\n",
    "                    continue\n",
    "                try:\n",
    "                    ref_cols = self.data_loader._get_table_details(\n",
    "                        ref_catalog,\n",
    "                        ref_schema,\n",
    "                        ref_table,\n",
    "                        apply_sampling=self.data_loader.enable_column_sampling\n",
    "                    )\n",
    "                    if ref_cols:\n",
    "                        additional.extend(ref_cols)\n",
    "                        tables_present.add(ref_tuple)\n",
    "                except Exception as e:\n",
    "                    self.logger.debug(f\"Could not load reference table {ref_tuple}: {str(e)[:80]}\")\n",
    "            reverse_relations = reverse_graph.get(base_table, [])\n",
    "            for rel in reverse_relations:\n",
    "                src_catalog = rel[0]\n",
    "                src_schema = rel[1]\n",
    "                src_table = rel[2]\n",
    "                src_fqtn = f\"{src_catalog}.{src_schema}.{src_table}\"\n",
    "                if src_fqtn not in reference_tables_set:\n",
    "                    continue\n",
    "                src_tuple = (src_catalog, src_schema, src_table)\n",
    "                if src_tuple in tables_present:\n",
    "                    continue\n",
    "                try:\n",
    "                    ref_cols = self.data_loader._get_table_details(\n",
    "                        src_catalog,\n",
    "                        src_schema,\n",
    "                        src_table,\n",
    "                        apply_sampling=self.data_loader.enable_column_sampling\n",
    "                    )\n",
    "                    if ref_cols:\n",
    "                        additional.extend(ref_cols)\n",
    "                        tables_present.add(src_tuple)\n",
    "                except Exception as e:\n",
    "                    self.logger.debug(f\"Could not load reference table {src_tuple}: {str(e)[:80]}\")\n",
    "        if additional:\n",
    "            return column_details + additional\n",
    "        return column_details\n",
    "\n",
    "    def _augment_columns_with_related_tables(self, column_details: list) -> list:\n",
    "        if not column_details or not self.data_loader or not getattr(self.data_loader, \"foreign_key_graph\", None):\n",
    "            return column_details\n",
    "        allowed_tables = None\n",
    "        if hasattr(self, \"data_category_map\"):\n",
    "            allowed_tables = {k for k, v in self.data_category_map.items() if v != \"TECHNICAL\"}\n",
    "        tables_present = {(c, s, t) for (c, s, t, _, _, _) in column_details}\n",
    "        additional = []\n",
    "        reverse_graph = self._get_reverse_foreign_key_graph()\n",
    "        for base_table in list(tables_present):\n",
    "            relations = self.data_loader.foreign_key_graph.get(base_table, [])\n",
    "            for rel in relations:\n",
    "                ref_catalog = rel[4] or base_table[0]\n",
    "                ref_schema = rel[5] or base_table[1]\n",
    "                ref_table = rel[6]\n",
    "                ref_fqtn = f\"{ref_catalog}.{ref_schema}.{ref_table}\"\n",
    "                if allowed_tables is not None and ref_fqtn not in allowed_tables:\n",
    "                    continue\n",
    "                ref_tuple = (ref_catalog, ref_schema, ref_table)\n",
    "                if ref_tuple in tables_present:\n",
    "                    continue\n",
    "                try:\n",
    "                    ref_cols = self.data_loader._get_table_details(\n",
    "                        ref_catalog,\n",
    "                        ref_schema,\n",
    "                        ref_table,\n",
    "                        apply_sampling=self.data_loader.enable_column_sampling\n",
    "                    )\n",
    "                    if ref_cols:\n",
    "                        additional.extend(ref_cols)\n",
    "                        tables_present.add(ref_tuple)\n",
    "                except Exception as e:\n",
    "                    self.logger.debug(f\"Could not load related table {ref_tuple}: {str(e)[:80]}\")\n",
    "            reverse_relations = reverse_graph.get(base_table, [])\n",
    "            for rel in reverse_relations:\n",
    "                src_catalog = rel[0]\n",
    "                src_schema = rel[1]\n",
    "                src_table = rel[2]\n",
    "                src_fqtn = f\"{src_catalog}.{src_schema}.{src_table}\"\n",
    "                if allowed_tables is not None and src_fqtn not in allowed_tables:\n",
    "                    continue\n",
    "                src_tuple = (src_catalog, src_schema, src_table)\n",
    "                if src_tuple in tables_present:\n",
    "                    continue\n",
    "                try:\n",
    "                    ref_cols = self.data_loader._get_table_details(\n",
    "                        src_catalog,\n",
    "                        src_schema,\n",
    "                        src_table,\n",
    "                        apply_sampling=self.data_loader.enable_column_sampling\n",
    "                    )\n",
    "                    if ref_cols:\n",
    "                        additional.extend(ref_cols)\n",
    "                        tables_present.add(src_tuple)\n",
    "                except Exception as e:\n",
    "                    self.logger.debug(f\"Could not load related table {src_tuple}: {str(e)[:80]}\")\n",
    "        if additional:\n",
    "            return column_details + additional\n",
    "        return column_details\n",
    "\n",
    "    def _expand_tables_with_foreign_keys(self, tables: set):\n",
    "        if not tables or not self.data_loader or not getattr(self.data_loader, \"foreign_key_graph\", None):\n",
    "            return tables, []\n",
    "        expanded = set()\n",
    "        relationships = []\n",
    "        for tbl in tables:\n",
    "            expanded.add(tbl)\n",
    "            cat, sch, tbl_name = parse_three_level_name(tbl)\n",
    "            if not (cat and sch and tbl_name):\n",
    "                continue\n",
    "            key = (cat, sch, tbl_name)\n",
    "            for rel in self.data_loader.foreign_key_graph.get(key, []):\n",
    "                ref_catalog = rel[4] or cat\n",
    "                ref_schema = rel[5] or sch\n",
    "                ref_table = rel[6]\n",
    "                ref_str = f\"{ref_catalog}.{ref_schema}.{ref_table}\"\n",
    "                expanded.add(ref_str)\n",
    "                relationships.append(f\"{cat}.{sch}.{tbl_name}.{rel[3]} → {ref_catalog}.{ref_schema}.{ref_table}.{rel[7]}\")\n",
    "        return expanded, relationships\n",
    "\n",
    "    def _apply_progressive_truncation(self, use_case_id: str, directly_involved_details: list, \n",
    "                                      additional_details: list, unstructured_docs: str, \n",
    "                                      max_schema_size: int, base_prompt_size: int,\n",
    "                                      directly_involved_tables: set = None) -> tuple:\n",
    "        \"\"\"\n",
    "        Apply progressive truncation strategy to fit schema within context limits.\n",
    "        \n",
    "        NEW STRATEGY (Updated):\n",
    "        1. Drop unstructured documents and check if it fits\n",
    "        2. Truncate tables with >250 columns to 250 columns (ALL tables, including directly involved)\n",
    "        3. If still doesn't fit, take your chances anyway and send the request to the LLM\n",
    "           - The LLM might succeed despite exceeding the limit\n",
    "        4. If SQL generation fails, the calling code should:\n",
    "           a. Switch model from Opus to Sonnet and retry\n",
    "           b. Last resort: Remove this use case and fetch the highest-rated deduplicated use case\n",
    "        \n",
    "        Args:\n",
    "            directly_involved_tables: Set of table names that are directly involved in the query\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (directly_involved_schema, additional_schema, final_unstructured_docs, was_truncated)\n",
    "        \"\"\"\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        # Initialize directly_involved_tables if not provided\n",
    "        if directly_involved_tables is None:\n",
    "            directly_involved_tables = set()\n",
    "        \n",
    "        # Get business scores (default to 50 if not available)\n",
    "        business_scores = getattr(self, 'business_scores', {})\n",
    "        \n",
    "        def is_table_directly_involved(table_name: str) -> bool:\n",
    "            \"\"\"Check if a table is directly involved in the query (must be protected).\"\"\"\n",
    "            if not directly_involved_tables:\n",
    "                return False\n",
    "            \n",
    "            # Normalize table name for comparison (remove backticks)\n",
    "            clean_table = table_name.replace('`', '')\n",
    "            \n",
    "            # Check against all directly involved tables\n",
    "            for involved_table in directly_involved_tables:\n",
    "                clean_involved = involved_table.replace('`', '')\n",
    "                if clean_table == clean_involved:\n",
    "                    return True\n",
    "            return False\n",
    "        \n",
    "        def group_columns_by_table(details):\n",
    "            \"\"\"Group column details by table.\"\"\"\n",
    "            tables = defaultdict(list)\n",
    "            for detail in details:\n",
    "                catalog, schema, table = detail[0], detail[1], detail[2]\n",
    "                fqtn = f\"{catalog}.{schema}.{table}\"\n",
    "                tables[fqtn].append(detail)\n",
    "            return tables\n",
    "        \n",
    "        def rebuild_details_from_tables(tables_dict):\n",
    "            \"\"\"Flatten table dict back to detail list.\"\"\"\n",
    "            details = []\n",
    "            for table_name in sorted(tables_dict.keys()):\n",
    "                details.extend(tables_dict[table_name])\n",
    "            return details\n",
    "        \n",
    "        def get_table_score(table_name):\n",
    "            \"\"\"Get business score for a table (0-100).\"\"\"\n",
    "            # Normalize table name (remove backticks)\n",
    "            clean_name = table_name.replace('`', '')\n",
    "            return business_scores.get(clean_name, 50)  # Default to 50 if not found\n",
    "        \n",
    "        def calculate_total_size(directly_inv_details, additional_det, unstructured):\n",
    "            \"\"\"Calculate total size of schema context.\"\"\"\n",
    "            # Load column tracking for SQL generation\n",
    "            directly_schema = self._format_schema_for_prompt(directly_inv_details, load_column_tracking=True)\n",
    "            additional_schema = \"\"\n",
    "            if additional_det:\n",
    "                additional_schema = self._format_schema_for_prompt(additional_det, load_column_tracking=True)\n",
    "            return len(directly_schema) + len(additional_schema) + len(unstructured)\n",
    "        \n",
    "        # Initial check\n",
    "        total_size = calculate_total_size(directly_involved_details, additional_details, unstructured_docs)\n",
    "        target_size = max_schema_size + len(unstructured_docs)  # Total allowed\n",
    "        \n",
    "        if total_size <= target_size:\n",
    "            # No truncation needed\n",
    "            directly_schema = self._format_schema_for_prompt(directly_involved_details, load_column_tracking=True)\n",
    "            additional_schema = \"\"\n",
    "            if additional_details:\n",
    "                additional_schema = self._format_schema_for_prompt(additional_details, load_column_tracking=True)\n",
    "            return (directly_schema, additional_schema, unstructured_docs, False)\n",
    "        \n",
    "        self.logger.warning(f\"Use case {use_case_id}: Schema exceeds limit. Starting progressive truncation...\")\n",
    "        \n",
    "        # STEP 1: Drop unstructured documents\n",
    "        self.logger.info(f\"Use case {use_case_id}: Step 1 - Dropping unstructured documents\")\n",
    "        unstructured_docs_truncated = \"\"\n",
    "        total_size = calculate_total_size(directly_involved_details, additional_details, unstructured_docs_truncated)\n",
    "        \n",
    "        if total_size <= target_size:\n",
    "            self.logger.info(f\"Use case {use_case_id}: Fits after dropping unstructured docs (size: {total_size:,} chars)\")\n",
    "            directly_schema = self._format_schema_for_prompt(directly_involved_details, load_column_tracking=True)\n",
    "            additional_schema = \"\"\n",
    "            if additional_details:\n",
    "                additional_schema = self._format_schema_for_prompt(additional_details, load_column_tracking=True)\n",
    "            return (directly_schema, additional_schema, unstructured_docs_truncated, True)\n",
    "        \n",
    "        # STEP 2: Truncate tables with >250 columns to 250 columns\n",
    "        # CRITICAL: Truncate ALL tables (including directly involved) if they exceed 250 columns\n",
    "        self.logger.info(f\"Use case {use_case_id}: Step 2 - Truncating tables >250 columns to 250 columns\")\n",
    "        \n",
    "        all_tables = group_columns_by_table(directly_involved_details)\n",
    "        truncated_count = 0\n",
    "        \n",
    "        for table_name, columns in list(all_tables.items()):\n",
    "            if len(columns) > 250:\n",
    "                all_tables[table_name] = columns[:250]\n",
    "                truncated_count += 1\n",
    "                self.logger.debug(f\"Use case {use_case_id}: Truncated {table_name} from {len(columns)} to 250 columns\")\n",
    "        \n",
    "        directly_involved_details = rebuild_details_from_tables(all_tables)\n",
    "        total_size = calculate_total_size(directly_involved_details, [], unstructured_docs_truncated)\n",
    "        \n",
    "        if total_size <= target_size:\n",
    "            self.logger.info(f\"Use case {use_case_id}: Fits after truncating {truncated_count} tables to 250 columns (size: {total_size:,} chars)\")\n",
    "            directly_schema = self._format_schema_for_prompt(directly_involved_details, load_column_tracking=True)\n",
    "            additional_schema = \"\"  # No additional tables left\n",
    "            return (directly_schema, additional_schema, unstructured_docs_truncated, True)\n",
    "        \n",
    "        # STEP 3: Take your chances anyway - send the request to the LLM\n",
    "        # The LLM might succeed despite exceeding the context limit\n",
    "        # If it fails, the calling code should:\n",
    "        #   a. Switch model from Opus to Sonnet and retry\n",
    "        #   b. Last resort: Remove this use case and fetch highest-rated deduplicated use case\n",
    "        self.logger.warning(f\"Use case {use_case_id}: Context still exceeds limit after truncation (size: {total_size:,} chars, limit: {target_size:,} chars)\")\n",
    "        self.logger.warning(f\"Use case {use_case_id}: Taking chances anyway - sending request to LLM (it might succeed)\")\n",
    "        \n",
    "        directly_schema = self._format_schema_for_prompt(directly_involved_details, load_column_tracking=True)\n",
    "        additional_schema = \"\"  # No additional tables\n",
    "        return (directly_schema, additional_schema, unstructured_docs_truncated, True)\n",
    "\n",
    "    def _sanitize_name(self, name: str) -> str:\n",
    "        if not name: return \"_\"\n",
    "        s = re.sub(r'[^a-z0-9_]', '_', str(name).lower())\n",
    "        s = re.sub(r'_+', '_', s).strip('_')\n",
    "        return s or \"_\"\n",
    "\n",
    "    def _save_usecases_catalog_json(self, final_consolidated_use_cases: list, english_translations: dict, summary_dict: dict = None) -> dict:\n",
    "        \"\"\"\n",
    "        Saves the usecases_catalog.json file with all the content for later doc generation.\n",
    "        Returns the summary_dict used (computed if not provided).\n",
    "        \n",
    "        JSON Structure:\n",
    "        {\n",
    "            \"title\": \"...\",\n",
    "            \"executive_summary\": \"...\",\n",
    "            \"domains\": \"domain1: X use cases, domain2: Y use cases, ...\",\n",
    "            \"domains\": [\n",
    "                {\n",
    "                    \"summary\": \"...\",\n",
    "                    \"use_cases\": [\n",
    "                        {id, title, type, statement, etc...}\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Generating JSON Catalog...\")\n",
    "            \n",
    "            # Group use cases by domain\n",
    "            flat_english_use_cases = final_consolidated_use_cases\n",
    "            _unsorted_grouped = self._group_use_cases_by_domain_flat(flat_english_use_cases)\n",
    "            english_grouped_data = {k: _unsorted_grouped[k] for k in sorted(_unsorted_grouped.keys())}\n",
    "            \n",
    "            # Get summary if not provided\n",
    "            if summary_dict is None:\n",
    "                (summary_dict, transliterated_name) = self._get_salesy_summary(english_grouped_data, self.business_name, \"English\", english_translations)\n",
    "            \n",
    "            # Build domain summary string\n",
    "            domain_counts = []\n",
    "            for domain_name, use_cases in english_grouped_data.items():\n",
    "                domain_counts.append(f\"{domain_name}: {len(use_cases)} use cases\")\n",
    "            domains_summary = \", \".join(domain_counts)\n",
    "            \n",
    "            # === NEW: Build Column Bitmap and replace names with IDs ===\n",
    "            import copy\n",
    "            column_registry = {}\n",
    "            # Format: ID -> \"FQN, Description\"\n",
    "            for col_id, info in self.id_column_map.items():\n",
    "                column_registry[col_id] = f\"{info['fqn']}, {info['description']}\"\n",
    "\n",
    "            # === NEW: Build Table Registry ===\n",
    "            table_registry = {}\n",
    "            # Format: ID -> Table FQN\n",
    "            for table_id, table_fqn in self.id_table_map.items():\n",
    "                table_registry[table_id] = table_fqn\n",
    "\n",
    "            # Create deep copy of use cases to modify for JSON output\n",
    "            json_english_grouped_data = copy.deepcopy(english_grouped_data)\n",
    "\n",
    "            # Pre-compute table -> column IDs map for faster lookup\n",
    "            table_to_col_ids = defaultdict(list)\n",
    "            for fqn, cid in self.column_id_map.items():\n",
    "                # fqn is catalog.schema.table.column\n",
    "                parts = fqn.split('.')\n",
    "                if len(parts) >= 2:\n",
    "                    table_fqn = \".\".join(parts[:-1])\n",
    "                    table_to_col_ids[table_fqn].append(cid)\n",
    "\n",
    "            # Replace column names with IDs in the JSON copy\n",
    "            for domain in json_english_grouped_data:\n",
    "                for uc in json_english_grouped_data[domain]:\n",
    "                    # 1. Process Columns Involved (Specific columns)\n",
    "                    cols_involved = uc.get(\"Columns Involved\", \"\")\n",
    "                    if cols_involved:\n",
    "                        # Split by comma or newline\n",
    "                        col_names = [c.strip() for c in re.split(r'[,\\n]', cols_involved) if c.strip()]\n",
    "                        col_ids = []\n",
    "                        for name in col_names:\n",
    "                            # 1. Try exact FQN match\n",
    "                            if name in self.column_id_map:\n",
    "                                col_ids.append(self.column_id_map[name])\n",
    "                            else:\n",
    "                                # 2. Fuzzy match: try to find a column ending with name\n",
    "                                found = False\n",
    "                                for fqn, cid in self.column_id_map.items():\n",
    "                                    if fqn.endswith(f\".{name}\"):\n",
    "                                         col_ids.append(cid)\n",
    "                                         found = True\n",
    "                                         break\n",
    "                                \n",
    "                                # 3. Super fuzzy: just check if name is in FQN (risky but better than nothing for IDs)\n",
    "                                if not found:\n",
    "                                    for fqn, cid in self.column_id_map.items():\n",
    "                                        if name in fqn:\n",
    "                                            col_ids.append(cid)\n",
    "                                            found = True\n",
    "                                            break\n",
    "\n",
    "                                if not found:\n",
    "                                    # Fallback: keep original name if not found in registry\n",
    "                                    col_ids.append(name)\n",
    "                        \n",
    "                        uc[\"Columns Involved\"] = \", \".join(col_ids)\n",
    "\n",
    "                    # 1b. Process Involved Columns (same logic as Columns Involved)\n",
    "                    involved_cols = uc.get(\"Involved Columns\", \"\")\n",
    "                    if involved_cols:\n",
    "                        col_names = [c.strip() for c in re.split(r'[,\\n]', involved_cols) if c.strip()]\n",
    "                        col_ids = []\n",
    "                        for name in col_names:\n",
    "                            if name in self.column_id_map:\n",
    "                                col_ids.append(self.column_id_map[name])\n",
    "                            else:\n",
    "                                found = False\n",
    "                                for fqn, cid in self.column_id_map.items():\n",
    "                                    if fqn.endswith(f\".{name}\"):\n",
    "                                         col_ids.append(cid)\n",
    "                                         found = True\n",
    "                                         break\n",
    "                                \n",
    "                                if not found:\n",
    "                                    for fqn, cid in self.column_id_map.items():\n",
    "                                        if name in fqn:\n",
    "                                            col_ids.append(cid)\n",
    "                                            found = True\n",
    "                                            break\n",
    "\n",
    "                                if not found:\n",
    "                                    col_ids.append(name)\n",
    "                        \n",
    "                        uc[\"Involved Columns\"] = \", \".join(col_ids)\n",
    "\n",
    "                    # 2. Process directly_involved_schema (Full table schemas -> IDs)\n",
    "                    # Requirement: directly_involved_schema should only have comma separated list of column ids from the registry\n",
    "                    involved_tables = uc.get('_directly_involved_tables', [])\n",
    "                    schema_col_ids = set()\n",
    "                    table_ids = []\n",
    "                    if involved_tables:\n",
    "                        for table in involved_tables:\n",
    "                            # table is usually FQN from discovery\n",
    "                            if table in table_to_col_ids:\n",
    "                                schema_col_ids.update(table_to_col_ids[table])\n",
    "                            # Convert table name to table ID\n",
    "                            if table in self.table_id_map:\n",
    "                                table_ids.append(self.table_id_map[table])\n",
    "                    \n",
    "                    # Always overwrite directly_involved_schema with IDs string (or empty string)\n",
    "                    if schema_col_ids:\n",
    "                         uc['directly_involved_schema'] = \", \".join(sorted(list(schema_col_ids), key=lambda x: int(x) if x.isdigit() else float('inf')))\n",
    "                    else:\n",
    "                         uc['directly_involved_schema'] = \"\"\n",
    "                    \n",
    "                    # Convert directly_involved_tables to table IDs string\n",
    "                    if table_ids:\n",
    "                        uc['directly_involved_tables'] = \", \".join(sorted(table_ids, key=lambda x: int(x) if x.isdigit() else float('inf')))\n",
    "                    else:\n",
    "                        uc['directly_involved_tables'] = \"\"\n",
    "                    \n",
    "                    # Remove old underscore-prefixed keys\n",
    "                    if '_directly_involved_schema' in uc:\n",
    "                        del uc['_directly_involved_schema']\n",
    "                    if '_directly_involved_tables' in uc:\n",
    "                        del uc['_directly_involved_tables']\n",
    "                    # Remove generated/validated from JSON - these are now stored in notebook cells\n",
    "                    if 'generated' in uc:\n",
    "                        del uc['generated']\n",
    "                    if 'validated' in uc:\n",
    "                        del uc['validated']\n",
    "            \n",
    "            # Build the JSON structure\n",
    "            catalog_json = {\n",
    "                \"business_name\": self.business_name,\n",
    "                \"title\": f\"{self.business_name} Use Cases Catalog\",\n",
    "                \"executive_summary\": summary_dict.get(\"Executive\", \"\"),\n",
    "                \"domains_summary\": domains_summary,\n",
    "                \"column_registry\": column_registry,\n",
    "                \"table_registry\": table_registry,\n",
    "                \"metadata\": {\n",
    "                    \"catalogs\": [c.strip() for c in self.catalogs_str.split(',') if c.strip()],\n",
    "                    \"schemas\": [s.strip() for s in self.schemas_str.split(',') if s.strip()],\n",
    "                    \"generation_path\": self.generation_path\n",
    "                },\n",
    "                \"domains\": []\n",
    "            }\n",
    "            \n",
    "            # Add each domain from the ID-ified data\n",
    "            for domain_name, use_cases in json_english_grouped_data.items():\n",
    "                domain_obj = {\n",
    "                    \"domain_name\": domain_name,\n",
    "                    \"summary\": summary_dict.get(domain_name, f\"Domain: {domain_name} with {len(use_cases)} use cases\"),\n",
    "                    \"use_cases\": use_cases\n",
    "                }\n",
    "                catalog_json[\"domains\"].append(domain_obj)\n",
    "            \n",
    "            # Save to workspace\n",
    "            json_path = os.path.join(self.docs_output_dir, f\"{self.business_name}-dbx_inspire.json\")\n",
    "            self.logger.info(f\"Saving JSON Catalog to: {json_path}\")\n",
    "            \n",
    "            json_content = json.dumps(catalog_json, indent=2, ensure_ascii=False)\n",
    "            json_data_b64 = base64.b64encode(json_content.encode('utf-8')).decode()\n",
    "            \n",
    "            self.w_client.workspace.import_(\n",
    "                path=json_path,\n",
    "                content=json_data_b64,\n",
    "                format=workspace.ImportFormat.AUTO,\n",
    "                overwrite=True\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f\"✅ JSON Catalog saved successfully to {json_path}\")\n",
    "            log_print(f\"✅ JSON Catalog saved to: {json_path}\")\n",
    "            \n",
    "            return summary_dict\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to save JSON Catalog: {e}\")\n",
    "            return summary_dict\n",
    "\n",
    "    def _run_queries_fixing_mode(self):\n",
    "        \"\"\"\n",
    "        SQL Regeneration Mode: Scans notebooks for failed SQL and regenerates them.\n",
    "        \n",
    "        This method:\n",
    "        1. Scans all notebooks in the notebook output directory\n",
    "        2. Finds SQL cells with \"Regenerate:Yes\" in the Inspire header\n",
    "        3. Loads the JSON file to get schema info for regeneration\n",
    "        4. Regenerates SQL using the WAVE PATTERN (same as normal generation)\n",
    "        5. Updates the notebook cells with new SQL and \"Regenerate:No\"\n",
    "        \n",
    "        NOTEBOOKS are the source of truth for SQL status.\n",
    "        \n",
    "        Can be run multiple times until no failed queries remain.\n",
    "        \"\"\"\n",
    "        import json\n",
    "        import base64\n",
    "        import re\n",
    "        from collections import defaultdict\n",
    "        from databricks.sdk.service import workspace\n",
    "        \n",
    "        log_print(f\"\\n\uD83D\uDCD3 SCANNING NOTEBOOKS FOR FAILED SQL...\")\n",
    "        self.logger.info(f\"Scanning notebooks in: {self.notebook_output_dir}\")\n",
    "        \n",
    "        json_file_path = os.path.join(self.docs_output_dir, f\"{self.business_name}-dbx_inspire.json\")\n",
    "        \n",
    "        log_print(f\"\uD83D\uDCC1 JSON File (for schema): {json_file_path}\")\n",
    "        self.logger.info(f\"Loading schema from: {json_file_path}\")\n",
    "        \n",
    "        try:\n",
    "            file_info = self.w_client.workspace.export(path=json_file_path, format=workspace.ExportFormat.AUTO)\n",
    "            json_content = base64.b64decode(file_info.content).decode('utf-8')\n",
    "            catalog_json = json.loads(json_content)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load JSON file: {e}\")\n",
    "            log_print(f\"❌ Failed to load JSON file: {e}\")\n",
    "            return\n",
    "        \n",
    "        json_business_name = catalog_json.get(\"business_name\", None)\n",
    "        if json_business_name:\n",
    "            self.business_name = json_business_name\n",
    "            self.logger.info(f\"Using business name from JSON: '{json_business_name}'\")\n",
    "        \n",
    "        json_generation_path = catalog_json.get(\"metadata\", {}).get(\"generation_path\", None)\n",
    "        if json_generation_path:\n",
    "            self.generation_path = json_generation_path\n",
    "            self.logger.info(f\"Using generation path from JSON: '{json_generation_path}'\")\n",
    "        \n",
    "        column_registry = catalog_json.get(\"column_registry\", {})\n",
    "        if not column_registry:\n",
    "            self.logger.error(\"No column_registry in JSON. Cannot regenerate SQL.\")\n",
    "            log_print(\"❌ No column_registry found in JSON file.\", level=\"ERROR\")\n",
    "            return\n",
    "        \n",
    "        self.logger.info(f\"Building schema from JSON column_registry ({len(column_registry)} columns)...\")\n",
    "        log_print(f\"\uD83D\uDCCA Building schema from JSON ({len(column_registry)} columns - NO database discovery)\")\n",
    "        \n",
    "        id_to_info = {}\n",
    "        full_schema_details = []\n",
    "        schema_by_table = defaultdict(list)\n",
    "        \n",
    "        for cid, val in column_registry.items():\n",
    "            parts = val.split(\",\", 1)\n",
    "            fqn = parts[0].strip()\n",
    "            description = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "            \n",
    "            fqn_parts = fqn.split(\".\")\n",
    "            if len(fqn_parts) >= 4:\n",
    "                catalog = fqn_parts[0].strip('`')\n",
    "                schema = fqn_parts[1].strip('`')\n",
    "                table = fqn_parts[2].strip('`')\n",
    "                column_name = \".\".join(fqn_parts[3:]).strip('`')\n",
    "            elif len(fqn_parts) == 3:\n",
    "                catalog = \"\"\n",
    "                schema = fqn_parts[0].strip('`')\n",
    "                table = fqn_parts[1].strip('`')\n",
    "                column_name = fqn_parts[2].strip('`')\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            desc_parts = description.split(\" - \", 1)\n",
    "            data_type = desc_parts[0].strip() if desc_parts else \"STRING\"\n",
    "            comment = desc_parts[1].strip() if len(desc_parts) > 1 else \"\"\n",
    "            \n",
    "            detail = (catalog, schema, table, column_name, data_type, comment)\n",
    "            full_schema_details.append(detail)\n",
    "            \n",
    "            fqtn = f\"{catalog}.{schema}.{table}\" if catalog else f\"{schema}.{table}\"\n",
    "            fqtn_backticks = f\"`{catalog}`.`{schema}`.`{table}`\" if catalog else f\"`{schema}`.`{table}`\"\n",
    "            schema_by_table[fqtn].append(detail)\n",
    "            schema_by_table[fqtn_backticks].append(detail)\n",
    "            \n",
    "            id_to_info[cid] = {\n",
    "                'fqn': fqn,\n",
    "                'catalog': catalog,\n",
    "                'schema': schema,\n",
    "                'table': table,\n",
    "                'column': column_name,\n",
    "                'data_type': data_type,\n",
    "                'comment': comment\n",
    "            }\n",
    "        \n",
    "        self.logger.info(f\"Rebuilt schema: {len(full_schema_details)} columns across {len(schema_by_table)//2} tables\")\n",
    "        \n",
    "        # === CRITICAL FIX: Initialize lightweight DataLoader for dynamic table loading ===\n",
    "        # This enables loading tables requested by users in regeneration instructions\n",
    "        # that weren't in the original generation (e.g., \"join with table X\")\n",
    "        if self.data_loader is None:\n",
    "            self.logger.info(\"\uD83D\uDD27 Initializing DataLoader for dynamic table loading in SQL Regeneration mode...\")\n",
    "            log_print(f\"   \uD83D\uDCE5 Enabling dynamic table loading for user-requested tables\")\n",
    "            try:\n",
    "                # Extract unique catalogs from the existing schema\n",
    "                existing_catalogs = set()\n",
    "                for detail in full_schema_details:\n",
    "                    if detail[0]:  # catalog name\n",
    "                        existing_catalogs.add(detail[0])\n",
    "                catalogs_str = \",\".join(sorted(existing_catalogs)) if existing_catalogs else \"\"\n",
    "                \n",
    "                self.data_loader = DataLoader(\n",
    "                    catalogs=catalogs_str,\n",
    "                    schemas=\"\",  # Don't restrict schemas - allow any table lookup\n",
    "                    tables=\"\",   # Don't restrict tables - allow any table lookup\n",
    "                    logger=self.logger,\n",
    "                    enable_two_pass=False,  # No bulk discovery needed\n",
    "                    enable_column_sampling=False,  # Get full column list for requested tables\n",
    "                    streaming_batch_size=100,\n",
    "                    max_parallelism=self.scan_parallelism,\n",
    "                    schema_timeout_seconds=300  # 5 min timeout for individual table lookups\n",
    "                )\n",
    "                self.logger.info(f\"✅ DataLoader initialized for catalogs: {catalogs_str if catalogs_str else '(all)'}\")\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"⚠️ Could not initialize DataLoader for dynamic loading: {e}\")\n",
    "                self.logger.warning(\"   User-requested tables not in JSON will cause hallucinated columns!\")\n",
    "                log_print(f\"   ⚠️ Dynamic table loading unavailable - user-requested tables may have hallucinated columns\", level=\"WARNING\")\n",
    "        \n",
    "        # Build lookup from JSON use cases for schema info\n",
    "        all_use_cases = []\n",
    "        use_case_lookup = {}\n",
    "        domain_lookup = {}\n",
    "        \n",
    "        for domain_idx, domain_obj in enumerate(catalog_json.get(\"domains\", []), start=1):\n",
    "            domain_name = domain_obj.get(\"domain_name\", \"General Operations\")\n",
    "            use_cases = domain_obj.get(\"use_cases\", [])\n",
    "            \n",
    "            for uc in use_cases:\n",
    "                schema_ids = uc.get(\"directly_involved_schema\", \"\") or uc.get(\"_directly_involved_schema\", \"\")\n",
    "                if schema_ids:\n",
    "                    schema_col_ids = [p.strip() for p in schema_ids.split(\",\")]\n",
    "                    schema_lines = []\n",
    "                    tables_seen = set()\n",
    "                    \n",
    "                    for cid in schema_col_ids:\n",
    "                        if cid in id_to_info:\n",
    "                            info = id_to_info[cid]\n",
    "                            fqtn = f\"{info['catalog']}.{info['schema']}.{info['table']}\" if info['catalog'] else f\"{info['schema']}.{info['table']}\"\n",
    "                            \n",
    "                            if fqtn not in tables_seen:\n",
    "                                if tables_seen:\n",
    "                                    schema_lines.append(\"\")\n",
    "                                schema_lines.append(f\"Table: {fqtn}\")\n",
    "                                schema_lines.append(\"Columns:\")\n",
    "                                tables_seen.add(fqtn)\n",
    "                            \n",
    "                            col_desc = f\"  - {info['column']} ({info['data_type']})\"\n",
    "                            if info['comment']:\n",
    "                                col_desc += f\": {info['comment']}\"\n",
    "                            schema_lines.append(col_desc)\n",
    "                    \n",
    "                    uc[\"directly_involved_schema\"] = \"\\n\".join(schema_lines)\n",
    "                \n",
    "                uc['Business Domain'] = domain_name\n",
    "                all_use_cases.append(uc)\n",
    "                uc_id = uc.get('No', '')\n",
    "                use_case_lookup[uc_id] = uc\n",
    "                # CRITICAL FIX: Extract domain prefix from use case ID, not from JSON domain order\n",
    "                # Use case IDs like N15-AI01 must match their notebook N15-xxx.ipynb\n",
    "                try:\n",
    "                    domain_prefix = uc_id.split('-')[0] if uc_id and '-' in uc_id else f\"N{domain_idx:02d}\"\n",
    "                except Exception:\n",
    "                    domain_prefix = f\"N{domain_idx:02d}\"\n",
    "                domain_lookup[uc_id] = (domain_name, domain_prefix)\n",
    "        \n",
    "        # === SCAN NOTEBOOKS FOR FAILED SQL ===\n",
    "        failed_use_cases = []\n",
    "        notebook_status = {}  # uc_id -> (notebook_path, generated, validated)\n",
    "        \n",
    "        # Log what use case IDs are in the lookup for debugging\n",
    "        self.logger.info(f\"Use case IDs in lookup from JSON: {list(use_case_lookup.keys())[:20]}...\")  # First 20\n",
    "        \n",
    "        log_print(f\"\\n\uD83D\uDD0D Scanning notebooks for SQL status...\")\n",
    "        \n",
    "        try:\n",
    "            notebook_list = list(self.w_client.workspace.list(self.notebook_output_dir))\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to list notebooks in {self.notebook_output_dir}: {e}\")\n",
    "            log_print(f\"❌ Failed to list notebooks: {e}\", level=\"ERROR\")\n",
    "            return\n",
    "        \n",
    "        # Match use case ID from header and find regenerate_sql status anywhere in the cell\n",
    "        # Also support legacy Regenerate: format for backwards compatibility\n",
    "        use_case_id_pattern = re.compile(r'--Use Case:\\s*([A-Za-z0-9_-]+)\\s*-', re.IGNORECASE)\n",
    "        regenerate_sql_pattern = re.compile(r'(?:regenerate_sql|Regenerate):\\s*(Yes|No)', re.IGNORECASE)\n",
    "        instructions_pattern = re.compile(r'/\\*\\*Regeneration Instruction Start\\s*(.*?)\\s*Regeneration Instruction End\\*\\*/', re.DOTALL)\n",
    "        \n",
    "        for item in notebook_list:\n",
    "            if not item.path.endswith('.ipynb'):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                file_info = self.w_client.workspace.export(path=item.path, format=workspace.ExportFormat.JUPYTER)\n",
    "                notebook_json_str = base64.b64decode(file_info.content).decode('utf-8')\n",
    "                notebook_json = json.loads(notebook_json_str)\n",
    "                \n",
    "                for cell in notebook_json.get('cells', []):\n",
    "                    if cell.get('cell_type') != 'code':\n",
    "                        continue\n",
    "                    \n",
    "                    source = cell.get('source', [])\n",
    "                    if isinstance(source, list):\n",
    "                        cell_content = ''.join(source)\n",
    "                    else:\n",
    "                        cell_content = source\n",
    "                    \n",
    "                    # Find use case ID from header\n",
    "                    uc_match = use_case_id_pattern.search(cell_content)\n",
    "                    if uc_match:\n",
    "                        uc_id = uc_match.group(1).strip()\n",
    "                        \n",
    "                        # Find regenerate_sql status anywhere in the cell\n",
    "                        regen_match = regenerate_sql_pattern.search(cell_content)\n",
    "                        regenerate_sql = regen_match.group(1) if regen_match else 'No'\n",
    "                        \n",
    "                        user_instructions = \"\"\n",
    "                        instructions_match = instructions_pattern.search(cell_content)\n",
    "                        if instructions_match:\n",
    "                            user_instructions = instructions_match.group(1).strip()\n",
    "                        \n",
    "                        needs_regenerate = (regenerate_sql.lower() == 'yes')\n",
    "                        notebook_status[uc_id] = (item.path, regenerate_sql, user_instructions)\n",
    "                        self.logger.info(f\"[{uc_id}] Found: regenerate_sql={regenerate_sql}, NeedsRegen={needs_regenerate}\")\n",
    "                        \n",
    "                        in_lookup = uc_id in use_case_lookup\n",
    "                        self.logger.info(f\"[{uc_id}] needs_regenerate={needs_regenerate}, in_lookup={in_lookup}\")\n",
    "                        \n",
    "                        if needs_regenerate and not in_lookup:\n",
    "                            self.logger.warning(f\"[{uc_id}] SKIPPED: Use case needs regeneration but NOT found in JSON lookup!\")\n",
    "                        \n",
    "                        if needs_regenerate and in_lookup:\n",
    "                            uc = use_case_lookup[uc_id].copy()\n",
    "                            uc['_notebook_path'] = item.path\n",
    "                            uc['_notebook_regenerate'] = regenerate_sql\n",
    "                            uc['_user_instructions'] = user_instructions\n",
    "                            failed_use_cases.append(uc)\n",
    "                            if user_instructions:\n",
    "                                self.logger.info(f\"[{uc_id}] Found in notebook with regenerate_sql={regenerate_sql}, User Instructions: {user_instructions[:100]}... -> NEEDS REGENERATION\")\n",
    "                                log_print(f\"   \uD83D\uDCDD [{uc_id}] regenerate_sql:Yes with instructions: {user_instructions[:80]}...\")\n",
    "                            else:\n",
    "                                self.logger.info(f\"[{uc_id}] Found in notebook with regenerate_sql={regenerate_sql} -> NEEDS REGENERATION\")\n",
    "                                log_print(f\"   \uD83D\uDD04 [{uc_id}] regenerate_sql:Yes\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                self.logger.debug(f\"Could not parse notebook {item.path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        log_print(f\"   • Notebooks scanned: {len(notebook_list)}\")\n",
    "        log_print(f\"   • Use cases found in notebooks: {len(notebook_status)}\")\n",
    "        \n",
    "        total_use_cases = len(all_use_cases)\n",
    "        failed_count = len(failed_use_cases)\n",
    "        \n",
    "        log_print(f\"\\n\uD83D\uDCCA USE CASE STATISTICS:\")\n",
    "        log_print(f\"   • Total Use Cases: {total_use_cases}\")\n",
    "        log_print(f\"   • Failed/Missing SQL: {failed_count}\")\n",
    "        log_print(f\"   • Valid SQL: {total_use_cases - failed_count}\")\n",
    "        \n",
    "        if failed_count == 0:\n",
    "            log_print(f\"\\n✅ SUCCESS: All {total_use_cases} use cases have valid SQL!\")\n",
    "            log_print(f\"   No queries need to be fixed.\")\n",
    "            self.logger.info(f\"No failed queries found. All {total_use_cases} use cases have valid SQL.\")\n",
    "            return\n",
    "        \n",
    "        log_print(f\"\\n\uD83D\uDD27 REGENERATING SQL FOR {failed_count} FAILED USE CASES (using wave pattern):\")\n",
    "        for uc in failed_use_cases:\n",
    "            uc_id = uc.get('No', 'UNKNOWN')\n",
    "            uc_name = uc.get('Name', '')\n",
    "            user_instructions = uc.get('_user_instructions', '')\n",
    "            if user_instructions:\n",
    "                log_print(f\"   • [{uc_id}] {uc_name} (with user instructions)\")\n",
    "            else:\n",
    "                log_print(f\"   • [{uc_id}] {uc_name}\")\n",
    "        \n",
    "        # === NEW: INTERPRET USER INSTRUCTIONS BEFORE SQL GENERATION (PARALLEL) ===\n",
    "        # For use cases with user instructions, run them through the interpretation prompt first\n",
    "        use_cases_with_instructions = [uc for uc in failed_use_cases if uc.get('_user_instructions', '').strip()]\n",
    "        \n",
    "        if use_cases_with_instructions:\n",
    "            log_print(f\"\\n\uD83D\uDD0D INTERPRETING USER INSTRUCTIONS FOR {len(use_cases_with_instructions)} USE CASES (parallel, max {self.max_parallelism} workers)...\")\n",
    "            self.logger.info(f\"Interpreting user instructions for {len(use_cases_with_instructions)} use cases in parallel...\")\n",
    "            \n",
    "            # Build available tables registry from column_registry (done once, shared by all workers)\n",
    "            available_tables = set()\n",
    "            for cid, val in column_registry.items():\n",
    "                parts = val.split(\",\", 1)\n",
    "                fqn = parts[0].strip()\n",
    "                fqn_parts = fqn.split(\".\")\n",
    "                if len(fqn_parts) >= 3:\n",
    "                    catalog = fqn_parts[0].strip('`')\n",
    "                    schema_name = fqn_parts[1].strip('`')\n",
    "                    table_name = fqn_parts[2].strip('`')\n",
    "                    fqtn = f\"{catalog}.{schema_name}.{table_name}\"\n",
    "                    available_tables.add(fqtn)\n",
    "            \n",
    "            available_tables_list = sorted(list(available_tables))\n",
    "            available_tables_registry = \"\\n\".join([f\"- {t}\" for t in available_tables_list])\n",
    "            \n",
    "            def interpret_single_use_case(uc: dict) -> dict:\n",
    "                \"\"\"Interpret user instructions for a single use case. Returns the updated use case.\"\"\"\n",
    "                uc_id = uc.get('No', 'UNKNOWN')\n",
    "                user_instructions = uc.get('_user_instructions', '')\n",
    "                previous_sql = uc.get('SQL', '')\n",
    "                \n",
    "                self.logger.info(f\"[{uc_id}] Interpreting user instructions: {user_instructions[:100]}...\")\n",
    "                \n",
    "                interpret_prompt_vars = {\n",
    "                    \"use_case_id\": uc_id,\n",
    "                    \"use_case_name\": uc.get('Name', ''),\n",
    "                    \"business_domain\": uc.get('Business Domain', ''),\n",
    "                    \"statement\": uc.get('Statement', ''),\n",
    "                    \"solution\": uc.get('Solution', ''),\n",
    "                    \"original_tables_involved\": uc.get('Tables Involved', ''),\n",
    "                    \"previous_sql\": previous_sql if previous_sql else \"(No previous SQL)\",\n",
    "                    \"available_tables_registry\": available_tables_registry,\n",
    "                    \"user_regeneration_instructions\": user_instructions\n",
    "                }\n",
    "                \n",
    "                try:\n",
    "                    interpretation_response = self.ai_agent.run_worker(\n",
    "                        step_name=f\"Interpret_Instructions_{uc_id}\",\n",
    "                        worker_prompt_path=\"INTERPRET_USER_SQL_REGENERATION_PROMPT\",\n",
    "                        prompt_vars=interpret_prompt_vars,\n",
    "                        response_schema=None\n",
    "                    )\n",
    "                    \n",
    "                    # Parse the JSON response\n",
    "                    interpretation_response_clean = clean_json_response(interpretation_response)\n",
    "                    interpretation_json = json.loads(interpretation_response_clean)\n",
    "                    \n",
    "                    # Extract interpreted information\n",
    "                    interpretation_summary = interpretation_json.get('interpretation_summary', '')\n",
    "                    tables_to_add = interpretation_json.get('tables_to_add', [])\n",
    "                    tables_to_remove = interpretation_json.get('tables_to_remove', [])\n",
    "                    final_tables_involved = interpretation_json.get('final_tables_involved', [])\n",
    "                    new_tables_need_loading = interpretation_json.get('new_tables_need_loading', False)\n",
    "                    technical_design_instructions = interpretation_json.get('technical_design_instructions', '')\n",
    "                    special_requirements = interpretation_json.get('special_requirements', '')\n",
    "                    \n",
    "                    self.logger.info(f\"[{uc_id}] Interpretation: {interpretation_summary[:100]}...\")\n",
    "                    self.logger.info(f\"[{uc_id}] Tables to add: {tables_to_add}\")\n",
    "                    self.logger.info(f\"[{uc_id}] Tables to remove: {tables_to_remove}\")\n",
    "                    self.logger.info(f\"[{uc_id}] Final tables: {final_tables_involved}\")\n",
    "                    \n",
    "                    # Update the use case's Tables Involved if interpretation changed them\n",
    "                    if final_tables_involved:\n",
    "                        uc['Tables Involved'] = ', '.join(final_tables_involved)\n",
    "                        self.logger.info(f\"[{uc_id}] Updated Tables Involved to: {uc['Tables Involved']}\")\n",
    "                    \n",
    "                    # Determine which tables need schema loaded - check FINAL tables, not just tables_to_add\n",
    "                    # The LLM may return tables_to_add=[] but have new tables in final_tables_involved\n",
    "                    self.logger.info(f\"[{uc_id}] Checking {len(final_tables_involved)} tables against registry ({len(schema_by_table)} entries)...\")\n",
    "                    tables_needing_schema = []\n",
    "                    for tbl in final_tables_involved:\n",
    "                        tbl_clean = tbl.replace('`', '').strip()\n",
    "                        tbl_variants = [tbl_clean, f\"`{tbl_clean.replace('.', '`.`')}`\"]\n",
    "                        found_in_registry = any(v in schema_by_table for v in tbl_variants)\n",
    "                        self.logger.debug(f\"[{uc_id}] Table '{tbl_clean}' - in registry: {found_in_registry}\")\n",
    "                        if not found_in_registry:\n",
    "                            tables_needing_schema.append(tbl_clean)\n",
    "                            self.logger.info(f\"[{uc_id}] \uD83D\uDD0D Table '{tbl_clean}' NOT in registry - will load from database\")\n",
    "                    \n",
    "                    # Also include any explicitly listed tables_to_add\n",
    "                    for tbl in tables_to_add:\n",
    "                        tbl_clean = tbl.replace('`', '').strip()\n",
    "                        if tbl_clean not in tables_needing_schema:\n",
    "                            tables_needing_schema.append(tbl_clean)\n",
    "                    \n",
    "                    self.logger.info(f\"[{uc_id}] Tables needing dynamic load: {len(tables_needing_schema)} - {tables_needing_schema}\")\n",
    "                    \n",
    "                    if tables_needing_schema:\n",
    "                        self.logger.info(f\"[{uc_id}] Loading schema for {len(tables_needing_schema)} tables not in registry: {tables_needing_schema}\")\n",
    "                        additional_schema_lines = []\n",
    "                        tables_loaded = []\n",
    "                        tables_not_found = []\n",
    "                        \n",
    "                        for tbl_name in tables_needing_schema:\n",
    "                            # Find columns for this table in schema_by_table\n",
    "                            table_variants = [tbl_name, f\"`{tbl_name.replace('.', '`.`')}`\"]\n",
    "                            found = False\n",
    "                            for variant in table_variants:\n",
    "                                if variant in schema_by_table:\n",
    "                                    additional_schema_lines.append(f\"\\nTable: {tbl_name}\")\n",
    "                                    additional_schema_lines.append(\"Columns:\")\n",
    "                                    for detail in schema_by_table[variant]:\n",
    "                                        cat, sch_name, tbl_nm, column_name, data_type, comment = detail\n",
    "                                        col_desc = f\"  - {column_name} ({data_type})\"\n",
    "                                        if comment:\n",
    "                                            col_desc += f\": {comment}\"\n",
    "                                        additional_schema_lines.append(col_desc)\n",
    "                                    tables_loaded.append(tbl_name)\n",
    "                                    found = True\n",
    "                                    break\n",
    "                            \n",
    "                            if not found:\n",
    "                                # Table not in registry - try to load dynamically from database\n",
    "                                self.logger.info(f\"[{uc_id}] \uD83D\uDCE5 Loading table '{tbl_name}' from database (not in JSON registry)...\")\n",
    "                                log_print(f\"   \uD83D\uDCE5 [{uc_id}] Loading schema for: {tbl_name}\")\n",
    "                                try:\n",
    "                                    tbl_parts = tbl_name.replace('`', '').split('.')\n",
    "                                    if len(tbl_parts) == 3:\n",
    "                                        cat, sch_name, tbl_nm = tbl_parts\n",
    "                                        self.logger.info(f\"[{uc_id}] Parsed table: catalog={cat}, schema={sch_name}, table={tbl_nm}\")\n",
    "                                        # Check if spark and data_loader are available for dynamic loading\n",
    "                                        if hasattr(self, 'data_loader') and self.data_loader is not None:\n",
    "                                            self.logger.info(f\"[{uc_id}] DataLoader available - calling _get_table_details...\")\n",
    "                                            dynamic_details = self.data_loader._get_table_details(cat, sch_name, tbl_nm, apply_sampling=False)\n",
    "                                            if dynamic_details:\n",
    "                                                self.logger.info(f\"[{uc_id}] ✅ Dynamically loaded {len(dynamic_details)} columns for table '{tbl_name}'\")\n",
    "                                                additional_schema_lines.append(f\"\\nTable: {tbl_name}\")\n",
    "                                                additional_schema_lines.append(\"Columns:\")\n",
    "                                                for detail in dynamic_details:\n",
    "                                                    d_cat, d_sch, d_tbl, column_name, data_type, comment = detail\n",
    "                                                    col_desc = f\"  - {column_name} ({data_type})\"\n",
    "                                                    if comment:\n",
    "                                                        col_desc += f\": {comment}\"\n",
    "                                                    additional_schema_lines.append(col_desc)\n",
    "                                                # Store for later merge into full_schema_details\n",
    "                                                if '_dynamic_column_details' not in uc:\n",
    "                                                    uc['_dynamic_column_details'] = []\n",
    "                                                uc['_dynamic_column_details'].extend(dynamic_details)\n",
    "                                                # Also add to schema_by_table for future lookups\n",
    "                                                schema_by_table[tbl_name] = dynamic_details\n",
    "                                                schema_by_table[f\"`{cat}`.`{sch_name}`.`{tbl_nm}`\"] = dynamic_details\n",
    "                                                tables_loaded.append(tbl_name)\n",
    "                                                found = True\n",
    "                                            else:\n",
    "                                                self.logger.warning(f\"[{uc_id}] Table '{tbl_name}' exists but returned no columns\")\n",
    "                                                tables_not_found.append(tbl_name)\n",
    "                                        else:\n",
    "                                            self.logger.warning(f\"[{uc_id}] DataLoader not available for dynamic table loading\")\n",
    "                                            tables_not_found.append(tbl_name)\n",
    "                                    else:\n",
    "                                        self.logger.warning(f\"[{uc_id}] Invalid table name format '{tbl_name}' - expected catalog.schema.table\")\n",
    "                                        tables_not_found.append(tbl_name)\n",
    "                                except Exception as e:\n",
    "                                    self.logger.warning(f\"[{uc_id}] Failed to load table '{tbl_name}' dynamically: {e}\")\n",
    "                                    tables_not_found.append(tbl_name)\n",
    "                        \n",
    "                        if additional_schema_lines:\n",
    "                            existing_schema = uc.get('directly_involved_schema', '')\n",
    "                            uc['directly_involved_schema'] = existing_schema + \"\\n\" + \"\\n\".join(additional_schema_lines)\n",
    "                            self.logger.info(f\"[{uc_id}] Successfully loaded schema for tables: {tables_loaded}\")\n",
    "                        \n",
    "                        if tables_not_found:\n",
    "                            self.logger.error(f\"[{uc_id}] ⚠️ CRITICAL: Could not load schema for tables: {tables_not_found}. SQL generation may produce invalid column names!\")\n",
    "                            log_print(f\"   ❌ [{uc_id}] FAILED to load tables: {', '.join(tables_not_found)}\", level=\"ERROR\")\n",
    "                            uc['_tables_not_found'] = tables_not_found\n",
    "                    \n",
    "                    # Build the interpreted regeneration context for the SQL generation prompt\n",
    "                    tables_not_found_warning = \"\"\n",
    "                    if tables_not_found:\n",
    "                        tables_not_found_warning = f\"\"\"\n",
    "**⛔⛔⛔ CRITICAL WARNING: SCHEMA NOT AVAILABLE FOR REQUESTED TABLES ⛔⛔⛔**\n",
    "\n",
    "The following tables requested by the user could NOT be loaded from Unity Catalog:\n",
    "{', '.join(tables_not_found)}\n",
    "\n",
    "**YOU MUST NOT USE THESE TABLES** - their schema is not available.\n",
    "DO NOT hallucinate column names for these tables.\n",
    "Instead, generate SQL using ONLY the tables that have schema available below.\n",
    "If the user's request REQUIRES these tables and cannot be fulfilled without them,\n",
    "generate a comment explaining which tables are missing.\n",
    "\n",
    "\"\"\"\n",
    "                    interpreted_context = f\"\"\"\n",
    "**\uD83D\uDD25\uD83D\uDD25\uD83D\uDD25 REGENERATION MODE - USER INSTRUCTIONS INTERPRETED \uD83D\uDD25\uD83D\uDD25\uD83D\uDD25**\n",
    "\n",
    "The user has provided regeneration instructions which have been interpreted into the following technical requirements:\n",
    "{tables_not_found_warning}\n",
    "**INTERPRETATION SUMMARY:**\n",
    "{interpretation_summary}\n",
    "\n",
    "**FINAL TABLES TO USE:**\n",
    "{', '.join(final_tables_involved) if final_tables_involved else uc.get('Tables Involved', '')}\n",
    "\n",
    "**TECHNICAL DESIGN INSTRUCTIONS (MUST FOLLOW):**\n",
    "{technical_design_instructions}\n",
    "\n",
    "**SPECIAL REQUIREMENTS:**\n",
    "{special_requirements if special_requirements else 'None'}\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL: You MUST follow the technical design instructions above. They take precedence over the default solution approach. \uD83D\uDEA8**\n",
    "\"\"\"\n",
    "                    uc['_interpreted_regeneration_context'] = interpreted_context\n",
    "                    uc['_interpretation_status'] = 'success'\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"[{uc_id}] Failed to interpret user instructions: {e}. Proceeding with raw instructions.\")\n",
    "                    # Fallback: use raw instructions in a simpler format\n",
    "                    uc['_interpreted_regeneration_context'] = f\"\"\"\n",
    "**\uD83D\uDD25 REGENERATION MODE - USER INSTRUCTIONS \uD83D\uDD25**\n",
    "\n",
    "The user has provided the following instructions for regenerating this SQL query. You MUST follow these instructions:\n",
    "\n",
    "**USER INSTRUCTIONS:**\n",
    "{user_instructions}\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL: Follow the user's instructions above. They take precedence over the default solution approach. \uD83D\uDEA8**\n",
    "\"\"\"\n",
    "                    uc['_interpretation_status'] = 'fallback'\n",
    "                \n",
    "                return uc\n",
    "            \n",
    "            # Execute interpretation in parallel using ThreadPoolExecutor\n",
    "            # ADAPTIVE PARALLELISM: Calculate based on use cases to interpret\n",
    "            from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "            \n",
    "            interpretation_parallelism, reason = calculate_adaptive_parallelism(\n",
    "                \"sql_generation\", self.max_parallelism,\n",
    "                num_items=len(use_cases_with_instructions),\n",
    "                is_llm_operation=True, logger=self.logger\n",
    "            )\n",
    "            log_adaptive_parallelism_decision(\"sql_generation\", interpretation_parallelism, self.max_parallelism, reason)\n",
    "            \n",
    "            interpretation_results = {}\n",
    "            with ThreadPoolExecutor(max_workers=interpretation_parallelism, thread_name_prefix=\"InterpretInstr\") as executor:\n",
    "                future_to_uc = {executor.submit(interpret_single_use_case, uc): uc.get('No', 'UNKNOWN') for uc in use_cases_with_instructions}\n",
    "                \n",
    "                completed_count = 0\n",
    "                for future in as_completed(future_to_uc):\n",
    "                    uc_id = future_to_uc[future]\n",
    "                    try:\n",
    "                        updated_uc = future.result(timeout=120)  # 2 min timeout per interpretation\n",
    "                        interpretation_results[uc_id] = updated_uc\n",
    "                        completed_count += 1\n",
    "                        status = updated_uc.get('_interpretation_status', 'unknown')\n",
    "                        if status == 'success':\n",
    "                            log_print(f\"   ✅ [{uc_id}] Interpretation complete ({completed_count}/{len(use_cases_with_instructions)})\")\n",
    "                        else:\n",
    "                            log_print(f\"   ⚠️ [{uc_id}] Using fallback instructions ({completed_count}/{len(use_cases_with_instructions)})\")\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"[{uc_id}] Interpretation failed with error: {e}\")\n",
    "                        log_print(f\"   ❌ [{uc_id}] Interpretation error: {str(e)[:50]}\")\n",
    "                        # Find the original use case and apply fallback\n",
    "                        for uc in use_cases_with_instructions:\n",
    "                            if uc.get('No') == uc_id:\n",
    "                                user_instr = uc.get('_user_instructions', '')\n",
    "                                uc['_interpreted_regeneration_context'] = f\"\"\"\n",
    "**\uD83D\uDD25 REGENERATION MODE - USER INSTRUCTIONS \uD83D\uDD25**\n",
    "\n",
    "The user has provided the following instructions for regenerating this SQL query. You MUST follow these instructions:\n",
    "\n",
    "**USER INSTRUCTIONS:**\n",
    "{user_instr}\n",
    "\n",
    "**\uD83D\uDEA8 CRITICAL: Follow the user's instructions above. They take precedence over the default solution approach. \uD83D\uDEA8**\n",
    "\"\"\"\n",
    "                                interpretation_results[uc_id] = uc\n",
    "                                break\n",
    "            \n",
    "            # Update the failed_use_cases list with interpreted results\n",
    "            for i, uc in enumerate(failed_use_cases):\n",
    "                uc_id = uc.get('No', 'UNKNOWN')\n",
    "                if uc_id in interpretation_results:\n",
    "                    failed_use_cases[i] = interpretation_results[uc_id]\n",
    "            \n",
    "            log_print(f\"   ✅ All {len(use_cases_with_instructions)} interpretations complete\")\n",
    "        \n",
    "        # Merge dynamically loaded column details into full_schema_details for SQL generation\n",
    "        dynamic_columns_added = 0\n",
    "        dynamic_tables_added = set()\n",
    "        for uc in failed_use_cases:\n",
    "            dynamic_details = uc.get('_dynamic_column_details', [])\n",
    "            if dynamic_details:\n",
    "                # Avoid duplicates by checking if table is already in full_schema_details\n",
    "                existing_tables = set()\n",
    "                for detail in full_schema_details:\n",
    "                    existing_tables.add(f\"{detail[0]}.{detail[1]}.{detail[2]}\")\n",
    "                \n",
    "                for detail in dynamic_details:\n",
    "                    fqtn = f\"{detail[0]}.{detail[1]}.{detail[2]}\"\n",
    "                    if fqtn not in existing_tables:\n",
    "                        full_schema_details.append(detail)\n",
    "                        dynamic_columns_added += 1\n",
    "                        dynamic_tables_added.add(fqtn)\n",
    "                        existing_tables.add(fqtn)\n",
    "        \n",
    "        if dynamic_columns_added > 0:\n",
    "            self.logger.info(f\"\uD83D\uDCE5 Merged {dynamic_columns_added} dynamically loaded columns from {len(dynamic_tables_added)} tables into schema\")\n",
    "            for tbl in sorted(dynamic_tables_added):\n",
    "                col_count = sum(1 for d in full_schema_details if f\"{d[0]}.{d[1]}.{d[2]}\" == tbl)\n",
    "                self.logger.info(f\"   \uD83D\uDCE5 {tbl}: {col_count} columns loaded\")\n",
    "        \n",
    "        self.logger.info(f\"Starting SQL regeneration using WAVE PATTERN for {failed_count} failed use cases...\")\n",
    "        \n",
    "        regenerated_use_cases = self._generate_sql_parallel(\n",
    "            failed_use_cases, \n",
    "            full_schema_details, \n",
    "            \"\"\n",
    "        )\n",
    "        \n",
    "        regenerated_count = 0\n",
    "        still_failed_count = 0\n",
    "        succeeded_list = []\n",
    "        failed_list = []\n",
    "        \n",
    "        for result in regenerated_use_cases:\n",
    "            uc_id = result.get('No', 'UNKNOWN')\n",
    "            uc_name = result.get('Name', '')\n",
    "            gen_status = result.get('generated', 'N')\n",
    "            val_status = result.get('validated', 'D')\n",
    "            \n",
    "            is_success = gen_status == 'Y' and val_status in ['Y', 'D']\n",
    "            if is_success:\n",
    "                regenerated_count += 1\n",
    "                succeeded_list.append((uc_id, uc_name, gen_status, val_status))\n",
    "                self.logger.info(f\"[{uc_id}] SQL regenerated successfully (generated={gen_status}, validated={val_status})\")\n",
    "            else:\n",
    "                still_failed_count += 1\n",
    "                failed_list.append((uc_id, uc_name, gen_status, val_status))\n",
    "                self.logger.warning(f\"[{uc_id}] SQL regeneration failed (generated={gen_status}, validated={val_status})\")\n",
    "            \n",
    "            for orig_uc in all_use_cases:\n",
    "                if orig_uc.get('No') == uc_id:\n",
    "                    orig_uc['SQL'] = result.get('SQL', '')\n",
    "                    orig_uc['sql_generation_status'] = result.get('sql_generation_status', '')\n",
    "                    orig_uc['sql_validation_status'] = result.get('sql_validation_status', '')\n",
    "                    orig_uc['generated'] = gen_status\n",
    "                    orig_uc['validated'] = val_status\n",
    "                    break\n",
    "        \n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"\uD83D\uDCCA QUERIES REGENERATION - FINAL REPORT\")\n",
    "        log_print(f\"{'='*80}\")\n",
    "        log_print(f\"\\n\uD83D\uDCC8 SUMMARY:\")\n",
    "        log_print(f\"   • Total attempted: {len(regenerated_use_cases)}\")\n",
    "        log_print(f\"   • ✅ Successfully regenerated: {regenerated_count}\")\n",
    "        log_print(f\"   • ❌ Still failed: {still_failed_count}\")\n",
    "        \n",
    "        if succeeded_list:\n",
    "            log_print(f\"\\n✅ SUCCEEDED ({len(succeeded_list)}):\")\n",
    "            for uc_id, uc_name, gen, val in succeeded_list:\n",
    "                log_print(f\"   • [{uc_id}] {uc_name} (generated={gen}, validated={val})\")\n",
    "        \n",
    "        if failed_list:\n",
    "            log_print(f\"\\n❌ STILL FAILED ({len(failed_list)}):\")\n",
    "            for uc_id, uc_name, gen, val in failed_list:\n",
    "                log_print(f\"   • [{uc_id}] {uc_name} (generated={gen}, validated={val})\")\n",
    "        \n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        \n",
    "        # Update JSON with new SQL (but NOT generated/validated - those are in notebooks now)\n",
    "        self.logger.info(\"Updating JSON file with regenerated SQL...\")\n",
    "        \n",
    "        for domain_obj in catalog_json.get(\"domains\", []):\n",
    "            domain_name = domain_obj.get(\"domain_name\", \"\")\n",
    "            for uc in domain_obj.get(\"use_cases\", []):\n",
    "                uc_id = uc.get('No', '')\n",
    "                for result in regenerated_use_cases:\n",
    "                    if result.get('No') == uc_id:\n",
    "                        uc['SQL'] = result.get('SQL', '')\n",
    "                        # Remove generated/validated from JSON - they are in notebooks\n",
    "                        if 'generated' in uc:\n",
    "                            del uc['generated']\n",
    "                        if 'validated' in uc:\n",
    "                            del uc['validated']\n",
    "                        break\n",
    "        \n",
    "        try:\n",
    "            updated_json = json.dumps(catalog_json, indent=2, ensure_ascii=False)\n",
    "            import_content = base64.b64encode(updated_json.encode('utf-8')).decode('utf-8')\n",
    "            self.w_client.workspace.import_(\n",
    "                path=json_file_path,\n",
    "                content=import_content,\n",
    "                format=workspace.ImportFormat.AUTO,\n",
    "                overwrite=True\n",
    "            )\n",
    "            log_print(f\"\\n✅ JSON file updated: {json_file_path}\")\n",
    "            self.logger.info(f\"JSON file updated successfully: {json_file_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to update JSON file: {e}\")\n",
    "            log_print(f\"⚠️ Failed to update JSON file: {e}\", level=\"WARNING\")\n",
    "        \n",
    "        # Update notebook cells with new SQL and reset header\n",
    "        # FIX: Batch updates by notebook to avoid lost updates from eventual consistency\n",
    "        # When multiple cells in the same notebook are updated sequentially, each load/save\n",
    "        # cycle can hit stale cache, causing later saves to overwrite earlier updates.\n",
    "        # Solution: Group by notebook, load ONCE, update ALL cells, save ONCE.\n",
    "        self.logger.info(\"Updating notebook cells with regenerated SQL (batched by notebook)...\")\n",
    "        log_print(f\"\\n\uD83D\uDCD3 UPDATING NOTEBOOKS (batched to prevent lost updates)...\")\n",
    "        \n",
    "        # Group use cases by notebook path\n",
    "        notebook_updates = {}  # notebook_path -> list of (use_case, domain_name, domain_prefix)\n",
    "        for result in regenerated_use_cases:\n",
    "            uc_id = result.get('No', 'UNKNOWN')\n",
    "            gen_status = result.get('generated', 'N')\n",
    "            val_status = result.get('validated', 'D')\n",
    "            \n",
    "            if gen_status == 'Y':\n",
    "                domain_info = domain_lookup.get(uc_id, ('General', 'N01'))\n",
    "                domain_name, domain_prefix = domain_info\n",
    "                result['generated'] = 'Y'\n",
    "                result['validated'] = 'Y' if val_status == 'Y' else ('Unknown' if val_status == 'D' else 'N')\n",
    "                \n",
    "                # Determine notebook path\n",
    "                uc_prefix_match = re.match(r'^(N\\d+)', uc_id)\n",
    "                actual_prefix = uc_prefix_match.group(1) if uc_prefix_match else domain_prefix\n",
    "                sanitized_domain = self._sanitize_name(domain_name)\n",
    "                notebook_name = f\"{actual_prefix}-{sanitized_domain}\"\n",
    "                notebook_path = os.path.join(self.notebook_output_dir, f\"{notebook_name}.ipynb\")\n",
    "                \n",
    "                if notebook_path not in notebook_updates:\n",
    "                    notebook_updates[notebook_path] = []\n",
    "                notebook_updates[notebook_path].append((result, domain_name, domain_prefix))\n",
    "        \n",
    "        # Update each notebook once with all its cells\n",
    "        notebooks_updated = 0\n",
    "        cells_updated = 0\n",
    "        for notebook_path, updates in notebook_updates.items():\n",
    "            batch_result = self._update_notebook_cells_batched(notebook_path, updates)\n",
    "            if batch_result > 0:\n",
    "                notebooks_updated += 1\n",
    "                cells_updated += batch_result\n",
    "        \n",
    "        self.logger.info(f\"Batched notebook update complete: {cells_updated} cells across {notebooks_updated} notebooks\")\n",
    "        \n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"✅ SQL REGENERATION COMPLETE\")\n",
    "        log_print(f\"{'='*80}\")\n",
    "        log_print(f\"   • SQL Regenerated: {regenerated_count}/{failed_count}\")\n",
    "        log_print(f\"   • Still Failed: {still_failed_count}\")\n",
    "        log_print(f\"   • Notebooks Updated: {notebooks_updated}\")\n",
    "        \n",
    "        if still_failed_count > 0:\n",
    "            log_print(f\"\\n⚠️ {still_failed_count} use cases still have failed SQL.\", level=\"WARNING\")\n",
    "            log_print(f\"   Run 'SQL Regeneration' mode again to retry these.\")\n",
    "        else:\n",
    "            log_print(f\"\\n\uD83C\uDF89 All use cases now have valid SQL!\")\n",
    "        \n",
    "        log_print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # === ALSO GENERATE SAMPLES IF generate_sample_result:Yes FOUND ===\n",
    "        # Re-generate SQL mode also handles sample generation (but not vice versa)\n",
    "        log_print(f\"\\n\uD83D\uDD0D Checking for generate_sample_result:Yes flags...\")\n",
    "        self.logger.info(\"Re-generate SQL mode: Also checking for sample generation requests...\")\n",
    "        try:\n",
    "            self._run_generate_sample_result_mode(called_from_sql_regen=True)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Sample generation after SQL regeneration failed: {e}\")\n",
    "            log_print(f\"⚠️ Sample generation encountered an issue: {e}\", level=\"WARNING\")\n",
    "        \n",
    "        self._upload_log_file()\n",
    "        AIAgent.get_summary_report()\n",
    "\n",
    "    def _update_notebook_cells_batched(self, notebook_path: str, updates: list) -> int:\n",
    "        \"\"\"\n",
    "        Update multiple cells in a single notebook with one load/save cycle.\n",
    "        \n",
    "        This prevents the lost update problem caused by eventual consistency in the\n",
    "        Databricks workspace API. When updating sequentially (load-update-save for each cell),\n",
    "        subsequent loads may return stale cached data, causing updates to be lost.\n",
    "        \n",
    "        Args:\n",
    "            notebook_path: Path to the notebook file\n",
    "            updates: List of tuples: (use_case_dict, domain_name, domain_prefix)\n",
    "            \n",
    "        Returns:\n",
    "            Number of cells successfully updated (0 if notebook load failed)\n",
    "        \"\"\"\n",
    "        import base64\n",
    "        import json\n",
    "        from databricks.sdk.service import workspace\n",
    "        \n",
    "        notebook_name = os.path.basename(notebook_path).replace('.ipynb', '')\n",
    "        self.logger.info(f\"[BATCH] Loading notebook: {notebook_path} to update {len(updates)} cells\")\n",
    "        \n",
    "        try:\n",
    "            file_info = self.w_client.workspace.export(path=notebook_path, format=workspace.ExportFormat.JUPYTER)\n",
    "            notebook_json_str = base64.b64decode(file_info.content).decode('utf-8')\n",
    "            notebook_json = json.loads(notebook_json_str)\n",
    "            self.logger.info(f\"[BATCH] Successfully loaded notebook with {len(notebook_json.get('cells', []))} cells\")\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"[BATCH] Could not load notebook {notebook_path}: {e}\")\n",
    "            return 0\n",
    "        \n",
    "        cells = notebook_json.get('cells', [])\n",
    "        inspire_header_pattern_template = r'--Use Case:\\s*{}'\n",
    "        generate_sample_pattern = re.compile(r'generate_sample_result:\\s*(Yes|No)', re.IGNORECASE)\n",
    "        \n",
    "        cells_updated = 0\n",
    "        \n",
    "        for use_case, domain_name, domain_prefix in updates:\n",
    "            uc_id = use_case.get('No', 'UNKNOWN')\n",
    "            sql_raw = use_case.get('SQL', '')\n",
    "            use_case_name = use_case.get('Name', '')\n",
    "            user_instructions = use_case.get('_user_instructions', '')\n",
    "            \n",
    "            sql_lines = sql_raw.split('\\n')\n",
    "            sql_lines_clean = []\n",
    "            skip_header = True\n",
    "            for line in sql_lines:\n",
    "                line_stripped = line.strip().lower()\n",
    "                if skip_header and (line_stripped.startswith('-- use case') or line_stripped.startswith('--use case')):\n",
    "                    continue\n",
    "                if skip_header and line_stripped.startswith('--') and not line_stripped.startswith('-- step') and not line_stripped.startswith('--step'):\n",
    "                    if len(line_stripped) > 2 and not any(kw in line_stripped for kw in ['with', 'select', 'cte', 'step']):\n",
    "                        continue\n",
    "                skip_header = False\n",
    "                sql_lines_clean.append(line)\n",
    "            sql = '\\n'.join(sql_lines_clean)\n",
    "            \n",
    "            if not sql:\n",
    "                self.logger.warning(f\"[{uc_id}] No SQL content to update\")\n",
    "                continue\n",
    "            \n",
    "            if user_instructions:\n",
    "                inspire_instructions_block = f\"/**Regeneration Instruction Start\\n{user_instructions}\\nRegeneration Instruction End**/\\n\\n\"\n",
    "            else:\n",
    "                inspire_instructions_block = \"/**Regeneration Instruction Start\\n\\nRegeneration Instruction End**/\\n\\n\"\n",
    "            \n",
    "            inspire_header_pattern = re.compile(inspire_header_pattern_template.format(re.escape(uc_id)))\n",
    "            \n",
    "            cell_found = False\n",
    "            for cell in cells:\n",
    "                if cell.get('cell_type') != 'code':\n",
    "                    continue\n",
    "                \n",
    "                source = cell.get('source', [])\n",
    "                if isinstance(source, list):\n",
    "                    cell_content = ''.join(source)\n",
    "                else:\n",
    "                    cell_content = source\n",
    "                \n",
    "                if inspire_header_pattern.search(cell_content):\n",
    "                    sample_match = generate_sample_pattern.search(cell_content)\n",
    "                    existing_sample_result = sample_match.group(1) if sample_match else 'No'\n",
    "                    \n",
    "                    updated_header = f\"--Use Case: {uc_id} - {use_case_name}\\n--generate_sample_result:{existing_sample_result}\\n--regenerate_sql:No\\n\"\n",
    "                    new_cell_content = updated_header + inspire_instructions_block + sql + \"\\n\"\n",
    "                    cell['source'] = [new_cell_content]\n",
    "                    cell_found = True\n",
    "                    cells_updated += 1\n",
    "                    self.logger.info(f\"[{uc_id}] Cell updated in batch (regenerate_sql:No, generate_sample_result:{existing_sample_result})\")\n",
    "                    break\n",
    "            \n",
    "            if not cell_found:\n",
    "                self.logger.warning(f\"[{uc_id}] Could not find matching cell in {notebook_path}\")\n",
    "        \n",
    "        if cells_updated > 0:\n",
    "            try:\n",
    "                updated_notebook_str = json.dumps(notebook_json, indent=2)\n",
    "                import_content = base64.b64encode(updated_notebook_str.encode('utf-8')).decode('utf-8')\n",
    "                self.w_client.workspace.import_(\n",
    "                    path=notebook_path,\n",
    "                    content=import_content,\n",
    "                    format=workspace.ImportFormat.JUPYTER,\n",
    "                    overwrite=True\n",
    "                )\n",
    "                self.logger.info(f\"[BATCH] Saved notebook {notebook_path} with {cells_updated} updated cells\")\n",
    "                log_print(f\"   ✅ [{notebook_name}] {cells_updated} cells updated (batched save)\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"[BATCH] Failed to save notebook {notebook_path}: {e}\")\n",
    "                return 0\n",
    "        \n",
    "        return cells_updated\n",
    "\n",
    "    def _update_notebook_sql_cell_with_header(self, use_case: dict, domain_name: str, domain_prefix: str) -> bool:\n",
    "        \"\"\"\n",
    "        Updates the SQL cell in an existing notebook with regenerated SQL and Inspire header.\n",
    "        \n",
    "        The header format is:\n",
    "        --Use Case: <ID> - <Name>\n",
    "        --Regenerate:No\n",
    "        /**Regeneration Instruction Start ... Regeneration Instruction End**/\n",
    "        \n",
    "        Returns True if the notebook was successfully updated.\n",
    "        \"\"\"\n",
    "        import base64\n",
    "        import json\n",
    "        from databricks.sdk.service import workspace\n",
    "        \n",
    "        uc_id = use_case.get('No', 'UNKNOWN')\n",
    "        sql_raw = use_case.get('SQL', '')\n",
    "        \n",
    "        # Strip LLM-generated header lines to avoid duplication (our header already has use case info)\n",
    "        sql_lines = sql_raw.split('\\n')\n",
    "        sql_lines_clean = []\n",
    "        skip_header = True\n",
    "        for line in sql_lines:\n",
    "            line_stripped = line.strip().lower()\n",
    "            if skip_header and (line_stripped.startswith('-- use case') or line_stripped.startswith('--use case')):\n",
    "                continue\n",
    "            if skip_header and line_stripped.startswith('--') and not line_stripped.startswith('-- step') and not line_stripped.startswith('--step'):\n",
    "                if len(line_stripped) > 2 and not any(kw in line_stripped for kw in ['with', 'select', 'cte', 'step']):\n",
    "                    continue\n",
    "            skip_header = False\n",
    "            sql_lines_clean.append(line)\n",
    "        sql = '\\n'.join(sql_lines_clean)\n",
    "        \n",
    "        if not sql:\n",
    "            self.logger.warning(f\"[{uc_id}] No SQL content to update\")\n",
    "            return False\n",
    "        \n",
    "        uc_prefix_match = re.match(r'^(N\\d+)', uc_id)\n",
    "        if uc_prefix_match:\n",
    "            actual_prefix = uc_prefix_match.group(1)\n",
    "        else:\n",
    "            actual_prefix = domain_prefix\n",
    "        \n",
    "        sanitized_domain = self._sanitize_name(domain_name)\n",
    "        notebook_name = f\"{actual_prefix}-{sanitized_domain}\"\n",
    "        notebook_path = os.path.join(self.notebook_output_dir, f\"{notebook_name}.ipynb\")\n",
    "        \n",
    "        self.logger.info(f\"[{uc_id}] Attempting to update notebook: {notebook_path}\")\n",
    "        \n",
    "        try:\n",
    "            file_info = self.w_client.workspace.export(path=notebook_path, format=workspace.ExportFormat.JUPYTER)\n",
    "            notebook_json_str = base64.b64decode(file_info.content).decode('utf-8')\n",
    "            notebook_json = json.loads(notebook_json_str)\n",
    "            self.logger.info(f\"[{uc_id}] Successfully loaded notebook with {len(notebook_json.get('cells', []))} cells\")\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"[{uc_id}] Could not find notebook for domain '{domain_name}' at {notebook_path}: {e}\")\n",
    "            return False\n",
    "        \n",
    "        cells = notebook_json.get('cells', [])\n",
    "        cell_updated = False\n",
    "        \n",
    "        use_case_name = use_case.get('Name', '')\n",
    "        \n",
    "        # Preserve user instructions if they were passed (from the notebook scan)\n",
    "        user_instructions = use_case.get('_user_instructions', '')\n",
    "        if user_instructions:\n",
    "            inspire_instructions_block = f\"/**Regeneration Instruction Start\\n{user_instructions}\\nRegeneration Instruction End**/\\n\\n\"\n",
    "        else:\n",
    "            inspire_instructions_block = \"/**Regeneration Instruction Start\\n\\nRegeneration Instruction End**/\\n\\n\"\n",
    "        \n",
    "        code_cells_count = 0\n",
    "        inspire_header_pattern = re.compile(r'--Use Case:\\s*' + re.escape(uc_id))\n",
    "        generate_sample_pattern = re.compile(r'generate_sample_result:\\s*(Yes|No)', re.IGNORECASE)\n",
    "        \n",
    "        for cell in cells:\n",
    "            if cell.get('cell_type') != 'code':\n",
    "                continue\n",
    "            \n",
    "            code_cells_count += 1\n",
    "            source = cell.get('source', [])\n",
    "            if isinstance(source, list):\n",
    "                cell_content = ''.join(source)\n",
    "            else:\n",
    "                cell_content = source\n",
    "            \n",
    "            # Match on Inspire header with this use case ID\n",
    "            if inspire_header_pattern.search(cell_content):\n",
    "                # Preserve existing generate_sample_result value\n",
    "                sample_match = generate_sample_pattern.search(cell_content)\n",
    "                existing_sample_result = sample_match.group(1) if sample_match else 'No'\n",
    "                \n",
    "                # Build header with regenerate_sql:No (just regenerated) but preserve generate_sample_result\n",
    "                updated_header = f\"--Use Case: {uc_id} - {use_case_name}\\n--generate_sample_result:{existing_sample_result}\\n--regenerate_sql:No\\n\"\n",
    "                \n",
    "                # Build new cell content with updated header and instructions block\n",
    "                new_cell_content = updated_header + inspire_instructions_block + sql + \"\\n\"\n",
    "                cell['source'] = [new_cell_content]\n",
    "                cell_updated = True\n",
    "                self.logger.info(f\"[{uc_id}] Found matching cell by Inspire header, updating SQL content (regenerate_sql:No, generate_sample_result:{existing_sample_result})\")\n",
    "                break\n",
    "        \n",
    "        if not cell_updated:\n",
    "            self.logger.warning(f\"[{uc_id}] Could not find Inspire header in notebook {notebook_path} (searched {code_cells_count} code cells)\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            updated_notebook_str = json.dumps(notebook_json, indent=2)\n",
    "            import_content = base64.b64encode(updated_notebook_str.encode('utf-8')).decode('utf-8')\n",
    "            self.w_client.workspace.import_(\n",
    "                path=notebook_path,\n",
    "                content=import_content,\n",
    "                format=workspace.ImportFormat.JUPYTER,\n",
    "                overwrite=True\n",
    "            )\n",
    "            self.logger.info(f\"Updated notebook cell for [{uc_id}] in {notebook_path}\")\n",
    "            log_print(f\"   ✅ [{uc_id}] Updated in {notebook_name}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to update notebook for [{uc_id}]: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _extract_clean_sql_error(self, error: Exception) -> str:\n",
    "        \"\"\"\n",
    "        Extract clean error message from SQL execution exception.\n",
    "        Removes JVM stack traces and internal details, keeping only the core error message.\n",
    "        \n",
    "        Args:\n",
    "            error: The exception raised during SQL execution\n",
    "            \n",
    "        Returns:\n",
    "            str: Clean, concise error message suitable for AI fix prompt\n",
    "        \"\"\"\n",
    "        error_str = str(error)\n",
    "        \n",
    "        if 'JVM stacktrace:' in error_str:\n",
    "            error_str = error_str.split('JVM stacktrace:')[0].strip()\n",
    "        \n",
    "        if 'SQLSTATE:' in error_str:\n",
    "            sqlstate_idx = error_str.find('SQLSTATE:')\n",
    "            semicolon_after = error_str.find(';', sqlstate_idx)\n",
    "            if semicolon_after != -1:\n",
    "                error_str = error_str[:semicolon_after + 20] if semicolon_after + 20 < len(error_str) else error_str[:semicolon_after + 1]\n",
    "        \n",
    "        lines = error_str.split('\\n')\n",
    "        clean_lines = []\n",
    "        for line in lines:\n",
    "            line_stripped = line.strip()\n",
    "            if line_stripped.startswith(\"'\") and ('+- ' in line_stripped or '   ' in line_stripped):\n",
    "                continue\n",
    "            if 'at org.apache' in line or 'at com.databricks' in line or 'at scala.' in line:\n",
    "                continue\n",
    "            if line_stripped:\n",
    "                clean_lines.append(line_stripped)\n",
    "        \n",
    "        clean_error = ' '.join(clean_lines[:5])\n",
    "        \n",
    "        if len(clean_error) > 500:\n",
    "            clean_error = clean_error[:500] + '...'\n",
    "        \n",
    "        return clean_error\n",
    "\n",
    "    def _fix_sql_with_retry(self, uc_id: str, uc_name: str, sql: str, error_msg: str, \n",
    "                            use_case_lookup: dict, schema_lookup: dict, max_retries: int = 2) -> tuple:\n",
    "        \"\"\"\n",
    "        Attempt to fix SQL using AI and retry execution.\n",
    "        \n",
    "        Args:\n",
    "            uc_id: Use case ID\n",
    "            uc_name: Use case name\n",
    "            sql: Original SQL query that failed\n",
    "            error_msg: Clean error message from execution failure\n",
    "            use_case_lookup: Dictionary mapping use case IDs to their details\n",
    "            schema_lookup: Dictionary mapping table names to their column schemas\n",
    "            max_retries: Maximum number of fix attempts (default: 2)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (success: bool, fixed_sql: str or None, result_df: DataFrame or None)\n",
    "        \"\"\"\n",
    "        current_sql = sql\n",
    "        current_error = error_msg\n",
    "        \n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            log_print(f\"   \uD83D\uDD27 [{uc_id}] Fix attempt {attempt}/{max_retries}...\")\n",
    "            \n",
    "            use_case_details = use_case_lookup.get(uc_id, {})\n",
    "            \n",
    "            tables_involved = use_case_details.get('Tables Involved', '')\n",
    "            directly_involved_schema = \"\"\n",
    "            if tables_involved and schema_lookup:\n",
    "                schema_parts = []\n",
    "                for tbl in tables_involved.split(','):\n",
    "                    tbl = tbl.strip()\n",
    "                    if tbl in schema_lookup:\n",
    "                        schema_parts.append(f\"-- Table: {tbl}\\n{schema_lookup[tbl]}\")\n",
    "                directly_involved_schema = '\\n\\n'.join(schema_parts) if schema_parts else \"\"\n",
    "            \n",
    "            fix_prompt_vars = {\n",
    "                \"use_case_id\": uc_id,\n",
    "                \"use_case_name\": uc_name,\n",
    "                \"business_domain\": use_case_details.get('Business Domain', ''),\n",
    "                \"statement\": use_case_details.get('Statement', ''),\n",
    "                \"tables_involved\": tables_involved,\n",
    "                \"directly_involved_schema\": directly_involved_schema,\n",
    "                \"original_sql\": current_sql,\n",
    "                \"explain_error\": current_error,\n",
    "                \"use_case_columns\": use_case_details.get('Involved Columns', '') or use_case_details.get('Columns Involved', '') or \"\"\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                fixed_sql = self.ai_agent.run_worker(\n",
    "                    step_name=f\"Fix_SQL_Sample_{uc_id}_Attempt{attempt}\",\n",
    "                    worker_prompt_path=\"USE_CASE_SQL_FIX_PROMPT\",\n",
    "                    prompt_vars=fix_prompt_vars,\n",
    "                    response_schema=None,\n",
    "                    timeout_override=120,\n",
    "                    max_retries_override=2\n",
    "                )\n",
    "                \n",
    "                if not fixed_sql or fixed_sql.strip() == current_sql.strip():\n",
    "                    log_print(f\"   ⚠️ [{uc_id}] Fix returned same SQL, skipping retry\", level=\"WARNING\")\n",
    "                    continue\n",
    "                \n",
    "                df = self.spark.sql(fixed_sql)\n",
    "                pdf = df.toPandas()\n",
    "                \n",
    "                log_print(f\"   ✅ [{uc_id}] SQL fixed successfully on attempt {attempt}\")\n",
    "                return (True, fixed_sql, pdf)\n",
    "                \n",
    "            except Exception as fix_error:\n",
    "                current_error = self._extract_clean_sql_error(fix_error)\n",
    "                current_sql = fixed_sql if 'fixed_sql' in dir() and fixed_sql else current_sql\n",
    "                log_print(f\"   ❌ [{uc_id}] Fix attempt {attempt} failed: {current_error[:80]}...\", level=\"ERROR\")\n",
    "        \n",
    "        return (False, None, None)\n",
    "\n",
    "    def _load_json_for_sample_fixing(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Load JSON catalog to get use case details and schema for SQL fixing.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (use_case_lookup: dict, schema_lookup: dict)\n",
    "        \"\"\"\n",
    "        from collections import defaultdict\n",
    "        \n",
    "        json_file_path = os.path.join(self.docs_output_dir, f\"{self.business_name}-dbx_inspire.json\")\n",
    "        use_case_lookup = {}\n",
    "        schema_lookup = {}\n",
    "        \n",
    "        try:\n",
    "            file_info = self.w_client.workspace.export(path=json_file_path, format=workspace.ExportFormat.AUTO)\n",
    "            json_content = base64.b64decode(file_info.content).decode('utf-8')\n",
    "            catalog_json = json.loads(json_content)\n",
    "            \n",
    "            domains_data = catalog_json.get(\"domains\", {})\n",
    "            if isinstance(domains_data, dict):\n",
    "                for domain_data in domains_data.values():\n",
    "                    if isinstance(domain_data, dict):\n",
    "                        for uc in domain_data.get(\"use_cases\", []):\n",
    "                            uc_id = uc.get('No', '')\n",
    "                            if uc_id:\n",
    "                                use_case_lookup[uc_id] = uc\n",
    "            elif isinstance(domains_data, list):\n",
    "                for domain_data in domains_data:\n",
    "                    if isinstance(domain_data, dict):\n",
    "                        for uc in domain_data.get(\"use_cases\", []):\n",
    "                            uc_id = uc.get('No', '')\n",
    "                            if uc_id:\n",
    "                                use_case_lookup[uc_id] = uc\n",
    "            \n",
    "            column_registry = catalog_json.get(\"column_registry\", {})\n",
    "            schema_by_table = defaultdict(list)\n",
    "            \n",
    "            for cid, val in column_registry.items():\n",
    "                parts = val.split(\",\", 1)\n",
    "                fqn = parts[0].strip()\n",
    "                description = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "                \n",
    "                fqn_parts = fqn.split('.')\n",
    "                if len(fqn_parts) >= 3:\n",
    "                    table_fqn = '.'.join(fqn_parts[:-1])\n",
    "                    col_name = fqn_parts[-1]\n",
    "                    schema_by_table[table_fqn].append(f\"  - {col_name}: {description[:100]}\" if description else f\"  - {col_name}\")\n",
    "            \n",
    "            for tbl, cols in schema_by_table.items():\n",
    "                schema_lookup[tbl] = '\\n'.join(cols)\n",
    "            \n",
    "            self.logger.info(f\"Loaded {len(use_case_lookup)} use cases and {len(schema_lookup)} table schemas for SQL fixing\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Could not load JSON for SQL fixing: {e}\")\n",
    "        \n",
    "        return (use_case_lookup, schema_lookup)\n",
    "\n",
    "    def _update_notebook_cell_with_fixed_sql(self, notebook_path: str, uc_id: str, fixed_sql: str) -> bool:\n",
    "        \"\"\"\n",
    "        Update a notebook cell with fixed SQL after successful fix.\n",
    "        \n",
    "        Args:\n",
    "            notebook_path: Path to the notebook file\n",
    "            uc_id: Use case ID to find the cell\n",
    "            fixed_sql: The fixed SQL to replace the original\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if update was successful\n",
    "        \"\"\"\n",
    "        try:\n",
    "            file_info = self.w_client.workspace.export(path=notebook_path, format=workspace.ExportFormat.JUPYTER)\n",
    "            notebook_json_str = base64.b64decode(file_info.content).decode('utf-8')\n",
    "            notebook_json = json.loads(notebook_json_str)\n",
    "            \n",
    "            inspire_header_pattern = re.compile(r'--Use Case:\\s*' + re.escape(uc_id))\n",
    "            \n",
    "            for cell in notebook_json.get('cells', []):\n",
    "                if cell.get('cell_type') != 'code':\n",
    "                    continue\n",
    "                \n",
    "                source = cell.get('source', [])\n",
    "                if isinstance(source, list):\n",
    "                    cell_content = ''.join(source)\n",
    "                else:\n",
    "                    cell_content = source\n",
    "                \n",
    "                if inspire_header_pattern.search(cell_content):\n",
    "                    header_lines = []\n",
    "                    for line in cell_content.split('\\n'):\n",
    "                        if line.strip().startswith('--') or line.strip().startswith('/**') or line.strip().startswith('*/'):\n",
    "                            header_lines.append(line)\n",
    "                        elif 'Regeneration Instruction' in line:\n",
    "                            header_lines.append(line)\n",
    "                        else:\n",
    "                            break\n",
    "                    \n",
    "                    new_cell_content = '\\n'.join(header_lines) + '\\n' + fixed_sql + '\\n'\n",
    "                    cell['source'] = [new_cell_content]\n",
    "                    \n",
    "                    updated_notebook_str = json.dumps(notebook_json, indent=2)\n",
    "                    import_content = base64.b64encode(updated_notebook_str.encode('utf-8')).decode('utf-8')\n",
    "                    self.w_client.workspace.import_(\n",
    "                        path=notebook_path,\n",
    "                        content=import_content,\n",
    "                        format=workspace.ImportFormat.JUPYTER,\n",
    "                        overwrite=True\n",
    "                    )\n",
    "                    \n",
    "                    self.logger.info(f\"Updated notebook cell for [{uc_id}] with fixed SQL\")\n",
    "                    return True\n",
    "            \n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to update notebook for [{uc_id}]: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _run_generate_sample_result_mode(self, called_from_sql_regen: bool = False):\n",
    "        \"\"\"\n",
    "        Generate Sample Result Mode: Scans notebooks for generate_sample_result:Yes and executes SQL.\n",
    "        \n",
    "        This method:\n",
    "        1. Scans all notebooks in the notebook output directory\n",
    "        2. Finds SQL cells with \"generate_sample_result:Yes\" in the Inspire header\n",
    "        3. Executes the SQL query using SparkSession\n",
    "        4. If execution fails, attempts to fix SQL up to 2 times using AI\n",
    "        5. Collects result where ai_sys_importance=High AND ai_sys_urgency=High, or first result\n",
    "        6. Generates one Excel per notebook with one sheet per use case ID (transposed)\n",
    "        7. Generates MD file with all results\n",
    "        8. Saves to /sample folder\n",
    "        \n",
    "        Args:\n",
    "            called_from_sql_regen: If True, this is being called from Re-generate SQL mode\n",
    "                                  (no samples found is INFO, not WARNING)\n",
    "        \"\"\"\n",
    "        import re\n",
    "        import json\n",
    "        import base64\n",
    "        import pandas as pd\n",
    "        \n",
    "        # Try to import openpyxl, install if needed, fall back to markdown-only if unavailable\n",
    "        excel_available = False\n",
    "        try:\n",
    "            from openpyxl import Workbook\n",
    "            from openpyxl.styles import Font, PatternFill, Alignment, Border, Side\n",
    "            from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "            excel_available = True\n",
    "        except ImportError:\n",
    "            self.logger.info(\"openpyxl not found, attempting to install...\")\n",
    "            try:\n",
    "                import subprocess\n",
    "                subprocess.check_call(['pip', 'install', 'openpyxl', '-q'])\n",
    "                from openpyxl import Workbook\n",
    "                from openpyxl.styles import Font, PatternFill, Alignment, Border, Side\n",
    "                from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "                excel_available = True\n",
    "                self.logger.info(\"Successfully installed openpyxl\")\n",
    "            except Exception as install_err:\n",
    "                self.logger.warning(f\"Could not install openpyxl: {install_err}. Will generate markdown only.\")\n",
    "                log_print(f\"⚠️ openpyxl unavailable - sample results will be generated as markdown only\", level=\"WARNING\")\n",
    "        \n",
    "        if not called_from_sql_regen:\n",
    "            log_print(f\"\\n\uD83D\uDCCA GENERATE SAMPLE RESULT MODE\")\n",
    "            log_print(f\"{'='*80}\")\n",
    "        log_print(f\"Scanning notebooks for generate_sample_result:Yes...\")\n",
    "        \n",
    "        sample_output_dir = os.path.join(self.base_output_dir, \"sample\")\n",
    "        excel_output_dir = os.path.join(sample_output_dir, \"excel\")\n",
    "        markdown_output_dir = os.path.join(sample_output_dir, \"markdown\")\n",
    "        \n",
    "        for dir_path in [sample_output_dir, excel_output_dir, markdown_output_dir]:\n",
    "            try:\n",
    "                self.w_client.workspace.mkdirs(dir_path)\n",
    "                self.logger.info(f\"Created directory: {dir_path}\")\n",
    "            except Exception as e:\n",
    "                self.logger.debug(f\"Directory may already exist: {dir_path}: {e}\")\n",
    "        \n",
    "        # Scan notebooks\n",
    "        try:\n",
    "            notebook_list = list(self.w_client.workspace.list(self.notebook_output_dir))\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to list notebooks in {self.notebook_output_dir}: {e}\")\n",
    "            log_print(f\"❌ Failed to list notebooks: {e}\", level=\"ERROR\")\n",
    "            return\n",
    "        \n",
    "        # Patterns for parsing notebook cells\n",
    "        use_case_id_pattern = re.compile(r'--Use Case:\\s*([A-Za-z0-9_-]+)\\s*-\\s*(.+?)$', re.MULTILINE)\n",
    "        generate_sample_pattern = re.compile(r'generate_sample_result:\\s*(Yes|No)', re.IGNORECASE)\n",
    "        \n",
    "        # Collect all use cases that need sample generation\n",
    "        notebooks_with_samples = {}  # notebook_path -> list of (use_case_id, use_case_name, sql, markdown_table)\n",
    "        \n",
    "        for item in notebook_list:\n",
    "            if not item.path.endswith('.ipynb'):\n",
    "                continue\n",
    "            \n",
    "            notebook_samples = []\n",
    "            notebook_name = os.path.basename(item.path).replace('.ipynb', '')\n",
    "            \n",
    "            try:\n",
    "                file_info = self.w_client.workspace.export(path=item.path, format=workspace.ExportFormat.JUPYTER)\n",
    "                notebook_json_str = base64.b64decode(file_info.content).decode('utf-8')\n",
    "                notebook_json = json.loads(notebook_json_str)\n",
    "                \n",
    "                # Track markdown table for each use case (from preceding markdown cell)\n",
    "                current_markdown_table = None\n",
    "                \n",
    "                for cell in notebook_json.get('cells', []):\n",
    "                    if cell.get('cell_type') == 'markdown':\n",
    "                        source = cell.get('source', [])\n",
    "                        if isinstance(source, list):\n",
    "                            current_markdown_table = ''.join(source)\n",
    "                        else:\n",
    "                            current_markdown_table = source\n",
    "                        continue\n",
    "                    \n",
    "                    if cell.get('cell_type') != 'code':\n",
    "                        continue\n",
    "                    \n",
    "                    source = cell.get('source', [])\n",
    "                    if isinstance(source, list):\n",
    "                        cell_content = ''.join(source)\n",
    "                    else:\n",
    "                        cell_content = source\n",
    "                    \n",
    "                    # Check for generate_sample_result:Yes\n",
    "                    sample_match = generate_sample_pattern.search(cell_content)\n",
    "                    if sample_match and sample_match.group(1).lower() == 'yes':\n",
    "                        # Extract use case ID and name\n",
    "                        uc_match = use_case_id_pattern.search(cell_content)\n",
    "                        if uc_match:\n",
    "                            uc_id = uc_match.group(1).strip()\n",
    "                            uc_name = uc_match.group(2).strip()\n",
    "                            \n",
    "                            # Extract SQL (remove header lines)\n",
    "                            sql_lines = []\n",
    "                            in_sql = False\n",
    "                            for line in cell_content.split('\\n'):\n",
    "                                line_stripped = line.strip()\n",
    "                                if line_stripped.startswith('--') or line_stripped.startswith('/**') or line_stripped.startswith('*/'):\n",
    "                                    continue\n",
    "                                if 'Regeneration Instruction' in line:\n",
    "                                    continue\n",
    "                                if line_stripped:\n",
    "                                    in_sql = True\n",
    "                                if in_sql:\n",
    "                                    sql_lines.append(line)\n",
    "                            \n",
    "                            sql = '\\n'.join(sql_lines).strip()\n",
    "                            if sql:\n",
    "                                notebook_samples.append((uc_id, uc_name, sql, current_markdown_table))\n",
    "                                log_print(f\"   \uD83D\uDCDD Found [{uc_id}] in {notebook_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.debug(f\"Could not parse notebook {item.path}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            if notebook_samples:\n",
    "                notebooks_with_samples[item.path] = notebook_samples\n",
    "        \n",
    "        if not notebooks_with_samples:\n",
    "            if called_from_sql_regen:\n",
    "                log_print(f\"ℹ️ No use cases with generate_sample_result:Yes found (sample generation skipped)\")\n",
    "            else:\n",
    "                log_print(f\"\\n⚠️ No SQL cells with generate_sample_result:Yes found in any notebook\", level=\"WARNING\")\n",
    "            return\n",
    "        \n",
    "        total_samples = sum(len(v) for v in notebooks_with_samples.values())\n",
    "        log_print(f\"\\n\uD83D\uDCCA Found {total_samples} use cases to sample across {len(notebooks_with_samples)} notebooks\")\n",
    "        \n",
    "        use_case_lookup, schema_lookup = self._load_json_for_sample_fixing()\n",
    "        \n",
    "        from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "        import threading\n",
    "        import gc\n",
    "        \n",
    "        WAVE_SIZE = 4\n",
    "        MAX_CONCURRENT_FIXES = 2\n",
    "        fix_semaphore = threading.Semaphore(MAX_CONCURRENT_FIXES)\n",
    "        \n",
    "        sample_parallelism = min(WAVE_SIZE, max(2, self.max_parallelism // 2))\n",
    "        \n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"\uD83D\uDD04 MEMORY-EFFICIENT SAMPLE EXECUTION: {total_samples} samples\")\n",
    "        log_print(f\"{'='*80}\")\n",
    "        log_print(f\"\uD83D\uDD27 [SAMPLE_EXECUTION] Wave size: {WAVE_SIZE}, Workers per wave: {sample_parallelism}\")\n",
    "        log_print(f\"   └─ Processing notebook-by-notebook with immediate memory release\")\n",
    "        log_print(f\"   └─ Max concurrent SQL fixes: {MAX_CONCURRENT_FIXES} (backpressure)\")\n",
    "        log_print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        def extract_transposed_data(pdf, uc_id: str, max_records: int = 10) -> list:\n",
    "            \"\"\"Extract transposed data for up to max_records rows.\n",
    "            \n",
    "            Returns list of records, where each record is a list of {Column, Value} dicts.\n",
    "            Prioritizes high importance/urgency rows if available.\n",
    "            Note: ai_sys_* columns are excluded from output as they are for internal use only.\n",
    "            Preserves numeric types to avoid 'Number stored as text' Excel errors.\n",
    "            \"\"\"\n",
    "            import numpy as np\n",
    "            \n",
    "            selected_rows = []\n",
    "            total_rows = len(pdf)\n",
    "            \n",
    "            if 'ai_sys_importance' in pdf.columns and 'ai_sys_urgency' in pdf.columns:\n",
    "                try:\n",
    "                    high_priority = pdf[\n",
    "                        (pdf['ai_sys_importance'].str.lower() == 'high') & \n",
    "                        (pdf['ai_sys_urgency'].str.lower() == 'high')\n",
    "                    ]\n",
    "                    if not high_priority.empty:\n",
    "                        for i in range(min(len(high_priority), max_records)):\n",
    "                            selected_rows.append(high_priority.iloc[i])\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            remaining_needed = max_records - len(selected_rows)\n",
    "            if remaining_needed > 0:\n",
    "                for i in range(min(total_rows, remaining_needed)):\n",
    "                    if len(selected_rows) < max_records:\n",
    "                        row = pdf.iloc[i]\n",
    "                        is_duplicate = False\n",
    "                        for existing in selected_rows:\n",
    "                            if existing.equals(row):\n",
    "                                is_duplicate = True\n",
    "                                break\n",
    "                        if not is_duplicate:\n",
    "                            selected_rows.append(row)\n",
    "            \n",
    "            if not selected_rows:\n",
    "                selected_rows = [pdf.iloc[0]]\n",
    "            \n",
    "            all_records = []\n",
    "            for row_idx, selected_row in enumerate(selected_rows):\n",
    "                transposed_data = []\n",
    "                for col_name in selected_row.index:\n",
    "                    col_name_str = str(col_name)\n",
    "                    if col_name_str.startswith('ai_sys_'):\n",
    "                        continue\n",
    "                    \n",
    "                    raw_value = selected_row[col_name]\n",
    "                    \n",
    "                    if pd.isna(raw_value):\n",
    "                        value = 'N/A'\n",
    "                    elif isinstance(raw_value, (int, np.integer)):\n",
    "                        value = int(raw_value)\n",
    "                    elif isinstance(raw_value, (float, np.floating)):\n",
    "                        if raw_value == int(raw_value):\n",
    "                            value = int(raw_value)\n",
    "                        else:\n",
    "                            value = round(float(raw_value), 6)\n",
    "                    elif isinstance(raw_value, bool):\n",
    "                        value = raw_value\n",
    "                    else:\n",
    "                        value = str(raw_value)[:500]\n",
    "                    \n",
    "                    transposed_data.append({'Column': col_name_str, 'Value': value})\n",
    "                \n",
    "                all_records.append(transposed_data)\n",
    "            \n",
    "            self.logger.debug(f\"[{uc_id}] Extracted {len(all_records)} records (of {total_rows} total)\")\n",
    "            return all_records\n",
    "        \n",
    "        def parse_markdown_table_to_info(markdown_text: str) -> list:\n",
    "            \"\"\"Parse markdown table to extract use case information rows.\n",
    "            \n",
    "            Expected format from notebooks:\n",
    "            | Aspect | Description |\n",
    "            |---|---|\n",
    "            | **Subdomain** | Value |\n",
    "            | **Statement** | Value |\n",
    "            \n",
    "            Returns list of tuples: [(field_name, field_value), ...]\n",
    "            \"\"\"\n",
    "            if not markdown_text:\n",
    "                return []\n",
    "            \n",
    "            info_rows = []\n",
    "            lines = markdown_text.strip().split('\\n')\n",
    "            skip_headers = {'aspect', 'description', 'field', 'attribute', 'column', 'name', 'value'}\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line.startswith('|'):\n",
    "                    continue\n",
    "                if '|---|' in line or '| --- |' in line or line.replace(' ', '').replace('|', '').replace('-', '') == '':\n",
    "                    continue\n",
    "                \n",
    "                parts = [p.strip() for p in line.split('|')]\n",
    "                parts = [p for p in parts if p]\n",
    "                \n",
    "                if len(parts) >= 2:\n",
    "                    field_name = parts[0].replace('**', '').strip()\n",
    "                    field_value = parts[1].replace('**', '').strip()\n",
    "                    \n",
    "                    if field_name and field_name.lower() not in skip_headers:\n",
    "                        if field_value:\n",
    "                            info_rows.append((field_name, field_value))\n",
    "            \n",
    "            return info_rows\n",
    "        \n",
    "        def execute_single_sample_memory_safe(sample_info: dict) -> dict:\n",
    "            \"\"\"Execute SQL and return only lightweight transposed data (no DataFrame retention).\"\"\"\n",
    "            uc_id = sample_info['uc_id']\n",
    "            uc_name = sample_info['uc_name']\n",
    "            sql = sample_info['sql']\n",
    "            notebook_path = sample_info['notebook_path']\n",
    "            markdown_table = sample_info['markdown_table']\n",
    "            \n",
    "            result = {\n",
    "                'uc_id': uc_id,\n",
    "                'uc_name': uc_name,\n",
    "                'notebook_path': notebook_path,\n",
    "                'markdown_table': markdown_table,\n",
    "                'success': False,\n",
    "                'transposed_data': None,\n",
    "                'final_sql': sql,\n",
    "                'sql_was_fixed': False,\n",
    "                'error': None\n",
    "            }\n",
    "            \n",
    "            pdf = None\n",
    "            try:\n",
    "                df = self.spark.sql(sql)\n",
    "                pdf = df.toPandas()\n",
    "                del df\n",
    "                \n",
    "                if pdf is not None and not pdf.empty:\n",
    "                    result['transposed_data'] = extract_transposed_data(pdf, uc_id)\n",
    "                    result['success'] = True\n",
    "                    self.logger.info(f\"[{uc_id}] SQL executed successfully ({len(pdf)} rows)\")\n",
    "                else:\n",
    "                    result['error'] = \"No results returned\"\n",
    "                    self.logger.warning(f\"[{uc_id}] SQL returned empty result\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                clean_error = self._extract_clean_sql_error(e)\n",
    "                self.logger.error(f\"[{uc_id}] SQL execution failed: {clean_error[:100]}\")\n",
    "                \n",
    "                with fix_semaphore:\n",
    "                    success, fixed_sql, fixed_pdf = self._fix_sql_with_retry(\n",
    "                        uc_id=uc_id,\n",
    "                        uc_name=uc_name,\n",
    "                        sql=sql,\n",
    "                        error_msg=clean_error,\n",
    "                        use_case_lookup=use_case_lookup,\n",
    "                        schema_lookup=schema_lookup,\n",
    "                        max_retries=2\n",
    "                    )\n",
    "                    \n",
    "                    if success and fixed_pdf is not None:\n",
    "                        result['transposed_data'] = extract_transposed_data(fixed_pdf, uc_id)\n",
    "                        result['success'] = True\n",
    "                        result['final_sql'] = fixed_sql\n",
    "                        result['sql_was_fixed'] = True\n",
    "                        del fixed_pdf\n",
    "                        \n",
    "                        if self._update_notebook_cell_with_fixed_sql(notebook_path, uc_id, fixed_sql):\n",
    "                            self.logger.info(f\"[{uc_id}] Notebook updated with fixed SQL\")\n",
    "                    else:\n",
    "                        result['error'] = f\"SQL fix failed after 2 attempts: {clean_error[:100]}\"\n",
    "            finally:\n",
    "                if pdf is not None:\n",
    "                    del pdf\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        global_succeeded = 0\n",
    "        global_failed = 0\n",
    "        global_fixed = 0\n",
    "        global_processed = 0\n",
    "        all_results_for_md = []\n",
    "        \n",
    "        # Style definitions for Excel (only if available)\n",
    "        if excel_available:\n",
    "            header_font = Font(bold=True, color=\"FFFFFF\", size=11)\n",
    "            header_fill = PatternFill(start_color=\"0066CC\", end_color=\"0066CC\", fill_type=\"solid\")\n",
    "            column_font = Font(bold=True, size=10)\n",
    "            value_font = Font(size=10)\n",
    "            thin_border = Border(\n",
    "                left=Side(style='thin'), right=Side(style='thin'),\n",
    "                top=Side(style='thin'), bottom=Side(style='thin')\n",
    "            )\n",
    "        \n",
    "        for notebook_path, samples in notebooks_with_samples.items():\n",
    "            notebook_name = os.path.basename(notebook_path).replace('.ipynb', '')\n",
    "            notebook_sample_count = len(samples)\n",
    "            log_print(f\"\\n\uD83D\uDCD3 Processing notebook: {notebook_name} ({notebook_sample_count} samples)\")\n",
    "            \n",
    "            # Only create workbook if Excel is available\n",
    "            wb = Workbook() if excel_available else None\n",
    "            if excel_available:\n",
    "                wb.remove(wb.active)\n",
    "            \n",
    "            notebook_succeeded = 0\n",
    "            notebook_failed = 0\n",
    "            \n",
    "            sample_list = [\n",
    "                {'notebook_path': notebook_path, 'uc_id': uc_id, 'uc_name': uc_name, \n",
    "                 'sql': sql, 'markdown_table': markdown_table}\n",
    "                for uc_id, uc_name, sql, markdown_table in samples\n",
    "            ]\n",
    "            \n",
    "            for wave_start in range(0, len(sample_list), WAVE_SIZE):\n",
    "                wave_samples = sample_list[wave_start:wave_start + WAVE_SIZE]\n",
    "                wave_num = (wave_start // WAVE_SIZE) + 1\n",
    "                total_waves = (len(sample_list) + WAVE_SIZE - 1) // WAVE_SIZE\n",
    "                \n",
    "                self.logger.info(f\"[{notebook_name}] Wave {wave_num}/{total_waves}: Processing {len(wave_samples)} samples\")\n",
    "                \n",
    "                with ThreadPoolExecutor(max_workers=sample_parallelism, thread_name_prefix=f\"Wave{wave_num}\") as executor:\n",
    "                    futures = {executor.submit(execute_single_sample_memory_safe, s): s['uc_id'] for s in wave_samples}\n",
    "                    \n",
    "                    for future in as_completed(futures):\n",
    "                        uc_id = futures[future]\n",
    "                        try:\n",
    "                            result = future.result(timeout=300)\n",
    "                            global_processed += 1\n",
    "                            \n",
    "                            if result['success'] and result['transposed_data']:\n",
    "                                global_succeeded += 1\n",
    "                                notebook_succeeded += 1\n",
    "                                sql_was_fixed = result.get('sql_was_fixed', False)\n",
    "                                if sql_was_fixed:\n",
    "                                    global_fixed += 1\n",
    "                                \n",
    "                                transposed_data = result['transposed_data']\n",
    "                                \n",
    "                                # Excel sheet creation (only if openpyxl available)\n",
    "                                if excel_available and wb is not None:\n",
    "                                    sheet_name = uc_id[:31]\n",
    "                                    ws = wb.create_sheet(title=sheet_name)\n",
    "                                    \n",
    "                                    right_align = Alignment(horizontal='right')\n",
    "                                    left_align_wrap = Alignment(horizontal='left', wrap_text=True)\n",
    "                                    info_fill = PatternFill(start_color=\"E6F3FF\", end_color=\"E6F3FF\", fill_type=\"solid\")\n",
    "                                    \n",
    "                                    ws['A1'] = f\"Use Case: {uc_id}\"\n",
    "                                    ws['A1'].font = Font(bold=True, size=14)\n",
    "                                    ws['A2'] = f\"Name: {result['uc_name']}\"\n",
    "                                    ws['A2'].font = Font(bold=True, size=12)\n",
    "                                    ws.merge_cells('A1:B1')\n",
    "                                    ws.merge_cells('A2:B2')\n",
    "                                    \n",
    "                                    current_row = 4\n",
    "                                    \n",
    "                                    uc_info = parse_markdown_table_to_info(result.get('markdown_table', ''))\n",
    "                                    if uc_info:\n",
    "                                        ws.cell(row=current_row, column=1, value='USE CASE INFORMATION')\n",
    "                                        ws.cell(row=current_row, column=1).font = Font(bold=True, size=11, color=\"0066CC\")\n",
    "                                        ws.merge_cells(f'A{current_row}:B{current_row}')\n",
    "                                        current_row += 1\n",
    "                                        \n",
    "                                        for field_name, field_value in uc_info:\n",
    "                                            ws.cell(row=current_row, column=1, value=field_name)\n",
    "                                            ws.cell(row=current_row, column=2, value=field_value)\n",
    "                                            ws.cell(row=current_row, column=1).font = Font(bold=True, size=10)\n",
    "                                            ws.cell(row=current_row, column=1).alignment = right_align\n",
    "                                            ws.cell(row=current_row, column=1).fill = info_fill\n",
    "                                            ws.cell(row=current_row, column=1).border = thin_border\n",
    "                                            ws.cell(row=current_row, column=2).font = Font(size=10)\n",
    "                                            ws.cell(row=current_row, column=2).alignment = left_align_wrap\n",
    "                                            ws.cell(row=current_row, column=2).fill = info_fill\n",
    "                                            ws.cell(row=current_row, column=2).border = thin_border\n",
    "                                            current_row += 1\n",
    "                                        \n",
    "                                        current_row += 1\n",
    "                                    \n",
    "                                    all_records = transposed_data\n",
    "                                    num_records = len(all_records)\n",
    "                                    \n",
    "                                    ws.cell(row=current_row, column=1, value=f'SAMPLE DATA ({num_records} record{\"s\" if num_records > 1 else \"\"})')\n",
    "                                    ws.cell(row=current_row, column=1).font = Font(bold=True, size=11, color=\"0066CC\")\n",
    "                                    ws.merge_cells(f'A{current_row}:B{current_row}')\n",
    "                                    current_row += 1\n",
    "                                    \n",
    "                                    separator_fill = PatternFill(start_color=\"D9D9D9\", end_color=\"D9D9D9\", fill_type=\"solid\")\n",
    "                                    \n",
    "                                    for record_idx, record_data in enumerate(all_records):\n",
    "                                        if record_idx > 0:\n",
    "                                            ws.cell(row=current_row, column=1, value=f'--- Record {record_idx + 1} ---')\n",
    "                                            ws.cell(row=current_row, column=1).font = Font(bold=True, size=10, italic=True)\n",
    "                                            ws.cell(row=current_row, column=1).fill = separator_fill\n",
    "                                            ws.cell(row=current_row, column=2).fill = separator_fill\n",
    "                                            ws.merge_cells(f'A{current_row}:B{current_row}')\n",
    "                                            current_row += 1\n",
    "                                        else:\n",
    "                                            ws.cell(row=current_row, column=1, value='Column')\n",
    "                                            ws.cell(row=current_row, column=2, value='Value')\n",
    "                                            ws.cell(row=current_row, column=1).font = header_font\n",
    "                                            ws.cell(row=current_row, column=2).font = header_font\n",
    "                                            ws.cell(row=current_row, column=1).fill = header_fill\n",
    "                                            ws.cell(row=current_row, column=2).fill = header_fill\n",
    "                                            ws.cell(row=current_row, column=1).border = thin_border\n",
    "                                            ws.cell(row=current_row, column=2).border = thin_border\n",
    "                                            ws.cell(row=current_row, column=1).alignment = right_align\n",
    "                                            current_row += 1\n",
    "                                        \n",
    "                                        for row_data in record_data:\n",
    "                                            col_name = row_data['Column']\n",
    "                                            if col_name.startswith('ai_sys_'):\n",
    "                                                continue\n",
    "                                            ws.cell(row=current_row, column=1, value=col_name)\n",
    "                                            ws.cell(row=current_row, column=2, value=row_data['Value'])\n",
    "                                            ws.cell(row=current_row, column=1).font = column_font\n",
    "                                            ws.cell(row=current_row, column=1).border = thin_border\n",
    "                                            ws.cell(row=current_row, column=1).alignment = right_align\n",
    "                                            ws.cell(row=current_row, column=2).font = value_font\n",
    "                                            ws.cell(row=current_row, column=2).border = thin_border\n",
    "                                            ws.cell(row=current_row, column=2).alignment = left_align_wrap\n",
    "                                            current_row += 1\n",
    "                                    \n",
    "                                    ws.column_dimensions['A'].width = 35\n",
    "                                    ws.column_dimensions['B'].width = 80\n",
    "                                \n",
    "                                # Markdown data collection (always, regardless of Excel availability)\n",
    "                                all_results_for_md.append({\n",
    "                                    'uc_id': uc_id,\n",
    "                                    'uc_name': result['uc_name'],\n",
    "                                    'notebook_path': notebook_path,\n",
    "                                    'markdown_table': result.get('markdown_table'),\n",
    "                                    'result_data': transposed_data\n",
    "                                })\n",
    "                                \n",
    "                                status = \"[FIXED]\" if sql_was_fixed else \"\"\n",
    "                                log_print(f\"   ✅ [{uc_id}] OK {status} ({global_processed}/{total_samples})\")\n",
    "                                \n",
    "                                del transposed_data\n",
    "                                del result['transposed_data']\n",
    "                            else:\n",
    "                                global_failed += 1\n",
    "                                notebook_failed += 1\n",
    "                                log_print(f\"   ❌ [{uc_id}] Failed: {result.get('error', 'Unknown')[:50]} ({global_processed}/{total_samples})\", level=\"ERROR\")\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            global_processed += 1\n",
    "                            global_failed += 1\n",
    "                            notebook_failed += 1\n",
    "                            log_print(f\"   ❌ [{uc_id}] Exception: {str(e)[:50]} ({global_processed}/{total_samples})\", level=\"ERROR\")\n",
    "                \n",
    "                gc.collect()\n",
    "                self.logger.debug(f\"[{notebook_name}] Wave {wave_num} complete, memory released\")\n",
    "            \n",
    "            # Save Excel workbook (only if available and has sheets)\n",
    "            if excel_available and wb is not None and wb.sheetnames:\n",
    "                try:\n",
    "                    import tempfile\n",
    "                    with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as tmp:\n",
    "                        excel_local_path = tmp.name\n",
    "                    \n",
    "                    wb.save(excel_local_path)\n",
    "                    \n",
    "                    with open(excel_local_path, 'rb') as f:\n",
    "                        excel_content = f.read()\n",
    "                    \n",
    "                    excel_workspace_path = os.path.join(excel_output_dir, f\"{notebook_name}_samples.xlsx\")\n",
    "                    excel_b64 = base64.b64encode(excel_content).decode('utf-8')\n",
    "                    self.w_client.workspace.import_(\n",
    "                        path=excel_workspace_path,\n",
    "                        content=excel_b64,\n",
    "                        format=workspace.ImportFormat.AUTO,\n",
    "                        overwrite=True\n",
    "                    )\n",
    "                    \n",
    "                    log_print(f\"   \uD83D\uDCCA Saved: {notebook_name}_samples.xlsx ({notebook_succeeded} sheets)\")\n",
    "                    os.remove(excel_local_path)\n",
    "                    del excel_content\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Failed to save Excel for {notebook_name}: {e}\")\n",
    "                    log_print(f\"   ❌ Failed to save Excel: {str(e)[:80]}\", level=\"ERROR\")\n",
    "            \n",
    "            if wb is not None:\n",
    "                del wb\n",
    "            gc.collect()\n",
    "            \n",
    "            log_print(f\"   \uD83D\uDCC8 Notebook complete: {notebook_succeeded} OK, {notebook_failed} failed\")\n",
    "        \n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"\uD83D\uDCCA SAMPLE EXECUTION COMPLETE\")\n",
    "        log_print(f\"   • Total: {total_samples}\")\n",
    "        log_print(f\"   • ✅ Succeeded: {global_succeeded} ({global_fixed} fixed)\")\n",
    "        log_print(f\"   • ❌ Failed: {global_failed}\")\n",
    "        log_print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        md_files_created = 0\n",
    "        if all_results_for_md:\n",
    "            md_by_notebook = {}\n",
    "            for result in all_results_for_md:\n",
    "                nb_path = result.get('notebook_path', 'unknown')\n",
    "                nb_name = os.path.basename(nb_path).replace('.ipynb', '') if nb_path else 'unknown'\n",
    "                if nb_name not in md_by_notebook:\n",
    "                    md_by_notebook[nb_name] = []\n",
    "                md_by_notebook[nb_name].append(result)\n",
    "            \n",
    "            for notebook_name, results in md_by_notebook.items():\n",
    "                md_content = f\"# Sample Results: {notebook_name}\\n\\n\"\n",
    "                md_content += f\"**Business:** {self.business_name}\\n\\n\"\n",
    "                md_content += f\"**Generated:** {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
    "                md_content += \"---\\n\\n\"\n",
    "                \n",
    "                for result in results:\n",
    "                    md_content += f\"## {result['uc_id']}: {result['uc_name']}\\n\\n\"\n",
    "                    \n",
    "                    if result.get('markdown_table'):\n",
    "                        md_content += \"### Use Case Information\\n\\n\"\n",
    "                        md_content += result['markdown_table'] + \"\\n\\n\"\n",
    "                    \n",
    "                    all_records = result['result_data']\n",
    "                    num_records = len(all_records) if isinstance(all_records, list) and all_records and isinstance(all_records[0], list) else 1\n",
    "                    \n",
    "                    if not isinstance(all_records[0], list):\n",
    "                        all_records = [all_records]\n",
    "                    \n",
    "                    md_content += f\"### Sample Result ({num_records} record{'s' if num_records > 1 else ''})\\n\\n\"\n",
    "                    \n",
    "                    for record_idx, record_data in enumerate(all_records):\n",
    "                        if record_idx > 0:\n",
    "                            md_content += f\"\\n**--- Record {record_idx + 1} ---**\\n\\n\"\n",
    "                        \n",
    "                        md_content += \"| Column | Value |\\n\"\n",
    "                        md_content += \"|-------:|-------|\\n\"\n",
    "                        \n",
    "                        for row in record_data:\n",
    "                            col_name = row['Column']\n",
    "                            if col_name.startswith('ai_sys_'):\n",
    "                                continue\n",
    "                            col = col_name.replace('|', '\\\\|')\n",
    "                            val = str(row['Value'])[:200].replace('|', '\\\\|').replace('\\n', ' ')\n",
    "                            md_content += f\"| {col} | {val} |\\n\"\n",
    "                    \n",
    "                    md_content += \"\\n---\\n\\n\"\n",
    "                \n",
    "                try:\n",
    "                    md_workspace_path = os.path.join(markdown_output_dir, f\"{notebook_name}_samples.md\")\n",
    "                    md_b64 = base64.b64encode(md_content.encode('utf-8')).decode('utf-8')\n",
    "                    self.w_client.workspace.import_(\n",
    "                        path=md_workspace_path,\n",
    "                        content=md_b64,\n",
    "                        format=workspace.ImportFormat.AUTO,\n",
    "                        overwrite=True\n",
    "                    )\n",
    "                    md_files_created += 1\n",
    "                    log_print(f\"   \uD83D\uDCDD Saved: {notebook_name}_samples.md ({len(results)} use cases)\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Failed to save MD file for {notebook_name}: {e}\")\n",
    "                    log_print(f\"   ❌ Failed to save MD: {notebook_name}_samples.md: {str(e)[:60]}\", level=\"ERROR\")\n",
    "        \n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"✅ GENERATE SAMPLE RESULT MODE COMPLETE\")\n",
    "        log_print(f\"   \uD83D\uDCC1 Output folder: {sample_output_dir}\")\n",
    "        if excel_available:\n",
    "            log_print(f\"   \uD83D\uDCC1 Excel files: {excel_output_dir}\")\n",
    "            log_print(f\"   \uD83D\uDCCA Excel files created: {len(notebooks_with_samples)}\")\n",
    "        else:\n",
    "            log_print(f\"   ⚠️ Excel files: Skipped (openpyxl unavailable)\")\n",
    "        log_print(f\"   \uD83D\uDCC1 Markdown files: {markdown_output_dir}\")\n",
    "        log_print(f\"   \uD83D\uDCDD Markdown files created: {md_files_created}\")\n",
    "        log_print(f\"   \uD83D\uDCCB Total use cases sampled: {len(all_results_for_md)}\")\n",
    "        log_print(f\"{'='*80}\\n\")\n",
    "\n",
    "    def _load_usecases_catalog_json(self, json_file_path: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Loads the JSON Catalog file for docs-only mode.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (final_consolidated_use_cases, summary_dict, english_grouped_data)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Loading JSON Catalog from: {json_file_path}\")\n",
    "            \n",
    "            # Read from workspace\n",
    "            file_info = self.w_client.workspace.export(path=json_file_path, format=workspace.ExportFormat.AUTO)\n",
    "            json_content = base64.b64decode(file_info.content).decode('utf-8')\n",
    "            catalog_json = json.loads(json_content)\n",
    "            \n",
    "            self.logger.info(f\"✅ Successfully loaded JSON Catalog\")\n",
    "            \n",
    "            # === NEW: Load business name from JSON, fallback to widget value ===\n",
    "            json_business_name = catalog_json.get(\"business_name\", None)\n",
    "            if json_business_name:\n",
    "                old_business_name = self.business_name\n",
    "                self.business_name = json_business_name\n",
    "                self.logger.info(f\"Using business name from JSON: '{json_business_name}' (widget value '{old_business_name}' ignored)\")\n",
    "                log_print(f\"\uD83D\uDCCC Using business name from JSON: '{json_business_name}'\")\n",
    "            else:\n",
    "                self.logger.info(f\"Business name not found in JSON. Using widget value: '{self.business_name}'\")\n",
    "                log_print(f\"\uD83D\uDCCC Using business name from widget: '{self.business_name}'\")\n",
    "            \n",
    "            # Extract data - summary_dict should match _get_salesy_summary format\n",
    "            # It uses \"Executive\" as key for executive summary, and domain names for domain summaries\n",
    "            summary_dict = {\n",
    "                \"Executive\": catalog_json.get(\"executive_summary\", \"\")\n",
    "            }\n",
    "            \n",
    "            # === NEW: Restore Column Names from IDs ===\n",
    "            column_registry = catalog_json.get(\"column_registry\", {})\n",
    "            # Parse registry: ID -> FQN\n",
    "            id_to_fqn = {}\n",
    "            for cid, val in column_registry.items():\n",
    "                # Value format: \"fqn, description\"\n",
    "                # Split only on first comma to separate FQN from description\n",
    "                parts = val.split(\",\", 1)\n",
    "                id_to_fqn[cid] = parts[0].strip()\n",
    "            \n",
    "            # Reconstruct grouped data and flat list\n",
    "            english_grouped_data = {}\n",
    "            final_consolidated_use_cases = []\n",
    "            \n",
    "            for domain_obj in catalog_json.get(\"domains\", []):\n",
    "                domain_name = domain_obj.get(\"domain_name\", \"General Operations\")\n",
    "                use_cases = domain_obj.get(\"use_cases\", [])\n",
    "                \n",
    "                # Restore column names in use cases\n",
    "                for uc in use_cases:\n",
    "                    cols_involved = uc.get(\"Columns Involved\", \"\")\n",
    "                    if cols_involved:\n",
    "                        parts = [p.strip() for p in cols_involved.split(\",\")]\n",
    "                        restored_names = []\n",
    "                        for p in parts:\n",
    "                            if p in id_to_fqn:\n",
    "                                restored_names.append(id_to_fqn[p])\n",
    "                            else:\n",
    "                                restored_names.append(p)\n",
    "                        uc[\"Columns Involved\"] = \", \".join(restored_names)\n",
    "\n",
    "                domain_summary = domain_obj.get(\"summary\", \"\")\n",
    "                \n",
    "                english_grouped_data[domain_name] = use_cases\n",
    "                final_consolidated_use_cases.extend(use_cases)\n",
    "                \n",
    "                # Add domain summary to summary_dict\n",
    "                summary_dict[domain_name] = domain_summary\n",
    "            \n",
    "            self.logger.info(f\"Loaded {len(final_consolidated_use_cases)} use cases from {len(english_grouped_data)} domains\")\n",
    "            \n",
    "            # === NEW: Filter out any use cases with \"Pending\" priority (safety check) ===\n",
    "            pending_use_cases = [uc for uc in final_consolidated_use_cases if uc.get('Priority') == 'Pending']\n",
    "            if pending_use_cases:\n",
    "                self.logger.warning(f\"⚠️ Found {len(pending_use_cases)} use cases with 'Pending' priority in JSON - these will be filtered out\")\n",
    "                for uc in pending_use_cases[:5]:  # Log first 5 for debugging\n",
    "                    self.logger.warning(f\"  - {uc.get('No', 'N/A')}: {uc.get('Name', 'N/A')}\")\n",
    "                \n",
    "                # Filter from flat list\n",
    "                final_consolidated_use_cases = [uc for uc in final_consolidated_use_cases if uc.get('Priority') != 'Pending']\n",
    "                \n",
    "                # Also filter from grouped data\n",
    "                for domain_name in list(english_grouped_data.keys()):\n",
    "                    english_grouped_data[domain_name] = [uc for uc in english_grouped_data[domain_name] if uc.get('Priority') != 'Pending']\n",
    "                \n",
    "                self.logger.info(f\"✅ Filtered to {len(final_consolidated_use_cases)} scored use cases (removed {len(pending_use_cases)} pending)\")\n",
    "            \n",
    "            return (final_consolidated_use_cases, summary_dict, english_grouped_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load JSON Catalog: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _upload_log_file(self):\n",
    "        log_file_path = None\n",
    "        try:\n",
    "            for handler in logging.root.handlers:\n",
    "                if isinstance(handler, logging.FileHandler):\n",
    "                    log_file_path = handler.baseFilename\n",
    "                    break\n",
    "            if not log_file_path:\n",
    "                self.logger.warning(\"Could not find FileHandler to upload log file.\")\n",
    "                return\n",
    "            if not os.path.exists(log_file_path):\n",
    "                self.logger.warning(f\"Log file not found at expected path: {log_file_path}\")\n",
    "                return\n",
    "            self.logger.info(f\"Reading log file from: {log_file_path}\")\n",
    "            with open(log_file_path, \"rb\") as f: log_data = f.read()\n",
    "            if not log_data:\n",
    "                self.logger.warning(\"Log file is empty. Skipping upload.\")\n",
    "                return\n",
    "            \n",
    "            # Copy log file to base output directory for easy access\n",
    "            output_log_path = os.path.join(self.base_output_dir, \"log.txt\")\n",
    "            try:\n",
    "                if self.base_output_dir.startswith(\"/tmp/\") or self.base_output_dir.startswith(\"/dbfs/\"):\n",
    "                    os.makedirs(self.base_output_dir, exist_ok=True)\n",
    "                    shutil.copy2(log_file_path, output_log_path)\n",
    "                    self.logger.info(f\"✅ Log file copied to output directory: {output_log_path}\")\n",
    "                    log_print(f\"✅ Log file available at: {output_log_path}\")\n",
    "                else:\n",
    "                    self.logger.info(f\"Skipping local copy of log file (non-local path): {output_log_path}\")\n",
    "            except Exception as copy_error:\n",
    "                self.logger.warning(f\"Failed to copy log file to output directory: {copy_error}\")\n",
    "            \n",
    "            # Also upload to workspace for Databricks UI access\n",
    "            workspace_log_path = os.path.join(self.docs_output_dir, \"generation_log.txt\")\n",
    "            \n",
    "            self.logger.info(f\"Uploading log file to workspace: {workspace_log_path}\")\n",
    "            log_data_b64 = base64.b64encode(log_data).decode()\n",
    "            self.w_client.workspace.import_(\n",
    "                path=workspace_log_path, content=log_data_b64,\n",
    "                format=workspace.ImportFormat.AUTO, overwrite=True\n",
    "            )\n",
    "            abs_path = self.w_client.workspace.get_status(workspace_log_path).path\n",
    "            self.logger.info(f\"Successfully uploaded log file to workspace: {abs_path}\")\n",
    "            log_print(f\"✅ Log file also uploaded to workspace: {abs_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to upload log file: {e}\")\n",
    "            if log_file_path:\n",
    "                self.logger.error(f\"Log file was at: {log_file_path}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Main\n",
    "# ==============================================================================\n",
    "# 4. MAIN EXECUTION METHOD (MODIFIED)\n",
    "# ==============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to read widget values, validate inputs,\n",
    "    and run the DatabricksInspire class.\n",
    "    \n",
    "    *** IMPORTANT ***\n",
    "    Run the `create_widgets()` cell first and fill in the UI values\n",
    "    BEFORE running this main() function.\n",
    "    \"\"\"\n",
    "    \n",
    "    print_ascii_banner()\n",
    "\n",
    "    # --- 1. Get Widget Values ---\n",
    "    \n",
    "    # --- Business Name ---\n",
    "    business_name = dbutils.widgets.get(\"00_business_name\")\n",
    "    \n",
    "    # --- UC Metadata ---\n",
    "    catalogs_and_schemas_str = dbutils.widgets.get(\"01_uc_metadata\")\n",
    "    \n",
    "    # --- Operation Mode ---\n",
    "    operation_mode = dbutils.widgets.get(\"02_operation\")\n",
    "    log_print(f\"\uD83C\uDFAF Operation Mode: {operation_mode}\")\n",
    "    \n",
    "    # --- Business Domains ---\n",
    "    business_domains_str = dbutils.widgets.get(\"03_business_domains\")\n",
    "    \n",
    "    # --- Business Priorities (multi-select) ---\n",
    "    business_priorities_str = dbutils.widgets.get(\"04_business_priorities\")\n",
    "    \n",
    "    # --- Strategic Goals ---\n",
    "    strategic_goals_str = dbutils.widgets.get(\"05_strategic_goals\")\n",
    "    \n",
    "    # Check if this is a JSON file path (docs-only mode)\n",
    "    json_file_path = None\n",
    "    catalogs_list = []\n",
    "    schemas_list = []\n",
    "    tables_list = []\n",
    "    \n",
    "    if catalogs_and_schemas_str:\n",
    "        catalogs_and_schemas_str = catalogs_and_schemas_str.strip()\n",
    "        # Check if it's a JSON file path (starts with /)\n",
    "        if catalogs_and_schemas_str.startswith('/'):\n",
    "            json_file_path = catalogs_and_schemas_str\n",
    "            log_print(f\"Detected JSON file path: {json_file_path}\")\n",
    "            log_print(\"Running in DOCS-ONLY mode: Will skip use case generation and notebook generation.\")\n",
    "        else:\n",
    "            # Parse catalogs, schemas, and tables from the merged widget\n",
    "            for item in catalogs_and_schemas_str.split(','):\n",
    "                item = item.strip()\n",
    "                if not item:\n",
    "                    continue\n",
    "                dot_count = item.count('.')\n",
    "                if dot_count == 2:\n",
    "                    # Fully qualified table (catalog.schema.table)\n",
    "                    tables_list.append(item)\n",
    "                elif dot_count == 1:\n",
    "                    # Fully qualified schema (catalog.schema)\n",
    "                    schemas_list.append(item)\n",
    "                elif dot_count == 0:\n",
    "                    # Catalog only\n",
    "                    catalogs_list.append(item)\n",
    "                else:\n",
    "                    # Invalid format - log warning\n",
    "                    log_print(f\"Invalid metadata format '{item}' - expected 0, 1, or 2 dots\", level=\"WARNING\")\n",
    "    \n",
    "    catalogs_str = ','.join(catalogs_list)\n",
    "    schemas_str = ','.join(schemas_list)\n",
    "    tables_str = ','.join(tables_list)\n",
    "    \n",
    "    # --- Generation Options ---\n",
    "    generate_str = dbutils.widgets.get(\"06_generation_options\")\n",
    "    # Force \"use cases\" to be included always\n",
    "    if generate_str:\n",
    "        if \"use cases\" not in generate_str:\n",
    "             generate_str += \", use cases\"\n",
    "    else:\n",
    "        generate_str = \"use cases\"\n",
    "    \n",
    "    # Parse generation options for special flags\n",
    "    generate_options_list = [opt.strip() for opt in generate_str.split(',') if opt.strip()]\n",
    "    \n",
    "    # Extract special options from generation options\n",
    "    use_unstructured_data = \"Unstructured Data Usecases\" in generate_options_list\n",
    "    technical_exclusion_strategy = \"Aggressive\"\n",
    "    \n",
    "    # Set use_unstructured_data_str based on Unstructured Data Usecases selection\n",
    "    use_unstructured_data_str = \"yes\" if use_unstructured_data else \"no\"\n",
    "    \n",
    "    # --- Generation Path ---\n",
    "    generation_path = dbutils.widgets.get(\"07_generation_path\")\n",
    "    \n",
    "    # --- Documents Languages (multiselect) ---\n",
    "    output_language_str = dbutils.widgets.get(\"08_documents_languages\") \n",
    "    \n",
    "    # --- AI Model (model endpoint for ai_query in generated SQL) ---\n",
    "    sql_model_serving = dbutils.widgets.get(\"09_ai_model\")\n",
    "    if not sql_model_serving or not sql_model_serving.strip():\n",
    "        sql_model_serving = \"databricks-gpt-oss-120b\"\n",
    "\n",
    "    # ============================================================================\n",
    "    # --- 2. VALIDATE ALL WIDGET VALUES (FAIL FAST BEFORE ANY PROCESSING) ---\n",
    "    log_print(\"=\" * 80)\n",
    "    log_print(\"\uD83D\uDD0D VALIDATING WIDGET INPUTS...\")\n",
    "    log_print(\"=\" * 80)\n",
    "    \n",
    "    validation_errors = []\n",
    "    \n",
    "    # Validate Business Name first\n",
    "    if not business_name:\n",
    "        validation_errors.append(\"❌ 'Business Name' (00_business_name) is REQUIRED\")\n",
    "    else:\n",
    "        log_print(f\"✅ Business Name: '{business_name}'\")\n",
    "    \n",
    "    # Validate Operation mode\n",
    "    valid_operations = [\"Discover Usecases\", \"Re-generate SQL\", \"Generate Sample Result\"]\n",
    "    if operation_mode not in valid_operations:\n",
    "        validation_errors.append(f\"❌ 'Operation' (02_operation) must be one of: {', '.join(valid_operations)}\")\n",
    "    else:\n",
    "        log_print(f\"✅ Operation: '{operation_mode}'\")\n",
    "    \n",
    "    # AUTO-ENABLE SQL Code generation for \"Re-generate SQL\" mode (regardless of checkbox)\n",
    "    if operation_mode == \"Re-generate SQL\" and \"SQL Code\" not in generate_options_list:\n",
    "        generate_options_list.append(\"SQL Code\")\n",
    "        generate_str = \", \".join(generate_options_list)\n",
    "        log_print(f\"ℹ️ Auto-enabled 'SQL Code' for Re-generate SQL mode\")\n",
    "    \n",
    "    # Log Business Priorities (optional)\n",
    "    if business_priorities_str:\n",
    "        log_print(f\"✅ Business Priorities: '{business_priorities_str}'\")\n",
    "    else:\n",
    "        log_print(f\"ℹ️ Business Priorities: Not provided\")\n",
    "    \n",
    "    # Log Business Domains (optional)\n",
    "    if business_domains_str:\n",
    "        log_print(f\"✅ Business Domains: '{business_domains_str}'\")\n",
    "    else:\n",
    "        log_print(f\"ℹ️ Business Domains: Not provided (domains will be inferred from data)\")\n",
    "    \n",
    "    # Log Strategic Goals (optional but HIGHEST PRIORITY when provided)\n",
    "    if strategic_goals_str:\n",
    "        log_print(f\"✅ Strategic Goals: '{strategic_goals_str[:100]}...' (HIGHEST PRIORITY)\")\n",
    "    else:\n",
    "        log_print(f\"ℹ️ Strategic Goals: Not provided\")\n",
    "    \n",
    "    # UC Metadata validation depends on operation mode\n",
    "    if not json_file_path:\n",
    "        if (operation_mode == \"Discover Usecases\" and \n",
    "            not catalogs_str and not schemas_str and not tables_str):\n",
    "            validation_errors.append(\"❌ 'UC Metadata' (01_uc_metadata) is REQUIRED when discovering use cases\")\n",
    "        elif operation_mode in [\"Re-generate SQL\", \"Generate Sample Result\"]:\n",
    "            # These modes work on existing notebooks, UC Metadata not required\n",
    "            log_print(f\"ℹ️ UC Metadata: Not required for '{operation_mode}' mode\")\n",
    "        else:\n",
    "            log_print(f\"✅ UC Metadata provided: catalogs={len(catalogs_str.split(',')) if catalogs_str else 0}, schemas={len(schemas_str.split(',')) if schemas_str else 0}, tables={len(tables_str.split(',')) if tables_str else 0}\")\n",
    "    else:\n",
    "        log_print(f\"✅ Docs-only mode: Using JSON file '{json_file_path}'\")\n",
    "    \n",
    "    if not generate_str:\n",
    "        validation_errors.append(\"❌ 'Generation Options' (06_generation_options) is REQUIRED - select at least one option\")\n",
    "    else:\n",
    "        log_print(f\"✅ Generation Options: {generate_str}\")\n",
    "    \n",
    "    if not generation_path:\n",
    "        validation_errors.append(\"❌ 'Generation Path' (07_generation_path) is REQUIRED\")\n",
    "    else:\n",
    "        log_print(f\"✅ Generation Path: '{generation_path}'\")\n",
    "    \n",
    "    \n",
    "    # Language is only REQUIRED for PDF/Presentation artifacts, optional for notebooks-only\n",
    "    requires_language = (\"PDF Catalog\" in generate_str or \n",
    "                        \"Presentation\" in generate_str or \n",
    "                        \"Use Cases Catalog PDF\" in generate_str)\n",
    "    \n",
    "    if requires_language:\n",
    "        if not output_language_str:\n",
    "            validation_errors.append(\"❌ 'Documents Languages' (08_documents_languages) is REQUIRED when generating PDF or Presentation\")\n",
    "        else:\n",
    "            languages = [lang.strip() for lang in output_language_str.split(',') if lang.strip()]\n",
    "            log_print(f\"✅ Documents Languages: {', '.join(languages)}\")\n",
    "    else:\n",
    "        # Default to English for notebooks-only mode (no PDF/Presentation)\n",
    "        if not output_language_str:\n",
    "            output_language_str = \"English\"\n",
    "            languages = [\"English\"]\n",
    "            log_print(f\"ℹ️ Documents Languages: Not required (no PDF/Presentation selected), defaulting to English\")\n",
    "        else:\n",
    "            languages = [lang.strip() for lang in output_language_str.split(',') if lang.strip()]\n",
    "            log_print(f\"ℹ️ Documents Languages: {', '.join(languages)} (optional for notebooks-only)\")\n",
    "    \n",
    "    # Log derived options\n",
    "    generate_sql_code = \"SQL Code\" in generate_options_list\n",
    "    log_print(f\"ℹ️ SQL Code Generation: {'Enabled' if generate_sql_code else 'DISABLED (notebooks will have placeholder SQL)'}\")\n",
    "    log_print(f\"ℹ️ Unstructured Data Usecases: {'Enabled' if use_unstructured_data else 'Disabled'}\")\n",
    "    log_print(\"ℹ️ Technical table filtering: Aggressive (mandatory)\")\n",
    "    if generate_sql_code:\n",
    "        log_print(f\"✅ AI Model: '{sql_model_serving}' (for ai_query in generated SQL)\")\n",
    "    \n",
    "    if validation_errors:\n",
    "        import sys as _sys\n",
    "        error_count = len(validation_errors)\n",
    "        error_summary = \"\\n\".join(validation_errors)\n",
    "        \n",
    "        log_print(\"=\" * 80, level=\"ERROR\")\n",
    "        log_print(f\"❌ VALIDATION FAILED - {error_count} ERROR(S) FOUND:\", level=\"ERROR\")\n",
    "        log_print(\"=\" * 80, level=\"ERROR\")\n",
    "        for error in validation_errors:\n",
    "            log_print(error, level=\"ERROR\")\n",
    "        log_print(\"=\" * 80, level=\"ERROR\")\n",
    "        \n",
    "        print(f\"\\n{'='*80}\\n❌ VALIDATION ERRORS ({error_count}):\\n{error_summary}\\n{'='*80}\\n\", file=_sys.stderr, flush=True)\n",
    "        _sys.stdout.flush()\n",
    "        _sys.stderr.flush()\n",
    "        \n",
    "        exit_msg = f\"Validation failed with {error_count} error(s):\\n{error_summary}\"\n",
    "        dbutils.notebook.exit(exit_msg)\n",
    "    \n",
    "    log_print(\"=\" * 80)\n",
    "    log_print(\"✅ ALL VALIDATIONS PASSED - Starting generation...\")\n",
    "    log_print(\"=\" * 80)\n",
    "\n",
    "    # --- 3. Pack values and Run ---\n",
    "    \n",
    "    widget_values = {\n",
    "        \"business\": business_name,\n",
    "        \"operation_mode\": operation_mode,\n",
    "        \"strategic_goals\": strategic_goals_str,\n",
    "        \"business_priorities\": business_priorities_str,\n",
    "        \"business_domains\": business_domains_str,\n",
    "        \"catalogs\": catalogs_str,\n",
    "        \"schemas\": schemas_str,\n",
    "        \"tables\": tables_str,\n",
    "        \"generate\": generate_str,\n",
    "        \"generation_path\": generation_path,\n",
    "        \"output_language\": output_language_str,\n",
    "        \"use_unstructured_data\": use_unstructured_data_str,\n",
    "        \"technical_exclusion_strategy\": technical_exclusion_strategy,\n",
    "        \"sql_model_serving\": sql_model_serving,\n",
    "        \"json_file_path\": json_file_path\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        inspirer = DatabricksInspire(**widget_values)\n",
    "        inspirer.run()\n",
    "    except NameError as ne:\n",
    "        if ('DataLoader' in str(ne) or 'AIAgent' in str(ne) or \n",
    "            'PROMPT_TEMPLATES' in str(ne) or 'DatabricksInspire' in str(ne) or \n",
    "            'setup_logging' in str(ne) or 'TranslationService' in str(ne)):\n",
    "            \n",
    "            print(f\"ERROR: A required class, function, or variable is missing: {ne}\", file=sys.stderr)\n",
    "            print(\"Please ensure `setup_logging`, `DataLoader`, `AIAgent`, `PROMPT_TEMPLATES`, `TranslationService`, and `DatabricksInspire` are defined in preceding cells.\", file=sys.stderr)\n",
    "        else:\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\", file=sys.stderr)\n",
    "        logging.getLogger(\"main\").critical(\"Main execution failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5686cd61-8913-476a-8068-931db592c1a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃    ____        _        _          _      _                             ┃\n┃   |  _ \\  __ _| |_ __ _| |__  _ __(_) ___| | _____                      ┃\n┃   | | | |/ _` | __/ _` | '_ \\| '__| |/ __| |/ / __|                     ┃\n┃   | |_| | (_| | || (_| | |_) | |  | | (__|   <\\__ \\                     ┃\n┃   |____/ \\__,_|\\__\\__,_|_.__/|_|  |_|\\___|_|\\_\\___/                     ┃\n┃       ___                      _                  _    ___              ┃\n┃      |_ _| _ __   ___  _ __   (_) _ __  ___      / \\  |_ _|             ┃\n┃       | | | '_ \\ / __|| '_ \\  | || '__|/ _ \\    / _ \\  | |              ┃\n┃       | | | | | |\\__ \\| |_) | | || |  |  __/   / ___ \\ | |              ┃\n┃      |___||_| |_||___/| .__/  |_||_|   \\___|  /_/   \\_\\___|             ┃\n┃                       |_|                                               ┃\n┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n\n01:19:07 - INFO - \uD83C\uDFAF Operation Mode: Discover Usecases\n01:19:07 - INFO - ================================================================================\n01:19:07 - INFO - \uD83D\uDD0D VALIDATING WIDGET INPUTS...\n01:19:07 - INFO - ================================================================================\n01:19:07 - INFO - ✅ Business Name: 'procore'\n01:19:07 - INFO - ✅ Operation: 'Discover Usecases'\n01:19:07 - INFO - ℹ️ Business Priorities: Not provided\n01:19:07 - INFO - ℹ️ Business Domains: Not provided (domains will be inferred from data)\n01:19:07 - INFO - ℹ️ Strategic Goals: Not provided\n01:19:07 - INFO - ✅ UC Metadata provided: catalogs=0, schemas=0, tables=1\n01:19:07 - INFO - ✅ Generation Options: PDF Catalog,SQL Code, use cases\n01:19:07 - INFO - ✅ Generation Path: './inspire_gen/'\n01:19:07 - INFO - ✅ Documents Languages: English\n01:19:07 - INFO - ℹ️ SQL Code Generation: Enabled\n01:19:07 - INFO - ℹ️ Unstructured Data Usecases: Disabled\n01:19:07 - INFO - ℹ️ Technical table filtering: Aggressive (mandatory)\n01:19:07 - INFO - ✅ AI Model: 'databricks-gpt-oss-120b' (for ai_query in generated SQL)\n01:19:07 - INFO - ================================================================================\n01:19:07 - INFO - ✅ ALL VALIDATIONS PASSED - Starting generation...\n01:19:07 - INFO - ================================================================================\n01:19:07 - INFO - Resolved relative generation path to: /Users/amr.ali@databricks.com/inspire/inspire_gen/\n01:19:07 - INFO - Logging configured. High-level logs to console, detailed logs to /tmp/procore/log.txt\n01:19:07 - INFO - DatabricksInspire initialized for business: procore. Target Language(s): ['English']. Base Output Dir: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore. Notebooks Dir: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/notebooks. Docs Dir: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/docs. Scan parallelism: 5. Max parallelism: auto\n01:19:07 - INFO - Cleaning up existing output directory...\n01:19:08 - INFO - Successfully deleted existing output directory: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore\n01:19:08 - INFO - Creating fresh workspace directories...\n01:19:08 - INFO - Successfully created all output directories.\n01:19:08 - INFO - Initializing capabilities for catalogs: {'virgin_02'}\n01:19:10 - INFO - Capabilities found: {'virgin_02': True}\n01:19:10 - INFO - Found 1 unique databases to process.\n01:19:10 - INFO - Found 1 individual tables to process across 1 databases:\n01:19:10 - INFO -   database virgin_02.procore_field: 1 tables loaded\n01:19:10 - INFO - Starting tasks: ['PDF Catalog', 'SQL Code', 'use cases'], Operation Mode: Discover Usecases\n01:19:10 - INFO - Dynamic parallelism set to 8 (memory_cap=8)\n01:19:10 - INFO - ✅ Dynamic parallelism: 8\n01:19:10 - INFO - Using default English UI translations.\n01:19:10 - INFO - ================================================================================\n01:19:10 - INFO - \uD83D\uDE80 STEP 1: EXTRACTING BUSINESS CONTEXT, STRATEGIC GOALS, AND PRIORITIES\n01:19:10 - INFO - ================================================================================\n01:19:10 - INFO - \uD83D\uDD0D Calling Business Context Worker for: procore\n01:19:10 - INFO - ⏳ Waiting for LLM response (Business Context extraction)...\n01:19:10 - INFO -    [BUSINESS_CONTEXT_WORKER_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:19:39 - INFO - ✅ Received LLM response, parsing business context...\n01:19:39 - INFO - ✅ Business Context Worker extracted 3 fields\n01:19:39 - INFO - ✅ Business context extraction completed\n01:19:39 - INFO - No user-provided business context. Using LLM context only.\n01:19:39 - INFO - ℹ️ No user strategic goals provided - will generate goals based on business context\n01:19:39 - INFO - ℹ️ No user business domains provided - domains will be inferred from data\n01:19:39 - INFO - ✅ Business context extracted and merged.\n01:19:39 - INFO - ================================================================================\n01:19:39 - INFO - Unstructured data generation is disabled. Skipping...\n01:19:39 - INFO - Collecting batches for parallel processing (MAXIMIZING context utilization)...\n01:19:39 - INFO - Initializing all tables from all databases...\n01:19:39 - INFO - Discovering all tables from 1 databases...\n01:19:39 - INFO - \uD83D\uDD04 TWO-PASS MODE ENABLED: Analyzing table sizes for intelligent batching...\n01:19:39 - INFO - ⚡ Using parallel schema discovery with 5 workers for speed\n01:19:39 - INFO - \uD83D\uDD27 [SCHEMA_DISCOVERY] Parallelism = 5 | FIXED=5 for metadata queries (DB connection limit)\n01:19:39 - INFO - \uD83D\uDD27 [SCHEMA_DISCOVERY] Workers: 5 (max=5)\n01:19:39 - INFO -    └─ Reason: FIXED=5 for metadata queries (DB connection limit)\n01:19:39 - INFO - \uD83D\uDD0D Starting discovery for `virgin_02`.`procore_field`...\n01:19:39 - INFO -       Submitted 1 schemas for parallel discovery...\n01:19:42 - INFO -    ✓ `virgin_02`.`procore_field`: Found 1 tables\n01:19:42 - INFO -       Progress: 1/1 schemas analyzed, 1 tables discovered so far\n01:19:42 - INFO - \uD83D\uDCCA Pass 1 complete: Analyzed 1 tables\n01:19:42 - INFO - \uD83C\uDFAF Pass 2: Creating optimized batches based on table sizes...\n01:19:42 - INFO - \uD83D\uDCCA Dynamic Batch Optimization Complete:\n01:19:42 - INFO -    • Total tables: 1\n01:19:42 - INFO -    • Created batches: 1\n01:19:42 - INFO -    • Average batch size: 1.0 tables\n01:19:42 - INFO -    • Table size distribution:\n01:19:42 - INFO -       - Small (<100 cols): 1\n01:19:42 - INFO -       - Medium (100-250 cols): 0\n01:19:42 - INFO -       - Wide (250-1000 cols): 0\n01:19:42 - INFO -       - Very Wide (>1000 cols): 0\n01:19:42 - INFO - ✅ Two-pass initialization complete: 1 tables in 1 optimized batches\n01:19:42 - INFO - \uD83D\uDCE6 Fetching optimized batch 1/1: 1 tables\n01:19:42 - INFO - \uD83D\uDD27 [COLUMN_FETCH] Parallelism = 5 | FIXED=5 for metadata queries (DB connection limit)\n01:19:43 - INFO -    ✓ Batch 1 loaded: 43 columns from 1 tables\n01:19:43 - INFO - Adding final batch 1 with 43 columns\n01:19:43 - INFO - Table-based batching: 1 tables per call, 1 batches\n01:19:43 - INFO - \uD83D\uDD0D Filtering tables into BUSINESS vs TECHNICAL categories...\n01:19:43 - INFO - \n================================================================================\n01:19:43 - INFO - \uD83D\uDD0D FILTERING TABLES: Business Data vs Technical/Metadata\n01:19:43 - INFO - ================================================================================\n\n01:19:43 - INFO - Dynamic parallelism set to 1 (tables=1, avg_table_chars=5213, tables_per_batch=133, est_batches=1, memory_cap=8)\n01:19:43 - INFO - ✅ Dynamic parallelism: 1 (tables=1, est_batches=1)\n01:19:43 - INFO - Filtering 1 tables into business vs technical categories with 'Aggressive' strategy...\n01:19:43 - INFO - Base prompt size: 32832 chars, Available for tables: 607168 chars\n01:19:43 - INFO - Estimated max tables per batch: 10119\n01:19:43 - INFO - All tables fit in one batch. Processing...\n01:19:43 - INFO - \uD83D\uDD27 [DOMAIN_CLUSTERING] Parallelism = 4 (from max=1) | calculated=4 based on: 1 items (small) + 1 domains (few) + domain detection is heavy\n01:19:43 - INFO - \uD83D\uDD27 [DOMAIN_CLUSTERING] Workers: 4 (max=1)\n01:19:43 - INFO -    └─ Reason: calculated=4 based on: 1 items (small) + 1 domains (few) + domain detection is heavy\n01:19:43 - INFO - Processing 1 classification batch(es) in parallel...\n01:19:43 - INFO - [Batch 1] Processing 1 tables (depth=0)...\n01:19:43 - INFO - ⏳ [Batch 1] Waiting for LLM response (filtering 1 tables into BUSINESS vs TECHNICAL)...\n01:19:43 - INFO -    [FILTER_BUSINESS_TABLES_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:19:48 - INFO - \uD83D\uDD2E✨ HONESTY CHECK [FILTER_BUSINESS_TABLES_PROMPT] Score: 95% | High confidence: timecard clearly records time-worked events with timestamps, core transactional HR/payroll data for construction/field operations ✨\uD83D\uDD2E\n01:19:48 - INFO - ✅ [Batch 1] Received LLM response, parsing classifications...\n01:19:48 - INFO - ✅ [Batch 1] Complete: 1 tables classified\n01:19:48 - INFO - ✅ Classification batch 1/1 completed\n01:19:48 - INFO - ✅ Filtering complete: 1 business tables, 0 technical tables, 0 reference tables\n01:19:48 - INFO - Top business tables by score: virgin_02.procore_field.timecard(90)\n01:19:48 - INFO - ✅ Filtering complete: Proceeding with 1 business tables, excluding 0 technical/metadata tables, excluding 0 reference tables\n01:19:48 - INFO - ✅ Business tables: 1\n01:19:48 - INFO -    \uD83D\uDCCA Master Data tables: 0\n01:19:48 - INFO -    \uD83D\uDCC8 Transactional tables: 1\n01:19:48 - INFO - \uD83D\uDFE1 Reference tables (excluded): 0\n01:19:48 - INFO - ❌ Technical tables (excluded): 0\n01:19:48 - INFO - ================================================================================\n\n01:19:48 - INFO - \uD83D\uDD27 [USE_CASE_GENERATION] Parallelism = 4 (from max=1) | calculated=4 based on: 1 items (small) + ~4K chars/prompt (small) + 43 cols (small schema) + LLM-intensive, 2-pass for transactional tables\n01:19:48 - INFO - \n================================================================================\n01:19:48 - INFO - \uD83D\uDD04 USE CASE GENERATION: SERIAL ENSEMBLE (2 PASSES)\n01:19:48 - INFO - ================================================================================\n01:19:48 - INFO - \uD83D\uDCCB PASS 1: Generate initial use cases from 1 batch(es)\n01:19:48 - INFO - \uD83D\uDCCB PASS 2: Generate NEW use cases not in PASS 1 (with feedback)\n01:19:48 - INFO - \uD83D\uDCBE Using file-based intermediate storage to prevent memory explosion\n01:19:48 - INFO - ================================================================================\n\n01:19:49 - INFO - \uD83D\uDCC1 Initialized intermediate storage at: /tmp/inspire_3jqpy6l2\n01:19:49 - INFO - \uD83D\uDD04 PASS 1: Generating initial use cases...\n01:19:49 - INFO - \n============================================================\n01:19:49 - INFO - \uD83D\uDD04 PASS 1: Initial Use Case Generation\n01:19:49 - INFO - ============================================================\n01:19:49 - INFO - [Batch P1_1] Starting batch processing with 43 columns from 1 tables\n01:19:49 - INFO - ✓ [PASS 1] Submitted batch 1\n01:19:49 - INFO - [Batch P1_1] Tables in call (1): virgin_02.procore_field.timecard\n01:19:49 - INFO - [Batch P1_1] Tables in call (1): virgin_02.procore_field.timecard\n01:19:49 - INFO - ⏳ [Batch P1_1] Sending batch to BOTH AI-focused and STATS-focused prompts in parallel...\n01:19:49 - INFO - ⏳ [Batch P1_1] [AI_USE_CASE_GEN_PROMPT] Waiting for LLM response (may take 3-5 min)...\n01:19:49 - INFO -    [AI_USE_CASE_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:19:49 - INFO - ⏳ [Batch P1_1] [STATS_USE_CASE_GEN_PROMPT] Waiting for LLM response (may take 3-5 min)...\n01:19:49 - INFO -    [STATS_USE_CASE_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:20:49 - INFO - [Batch_P1_1_AI] Still waiting... 60s elapsed (timeout: 300s)01:20:49 - INFO - [Batch_P1_1_STATS] Still waiting... 60s elapsed (timeout: 300s)\n\n01:21:01 - INFO - ✅ [Batch P1_1] [STATS_USE_CASE_GEN_PROMPT] Received LLM response\n01:21:35 - INFO - ✅ [Batch P1_1] [AI_USE_CASE_GEN_PROMPT] Received LLM response\n01:21:35 - INFO - ✅ [Batch P1_1] Received both responses, parsing CSVs...\n01:21:35 - INFO - [Batch P1_1][AI] Starting robust 11-column CSV parsing (SQL and scoring metrics will be assigned separately)...\n01:21:35 - INFO - [Batch P1_1][AI] Robust parsing complete. Found 10 rows.\n01:21:35 - INFO - [Batch P1_1][STATS] Starting robust 11-column CSV parsing (SQL and scoring metrics will be assigned separately)...\n01:21:35 - INFO - [Batch P1_1][STATS] Robust parsing complete. Found 1 rows.\n01:21:35 - INFO - ✅ [Batch P1_1] Merged results: 10 AI use cases + 1 STATS use cases = 11 total\n01:21:35 - INFO - [Batch P1_1] ✓ Table validation passed: All 11 use cases reference existing tables\n01:21:35 - INFO - \n================================================================================\n01:21:35 - INFO - \uD83D\uDCCA TOP USE CASES FROM [Batch P1_1] (for early quality review):\n01:21:35 - INFO - ================================================================================\n\n01:21:35 - INFO - \uD83E\uDD16 Top 5 AI-focused use cases:\n01:21:35 - INFO -    AI-FP1_1-U1: Anticipate Labor Cost Overruns with Proactive Budget Alerts\n01:21:35 - INFO -    AI-FP1_1-U2: Predict Project Completion Delays from Labor Productivity Trends\n01:21:35 - INFO -    AI-FP1_1-U3: Classify Timecards by Approval Risk to Accelerate Payroll Processing\n01:21:35 - INFO -    AI-FP1_1-U4: Detect Fraudulent Timecard Patterns Using Behavioral Anomaly Analysis\n01:21:35 - INFO -    AI-FP1_1-U5: Segment Crews by Productivity Performance to Optimize Project Assignments\n\n01:21:35 - INFO - \uD83D\uDCCA Top 1 STATS-focused use cases:\n01:21:35 - INFO -    AI-SP1_1-U1: Detect Anomalous Labor Cost Patterns by Crew and Site\n\n01:21:35 - INFO - ================================================================================\n\n01:21:35 - INFO - [Batch P1_1] ✓ Subdomain validation passed: All domains have ≥2 subdomains, all subdomains have ≥2 use cases\n01:21:35 - INFO - ✓ [PASS 1] Batch P1_1: 11 use cases (1/1)\n01:21:35 - INFO - ✓ [PASS 1] Batch complete (1/1)\n01:21:35 - INFO - ✅ PASS 1 complete: Generated 11 use cases\n01:21:35 - INFO - ✅ PASS 1 complete: 11 use cases generated\n01:21:35 - INFO - \uD83D\uDD04 PASS 2: Generating NEW use cases from TRANSACTIONAL TABLES (with PASS 1 feedback)...\n01:21:35 - INFO - \n============================================================\n01:21:35 - INFO - \uD83D\uDD04 PASS 2: Ensemble - TRANSACTIONAL TABLES ONLY\n01:21:35 - INFO - ============================================================\n01:21:35 - INFO - \uD83D\uDCCB Feedback: 11 use cases from PASS 1\n01:21:35 - INFO - \uD83D\uDCCA Transactional batches: 1 (focusing on event/transaction data)\n01:21:35 - INFO - \uD83C\uDFAF Goal: Find NEW use cases from transactional data NOT covered in PASS 1\n01:21:36 - INFO - [Batch P2_1] Starting batch processing with 43 columns from 1 tables\n01:21:36 - INFO - ✓ [PASS 2] Submitted transactional batch 1\n01:21:36 - INFO - [Batch P2_1] Tables in call (1): virgin_02.procore_field.timecard\n01:21:36 - INFO - [Batch P2_1] Tables in call (1): virgin_02.procore_field.timecard\n01:21:36 - INFO - ⏳ [Batch P2_1] Sending batch to BOTH AI-focused and STATS-focused prompts in parallel...\n01:21:36 - INFO - ⏳ [Batch P2_1] [AI_USE_CASE_GEN_PROMPT] Waiting for LLM response (may take 3-5 min)...\n01:21:36 - INFO - ⏳ [Batch P2_1] [STATS_USE_CASE_GEN_PROMPT] Waiting for LLM response (may take 3-5 min)...\n01:21:36 - INFO -    [AI_USE_CASE_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:21:36 - INFO -    [STATS_USE_CASE_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:22:36 - INFO - [Batch_P2_1_AI] Still waiting... 60s elapsed (timeout: 300s)\n01:22:36 - INFO - [Batch_P2_1_STATS] Still waiting... 60s elapsed (timeout: 300s)\n01:22:37 - INFO - ✅ [Batch P2_1] [AI_USE_CASE_GEN_PROMPT] Received LLM response\n01:23:20 - INFO - ✅ [Batch P2_1] [STATS_USE_CASE_GEN_PROMPT] Received LLM response\n01:23:20 - INFO - ✅ [Batch P2_1] Received both responses, parsing CSVs...\n01:23:20 - INFO - [Batch P2_1][AI] Starting robust 11-column CSV parsing (SQL and scoring metrics will be assigned separately)...\n01:23:20 - INFO - [Batch P2_1][AI] Robust parsing complete. Found 10 rows.\n01:23:20 - INFO - [Batch P2_1][STATS] Starting robust 11-column CSV parsing (SQL and scoring metrics will be assigned separately)...\n01:23:20 - INFO - [Batch P2_1][STATS] Robust parsing complete. Found 10 rows.\n01:23:20 - INFO - ✅ [Batch P2_1] Merged results: 10 AI use cases + 10 STATS use cases = 20 total\n01:23:20 - INFO - [Batch P2_1] ✓ Table validation passed: All 20 use cases reference existing tables\n01:23:20 - INFO - \n================================================================================\n01:23:20 - INFO - \uD83D\uDCCA TOP USE CASES FROM [Batch P2_1] (for early quality review):\n01:23:20 - INFO - ================================================================================\n\n01:23:20 - INFO - \uD83E\uDD16 Top 5 AI-focused use cases:\n01:23:20 - INFO -    AI-FP2_1-U1: Predict Overtime Cost Escalation by Project with Proactive Budget Reallocation\n01:23:20 - INFO -    AI-FP2_1-U2: Optimize Equipment Utilization ROI by Analyzing Idle Time and Labor Productivity\n01:23:20 - INFO -    AI-FP2_1-U3: Detect Payroll Processing Errors Before Submission Using Pattern Anomaly Detection\n01:23:20 - INFO -    AI-FP2_1-U4: Predict Material Waste by Crew to Reduce Project Costs and Improve Sustainability\n01:23:20 - INFO -    AI-FP2_1-U5: Classify Weather-Related Safety Risks to Prevent Incidents and Reduce Insurance Costs\n\n01:23:20 - INFO - \uD83D\uDCCA Top 5 STATS-focused use cases:\n01:23:20 - INFO -    AI-SP2_1-U1: Simulate Impact of Labor Rate Changes on Project Profitability\n01:23:20 - INFO -    AI-SP2_1-U2: Detect Systematic Timecard Manipulation Patterns Using Multi-Dimensional Anomaly Detection\n01:23:20 - INFO -    AI-SP2_1-U3: Optimize Equipment Utilization Through Geospatial Crew-Equipment Proximity Analysis\n01:23:20 - INFO -    AI-SP2_1-U4: Predict Payroll Processing Errors Before Submission Using Classification\n01:23:20 - INFO -    AI-SP2_1-U5: Analyze Labor Productivity Volatility by Cost Code to Identify Training Needs\n\n01:23:20 - INFO - ================================================================================\n\n01:23:20 - INFO - [Batch P2_1] ✓ Subdomain validation passed: All domains have ≥2 subdomains, all subdomains have ≥2 use cases\n01:23:20 - INFO - ✓ [PASS 2] Batch P2_1: 20 NEW use cases (1/1)\n01:23:20 - INFO - ✓ [PASS 2] Batch complete (1/1)\n01:23:20 - INFO - ✅ PASS 2 complete: Generated 20 additional NEW use cases from transactional tables\n01:23:20 - INFO - ✅ PASS 2 complete: 20 NEW use cases generated\n01:23:20 - INFO - \n============================================================\n01:23:20 - INFO - ✅ SERIAL ENSEMBLE COMPLETE\n01:23:20 - INFO - ============================================================\n01:23:20 - INFO - \uD83D\uDCCA Batch processing complete. Storage stats: 2 batches, 31 use cases, 0.08 MB on disk\n01:23:20 - INFO - Loading use cases from disk for deduplication...\n01:23:20 - INFO - \uD83D\uDD04 Starting domain-level parallel deduplication (skipping global deduplication for max parallelization)...\n01:23:21 - INFO - \uD83D\uDD0D Checking table coverage before clustering and scoring...\n01:23:21 - INFO - \uD83D\uDD0D CATCH-ALL MODE: Checking for BUSINESS tables that were never involved in use cases...\n01:23:21 - INFO - ✅ All BUSINESS tables have been involved in use cases\n01:23:21 - INFO - ✅ All tables have at least one use case - no retry needed\n01:23:21 - INFO - ✅ All tables covered by initial use cases\n01:23:21 - INFO - \uD83D\uDCCA Clustering 31 deduplicated use cases into domains and subdomains...\n01:23:21 - INFO - \uD83C\uDFAF Starting TWO-STEP domain/subdomain clustering for 31 use cases...\n01:23:21 - INFO - \uD83D\uDCCD STEP 1: Detecting domains for all 31 use cases (40,964 chars) - 1/2\n01:23:21 - INFO -    [DOMAIN_FINDER_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:23:37 - INFO - \uD83D\uDD2E✨ HONESTY CHECK [DOMAIN_FINDER_PROMPT] Score: 85% | Created 7 domains for 34 use cases (target ~3-4). All domains are single words and industry-specific to construction. Some domains have <6 cases but acceptable for dataset size. No overlapping words. ✨\uD83D\uDD2E\n01:23:37 - INFO - ✅ LLM response received in 16.6 seconds\n01:23:37 - INFO - ℹ️ Soft warnings (acceptable for small datasets): Domain 'Schedule' has 3 use case(s) (acceptable for small dataset); Domain 'Fraud' has 2 use case(s) (acceptable for small dataset); Domain 'Estimating' has 2 use case(s) (acceptable for small dataset)\n01:23:37 - INFO - ✅ Domain detection successful on attempt 1! Created 13 domains\n01:23:37 - INFO - ℹ️ Note: 10 domains have <4 use cases (acceptable for dataset of 31 total use cases)\n01:23:37 - INFO - \uD83D\uDCCD STEP 1.5: Merging small domains (if any)...\n01:23:37 - INFO -    [DOMAINS_MERGER_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:23:48 - INFO - Merging 'Schedule' (3 cases) → 'Workforce'\n01:23:48 - INFO - Merging 'Fraud' (2 cases) → 'Payroll'\n01:23:48 - INFO - Merging 'Estimating' (2 cases) → 'Procurement'\n01:23:48 - INFO - Merging 'Retention' (1 cases) → 'Workforce'\n01:23:48 - INFO - Merging 'Estimating' (2 cases) → 'Procurement'\n01:23:48 - INFO - Merging 'Equipment' (2 cases) → 'Procurement'\n01:23:48 - INFO - Merging 'Materials' (1 cases) → 'Procurement'\n01:23:48 - INFO - Merging 'Safety' (1 cases) → 'Operations'\n01:23:48 - INFO - Merging 'Training' (2 cases) → 'Workforce'\n01:23:48 - INFO - Merging 'Accounting' (2 cases) → 'Budget'\n01:23:48 - INFO - Merging 'Fraud' (2 cases) → 'Payroll'\n01:23:48 - INFO - Merging 'Equipment' (2 cases) → 'Procurement'\n01:23:48 - INFO - Merging 'Training' (2 cases) → 'Workforce'\n01:23:48 - INFO - Merging 'Schedule' (3 cases) → 'Workforce'\n01:23:48 - INFO - Merging 'Schedule' (3 cases) → 'Workforce'\n01:23:48 - INFO - Merging 'Accounting' (2 cases) → 'Budget'\n01:23:48 - WARNING - WARNING: 1 domains still have fewer than 4 use cases: ['Operations']\n01:23:48 - INFO - \uD83D\uDCCD STEP 2: Detecting subdomains for each domain in parallel...\n01:23:48 - INFO - \uD83D\uDD27 [SUBDOMAIN_DETECTION] Parallelism = 4 (from max=1) | calculated=4 based on: 31 items (large) + 5 domains (few) + subdomain per domain\n01:23:48 - INFO - \uD83D\uDD27 [SUBDOMAIN_DETECTION] Workers: 4 (max=1)\n01:23:48 - INFO -    └─ Reason: calculated=4 based on: 31 items (large) + 5 domains (few) + subdomain per domain\n01:23:48 - INFO - Processing 5 domains for subdomain detection...\n01:23:48 - INFO -    [SUBDOMAIN_DETECTOR_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:23:48 - INFO -    [SUBDOMAIN_DETECTOR_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:23:48 - INFO -    [SUBDOMAIN_DETECTOR_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:23:48 - INFO -    [SUBDOMAIN_DETECTOR_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:23:55 - INFO - \uD83D\uDD2E✨ HONESTY CHECK [SUBDOMAIN_DETECTOR_PROMPT] Score: 85% | All subdomains are exactly 2 words and business-focused. Each has 2+ use cases. Distribution is balanced across 3 subdomains (2-2-2). No overlapping words. Meets all mandatory requirements. ✨\uD83D\uDD2E\n01:23:55 - WARNING - [Domain: Procurement] ⚠️ Subdomain detection attempt 1 has 1 violations\n01:23:55 - INFO -    [SUBDOMAIN_DETECTOR_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:23:55 - INFO - \uD83D\uDD2E✨ HONESTY CHECK [SUBDOMAIN_DETECTOR_PROMPT] Score: 95% | All rules met: 3 subdomains (2-10), all 2-word names, 2+ cases each, no overlaps, business-focused. Minor concern: could debate if groupings are optimal but they are logically sound and meet all mandatory requirements. ✨\uD83D\uDD2E\n01:23:55 - INFO -    [SUBDOMAIN_DETECTOR_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:23:55 - INFO - ✅ Domain 'Budget': Subdomain detection complete (7 use cases)\n01:23:55 - INFO - \uD83D\uDD2E✨ HONESTY CHECK [SUBDOMAIN_DETECTOR_PROMPT] Score: 85% | All rules met: 3 subdomains (2-10 range) ✨\uD83D\uDD2E\n01:23:55 - INFO - ✅ Domain 'Payroll': Subdomain detection complete (6 use cases)\n01:23:57 - INFO - \uD83D\uDD2E✨ HONESTY CHECK [SUBDOMAIN_DETECTOR_PROMPT] Score: 85% | All subdomains are exactly 2 words and business-focused. Each has 2+ use cases. Distribution is balanced across 5 subdomains (2-3 cases each). No overlapping words. Meets all validation requirements. ✨\uD83D\uDD2E\n01:23:57 - WARNING - [Domain: Workforce] ⚠️ Subdomain detection attempt 1 has 1 violations\n01:23:57 - INFO -    [SUBDOMAIN_DETECTOR_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:24:00 - INFO - \uD83D\uDD2E✨ HONESTY CHECK [SUBDOMAIN_DETECTOR_PROMPT] Score: 85% | Met all requirements: 2 subdomains (2-10 range), each with exactly 2 words, no overlapping words, business-focused names, minimum 1 use case per subdomain. However only 2 use cases total limits distribution options. ✨\uD83D\uDD2E\n01:24:00 - WARNING - [Domain: Operations] ⚠️ Subdomain detection attempt 1 has 2 violations\n01:24:00 - INFO -    [SUBDOMAIN_DETECTOR_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:24:02 - INFO - \uD83D\uDD2E✨ HONESTY CHECK [SUBDOMAIN_DETECTOR_PROMPT] Score: 85% | All subdomains have exactly 2 words and 2+ use cases. Consolidated to 3 subdomains to meet requirements. Names are business-focused. Minor concern: could have more granular groupings but constrained by minimum use case rule. ✨\uD83D\uDD2E\n01:24:02 - INFO - ✅ Domain 'Procurement': Subdomain detection complete (5 use cases)\n01:24:06 - INFO - \uD83D\uDD2E✨ HONESTY CHECK [SUBDOMAIN_DETECTOR_PROMPT] Score: 100% | Only 2 use cases available. Merged both into single subdomain to meet minimum 2 use cases per subdomain requirement. Subdomain name is exactly 2 words and business-focused. ✨\uD83D\uDD2E\n01:24:06 - WARNING - [Domain: Operations] ⚠️ Subdomain detection attempt 2 has 1 violations\n01:24:06 - WARNING - [Domain: Operations] Max attempts reached. Using subdomain assignments despite 1 violations\n01:24:06 - WARNING - [Domain: Operations] Violations: Only 1 subdomain(s), minimum required: 2\n01:24:06 - INFO - ✅ Domain 'Operations': Subdomain detection complete (2 use cases)\n01:24:06 - INFO - \uD83D\uDD2E✨ HONESTY CHECK [SUBDOMAIN_DETECTOR_PROMPT] Score: 85% | Met all rules: 2-10 subdomains (6 total) all 2-word names no overlaps 2+ cases each. Balanced distribution across workforce business functions. Minor concern: some groupings could be debated but all are defensible. ✨\uD83D\uDD2E\n01:24:06 - WARNING - [Domain: Workforce] ⚠️ Subdomain detection attempt 2 has 2 violations\n01:24:06 - WARNING - [Domain: Workforce] Max attempts reached. Using subdomain assignments despite 2 violations\n01:24:06 - WARNING - [Domain: Workforce] Violations: Subdomain 'Employee Retention' has only 1 use case(s), minimum required: 2; Subdomain 'Resource Allocation' has only 1 use case(s), minimum required: 2\n01:24:06 - INFO - ✅ Domain 'Workforce': Subdomain detection complete (11 use cases)\n01:24:06 - INFO - ✅ Two-step clustering complete! 31 use cases processed\n01:24:06 - INFO - \uD83D\uDD04 PHASE 1: Scoring use cases per domain in parallel...\n01:24:06 - INFO - \uD83D\uDD27 [SCORING] Parallelism = 4 (from max=1) | calculated=4 based on: 31 items (large) + 5 domains (few) + scoring is rate-limit sensitive\n01:24:06 - INFO - \uD83D\uDCCA PHASE 1: Scoring 31 use cases per domain in parallel\n01:24:06 - INFO - \uD83D\uDCCA Grouped into 5 domains\n01:24:06 - INFO - \n================================================================================\n01:24:06 - INFO - \uD83D\uDCCA PHASE 1: SCORING PER DOMAIN (PARALLEL)\n01:24:06 - INFO - ================================================================================\n01:24:06 - INFO - Total use cases: 31\n01:24:06 - INFO - Total domains: 5\n01:24:06 - INFO - \uD83D\uDD27 [SCORING] Workers: 4 (max=1)\n01:24:06 - INFO -    └─ Reason: calculated=4 based on: 31 items (large) + 5 domains (few) + scoring is rate-limit sensitive\n01:24:06 - INFO - Total timeout: 1800s (30 min)\n01:24:06 - INFO - ================================================================================\n\n01:24:06 - INFO -    - Budget: 7 use cases\n01:24:06 - INFO -    - Operations: 2 use cases\n01:24:06 - INFO -    - Payroll: 6 use cases\n01:24:06 - INFO -    - Procurement: 5 use cases\n01:24:06 - INFO -    - Workforce: 11 use cases\n01:24:06 - INFO - \uD83D\uDCCA [Budget] Scoring 7 use cases...\n01:24:06 - INFO - Starting LLM-based scoring for 7 use cases...\n01:24:06 - INFO - \uD83D\uDCCA [Payroll] Scoring 6 use cases...\n01:24:06 - INFO - \uD83D\uDCCA [Procurement] Scoring 5 use cases...\n01:24:06 - INFO - ⏳ [Budget] Waiting for LLM response (scoring 7 use cases)...\n01:24:06 - INFO - Starting LLM-based scoring for 6 use cases...\n01:24:06 - INFO - \uD83D\uDCCA [Operations] Scoring 2 use cases...\n01:24:06 - INFO - Starting LLM-based scoring for 5 use cases...\n01:24:06 - INFO -    [SCORE_USE_CASES_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:24:06 - INFO - ⏳ [Payroll] Waiting for LLM response (scoring 6 use cases)...\n01:24:06 - INFO - Starting LLM-based scoring for 2 use cases...\n01:24:06 - INFO - ⏳ [Operations] Waiting for LLM response (scoring 2 use cases)...\n01:24:06 - INFO -    [SCORE_USE_CASES_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:24:06 - INFO - ⏳ [Procurement] Waiting for LLM response (scoring 5 use cases)...\n01:24:06 - INFO -    [SCORE_USE_CASES_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:24:06 - INFO -    [SCORE_USE_CASES_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:24:23 - INFO - ✅ [Operations] Received LLM response, processing results...\n01:24:23 - INFO - Received scoring for 2 use cases from LLM for domain 'Operations'\n01:24:23 - INFO - Pass 1 scoring complete for 2 use cases in domain 'Operations'\n01:24:23 - INFO - Scoring complete for 2 use cases in domain 'Operations'\n01:24:23 - INFO - ✅ [Operations] Scoring complete\n01:24:23 - INFO - \uD83D\uDCCA [Workforce] Scoring 11 use cases...\n01:24:23 - INFO - ✓ Scored domain 1/5: Operations (2 use cases)\n01:24:23 - INFO - Starting LLM-based scoring for 11 use cases...\n01:24:23 - INFO - ⏳ [Workforce] Waiting for LLM response (scoring 11 use cases)...\n01:24:23 - INFO - ✓ Domain 1/5 scoring complete: Operations\n01:24:23 - INFO -    [SCORE_USE_CASES_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:24:28 - INFO - ✅ [Procurement] Received LLM response, processing results...\n01:24:28 - INFO - Received scoring for 5 use cases from LLM for domain 'Procurement'\n01:24:28 - INFO - Pass 1 scoring complete for 5 use cases in domain 'Procurement'\n01:24:28 - INFO - Scoring complete for 5 use cases in domain 'Procurement'\n01:24:28 - INFO - ✅ [Procurement] Scoring complete\n01:24:28 - INFO - ✓ Scored domain 2/5: Procurement (5 use cases)\n01:24:28 - INFO - ✓ Domain 2/5 scoring complete: Procurement\n01:24:29 - INFO - ✅ [Payroll] Received LLM response, processing results...\n01:24:29 - INFO - Received scoring for 6 use cases from LLM for domain 'Payroll'\n01:24:29 - INFO - Pass 1 scoring complete for 6 use cases in domain 'Payroll'\n01:24:29 - INFO - Scoring complete for 6 use cases in domain 'Payroll'\n01:24:29 - INFO - ✅ [Payroll] Scoring complete\n01:24:29 - INFO - ✓ Scored domain 3/5: Payroll (6 use cases)\n01:24:29 - INFO - ✓ Domain 3/5 scoring complete: Payroll\n01:24:33 - INFO - ✅ [Budget] Received LLM response, processing results...\n01:24:33 - INFO - Received scoring for 7 use cases from LLM for domain 'Budget'\n01:24:33 - INFO - Pass 1 scoring complete for 7 use cases in domain 'Budget'\n01:24:33 - INFO - Scoring complete for 7 use cases in domain 'Budget'\n01:24:33 - INFO - ✅ [Budget] Scoring complete\n01:24:33 - INFO - ✓ Scored domain 4/5: Budget (7 use cases)\n01:24:33 - INFO - ✓ Domain 4/5 scoring complete: Budget\n01:25:03 - INFO - ✅ [Workforce] Received LLM response, processing results...\n01:25:03 - INFO - Received scoring for 11 use cases from LLM for domain 'Workforce'\n01:25:03 - INFO - Pass 1 scoring complete for 11 use cases in domain 'Workforce'\n01:25:03 - INFO - Scoring complete for 11 use cases in domain 'Workforce'\n01:25:03 - INFO - ✅ [Workforce] Scoring complete\n01:25:03 - INFO - ✓ Scored domain 5/5: Workforce (11 use cases)\n01:25:03 - INFO - ✓ Domain 5/5 scoring complete: Workforce\n01:25:03 - INFO - \n================================================================================\n01:25:03 - INFO - ✅ PHASE 1 COMPLETE: ALL DOMAINS SCORED\n01:25:03 - INFO - ================================================================================\n01:25:03 - INFO - Total scored use cases: 31\n01:25:03 - INFO - ================================================================================\n\n01:25:03 - INFO - ✅ Phase 1 complete: 31 use cases scored across all domains\n01:25:03 - INFO - Applied priority normalization scale factor 1.07 with target max 9.69\n01:25:03 - INFO - ✅ Phase 1 normalized across 31 use cases\n01:25:03 - INFO - ✅ Phase 1 complete: All use cases scored\n01:25:03 - INFO - \uD83D\uDD04 Starting INTELLIGENT domain-level deduplication (using scores)...\n01:25:03 - INFO - \uD83D\uDD04 Starting intelligent domain-level deduplication for 31 scored use cases...\n01:25:03 - INFO - \uD83D\uDCCA Grouped use cases into 5 domains\n01:25:03 - INFO - \uD83D\uDD27 [DEDUPLICATION] Parallelism = 4 (from max=1) | calculated=4 based on: 31 items (large) + 5 domains (few) + dedup needs LLM per domain\n01:25:03 - INFO - \n================================================================================\n01:25:03 - INFO - \uD83D\uDD04 DEDUPLICATION: Processing 5 domains (31 total use cases)\n01:25:03 - INFO - ================================================================================\n01:25:03 - INFO - \uD83D\uDD27 [DEDUPLICATION] Workers: 4 (max=1)\n01:25:03 - INFO -    └─ Reason: calculated=4 based on: 31 items (large) + 5 domains (few) + dedup needs LLM per domain\n01:25:03 - INFO - Overall timeout: 675s (11 min)\n01:25:03 - INFO - ================================================================================\n\n01:25:03 - INFO - [Operations] Deduplicating 2 use cases...\n01:25:03 - INFO - [Procurement] Deduplicating 5 use cases...\n01:25:03 - INFO - [Payroll] Deduplicating 6 use cases...\n01:25:03 - INFO - [Budget] Deduplicating 7 use cases...\n01:25:03 - INFO - ⏳ [Budget] Waiting for LLM response...\n01:25:03 - INFO -    [REVIEW_USE_CASES_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:25:03 - INFO - ⏳ [Payroll] Waiting for LLM response...\n01:25:03 - INFO - ⏳ [Operations] Waiting for LLM response...\n01:25:03 - INFO -    [REVIEW_USE_CASES_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:25:03 - INFO -    [REVIEW_USE_CASES_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:25:03 - INFO - ⏳ [Procurement] Waiting for LLM response...\n01:25:03 - INFO -    [REVIEW_USE_CASES_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:25:08 - INFO - \uD83D\uDD2E✨ HONESTY CHECK [REVIEW_USE_CASES_PROMPT] Score: 95% | Both use cases kept - they address distinct business problems (operational insights vs safety risks) despite using same table. Different analytical focus and outcomes. No duplicates detected in 2-item set. ✨\uD83D\uDD2E\n01:25:08 - INFO - ✅ [Operations] Retained 2 use cases, removed 0\n01:25:08 - INFO - [Workforce] Deduplicating 11 use cases...\n01:25:08 - INFO - [Deduplication] ✓ Domain 1/5 complete: Operations (5.1s elapsed)\n01:25:08 - INFO - ⏳ [Workforce] Waiting for LLM response...\n01:25:08 - INFO -    [REVIEW_USE_CASES_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:25:09 - INFO - \uD83D\uDD2E✨ HONESTY CHECK [REVIEW_USE_CASES_PROMPT] Score: 85% | All 5 use cases are distinct with different business objectives. AI-FP2_1-U4 and AI-SP2_1-U3 both optimize equipment but focus on different aspects (waste vs proximity). No clear duplicates found despite aggressive review. ✨\uD83D\uDD2E\n01:25:09 - INFO - ✅ [Procurement] Retained 5 use cases, removed 0\n01:25:09 - INFO - [Deduplication] ✓ Domain 2/5 complete: Procurement (5.9s elapsed)\n01:25:09 - INFO - \uD83D\uDD2E✨ HONESTY CHECK [REVIEW_USE_CASES_PROMPT] Score: 85% | Kept 5/7 use cases. Removed AI-FP2_1-U9 and AI-SP2_1-U8 as duplicates (both detect cost code misallocation/misclassification - same core concept). All others address distinct business problems despite similar domain. ✨\uD83D\uDD2E\n01:25:09 - INFO - ✅ [Budget] Retained 5 use cases, removed 2\n01:25:09 - INFO - [Deduplication] ✓ Domain 3/5 complete: Budget (6.3s elapsed)\n01:25:09 - INFO - \uD83D\uDD2E✨ HONESTY CHECK [REVIEW_USE_CASES_PROMPT] Score: 85% | Removed 2 duplicates: AI-FP2_1-U7 (duplicate of U3 - both classify/predict approval bottlenecks) and AI-SP2_1-U4 (duplicate of AI-FP2_1-U3 - both predict payroll errors). Kept 4 distinct use cases with different core intents. ✨\uD83D\uDD2E\n01:25:09 - INFO - ✅ [Payroll] Retained 4 use cases, removed 2\n01:25:09 - INFO - [Deduplication] ✓ Domain 4/5 complete: Payroll (6.7s elapsed)\n01:25:15 - INFO - \uD83D\uDD2E✨ HONESTY CHECK [REVIEW_USE_CASES_PROMPT] Score: 85% | Removed 4 duplicates: U5/U8 (crew optimization overlap), U7 (distinct-turnover), U6/U8/SP variants (training/scheduling/crew analysis overlap). Kept highest ROI representatives. Some judgment calls on similarity thresholds. ✨\uD83D\uDD2E\n01:25:15 - INFO - ✅ [Workforce] Retained 7 use cases, removed 4\n01:25:15 - INFO - [Deduplication] ✓ Domain 5/5 complete: Workforce (12.6s elapsed)\n01:25:15 - INFO - ✅ Domain-level deduplication complete: Retained 23 use cases, removed 8 (25.8%)\n01:25:15 - INFO - ✅ Volume filter applied (Base (All)), but no use cases were dropped.\n01:25:15 - INFO - Generating English Excel before SQL generation...\n01:25:15 - INFO - --- Starting Excel Catalog Generation with XlsxWriter for English ---\n01:25:15 - INFO - Installing required Excel package: xlsxwriter...\nCollecting xlsxwriter\n  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\nDownloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\nInstalling collected packages: xlsxwriter\nSuccessfully installed xlsxwriter-3.2.9\n01:25:17 - INFO - Successfully installed xlsxwriter.\n01:25:17 - INFO - Creating Excel file at /tmp/tmp081k2n_d.xlsx\n01:25:17 - INFO - Excel file created successfully with native tables\n01:25:17 - INFO - Uploading Excel to workspace path: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/docs/procore-dbx_inspire.xlsx\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0.1\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:25:18 - INFO - Success! Excel Catalog uploaded to: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/docs/procore-dbx_inspire.xlsx\n01:25:18 - INFO - Success! Excel Catalog (English) generated: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/docs/procore-dbx_inspire.xlsx\n01:25:18 - INFO - Generating executive summaries for notebooks...\n01:25:18 - INFO -    [SUMMARY_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:25:41 - INFO - \uD83D\uDD2E✨ HONESTY CHECK [SUMMARY_GEN_PROMPT] Score: 72% | Generic summaries lacking specific Procore context or construction industry depth. No actual use case details provided. Formulaic structure without deep strategic insight or differentiation. ✨\uD83D\uDD2E\n01:25:41 - INFO - LLM summaries (CSV) received for English.\n01:25:41 - INFO - Found CSV header at index 0. Parsing as 3-column.\n01:25:41 - INFO - Successfully parsed 6 summaries for English. Transliterated name: Procore\n01:25:41 - INFO - \uD83D\uDE80 Starting PDF/PPTX documentation generation in parallel with SQL...\n01:25:41 - INFO - \uD83D\uDCC4 Documentation generation starting in background (languages: English)\n01:25:41 - INFO - --- Starting Document Generation for Languages: ['English'] ---\n01:25:41 - INFO - \uD83D\uDD04 PHASE 2: Domain-by-domain SQL generation & notebook creation...\n01:25:41 - INFO - \n================================================================================\n01:25:41 - INFO - Checking and installing required dependencies before starting translations...\n01:25:41 - INFO - \uD83D\uDD27 PHASE 2: DOMAIN-BY-DOMAIN SQL & NOTEBOOKS\n01:25:41 - INFO - Checking PDF dependencies (weasyprint)...\n01:25:41 - INFO - ================================================================================\n01:25:41 - INFO - Total use cases: 23\n01:25:41 - INFO - PDF package (weasyprint) not found. Installing...\n01:25:41 - INFO - Parallel workers: 1\n01:25:41 - INFO - Strategy: Generate SQL per domain → Create notebook → Move to next domain\n01:25:41 - INFO - Order: Smallest domains first (for quick demo testing)\n01:25:41 - INFO - ================================================================================\n\n01:25:41 - INFO - \n================================================================================\n01:25:41 - INFO - \uD83C\uDFAF DOMAIN-BY-DOMAIN SQL GENERATION & NOTEBOOK CREATION\n01:25:41 - INFO - ================================================================================\n01:25:41 - INFO - \uD83D\uDCCA Total: 23 use cases across 5 domains\n01:25:41 - INFO - \uD83D\uDCCB Processing order (smallest domains first for quick testing):\n01:25:41 - INFO -    1. N05-Operations: 2 use cases (impact: 8.6)\n01:25:41 - INFO -    2. N02-Payroll: 4 use cases (impact: 9.0)\n01:25:41 - INFO -    3. N01-Budget: 5 use cases (impact: 9.0)\n01:25:41 - INFO -    4. N04-Procurement: 5 use cases (impact: 8.8)\n01:25:41 - INFO -    5. N03-Workforce: 7 use cases (impact: 9.0)\n01:25:41 - INFO - ================================================================================\n\n01:25:41 - INFO - Starting domain-by-domain SQL generation for 23 use cases across 5 domains\n01:25:41 - INFO - Domain order (by size): [(2, 'N05', 'Operations'), (4, 'N02', 'Payroll'), (5, 'N01', 'Budget'), (5, 'N04', 'Procurement'), (7, 'N03', 'Workforce')]\n01:25:41 - INFO - \uD83D\uDD27 Building schema index from 43 columns for fast lookup...\n01:25:41 - INFO -    ✓ Schema index built with 2 table entries\n01:25:41 - INFO - \n================================================================================\n01:25:41 - INFO - \uD83C\uDFE2 DOMAIN 1/5: OPERATIONS (Notebook: N05)\n01:25:41 - INFO - ================================================================================\n01:25:41 - INFO -    \uD83D\uDCCA Use cases in this domain: 2\n01:25:41 - INFO -    \uD83D\uDD04 Progress: 0/23 use cases completed so far\n01:25:41 - INFO - \n\uD83C\uDFE2 [1/5] Starting domain: Operations (N05, 2 use cases)\n01:25:41 - INFO - \n   \uD83D\uDCDD PHASE 1: Generating SQL for 2 use cases (wave pattern)...\n01:25:41 - INFO - \uD83D\uDD27 [SQL_GENERATION] Parallelism = 4 (from max=1) | calculated=4 based on: 2 items (small) + ~2K chars/prompt (small) + 43 cols (small schema) + SQL gen is complex\n01:25:41 - INFO - \uD83D\uDD27 [SQL_GENERATION] Workers: 4 (max=1)\n01:25:41 - INFO -    └─ Reason: calculated=4 based on: 2 items (small) + ~2K chars/prompt (small) + 43 cols (small schema) + SQL gen is complex\n01:25:41 - INFO -    \uD83D\uDD01 Wave 1: processing 2 use cases with parallelism 4\n01:25:41 - INFO -       ▶️ Wave 1: 2 use cases, parallelism 4\n01:25:41 - INFO - \uD83D\uDD27 [N05-AI01] Starting SQL generation...\n01:25:41 - INFO -    [N05-AI01] 4 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:25:41 - INFO - \uD83D\uDD27 [N05-AI02] Starting SQL generation...\n01:25:42 - INFO -    [N05-AI02] 6 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:25:41 - INFO - ⏳ [N05-AI01] Waiting for LLM response (SQL generation, timeout=300s)...\n01:25:42 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:25:42 - INFO - ⏳ [N05-AI02] Waiting for LLM response (SQL generation, timeout=360s)...\n01:25:42 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\nCollecting weasyprint\n  Downloading weasyprint-68.0-py3-none-any.whl.metadata (3.7 kB)\nCollecting pydyf>=0.11.0 (from weasyprint)\n  Downloading pydyf-0.12.1-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: cffi>=0.6 in /databricks/python3/lib/python3.12/site-packages (from weasyprint) (1.16.0)\nCollecting tinyhtml5>=2.0.0b1 (from weasyprint)\n  Downloading tinyhtml5-2.0.0-py3-none-any.whl.metadata (2.9 kB)\nCollecting tinycss2>=1.5.0 (from weasyprint)\n  Downloading tinycss2-1.5.1-py3-none-any.whl.metadata (3.0 kB)\nCollecting cssselect2>=0.8.0 (from weasyprint)\n  Downloading cssselect2-0.8.0-py3-none-any.whl.metadata (2.9 kB)\nCollecting Pyphen>=0.9.1 (from weasyprint)\n  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: Pillow>=9.1.0 in /databricks/python3/lib/python3.12/site-packages (from weasyprint) (10.3.0)\nCollecting fonttools>=4.59.2 (from fonttools[woff]>=4.59.2->weasyprint)\n  Downloading fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (114 kB)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.12/site-packages (from cffi>=0.6->weasyprint) (2.21)\nCollecting webencodings (from cssselect2>=0.8.0->weasyprint)\n  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\nCollecting brotli>=1.0.1 (from fonttools[woff]>=4.59.2->weasyprint)\n  Downloading brotli-1.2.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.1 kB)\nCollecting zopfli>=0.1.4 (from fonttools[woff]>=4.59.2->weasyprint)\n  Downloading zopfli-0.4.0-cp310-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.5 kB)\nDownloading weasyprint-68.0-py3-none-any.whl (319 kB)\nDownloading cssselect2-0.8.0-py3-none-any.whl (15 kB)\nDownloading fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (5.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/5.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.0/5.0 MB\u001B[0m \u001B[31m82.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading pydyf-0.12.1-py3-none-any.whl (8.0 kB)\nDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.1 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m112.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tinycss2-1.5.1-py3-none-any.whl (28 kB)\nDownloading tinyhtml5-2.0.0-py3-none-any.whl (39 kB)\nDownloading brotli-1.2.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.4 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.4/1.4 MB\u001B[0m \u001B[31m121.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\nDownloading zopfli-0.4.0-cp310-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (847 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/847.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m847.1/847.1 kB\u001B[0m \u001B[31m98.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: webencodings, brotli, zopfli, tinyhtml5, tinycss2, Pyphen, pydyf, fonttools, cssselect2, weasyprint\n  Attempting uninstall: fonttools\n    Found existing installation: fonttools 4.51.0\n    Not uninstalling fonttools at /databricks/python3/lib/python3.12/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-8dab356a-3762-4dad-8e65-2e185c41fc88\n    Can't uninstall 'fonttools'. No files were found to uninstall.\nSuccessfully installed Pyphen-0.17.2 brotli-1.2.0 cssselect2-0.8.0 fonttools-4.61.1 pydyf-0.12.1 tinycss2-1.5.1 tinyhtml5-2.0.0 weasyprint-68.0 webencodings-0.5.1 zopfli-0.4.0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0.1\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:25:46 - INFO - ✓ PDF package (weasyprint) installed successfully.\n01:25:46 - INFO - Checking Excel dependencies (pandas, openpyxl)...\n01:25:46 - INFO - Excel packages not found. Installing...\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.12/site-packages (1.5.3)\nCollecting openpyxl\n  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: numpy>=1.21.0 in /databricks/python3/lib/python3.12/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas) (2024.1)\nCollecting et-xmlfile (from openpyxl)\n  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\nDownloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\nDownloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\nInstalling collected packages: et-xmlfile, openpyxl\nSuccessfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0.1\u001B[0m\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:25:48 - INFO - ✓ Excel packages (pandas, openpyxl) installed successfully.\n01:25:48 - INFO - ✓ Proceeding with translations and artifact generation (fallback to .md/.csv if needed)...\n01:25:48 - INFO - Using default English UI translations.\n01:25:48 - INFO - \uD83D\uDD27 [TRANSLATION] Parallelism = 4 (from max=1) | calculated=4 based on: 23 items (medium) + translation LLM calls\n01:25:48 - INFO - \uD83D\uDD27 [TRANSLATION] Workers: 4 (max=1)\n01:25:48 - INFO -    └─ Reason: calculated=4 based on: 23 items (medium) + translation LLM calls\n01:25:48 - INFO - Preparing English artifacts (no translation needed).\n01:25:48 - INFO -    [SUMMARY_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:26:16 - INFO - \uD83D\uDD2E✨ HONESTY CHECK [SUMMARY_GEN_PROMPT] Score: 72% | Generic summaries lacking specific Procore context or construction industry depth. No actual use case details provided. Formulaic structure without deep strategic insight or differentiation. ✨\uD83D\uDD2E\n01:26:16 - INFO - LLM summaries (CSV) received for English.\n01:26:16 - INFO - Found CSV header at index 0. Parsing as 3-column.\n01:26:16 - INFO - Successfully parsed 6 summaries for English. Transliterated name: Procore\n01:26:16 - INFO - Waiting for 0 language packs to complete...\n01:26:16 - INFO - Language pack processing timeout set to 0s (0 minutes)\n01:26:16 - INFO - \uD83D\uDD27 [ARTIFACT_WRITING] Parallelism = 4 | few items (3), using 1 workers\n01:26:16 - INFO - \uD83D\uDD27 [ARTIFACT_WRITING] Workers: 4 (max=1)\n01:26:16 - INFO -    └─ Reason: few items (3), using 1 workers\n01:26:16 - INFO - Translations complete. Starting artifact generation for 1 languages...\n01:26:16 - INFO - Submitting writing jobs for English...\n01:26:16 - INFO - --- Starting Markdown Catalog Generation for English ---\n01:26:16 - INFO - --- Starting CSV Catalog Generation for English ---\n01:26:16 - INFO - --- Starting PDF Catalog Generation for English ---\n01:26:16 - INFO - Skipping Excel generation for English (already generated).\n01:26:16 - INFO - PDF package (weasyprint) already installed.\n01:26:16 - INFO - Building HTML for PDF (English)...\n01:26:16 - INFO - Generating PDF at local temp path: /tmp/tmpcpa89ase.pdf\n01:26:17 - INFO - Success! CSV Catalog uploaded to: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/docs/procore-dbx_inspire.csv\n01:26:17 - INFO - Success! CSV Catalog (English) generated: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/docs/procore-dbx_inspire.csv\n01:26:17 - INFO - Success! Markdown Catalog uploaded to: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/docs/procore-dbx_inspire.md\n01:26:17 - INFO - Success! Markdown Catalog (English) generated: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/docs/procore-dbx_inspire.md\n01:26:17 - INFO - ✓ English Markdown completed\n01:26:17 - INFO - ✓ English CSV completed\n01:26:19 - INFO - Uploading PDF to workspace path: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/docs/procore-dbx_inspire_en.pdf\n01:26:20 - INFO - Success! PDF Catalog uploaded to: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/docs/procore-dbx_inspire_en.pdf\n01:26:20 - INFO - Success! PDF Catalog (English) generated: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/docs/procore-dbx_inspire_en.pdf\n01:26:20 - INFO - ✓ English PDF completed\n01:26:20 - INFO - All artifact writing jobs completed.\n01:26:42 - INFO - [Generate_SQL_N05-AI01_Wave] Still waiting... 60s elapsed (timeout: 300s)\n01:26:42 - INFO - [Generate_SQL_N05-AI02_Wave] Still waiting... 60s elapsed (timeout: 360s)\n01:26:53 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:26:53 - INFO - ✅ [N05-AI02] Received LLM response (18806 chars)\n01:26:53 - INFO - ✓ SQL generated for N05-AI02 in 71.7s\n01:27:07 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:27:07 - INFO - ✅ [N05-AI01] Received LLM response (20852 chars)\n01:27:07 - INFO - ✓ SQL generated for N05-AI01 in 86.0s\n01:27:07 - INFO -       ✅ Wave 1 done in 86.0s: 2 OK, 0 Failed\n01:27:07 - INFO - \n   ✅ SQL Generation Complete: 2 succeeded, 0 failed (86.0s)\n01:27:07 - INFO - \n   \uD83D\uDCD3 PHASE 2: Creating notebook for domain 'Operations'...\n01:27:07 - INFO - Added domain overview cell for 'Operations'\n01:27:07 - INFO - Importing notebook 'N05-operations.ipynb' (attempt 1/2)...\n01:27:08 - INFO - ✓ Notebook 'N05-operations.ipynb' imported successfully in 0.4s\n01:27:08 - INFO - Notebook is located at: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/notebooks/N05-operations.ipynb\n01:27:08 - INFO -    ✓ Notebook saved to: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/notebooks/N05-operations.ipynb\n01:27:08 - INFO - \n********************************************************************************\n01:27:08 - INFO - \uD83C\uDF89 DOMAIN 'OPERATIONS' COMPLETE!\n01:27:08 - INFO - ********************************************************************************\n01:27:08 - INFO -    \uD83D\uDCD3 Notebook: N05-operations.ipynb\n01:27:08 - INFO -    \uD83D\uDCCA Use cases: 2 (2 SQL OK, 0 SQL Failed)\n01:27:08 - INFO -    ⏱️  Total time: 86.6s (SQL: 86.0s, Notebook: 0.7s)\n01:27:08 - INFO -    ✅ READY FOR TESTING!\n01:27:08 - INFO - ********************************************************************************\n\n01:27:08 - INFO - \uD83C\uDF89 [1/5] Domain 'Operations' notebook 'N05-operations.ipynb' READY FOR INSPECTION\n01:27:08 - INFO -    \uD83D\uDCC8 Progress: 2/23 use cases (8%)\n01:27:08 - INFO -    ⏳ Estimated time remaining: 15.2 minutes (4 domains left)\n01:27:08 - INFO - \n================================================================================\n01:27:08 - INFO - \uD83C\uDFE2 DOMAIN 2/5: PAYROLL (Notebook: N02)\n01:27:08 - INFO - ================================================================================\n01:27:08 - INFO -    \uD83D\uDCCA Use cases in this domain: 4\n01:27:08 - INFO -    \uD83D\uDD04 Progress: 2/23 use cases completed so far\n01:27:08 - INFO - \n\uD83C\uDFE2 [2/5] Starting domain: Payroll (N02, 4 use cases)\n01:27:08 - INFO - \n   \uD83D\uDCDD PHASE 1: Generating SQL for 4 use cases (wave pattern)...\n01:27:08 - INFO - \uD83D\uDD27 [SQL_GENERATION] Parallelism = 4 (from max=1) | calculated=4 based on: 4 items (small) + ~2K chars/prompt (small) + 43 cols (small schema) + SQL gen is complex\n01:27:08 - INFO - \uD83D\uDD27 [SQL_GENERATION] Workers: 4 (max=1)\n01:27:08 - INFO -    └─ Reason: calculated=4 based on: 4 items (small) + ~2K chars/prompt (small) + 43 cols (small schema) + SQL gen is complex\n01:27:08 - INFO -    \uD83D\uDD01 Wave 1: processing 4 use cases with parallelism 4\n01:27:08 - INFO -       ▶️ Wave 1: 4 use cases, parallelism 4\n01:27:08 - INFO - \uD83D\uDD27 [N02-AI01] Starting SQL generation...\n01:27:08 - INFO - \uD83D\uDD27 [N02-AI02] Starting SQL generation...\n01:27:08 - INFO -    [N02-AI01] 6 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:27:08 - INFO -    [N02-AI02] 5 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:27:08 - INFO - \uD83D\uDD27 [N02-ST01] Starting SQL generation...\n01:27:08 - INFO - \uD83D\uDD27 [N02-AI03] Starting SQL generation...\n01:27:08 - INFO -    [N02-AI03] 4 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:27:08 - INFO - ⏳ [N02-AI02] Waiting for LLM response (SQL generation, timeout=330s)...\n01:27:08 - INFO -    [N02-ST01] 6 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:27:08 - INFO - ⏳ [N02-ST01] Waiting for LLM response (SQL generation, timeout=360s)...\n01:27:08 - INFO - ⏳ [N02-AI03] Waiting for LLM response (SQL generation, timeout=300s)...\n01:27:08 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:27:08 - INFO - ⏳ [N02-AI01] Waiting for LLM response (SQL generation, timeout=360s)...\n01:27:08 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:27:08 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:27:08 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:28:08 - INFO - [Generate_SQL_N02-AI02_Wave] Still waiting... 60s elapsed (timeout: 330s)\n01:28:09 - INFO - [Generate_SQL_N02-ST01_Wave] Still waiting... 60s elapsed (timeout: 360s)\n01:28:09 - INFO - [Generate_SQL_N02-AI03_Wave] Still waiting... 60s elapsed (timeout: 300s)\n01:28:09 - INFO - [Generate_SQL_N02-AI01_Wave] Still waiting... 60s elapsed (timeout: 360s)\n01:28:34 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:28:34 - INFO - ✅ [N02-AI03] Received LLM response (21327 chars)\n01:28:34 - INFO - ✓ SQL generated for N02-AI03 in 85.8s\n01:28:46 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:28:46 - INFO - ✅ [N02-AI01] Received LLM response (25752 chars)\n01:28:46 - INFO - ✓ SQL generated for N02-AI01 in 98.2s\n01:28:52 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:28:52 - INFO - ✅ [N02-AI02] Received LLM response (26748 chars)\n01:28:52 - INFO - ✓ SQL generated for N02-AI02 in 103.6s\n01:29:08 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:29:08 - INFO - ✅ [N02-ST01] Received LLM response (32496 chars)\n01:29:08 - INFO - ✓ SQL generated for N02-ST01 in 119.5s\n01:29:08 - INFO -       ✅ Wave 1 done in 119.6s: 4 OK, 0 Failed\n01:29:08 - INFO - \n   ✅ SQL Generation Complete: 4 succeeded, 0 failed (119.6s)\n01:29:08 - INFO - \n   \uD83D\uDCD3 PHASE 2: Creating notebook for domain 'Payroll'...\n01:29:08 - INFO - Added domain overview cell for 'Payroll'\n01:29:08 - INFO - Importing notebook 'N02-payroll.ipynb' (attempt 1/2)...\n01:29:08 - INFO - ✓ Notebook 'N02-payroll.ipynb' imported successfully in 0.4s\n01:29:08 - INFO - Notebook is located at: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/notebooks/N02-payroll.ipynb\n01:29:08 - INFO -    ✓ Notebook saved to: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/notebooks/N02-payroll.ipynb\n01:29:08 - INFO - \n********************************************************************************\n01:29:08 - INFO - \uD83C\uDF89 DOMAIN 'PAYROLL' COMPLETE!\n01:29:08 - INFO - ********************************************************************************\n01:29:08 - INFO -    \uD83D\uDCD3 Notebook: N02-payroll.ipynb\n01:29:08 - INFO -    \uD83D\uDCCA Use cases: 4 (4 SQL OK, 0 SQL Failed)\n01:29:08 - INFO -    ⏱️  Total time: 120.2s (SQL: 119.6s, Notebook: 0.6s)\n01:29:08 - INFO -    ✅ READY FOR TESTING!\n01:29:08 - INFO - ********************************************************************************\n\n01:29:08 - INFO - \uD83C\uDF89 [2/5] Domain 'Payroll' notebook 'N02-payroll.ipynb' READY FOR INSPECTION\n01:29:08 - INFO -    \uD83D\uDCC8 Progress: 6/23 use cases (26%)\n01:29:08 - INFO -    ⏳ Estimated time remaining: 9.8 minutes (3 domains left)\n01:29:08 - INFO - \n================================================================================\n01:29:08 - INFO - \uD83C\uDFE2 DOMAIN 3/5: BUDGET (Notebook: N01)\n01:29:08 - INFO - ================================================================================\n01:29:08 - INFO -    \uD83D\uDCCA Use cases in this domain: 5\n01:29:08 - INFO -    \uD83D\uDD04 Progress: 6/23 use cases completed so far\n01:29:08 - INFO - \n\uD83C\uDFE2 [3/5] Starting domain: Budget (N01, 5 use cases)\n01:29:08 - INFO - \n   \uD83D\uDCDD PHASE 1: Generating SQL for 5 use cases (wave pattern)...\n01:29:08 - INFO - \uD83D\uDD27 [SQL_GENERATION] Parallelism = 4 (from max=1) | calculated=4 based on: 5 items (small) + ~2K chars/prompt (small) + 43 cols (small schema) + SQL gen is complex\n01:29:08 - INFO - \uD83D\uDD27 [SQL_GENERATION] Workers: 4 (max=1)\n01:29:08 - INFO -    └─ Reason: calculated=4 based on: 5 items (small) + ~2K chars/prompt (small) + 43 cols (small schema) + SQL gen is complex\n01:29:08 - INFO -    \uD83D\uDD01 Wave 1: processing 5 use cases with parallelism 4\n01:29:08 - INFO -       ▶️ Wave 1: 5 use cases, parallelism 4\n01:29:08 - INFO - \uD83D\uDD27 [N01-AI01] Starting SQL generation...\n01:29:08 - INFO - \uD83D\uDD27 [N01-AI02] Starting SQL generation...\n01:29:08 - INFO - \uD83D\uDD27 [N01-ST01] Starting SQL generation...\n01:29:08 - INFO -    [N01-AI01] 5 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:29:08 - INFO - \uD83D\uDD27 [N01-ST02] Starting SQL generation...\n01:29:08 - INFO -    [N01-AI02] 4 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:29:08 - INFO -    [N01-ST01] 6 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:29:08 - INFO - ⏳ [N01-AI01] Waiting for LLM response (SQL generation, timeout=330s)...\n01:29:08 - INFO -    [N01-ST02] 4 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:29:08 - INFO - ⏳ [N01-AI02] Waiting for LLM response (SQL generation, timeout=300s)...\n01:29:08 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:29:08 - INFO - ⏳ [N01-ST01] Waiting for LLM response (SQL generation, timeout=360s)...\n01:29:08 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:29:08 - INFO - ⏳ [N01-ST02] Waiting for LLM response (SQL generation, timeout=300s)...\n01:29:08 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:29:09 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:30:08 - INFO - [Generate_SQL_N01-AI01_Wave] Still waiting... 60s elapsed (timeout: 330s)\n01:30:09 - INFO - [Generate_SQL_N01-AI02_Wave] Still waiting... 60s elapsed (timeout: 300s)\n01:30:09 - INFO - [Generate_SQL_N01-ST01_Wave] Still waiting... 60s elapsed (timeout: 360s)\n01:30:09 - INFO - [Generate_SQL_N01-ST02_Wave] Still waiting... 60s elapsed (timeout: 300s)\n01:30:49 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:30:49 - INFO - ✅ [N01-AI01] Received LLM response (22280 chars)\n01:30:49 - INFO - ✓ SQL generated for N01-AI01 in 100.8s\n01:30:49 - INFO - \uD83D\uDD27 [N01-ST03] Starting SQL generation...\n01:30:49 - INFO -    [N01-ST03] 6 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:30:49 - INFO - ⏳ [N01-ST03] Waiting for LLM response (SQL generation, timeout=360s)...\n01:30:49 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:31:09 - INFO - [Generate_SQL_N01-AI02_Wave] Still waiting... 120s elapsed (timeout: 300s)\n01:31:09 - INFO - [Generate_SQL_N01-ST01_Wave] Still waiting... 120s elapsed (timeout: 360s)\n01:31:09 - INFO - [Generate_SQL_N01-ST02_Wave] Still waiting... 120s elapsed (timeout: 300s)\n01:31:24 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:31:24 - INFO - ✅ [N01-ST01] Received LLM response (36087 chars)\n01:31:24 - INFO - ✓ SQL generated for N01-ST01 in 135.3s\n01:31:38 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:31:38 - INFO - ✅ [N01-ST02] Received LLM response (23623 chars)\n01:31:38 - INFO - ✓ SQL generated for N01-ST02 in 150.1s\n01:31:49 - INFO - [Generate_SQL_N01-ST03_Wave] Still waiting... 60s elapsed (timeout: 360s)\n01:32:09 - INFO - [Generate_SQL_N01-AI02_Wave] Still waiting... 180s elapsed (timeout: 300s)\n01:32:11 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:32:11 - INFO - ✅ [N01-AI02] Received LLM response (21308 chars)\n01:32:11 - INFO - ✓ SQL generated for N01-AI02 in 183.1s\n01:32:49 - INFO - [Generate_SQL_N01-ST03_Wave] Still waiting... 120s elapsed (timeout: 360s)\n01:32:51 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:32:51 - INFO - ✅ [N01-ST03] Received LLM response (24478 chars)\n01:32:51 - INFO - ✓ SQL generated for N01-ST03 in 122.0s\n01:32:51 - INFO -       ✅ Wave 1 done in 222.7s: 5 OK, 0 Failed\n01:32:51 - INFO - \n   ✅ SQL Generation Complete: 5 succeeded, 0 failed (222.7s)\n01:32:51 - INFO - \n   \uD83D\uDCD3 PHASE 2: Creating notebook for domain 'Budget'...\n01:32:51 - INFO - Added domain overview cell for 'Budget'\n01:32:51 - INFO - Importing notebook 'N01-budget.ipynb' (attempt 1/2)...\n01:32:51 - INFO - ✓ Notebook 'N01-budget.ipynb' imported successfully in 0.4s\n01:32:52 - INFO - Notebook is located at: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/notebooks/N01-budget.ipynb\n01:32:52 - INFO -    ✓ Notebook saved to: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/notebooks/N01-budget.ipynb\n01:32:52 - INFO - \n********************************************************************************\n01:32:52 - INFO - \uD83C\uDF89 DOMAIN 'BUDGET' COMPLETE!\n01:32:52 - INFO - ********************************************************************************\n01:32:52 - INFO -    \uD83D\uDCD3 Notebook: N01-budget.ipynb\n01:32:52 - INFO -    \uD83D\uDCCA Use cases: 5 (5 SQL OK, 0 SQL Failed)\n01:32:52 - INFO -    ⏱️  Total time: 223.4s (SQL: 222.7s, Notebook: 0.6s)\n01:32:52 - INFO -    ✅ READY FOR TESTING!\n01:32:52 - INFO - ********************************************************************************\n\n01:32:52 - INFO - \uD83C\uDF89 [3/5] Domain 'Budget' notebook 'N01-budget.ipynb' READY FOR INSPECTION\n01:32:52 - INFO -    \uD83D\uDCC8 Progress: 11/23 use cases (47%)\n01:32:52 - INFO -    ⏳ Estimated time remaining: 7.8 minutes (2 domains left)\n01:32:52 - INFO - \n================================================================================\n01:32:52 - INFO - \uD83C\uDFE2 DOMAIN 4/5: PROCUREMENT (Notebook: N04)\n01:32:52 - INFO - ================================================================================\n01:32:52 - INFO -    \uD83D\uDCCA Use cases in this domain: 5\n01:32:52 - INFO -    \uD83D\uDD04 Progress: 11/23 use cases completed so far\n01:32:52 - INFO - \n\uD83C\uDFE2 [4/5] Starting domain: Procurement (N04, 5 use cases)\n01:32:52 - INFO - \n   \uD83D\uDCDD PHASE 1: Generating SQL for 5 use cases (wave pattern)...\n01:32:52 - INFO - \uD83D\uDD27 [SQL_GENERATION] Parallelism = 4 (from max=1) | calculated=4 based on: 5 items (small) + ~2K chars/prompt (small) + 43 cols (small schema) + SQL gen is complex\n01:32:52 - INFO - \uD83D\uDD27 [SQL_GENERATION] Workers: 4 (max=1)\n01:32:52 - INFO -    └─ Reason: calculated=4 based on: 5 items (small) + ~2K chars/prompt (small) + 43 cols (small schema) + SQL gen is complex\n01:32:52 - INFO -    \uD83D\uDD01 Wave 1: processing 5 use cases with parallelism 4\n01:32:52 - INFO -       ▶️ Wave 1: 5 use cases, parallelism 4\n01:32:52 - INFO - \uD83D\uDD27 [N04-AI01] Starting SQL generation...\n01:32:52 - INFO -    [N04-AI01] 6 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:32:52 - INFO - \uD83D\uDD27 [N04-AI02] Starting SQL generation...\n01:32:52 - INFO - \uD83D\uDD27 [N04-AI03] Starting SQL generation...\n01:32:52 - INFO - \uD83D\uDD27 [N04-AI04] Starting SQL generation...\n01:32:52 - INFO - ⏳ [N04-AI01] Waiting for LLM response (SQL generation, timeout=360s)...\n01:32:52 - INFO -    [N04-AI02] 4 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:32:52 - INFO -    [N04-AI03] 5 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:32:52 - INFO -    [N04-AI04] 6 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:32:52 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:32:52 - INFO - ⏳ [N04-AI02] Waiting for LLM response (SQL generation, timeout=300s)...\n01:32:52 - INFO - ⏳ [N04-AI04] Waiting for LLM response (SQL generation, timeout=360s)...\n01:32:52 - INFO - ⏳ [N04-AI03] Waiting for LLM response (SQL generation, timeout=330s)...\n01:32:52 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:32:52 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:32:52 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:33:52 - INFO - [Generate_SQL_N04-AI01_Wave] Still waiting... 60s elapsed (timeout: 360s)\n01:33:52 - INFO - [Generate_SQL_N04-AI02_Wave] Still waiting... 60s elapsed (timeout: 300s)\n01:33:52 - INFO - [Generate_SQL_N04-AI04_Wave] Still waiting... 60s elapsed (timeout: 360s)\n01:33:52 - INFO - [Generate_SQL_N04-AI03_Wave] Still waiting... 60s elapsed (timeout: 330s)\n01:34:11 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:34:11 - INFO - ✅ [N04-AI03] Received LLM response (21013 chars)\n01:34:11 - INFO - ✓ SQL generated for N04-AI03 in 79.6s\n01:34:11 - INFO - \uD83D\uDD27 [N04-ST01] Starting SQL generation...\n01:34:11 - INFO -    [N04-ST01] 6 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:34:11 - INFO - ⏳ [N04-ST01] Waiting for LLM response (SQL generation, timeout=360s)...\n01:34:11 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:34:16 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:34:16 - INFO - ✅ [N04-AI02] Received LLM response (22260 chars)\n01:34:16 - INFO - ✓ SQL generated for N04-AI02 in 84.6s\n01:34:18 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:34:18 - INFO - ✅ [N04-AI04] Received LLM response (21260 chars)\n01:34:18 - INFO - ✓ SQL generated for N04-AI04 in 86.2s\n01:34:23 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:34:23 - INFO - ✅ [N04-AI01] Received LLM response (23872 chars)\n01:34:23 - INFO - ✓ SQL generated for N04-AI01 in 91.7s\n01:35:11 - INFO - [Generate_SQL_N04-ST01_Wave] Still waiting... 60s elapsed (timeout: 360s)\n01:36:11 - INFO - [Generate_SQL_N04-ST01_Wave] Still waiting... 120s elapsed (timeout: 360s)\n01:36:16 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:36:16 - INFO - ✅ [N04-ST01] Received LLM response (21285 chars)\n01:36:16 - INFO - ✓ SQL generated for N04-ST01 in 125.0s\n01:36:16 - INFO -       ✅ Wave 1 done in 204.6s: 5 OK, 0 Failed\n01:36:16 - INFO - \n   ✅ SQL Generation Complete: 5 succeeded, 0 failed (204.6s)\n01:36:16 - INFO - \n   \uD83D\uDCD3 PHASE 2: Creating notebook for domain 'Procurement'...\n01:36:16 - INFO - Added domain overview cell for 'Procurement'\n01:36:16 - INFO - Importing notebook 'N04-procurement.ipynb' (attempt 1/2)...\n01:36:17 - INFO - ✓ Notebook 'N04-procurement.ipynb' imported successfully in 0.3s\n01:36:17 - INFO - Notebook is located at: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/notebooks/N04-procurement.ipynb\n01:36:17 - INFO -    ✓ Notebook saved to: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/notebooks/N04-procurement.ipynb\n01:36:17 - INFO - \n********************************************************************************\n01:36:17 - INFO - \uD83C\uDF89 DOMAIN 'PROCUREMENT' COMPLETE!\n01:36:17 - INFO - ********************************************************************************\n01:36:17 - INFO -    \uD83D\uDCD3 Notebook: N04-procurement.ipynb\n01:36:17 - INFO -    \uD83D\uDCCA Use cases: 5 (5 SQL OK, 0 SQL Failed)\n01:36:17 - INFO -    ⏱️  Total time: 205.1s (SQL: 204.6s, Notebook: 0.5s)\n01:36:17 - INFO -    ✅ READY FOR TESTING!\n01:36:17 - INFO - ********************************************************************************\n\n01:36:17 - INFO - \uD83C\uDF89 [4/5] Domain 'Procurement' notebook 'N04-procurement.ipynb' READY FOR INSPECTION\n01:36:17 - INFO -    \uD83D\uDCC8 Progress: 16/23 use cases (69%)\n01:36:17 - INFO -    ⏳ Estimated time remaining: 4.6 minutes (1 domains left)\n01:36:17 - INFO - \n================================================================================\n01:36:17 - INFO - \uD83C\uDFE2 DOMAIN 5/5: WORKFORCE (Notebook: N03)\n01:36:17 - INFO - ================================================================================\n01:36:17 - INFO -    \uD83D\uDCCA Use cases in this domain: 7\n01:36:17 - INFO -    \uD83D\uDD04 Progress: 16/23 use cases completed so far\n01:36:17 - INFO - \n\uD83C\uDFE2 [5/5] Starting domain: Workforce (N03, 7 use cases)\n01:36:17 - INFO - \n   \uD83D\uDCDD PHASE 1: Generating SQL for 7 use cases (wave pattern)...\n01:36:17 - INFO - \uD83D\uDD27 [SQL_GENERATION] Parallelism = 4 (from max=1) | calculated=4 based on: 7 items (small) + ~2K chars/prompt (small) + 43 cols (small schema) + SQL gen is complex\n01:36:17 - INFO - \uD83D\uDD27 [SQL_GENERATION] Workers: 4 (max=1)\n01:36:17 - INFO -    └─ Reason: calculated=4 based on: 7 items (small) + ~2K chars/prompt (small) + 43 cols (small schema) + SQL gen is complex\n01:36:17 - INFO -    \uD83D\uDD01 Wave 1: processing 7 use cases with parallelism 4\n01:36:17 - INFO -       ▶️ Wave 1: 7 use cases, parallelism 4\n01:36:17 - INFO - \uD83D\uDD27 [N03-AI05] Starting SQL generation...\n01:36:17 - INFO - \uD83D\uDD27 [N03-AI01] Starting SQL generation...\n01:36:17 - INFO - \uD83D\uDD27 [N03-AI02] Starting SQL generation...\n01:36:17 - INFO -    [N03-AI05] 5 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:36:17 - INFO - \uD83D\uDD27 [N03-AI03] Starting SQL generation...\n01:36:17 - INFO -    [N03-AI01] 5 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:36:17 - INFO -    [N03-AI02] 5 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:36:17 - INFO -    [N03-AI03] 5 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:36:17 - INFO - ⏳ [N03-AI05] Waiting for LLM response (SQL generation, timeout=330s)...\n01:36:17 - INFO - ⏳ [N03-AI01] Waiting for LLM response (SQL generation, timeout=330s)...\n01:36:17 - INFO - ⏳ [N03-AI02] Waiting for LLM response (SQL generation, timeout=330s)...\n01:36:17 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:36:17 - INFO - ⏳ [N03-AI03] Waiting for LLM response (SQL generation, timeout=330s)...\n01:36:17 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:36:17 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:36:17 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:37:17 - INFO - [Generate_SQL_N03-AI05_Wave] Still waiting... 60s elapsed (timeout: 330s)\n01:37:17 - INFO - [Generate_SQL_N03-AI01_Wave] Still waiting... 60s elapsed (timeout: 330s)\n01:37:17 - INFO - [Generate_SQL_N03-AI02_Wave] Still waiting... 61s elapsed (timeout: 330s)\n01:37:17 - INFO - [Generate_SQL_N03-AI03_Wave] Still waiting... 60s elapsed (timeout: 330s)\n01:37:34 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:37:34 - INFO - ✅ [N03-AI03] Received LLM response (19799 chars)\n01:37:34 - INFO - ✓ SQL generated for N03-AI03 in 77.4s\n01:37:34 - INFO - \uD83D\uDD27 [N03-AI04] Starting SQL generation...\n01:37:34 - INFO -    [N03-AI04] 5 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:37:34 - INFO - ⏳ [N03-AI04] Waiting for LLM response (SQL generation, timeout=330s)...\n01:37:34 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:37:36 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:37:36 - INFO - ✅ [N03-AI01] Received LLM response (21005 chars)\n01:37:36 - INFO - ✓ SQL generated for N03-AI01 in 79.3s\n01:37:36 - INFO - \uD83D\uDD27 [N03-ST01] Starting SQL generation...\n01:37:36 - INFO -    [N03-ST01] 7 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:37:36 - INFO - ⏳ [N03-ST01] Waiting for LLM response (SQL generation, timeout=360s)...\n01:37:36 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:37:47 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:37:47 - INFO - ✅ [N03-AI05] Received LLM response (21407 chars)\n01:37:47 - INFO - ✓ SQL generated for N03-AI05 in 89.8s\n01:37:47 - INFO - \uD83D\uDD27 [N03-ST02] Starting SQL generation...\n01:37:47 - INFO -    [N03-ST02] 7 CTEs from Technical Design, 43 columns from directly involved tables, 0 additional columns\n01:37:47 - INFO - ⏳ [N03-ST02] Waiting for LLM response (SQL generation, timeout=360s)...\n01:37:47 - INFO -    [USE_CASE_SQL_GEN_PROMPT] Setting max_tokens=115,200 (model limit: 128,000)\n01:37:53 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:37:53 - INFO - ✅ [N03-AI02] Received LLM response (23316 chars)\n01:37:53 - INFO - ✓ SQL generated for N03-AI02 in 96.3s\n01:38:34 - INFO - [Generate_SQL_N03-AI04_Wave] Still waiting... 60s elapsed (timeout: 330s)\n01:38:36 - INFO - [Generate_SQL_N03-ST01_Wave] Still waiting... 60s elapsed (timeout: 360s)\n01:38:47 - INFO - [Generate_SQL_N03-ST02_Wave] Still waiting... 60s elapsed (timeout: 360s)\n01:39:33 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:39:33 - INFO - ✅ [N03-AI04] Received LLM response (32340 chars)\n01:39:33 - INFO - ✓ SQL generated for N03-AI04 in 118.5s\n01:39:36 - INFO - [Generate_SQL_N03-ST01_Wave] Still waiting... 120s elapsed (timeout: 360s)\n01:39:47 - INFO - [Generate_SQL_N03-ST02_Wave] Still waiting... 120s elapsed (timeout: 360s)\n01:40:21 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:40:21 - INFO - ✅ [N03-ST02] Received LLM response (21036 chars)\n01:40:21 - INFO - ✓ SQL generated for N03-ST02 in 154.0s\n01:40:36 - INFO - [Generate_SQL_N03-ST01_Wave] Still waiting... 180s elapsed (timeout: 360s)\n01:41:11 - INFO -    [USE_CASE_SQL_GEN_PROMPT] SQL complete - END marker found\n01:41:11 - INFO - ✅ [N03-ST01] Received LLM response (22500 chars)\n01:41:11 - INFO - ✓ SQL generated for N03-ST01 in 214.9s\n01:41:11 - INFO -       ✅ Wave 1 done in 294.2s: 7 OK, 0 Failed\n01:41:11 - INFO - \n   ✅ SQL Generation Complete: 7 succeeded, 0 failed (294.2s)\n01:41:11 - INFO - \n   \uD83D\uDCD3 PHASE 2: Creating notebook for domain 'Workforce'...\n01:41:11 - INFO - Added domain overview cell for 'Workforce'\n01:41:11 - INFO - Importing notebook 'N03-workforce.ipynb' (attempt 1/2)...\n01:41:12 - INFO - ✓ Notebook 'N03-workforce.ipynb' imported successfully in 0.5s\n01:41:12 - INFO - Notebook is located at: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/notebooks/N03-workforce.ipynb\n01:41:12 - INFO -    ✓ Notebook saved to: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/notebooks/N03-workforce.ipynb\n01:41:12 - INFO - \n********************************************************************************\n01:41:12 - INFO - \uD83C\uDF89 DOMAIN 'WORKFORCE' COMPLETE!\n01:41:12 - INFO - ********************************************************************************\n01:41:12 - INFO -    \uD83D\uDCD3 Notebook: N03-workforce.ipynb\n01:41:12 - INFO -    \uD83D\uDCCA Use cases: 7 (7 SQL OK, 0 SQL Failed)\n01:41:12 - INFO -    ⏱️  Total time: 295.0s (SQL: 294.2s, Notebook: 0.8s)\n01:41:12 - INFO -    ✅ READY FOR TESTING!\n01:41:12 - INFO - ********************************************************************************\n\n01:41:12 - INFO - \uD83C\uDF89 [5/5] Domain 'Workforce' notebook 'N03-workforce.ipynb' READY FOR INSPECTION\n01:41:12 - INFO - \n================================================================================\n01:41:12 - INFO - \uD83C\uDFC1 ALL DOMAINS PROCESSED\n01:41:12 - INFO - ================================================================================\n01:41:12 - INFO -    \uD83D\uDCCA Total use cases: 23\n01:41:12 - INFO -    \uD83D\uDCD3 Notebooks created: 5/5\n01:41:12 - INFO -    ⏱️  Total time: 15.5 minutes\n01:41:12 - INFO - \n   \uD83D\uDCD3 Notebooks ready for testing:\n01:41:12 - INFO -       ✅ N05-operations.ipynb\n01:41:12 - INFO -       ✅ N02-payroll.ipynb\n01:41:12 - INFO -       ✅ N01-budget.ipynb\n01:41:12 - INFO -       ✅ N04-procurement.ipynb\n01:41:12 - INFO -       ✅ N03-workforce.ipynb\n01:41:12 - INFO - ================================================================================\n\n01:41:12 - INFO - ✅ Domain-by-domain SQL generation complete: 23 use cases, 5 notebooks in 930.3s\n01:41:12 - INFO - ✅ Phase 2 complete: All domains processed (SQL + Notebooks)\n01:41:12 - INFO - ⏳ Waiting for documentation generation to complete...\n01:41:12 - INFO - ✅ Documentation generation completed\n01:41:12 - INFO - ✅ Documentation generation completed\n01:41:12 - INFO - ✅ Populated Primary Table for 23 use cases\n01:41:12 - INFO - Saving JSON Catalog with generated SQL and columns...\n01:41:12 - INFO - Generating JSON Catalog...\n01:41:12 - INFO - Saving JSON Catalog to: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/docs/procore-dbx_inspire.json\n01:41:12 - INFO - ✅ JSON Catalog saved successfully to /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/docs/procore-dbx_inspire.json\n01:41:12 - INFO - ✅ JSON Catalog saved to: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/docs/procore-dbx_inspire.json\n01:41:12 - WARNING - No available tables found in data loader cache. Skipping table statistics report.\n01:41:12 - INFO - \uD83E\uDDF9 Cleaned up intermediate storage: /tmp/inspire_3jqpy6l2\n01:41:12 - INFO - ✅ All Use cases for procore generated successfully\n01:41:12 - INFO - Uploading log file...\n01:41:12 - INFO - Reading log file from: /tmp/procore/log.txt\n01:41:12 - INFO - Skipping local copy of log file (non-local path): /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/log.txt\n01:41:12 - INFO - Uploading log file to workspace: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/docs/generation_log.txt\n01:41:13 - INFO - Successfully uploaded log file to workspace: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/docs/generation_log.txt\n01:41:13 - INFO - ✅ Log file also uploaded to workspace: /Users/amr.ali@databricks.com/inspire/inspire_gen/procore/docs/generation_log.txt\n01:41:13 - INFO - \n================================================================================\n01:41:13 - INFO - \uD83D\uDCCA PROCESSING HONESTY REPORT\n01:41:13 - INFO - ================================================================================\n\n01:41:13 - INFO - \uD83D\uDCC8 Overall Statistics:\n01:41:13 - INFO -    • Total tables discovered: 1\n01:41:13 - INFO -    • Total tables processed: 1\n01:41:13 - INFO -    • Total batches created: 1\n01:41:13 - INFO -    • Total batch splits performed: 0\n01:41:13 - INFO - \n✅ No batch splitting was required - all batches fit within LLM context limits\n01:41:13 - INFO - \n✅ No columns were dropped - all tables processed with full schema\n01:41:13 - INFO - \n================================================================================\n01:41:13 - INFO - \uD83C\uDFAF HONESTY ASSESSMENT:\n01:41:13 - INFO - ================================================================================\n01:41:13 - INFO - \n✅ 100% HONEST - All tables processed completely with all columns\n01:41:13 - INFO -    No compromises were made during processing.\n01:41:13 - INFO - \n================================================================================\n\n01:41:13 - INFO - \n======================================================================\n--- \uD83D\uDCCA AI Usage Summary ---\n======================================================================\nTotal AI Calls:     51\nTotal Input Tokens:  ~2,686,101.25  (10,744,405 chars)\nTotal Output Tokens: ~172,707.50  (690,830 chars)\n\n--- Prompt Type Details ---\nPrompt Type                              | Calls   | Input Tokens       | Output Tokens     \n--------------------------------------------------------------------------------------------\nUSE_CASE_SQL_GEN_PROMPT                  | 23      | ~2,555,253        | ~136,212         \nSUBDOMAIN_DETECTOR_PROMPT                | 8       | ~27,133           | ~688             \nSCORE_USE_CASES_PROMPT                   | 5       | ~24,775           | ~6,786           \nREVIEW_USE_CASES_PROMPT                  | 5       | ~13,896           | ~89              \nSTATS_USE_CASE_GEN_PROMPT                | 2       | ~21,356           | ~12,139          \nAI_USE_CASE_GEN_PROMPT                   | 2       | ~20,620           | ~11,296          \nSUMMARY_GEN_PROMPT                       | 2       | ~3,076            | ~3,144           \nBUSINESS_CONTEXT_WORKER_PROMPT           | 1       | ~1,132            | ~1,718           \nFILTER_BUSINESS_TABLES_PROMPT            | 1       | ~8,202            | ~48              \nDOMAIN_FINDER_PROMPT                     | 1       | ~9,981            | ~175             \nDOMAINS_MERGER_PROMPT                    | 1       | ~678              | ~412             \n======================================================================\n01:41:13 - INFO - ✅ All Use cases for procore generated successfully\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": {
    "autoRunOnWidgetChange": "no-auto-run"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8092587260709082,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2,
    "widgetLayout": [
     {
      "breakBefore": false,
      "name": "00_business",
      "width": 172
     },
     {
      "breakBefore": false,
      "name": "02_business_domains",
      "width": 172
     },
     {
      "breakBefore": false,
      "name": "01_business_priorities",
      "width": 172
     },
     {
      "breakBefore": false,
      "name": "06_generation_path",
      "width": 172
     },
     {
      "breakBefore": false,
      "name": "03_strategic_goals",
      "width": 172
     },
     {
      "breakBefore": false,
      "name": "04_catalogs_and_schemas",
      "width": 172
     },
     {
      "breakBefore": false,
      "name": "05_generate",
      "width": 172
     },
     {
      "breakBefore": false,
      "name": "07_output_language",
      "width": 172
     },
     {
      "breakBefore": false,
      "name": "10_sql_model_serving",
      "width": 172
     },
     {
      "breakBefore": false,
      "name": "09_sql_warehouse",
      "width": 172
     }
    ]
   },
   "notebookName": "databricks_inspire_v34",
   "widgets": {
    "00_business_name": {
     "currentValue": "procore",
     "nuid": "c0155531-a622-42a5-90e1-92d9afe47bab",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "01. Business Name",
      "name": "00_business_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "01. Business Name",
      "name": "00_business_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "01_uc_metadata": {
     "currentValue": "virgin_02.procore_field.timecard",
     "nuid": "3fdb1122-ad52-4ae9-b558-ba3509d5012b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "02. UC Metadata",
      "name": "01_uc_metadata",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "02. UC Metadata",
      "name": "01_uc_metadata",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "02_operation": {
     "currentValue": "Discover Usecases",
     "nuid": "4dfaeb94-19e1-43cb-a5c6-efbf9600fb66",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Discover Usecases",
      "label": "03. Operation",
      "name": "02_operation",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "Discover Usecases",
        "Re-generate SQL",
        "Generate Sample Result"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "Discover Usecases",
      "label": "03. Operation",
      "name": "02_operation",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "Discover Usecases",
        "Re-generate SQL",
        "Generate Sample Result"
       ]
      }
     }
    },
    "03_business_domains": {
     "currentValue": "",
     "nuid": "4bc447b5-cef0-4e1a-9de1-bdd5bf03535a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "04. Business Domains",
      "name": "03_business_domains",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "04. Business Domains",
      "name": "03_business_domains",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "04_business_priorities": {
     "currentValue": "",
     "nuid": "71116d3f-01f8-4e4a-80da-8b010381094f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Increase Revenue",
      "label": "05. Business Priorities",
      "name": "04_business_priorities",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "Increase Revenue",
        "Reduce Cost",
        "Optimize Operations",
        "Mitigate Risk",
        "Empower Talent",
        "Enhance Experience",
        "Drive Innovation",
        "Achieve ESG",
        "Protect Revenue",
        "Execute Strategy"
       ],
       "fixedDomain": true,
       "multiselect": true
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "multiselect",
      "defaultValue": "Increase Revenue",
      "label": "05. Business Priorities",
      "name": "04_business_priorities",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "Increase Revenue",
        "Reduce Cost",
        "Optimize Operations",
        "Mitigate Risk",
        "Empower Talent",
        "Enhance Experience",
        "Drive Innovation",
        "Achieve ESG",
        "Protect Revenue",
        "Execute Strategy"
       ]
      }
     }
    },
    "05_strategic_goals": {
     "currentValue": "",
     "nuid": "311bbc50-1039-4483-a5ee-e168c43a9bdc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "06. Strategic Goals",
      "name": "05_strategic_goals",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "06. Strategic Goals",
      "name": "05_strategic_goals",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "06_generation_options": {
     "currentValue": "PDF Catalog,SQL Code",
     "nuid": "600fd5ae-f70a-4262-bb82-bc271d999304",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "SQL Code",
      "label": "07. Generation Options",
      "name": "06_generation_options",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "SQL Code",
        "PDF Catalog",
        "Presentation",
        "dashboards",
        "Unstructured Data Usecases"
       ],
       "fixedDomain": true,
       "multiselect": true
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "multiselect",
      "defaultValue": "SQL Code",
      "label": "07. Generation Options",
      "name": "06_generation_options",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "SQL Code",
        "PDF Catalog",
        "Presentation",
        "dashboards",
        "Unstructured Data Usecases"
       ]
      }
     }
    },
    "07_generation_path": {
     "currentValue": "./inspire_gen/",
     "nuid": "f4035006-b0fb-4885-b7bf-a5678289c8e9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "./inspire_gen/",
      "label": "08. Generation Path",
      "name": "07_generation_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "./inspire_gen/",
      "label": "08. Generation Path",
      "name": "07_generation_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "08_documents_languages": {
     "currentValue": "English",
     "nuid": "e8e58a1a-9dad-49d0-9089-0459c438358c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "English",
      "label": "09. Documents Languages",
      "name": "08_documents_languages",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "English",
        "French",
        "German",
        "Spanish",
        "Hindi",
        "Chinese (Mandarin)",
        "Japanese",
        "Arabic",
        "Portuguese",
        "Russian",
        "Swedish",
        "Danish",
        "Norwegian",
        "Finnish",
        "Italian",
        "Polish",
        "Romanian",
        "Ukrainian",
        "Dutch",
        "Korean",
        "Indonesian",
        "Malay",
        "Tamil"
       ],
       "fixedDomain": true,
       "multiselect": true
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "multiselect",
      "defaultValue": "English",
      "label": "09. Documents Languages",
      "name": "08_documents_languages",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "English",
        "French",
        "German",
        "Spanish",
        "Hindi",
        "Chinese (Mandarin)",
        "Japanese",
        "Arabic",
        "Portuguese",
        "Russian",
        "Swedish",
        "Danish",
        "Norwegian",
        "Finnish",
        "Italian",
        "Polish",
        "Romanian",
        "Ukrainian",
        "Dutch",
        "Korean",
        "Indonesian",
        "Malay",
        "Tamil"
       ]
      }
     }
    },
    "09_ai_model": {
     "currentValue": "databricks-gpt-oss-120b",
     "nuid": "5d70f659-3b7c-4778-9368-2da8b4ee2b00",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-gpt-oss-120b",
      "label": "10. AI Model",
      "name": "09_ai_model",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "databricks-gpt-oss-120b",
      "label": "10. AI Model",
      "name": "09_ai_model",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}